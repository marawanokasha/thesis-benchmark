{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates the doc2vec vector embeddings for a specific configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "import gzip\n",
    "\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "from functools import partial\n",
    "\n",
    "from thesis.utils.metrics import *\n",
    "from thesis.utils.file import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables used throughout the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_SEED = 1234\n",
    "DOC2VEC_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_WORD_COUNT = 100\n",
    "NUM_CORES = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GLOBAL_VARS = namedtuple('GLOBAL_VARS', ['MODEL_NAME', 'DOC2VEC_MODEL_NAME', 'DOC2VEC_MODEL', \n",
    "                                         'SVM_MODEL_NAME', 'NN_MODEL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_MODEL = \"vocab_model\"\n",
    "MODEL_PREFIX = \"model\"\n",
    "VALIDATION_DICT = \"validation_dict.pkl\"\n",
    "TEST_MATRIX = \"test_matrix.pkl\"\n",
    "TEST_DICT = \"test_dict.pkl\"\n",
    "METRICS = \"metrics.pkl\"\n",
    "CLASSIFIER = \"classifier.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_location = \"/mnt/virtual-machines/data/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "training_docs_list_file = exports_location + \"training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"validation_docs_list.pkl\"\n",
    "test_docs_list_file = exports_location + \"test_docs_list.pkl\"\n",
    "\n",
    "preprocessed_location = root_location + \"preprocessed_data/extended_pv_abs_desc_claims_full_chunks/\"\n",
    "\n",
    "training_preprocessed_files_prefix = preprocessed_location + \"extended_pv_training_docs_data_preprocessed-\"\n",
    "validation_preprocessed_files_prefix = preprocessed_location + \"extended_pv_validation_docs_data_preprocessed-\"\n",
    "test_preprocessed_files_prefix = preprocessed_location + \"extended_pv_test_docs_data_preprocessed-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load general data required for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 s, sys: 1.24 s, total: 19.2 s\n",
      "Wall time: 19.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286325"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321473"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401877"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExtendedPVDocumentBatchGenerator(Process):\n",
    "    def __init__(self, filename_prefix, queue, batch_size=10000, start_file=0, offset=10000):\n",
    "        super(ExtendedPVDocumentBatchGenerator, self).__init__()\n",
    "        self.queue = queue\n",
    "        self.offset = offset\n",
    "        self.filename_prefix = filename_prefix\n",
    "        self.files_loaded = start_file - offset\n",
    "\n",
    "    def run(self):\n",
    "        cur_file = None\n",
    "        while True:\n",
    "            try:\n",
    "                if cur_file is None:\n",
    "                    info(\"Loading new file for index: {}\".format(str(self.files_loaded + self.offset)))\n",
    "#                     cur_file = gzip.open(self.filename_prefix + str(self.files_loaded + self.offset) + '.gz')\n",
    "                    cur_file = open(self.filename_prefix + str(self.files_loaded + self.offset))\n",
    "                    self.files_loaded += self.offset\n",
    "                for line in cur_file:\n",
    "                    self.queue.put(line)\n",
    "                cur_file.close()\n",
    "                cur_file = None\n",
    "            except IOError:\n",
    "                self.queue.put(False, block=True, timeout=None)\n",
    "                info(\"All files are loaded - last file: {}\".format(str(self.files_loaded + self.offset)))\n",
    "                return\n",
    "\n",
    "\n",
    "class BatchWrapper(object):\n",
    "    def __init__(self, training_preprocessed_files_prefix, buffer_size=10000, batch_size=10000, level=1, level_type=None):\n",
    "        assert batch_size <= 10000 or batch_size is None\n",
    "        self.level = level\n",
    "        self.level_type = level_type[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.q = Queue(maxsize=buffer_size)\n",
    "        self.p = ExtendedPVDocumentBatchGenerator(training_preprocessed_files_prefix, queue=self.q,\n",
    "                                                  batch_size=batch_size, start_file=0, offset=10000)\n",
    "        self.p.start()\n",
    "        self.cur_data = []\n",
    "\n",
    "    def is_correct_type(self, doc_id):\n",
    "        parts = doc_id.split(\"_\")\n",
    "        len_parts = len(parts)\n",
    "        if len_parts == self.level:\n",
    "            if len_parts == 1:\n",
    "                return True\n",
    "            if len_parts == self.level and (parts[1][0] == self.level_type or self.level_type is None):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def return_sentences(self, line):\n",
    "        line_array = tuple(line.split(\" \"))\n",
    "        doc_id = line_array[0]\n",
    "        if not self.is_correct_type(doc_id):\n",
    "            return False\n",
    "        line_array = line_array[1:]\n",
    "        len_line_array = len(line_array)\n",
    "        curr_batch_iter = 0\n",
    "        # divide the document to batches according to the batch size\n",
    "        sentences = []\n",
    "        while curr_batch_iter < len_line_array:\n",
    "            sentences.append(LabeledSentence(words=line_array[curr_batch_iter: curr_batch_iter + self.batch_size], tags=[doc_id]))\n",
    "            curr_batch_iter += self.batch_size\n",
    "        return tuple(sentences)\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            item = self.q.get(block=True)\n",
    "            if item is False:\n",
    "                raise StopIteration()\n",
    "            else:\n",
    "                sentences = self.return_sentences(item)\n",
    "                if not sentences:\n",
    "                    None\n",
    "                else:\n",
    "                    for sentence in sentences:\n",
    "                        yield sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2vec and SVM Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOC2VEC_SIZE = 200\n",
    "DOC2VEC_WINDOW = 2\n",
    "DOC2VEC_MAX_VOCAB_SIZE = None\n",
    "DOC2VEC_SAMPLE = 1e-3\n",
    "DOC2VEC_TYPE = 1\n",
    "DOC2VEC_HIERARCHICAL_SAMPLE = 0\n",
    "DOC2VEC_NEGATIVE_SAMPLE_SIZE = 10\n",
    "DOC2VEC_CONCAT = 0\n",
    "DOC2VEC_MEAN = 1\n",
    "DOC2VEC_TRAIN_WORDS = 0\n",
    "DOC2VEC_EPOCHS = 1 # we do our training manually one epoch at a time\n",
    "DOC2VEC_MAX_EPOCHS = 8\n",
    "REPORT_DELAY = 20 # report the progress every x seconds\n",
    "REPORT_VOCAB_PROGRESS = 10000 # report vocab progress every x documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    (3, 'abstract'),\n",
    "    (3, 'claims')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/virtual-machines/data/preprocessed_data/extended_pv_abs_desc_claims_full_chunks/extended_pv_training_docs_data_preprocessed-'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_preprocessed_files_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-09 02:11:12,582 : INFO : creating vocabulary for 3 abstract in \n",
      "2017-04-09 02:11:12,584 : INFO : FILE /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_abstract/full/vocab_model/model\n",
      "2017-04-09 02:11:12,626 : INFO : collecting all words and their counts\n",
      "2017-04-09 02:11:12,628 : INFO : Loading new file for index: 0\n",
      "2017-04-09 02:11:12,643 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-04-09 02:11:29,764 : INFO : Loading new file for index: 10000\n",
      "2017-04-09 02:11:46,761 : INFO : Loading new file for index: 20000\n",
      "2017-04-09 02:12:04,549 : INFO : Loading new file for index: 30000\n",
      "2017-04-09 02:12:10,867 : INFO : PROGRESS: at example #100000, processed 3902652 words (67029/s), 64111 word types, 100000 tags\n",
      "2017-04-09 02:12:21,967 : INFO : Loading new file for index: 40000\n",
      "2017-04-09 02:12:39,445 : INFO : Loading new file for index: 50000\n",
      "2017-04-09 02:12:57,212 : INFO : Loading new file for index: 60000\n",
      "2017-04-09 02:13:09,027 : INFO : PROGRESS: at example #200000, processed 7804234 words (67084/s), 96496 word types, 200000 tags\n",
      "2017-04-09 02:13:14,199 : INFO : Loading new file for index: 70000\n",
      "2017-04-09 02:13:31,729 : INFO : Loading new file for index: 80000\n",
      "2017-04-09 02:13:49,230 : INFO : Loading new file for index: 90000\n",
      "2017-04-09 02:14:06,404 : INFO : Loading new file for index: 100000\n",
      "2017-04-09 02:14:06,870 : INFO : PROGRESS: at example #300000, processed 11719671 words (67693/s), 122337 word types, 300000 tags\n",
      "2017-04-09 02:14:25,093 : INFO : Loading new file for index: 110000\n",
      "2017-04-09 02:14:43,259 : INFO : Loading new file for index: 120000\n",
      "2017-04-09 02:15:00,279 : INFO : Loading new file for index: 130000\n",
      "2017-04-09 02:15:06,172 : INFO : PROGRESS: at example #400000, processed 15621231 words (65793/s), 145461 word types, 400000 tags\n",
      "2017-04-09 02:15:17,375 : INFO : Loading new file for index: 140000\n",
      "2017-04-09 02:15:34,454 : INFO : Loading new file for index: 150000\n",
      "2017-04-09 02:15:51,953 : INFO : Loading new file for index: 160000\n",
      "2017-04-09 02:16:03,790 : INFO : PROGRESS: at example #500000, processed 19514396 words (67569/s), 166250 word types, 500000 tags\n",
      "2017-04-09 02:16:09,114 : INFO : Loading new file for index: 170000\n",
      "2017-04-09 02:16:26,670 : INFO : Loading new file for index: 180000\n",
      "2017-04-09 02:16:43,455 : INFO : Loading new file for index: 190000\n",
      "2017-04-09 02:17:00,302 : INFO : Loading new file for index: 200000\n",
      "2017-04-09 02:17:00,801 : INFO : PROGRESS: at example #600000, processed 23413223 words (68389/s), 185560 word types, 600000 tags\n",
      "2017-04-09 02:17:17,400 : INFO : Loading new file for index: 210000\n",
      "2017-04-09 02:17:34,435 : INFO : Loading new file for index: 220000\n",
      "2017-04-09 02:17:51,526 : INFO : Loading new file for index: 230000\n",
      "2017-04-09 02:17:57,720 : INFO : PROGRESS: at example #700000, processed 27332982 words (68866/s), 203508 word types, 700000 tags\n",
      "2017-04-09 02:18:08,989 : INFO : Loading new file for index: 240000\n",
      "2017-04-09 02:18:26,319 : INFO : Loading new file for index: 250000\n",
      "2017-04-09 02:18:43,690 : INFO : Loading new file for index: 260000\n",
      "2017-04-09 02:18:55,492 : INFO : PROGRESS: at example #800000, processed 31251636 words (67832/s), 220310 word types, 800000 tags\n",
      "2017-04-09 02:19:00,575 : INFO : Loading new file for index: 270000\n",
      "2017-04-09 02:19:18,441 : INFO : Loading new file for index: 280000\n",
      "2017-04-09 02:19:35,382 : INFO : Loading new file for index: 290000\n",
      "2017-04-09 02:19:52,990 : INFO : Loading new file for index: 300000\n",
      "2017-04-09 02:19:53,458 : INFO : PROGRESS: at example #900000, processed 35150997 words (67271/s), 236359 word types, 900000 tags\n",
      "2017-04-09 02:20:11,578 : INFO : Loading new file for index: 310000\n",
      "2017-04-09 02:20:28,861 : INFO : Loading new file for index: 320000\n",
      "2017-04-09 02:20:45,777 : INFO : Loading new file for index: 330000\n",
      "2017-04-09 02:20:51,883 : INFO : PROGRESS: at example #1000000, processed 39052255 words (66775/s), 252246 word types, 1000000 tags\n",
      "2017-04-09 02:21:02,811 : INFO : Loading new file for index: 340000\n",
      "2017-04-09 02:21:19,778 : INFO : Loading new file for index: 350000\n",
      "2017-04-09 02:21:37,473 : INFO : Loading new file for index: 360000\n",
      "2017-04-09 02:21:49,592 : INFO : PROGRESS: at example #1100000, processed 42965620 words (67813/s), 267407 word types, 1100000 tags\n",
      "2017-04-09 02:21:54,724 : INFO : Loading new file for index: 370000\n",
      "2017-04-09 02:22:11,302 : INFO : Loading new file for index: 380000\n",
      "2017-04-09 02:22:28,651 : INFO : Loading new file for index: 390000\n",
      "2017-04-09 02:22:45,361 : INFO : Loading new file for index: 400000\n",
      "2017-04-09 02:22:45,791 : INFO : PROGRESS: at example #1200000, processed 46871348 words (69500/s), 282035 word types, 1200000 tags\n",
      "2017-04-09 02:23:02,625 : INFO : Loading new file for index: 410000\n",
      "2017-04-09 02:23:19,782 : INFO : Loading new file for index: 420000\n",
      "2017-04-09 02:23:36,882 : INFO : Loading new file for index: 430000\n",
      "2017-04-09 02:23:43,017 : INFO : PROGRESS: at example #1300000, processed 50768188 words (68097/s), 296210 word types, 1300000 tags\n",
      "2017-04-09 02:23:53,818 : INFO : Loading new file for index: 440000\n",
      "2017-04-09 02:24:10,431 : INFO : Loading new file for index: 450000\n",
      "2017-04-09 02:24:27,421 : INFO : Loading new file for index: 460000\n",
      "2017-04-09 02:24:39,061 : INFO : PROGRESS: at example #1400000, processed 54693714 words (70045/s), 309835 word types, 1400000 tags\n",
      "2017-04-09 02:24:44,013 : INFO : Loading new file for index: 470000\n",
      "2017-04-09 02:25:01,102 : INFO : Loading new file for index: 480000\n",
      "2017-04-09 02:25:17,727 : INFO : Loading new file for index: 490000\n",
      "2017-04-09 02:25:34,468 : INFO : Loading new file for index: 500000\n",
      "2017-04-09 02:25:34,951 : INFO : PROGRESS: at example #1500000, processed 58597352 words (69847/s), 322942 word types, 1500000 tags\n",
      "2017-04-09 02:25:51,338 : INFO : Loading new file for index: 510000\n",
      "2017-04-09 02:26:08,290 : INFO : Loading new file for index: 520000\n",
      "2017-04-09 02:26:24,914 : INFO : Loading new file for index: 530000\n",
      "2017-04-09 02:26:30,879 : INFO : PROGRESS: at example #1600000, processed 62510574 words (69970/s), 335969 word types, 1600000 tags\n",
      "2017-04-09 02:26:41,703 : INFO : Loading new file for index: 540000\n",
      "2017-04-09 02:26:58,556 : INFO : Loading new file for index: 550000\n",
      "2017-04-09 02:27:16,569 : INFO : Loading new file for index: 560000\n",
      "2017-04-09 02:27:28,475 : INFO : PROGRESS: at example #1700000, processed 66430122 words (68054/s), 348421 word types, 1700000 tags\n",
      "2017-04-09 02:27:33,669 : INFO : Loading new file for index: 570000\n",
      "2017-04-09 02:27:50,353 : INFO : Loading new file for index: 580000\n",
      "2017-04-09 02:28:07,174 : INFO : Loading new file for index: 590000\n",
      "2017-04-09 02:28:24,179 : INFO : Loading new file for index: 600000\n",
      "2017-04-09 02:28:24,659 : INFO : PROGRESS: at example #1800000, processed 70348632 words (69745/s), 360435 word types, 1800000 tags\n",
      "2017-04-09 02:28:40,890 : INFO : Loading new file for index: 610000\n",
      "2017-04-09 02:28:57,572 : INFO : Loading new file for index: 620000\n",
      "2017-04-09 02:29:14,238 : INFO : Loading new file for index: 630000\n",
      "2017-04-09 02:29:20,570 : INFO : PROGRESS: at example #1900000, processed 74267552 words (70094/s), 372357 word types, 1900000 tags\n",
      "2017-04-09 02:29:31,533 : INFO : Loading new file for index: 640000\n",
      "2017-04-09 02:29:48,928 : INFO : Loading new file for index: 650000\n",
      "2017-04-09 02:30:06,622 : INFO : Loading new file for index: 660000\n",
      "2017-04-09 02:30:18,360 : INFO : PROGRESS: at example #2000000, processed 78166949 words (67477/s), 384162 word types, 2000000 tags\n",
      "2017-04-09 02:30:23,477 : INFO : Loading new file for index: 670000\n",
      "2017-04-09 02:30:40,471 : INFO : Loading new file for index: 680000\n",
      "2017-04-09 02:30:57,734 : INFO : Loading new file for index: 690000\n",
      "2017-04-09 02:31:14,542 : INFO : Loading new file for index: 700000\n",
      "2017-04-09 02:31:15,018 : INFO : PROGRESS: at example #2100000, processed 82083149 words (69122/s), 395566 word types, 2100000 tags\n",
      "2017-04-09 02:31:31,831 : INFO : Loading new file for index: 710000\n",
      "2017-04-09 02:31:49,445 : INFO : Loading new file for index: 720000\n",
      "2017-04-09 02:32:06,356 : INFO : Loading new file for index: 730000\n",
      "2017-04-09 02:32:12,541 : INFO : PROGRESS: at example #2200000, processed 85975245 words (67662/s), 406986 word types, 2200000 tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-09 02:32:23,473 : INFO : Loading new file for index: 740000\n",
      "2017-04-09 02:32:40,656 : INFO : Loading new file for index: 750000\n",
      "2017-04-09 02:32:57,745 : INFO : Loading new file for index: 760000\n",
      "2017-04-09 02:33:09,551 : INFO : PROGRESS: at example #2300000, processed 89888026 words (68635/s), 418070 word types, 2300000 tags\n",
      "2017-04-09 02:33:14,794 : INFO : Loading new file for index: 770000\n",
      "2017-04-09 02:33:31,657 : INFO : Loading new file for index: 780000\n",
      "2017-04-09 02:33:49,452 : INFO : Loading new file for index: 790000\n",
      "2017-04-09 02:34:06,373 : INFO : Loading new file for index: 800000\n",
      "2017-04-09 02:34:06,876 : INFO : PROGRESS: at example #2400000, processed 93792971 words (68120/s), 428930 word types, 2400000 tags\n",
      "2017-04-09 02:34:23,839 : INFO : Loading new file for index: 810000\n",
      "2017-04-09 02:34:40,929 : INFO : Loading new file for index: 820000\n",
      "2017-04-09 02:34:57,340 : INFO : Loading new file for index: 830000\n",
      "2017-04-09 02:35:03,595 : INFO : PROGRESS: at example #2500000, processed 97697451 words (68840/s), 439597 word types, 2500000 tags\n",
      "2017-04-09 02:35:14,594 : INFO : Loading new file for index: 840000\n",
      "2017-04-09 02:35:31,620 : INFO : Loading new file for index: 850000\n",
      "2017-04-09 02:35:48,635 : INFO : Loading new file for index: 860000\n",
      "2017-04-09 02:36:00,626 : INFO : PROGRESS: at example #2600000, processed 101596492 words (68369/s), 450270 word types, 2600000 tags\n",
      "2017-04-09 02:36:05,773 : INFO : Loading new file for index: 870000\n",
      "2017-04-09 02:36:24,485 : INFO : Loading new file for index: 880000\n",
      "2017-04-09 02:36:41,057 : INFO : Loading new file for index: 890000\n",
      "2017-04-09 02:36:58,289 : INFO : Loading new file for index: 900000\n",
      "2017-04-09 02:36:58,772 : INFO : PROGRESS: at example #2700000, processed 105498645 words (67111/s), 460581 word types, 2700000 tags\n",
      "2017-04-09 02:37:14,979 : INFO : Loading new file for index: 910000\n",
      "2017-04-09 02:37:31,693 : INFO : Loading new file for index: 920000\n",
      "2017-04-09 02:37:48,538 : INFO : Loading new file for index: 930000\n",
      "2017-04-09 02:37:54,936 : INFO : PROGRESS: at example #2800000, processed 109421136 words (69841/s), 470817 word types, 2800000 tags\n",
      "2017-04-09 02:38:05,574 : INFO : Loading new file for index: 940000\n",
      "2017-04-09 02:38:22,172 : INFO : Loading new file for index: 950000\n",
      "2017-04-09 02:38:39,013 : INFO : Loading new file for index: 960000\n",
      "2017-04-09 02:38:51,482 : INFO : PROGRESS: at example #2900000, processed 113328933 words (69110/s), 480955 word types, 2900000 tags\n",
      "2017-04-09 02:38:56,576 : INFO : Loading new file for index: 970000\n",
      "2017-04-09 02:39:13,833 : INFO : Loading new file for index: 980000\n",
      "2017-04-09 02:39:30,732 : INFO : Loading new file for index: 990000\n",
      "2017-04-09 02:39:47,634 : INFO : Loading new file for index: 1000000\n",
      "2017-04-09 02:39:48,116 : INFO : PROGRESS: at example #3000000, processed 117231794 words (68915/s), 490917 word types, 3000000 tags\n",
      "2017-04-09 02:40:04,476 : INFO : Loading new file for index: 1010000\n",
      "2017-04-09 02:40:21,914 : INFO : Loading new file for index: 1020000\n",
      "2017-04-09 02:40:38,896 : INFO : Loading new file for index: 1030000\n",
      "2017-04-09 02:40:45,048 : INFO : PROGRESS: at example #3100000, processed 121150609 words (68835/s), 500670 word types, 3100000 tags\n",
      "2017-04-09 02:40:56,086 : INFO : Loading new file for index: 1040000\n",
      "2017-04-09 02:41:14,350 : INFO : Loading new file for index: 1050000\n",
      "2017-04-09 02:41:31,255 : INFO : Loading new file for index: 1060000\n",
      "2017-04-09 02:41:42,879 : INFO : PROGRESS: at example #3200000, processed 125045436 words (67350/s), 510192 word types, 3200000 tags\n",
      "2017-04-09 02:41:47,867 : INFO : Loading new file for index: 1070000\n",
      "2017-04-09 02:42:04,349 : INFO : Loading new file for index: 1080000\n",
      "2017-04-09 02:42:21,328 : INFO : Loading new file for index: 1090000\n",
      "2017-04-09 02:42:38,660 : INFO : Loading new file for index: 1100000\n",
      "2017-04-09 02:42:39,140 : INFO : PROGRESS: at example #3300000, processed 128957120 words (69530/s), 519792 word types, 3300000 tags\n",
      "2017-04-09 02:42:55,404 : INFO : Loading new file for index: 1110000\n",
      "2017-04-09 02:43:12,130 : INFO : Loading new file for index: 1120000\n",
      "2017-04-09 02:43:29,431 : INFO : Loading new file for index: 1130000\n",
      "2017-04-09 02:43:35,636 : INFO : PROGRESS: at example #3400000, processed 132878374 words (69409/s), 529233 word types, 3400000 tags\n",
      "2017-04-09 02:43:46,079 : INFO : Loading new file for index: 1140000\n",
      "2017-04-09 02:44:02,699 : INFO : Loading new file for index: 1150000\n",
      "2017-04-09 02:44:20,283 : INFO : Loading new file for index: 1160000\n",
      "2017-04-09 02:44:31,913 : INFO : PROGRESS: at example #3500000, processed 136779790 words (69326/s), 538030 word types, 3500000 tags\n",
      "2017-04-09 02:44:37,158 : INFO : Loading new file for index: 1170000\n",
      "2017-04-09 02:44:53,678 : INFO : Loading new file for index: 1180000\n",
      "2017-04-09 02:45:10,827 : INFO : Loading new file for index: 1190000\n",
      "2017-04-09 02:45:27,324 : INFO : Loading new file for index: 1200000\n",
      "2017-04-09 02:45:27,843 : INFO : PROGRESS: at example #3600000, processed 140702103 words (70130/s), 547765 word types, 3600000 tags\n",
      "2017-04-09 02:45:44,215 : INFO : Loading new file for index: 1210000\n",
      "2017-04-09 02:46:01,287 : INFO : Loading new file for index: 1220000\n",
      "2017-04-09 02:46:17,904 : INFO : Loading new file for index: 1230000\n",
      "2017-04-09 02:46:23,857 : INFO : PROGRESS: at example #3700000, processed 144628379 words (70096/s), 556875 word types, 3700000 tags\n",
      "2017-04-09 02:46:34,265 : INFO : Loading new file for index: 1240000\n",
      "2017-04-09 02:46:50,843 : INFO : Loading new file for index: 1250000\n",
      "2017-04-09 02:47:07,456 : INFO : Loading new file for index: 1260000\n",
      "2017-04-09 02:47:18,929 : INFO : PROGRESS: at example #3800000, processed 148535617 words (70949/s), 565516 word types, 3800000 tags\n",
      "2017-04-09 02:47:24,009 : INFO : Loading new file for index: 1270000\n",
      "2017-04-09 02:47:43,156 : INFO : Loading new file for index: 1280000\n",
      "2017-04-09 02:47:54,049 : INFO : Loading new file for index: 1290000\n",
      "2017-04-09 02:47:54,051 : INFO : All files are loaded - last file: 1290000\n",
      "2017-04-09 02:47:54,561 : INFO : collected 570719 word types and 3858975 unique tags from a corpus of 3858975 examples and 150842878 words\n",
      "2017-04-09 02:47:54,889 : INFO : min_count=100 retains 23510 unique words (drops 547209)\n",
      "2017-04-09 02:47:54,890 : INFO : min_count leaves 148025203 word corpus (98% of original 150842878)\n",
      "2017-04-09 02:47:54,941 : INFO : deleting the raw counts dictionary of 570719 items\n",
      "2017-04-09 02:47:55,004 : INFO : sample=0.001 downsamples 37 most-common words\n",
      "2017-04-09 02:47:55,005 : INFO : downsampling leaves estimated 102385809 word corpus (69.2% of prior 148025203)\n",
      "2017-04-09 02:47:55,006 : INFO : estimated required memory for 23510 words and 200 dimensions: 3908346000 bytes\n",
      "2017-04-09 02:47:55,060 : INFO : resetting layer weights\n",
      "2017-04-09 02:48:37,974 : INFO : saving Doc2Vec object under /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_abstract/full/vocab_model/model, separately None\n",
      "2017-04-09 02:48:37,976 : INFO : storing numpy array 'doctag_syn0' to /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_abstract/full/vocab_model/model.docvecs.doctag_syn0.npy\n",
      "2017-04-09 02:48:40,258 : INFO : not storing attribute syn0norm\n",
      "2017-04-09 02:48:40,259 : INFO : not storing attribute cum_table\n",
      "2017-04-09 02:49:07,182 : INFO : creating vocabulary for 3 claims in \n",
      "2017-04-09 02:49:07,183 : INFO : FILE /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_claims/full/vocab_model/model\n",
      "2017-04-09 02:49:08,522 : INFO : collecting all words and their counts\n",
      "2017-04-09 02:49:08,524 : INFO : Loading new file for index: 0\n",
      "2017-04-09 02:49:08,540 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-04-09 02:49:28,213 : INFO : Loading new file for index: 10000\n",
      "2017-04-09 02:49:48,070 : INFO : Loading new file for index: 20000\n",
      "2017-04-09 02:49:59,384 : INFO : PROGRESS: at example #100000, processed 26951598 words (530087/s), 126142 word types, 100000 tags\n",
      "2017-04-09 02:50:08,870 : INFO : Loading new file for index: 30000\n",
      "2017-04-09 02:50:29,176 : INFO : Loading new file for index: 40000\n",
      "2017-04-09 02:50:49,185 : INFO : Loading new file for index: 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-09 02:50:49,786 : INFO : PROGRESS: at example #200000, processed 53913100 words (534946/s), 197566 word types, 200000 tags\n",
      "2017-04-09 02:51:09,706 : INFO : Loading new file for index: 60000\n",
      "2017-04-09 02:51:29,593 : INFO : Loading new file for index: 70000\n",
      "2017-04-09 02:51:40,544 : INFO : PROGRESS: at example #300000, processed 81249829 words (538581/s), 261261 word types, 299996 tags\n",
      "2017-04-09 02:51:49,911 : INFO : Loading new file for index: 80000\n",
      "2017-04-09 02:52:10,324 : INFO : Loading new file for index: 90000\n",
      "2017-04-09 02:52:30,372 : INFO : Loading new file for index: 100000\n",
      "2017-04-09 02:52:30,902 : INFO : PROGRESS: at example #400000, processed 108507954 words (541299/s), 322438 word types, 399996 tags\n",
      "2017-04-09 02:52:50,831 : INFO : Loading new file for index: 110000\n",
      "2017-04-09 02:53:11,776 : INFO : Loading new file for index: 120000\n",
      "2017-04-09 02:53:22,526 : INFO : PROGRESS: at example #500000, processed 135625272 words (525301/s), 376791 word types, 499992 tags\n",
      "2017-04-09 02:53:31,943 : INFO : Loading new file for index: 130000\n",
      "2017-04-09 02:53:51,726 : INFO : Loading new file for index: 140000\n",
      "2017-04-09 02:54:11,598 : INFO : Loading new file for index: 150000\n",
      "2017-04-09 02:54:12,212 : INFO : PROGRESS: at example #600000, processed 162586082 words (542630/s), 429369 word types, 599992 tags\n",
      "2017-04-09 02:54:32,226 : INFO : Loading new file for index: 160000\n",
      "2017-04-09 02:54:52,083 : INFO : Loading new file for index: 170000\n",
      "2017-04-09 02:55:03,034 : INFO : PROGRESS: at example #700000, processed 189734898 words (534212/s), 477960 word types, 699992 tags\n",
      "2017-04-09 02:55:12,586 : INFO : Loading new file for index: 180000\n",
      "2017-04-09 02:55:32,333 : INFO : Loading new file for index: 190000\n",
      "2017-04-09 02:55:52,190 : INFO : Loading new file for index: 200000\n",
      "2017-04-09 02:55:52,781 : INFO : PROGRESS: at example #800000, processed 216811501 words (544304/s), 523780 word types, 799992 tags\n",
      "2017-04-09 02:56:12,125 : INFO : Loading new file for index: 210000\n",
      "2017-04-09 02:56:32,307 : INFO : Loading new file for index: 220000\n",
      "2017-04-09 02:56:43,355 : INFO : PROGRESS: at example #900000, processed 244107152 words (539721/s), 568757 word types, 899992 tags\n",
      "2017-04-09 02:56:52,631 : INFO : Loading new file for index: 230000\n",
      "2017-04-09 02:57:12,963 : INFO : Loading new file for index: 240000\n",
      "2017-04-09 02:57:33,621 : INFO : Loading new file for index: 250000\n",
      "2017-04-09 02:57:34,192 : INFO : PROGRESS: at example #1000000, processed 271231381 words (533571/s), 611300 word types, 999992 tags\n",
      "2017-04-09 02:57:54,131 : INFO : Loading new file for index: 260000\n",
      "2017-04-09 02:58:13,724 : INFO : Loading new file for index: 270000\n",
      "2017-04-09 02:58:24,625 : INFO : PROGRESS: at example #1100000, processed 298520921 words (541119/s), 651782 word types, 1099988 tags\n",
      "2017-04-09 02:58:34,419 : INFO : Loading new file for index: 280000\n",
      "2017-04-09 02:58:54,116 : INFO : Loading new file for index: 290000\n",
      "2017-04-09 02:59:14,225 : INFO : Loading new file for index: 300000\n",
      "2017-04-09 02:59:14,773 : INFO : PROGRESS: at example #1200000, processed 325376855 words (535549/s), 691154 word types, 1199988 tags\n",
      "2017-04-09 02:59:34,424 : INFO : Loading new file for index: 310000\n",
      "2017-04-09 02:59:54,384 : INFO : Loading new file for index: 320000\n",
      "2017-04-09 03:00:04,637 : INFO : PROGRESS: at example #1300000, processed 352405658 words (542056/s), 731014 word types, 1299984 tags\n",
      "2017-04-09 03:00:13,996 : INFO : Loading new file for index: 330000\n",
      "2017-04-09 03:00:33,674 : INFO : Loading new file for index: 340000\n",
      "2017-04-09 03:00:52,924 : INFO : Loading new file for index: 350000\n",
      "2017-04-09 03:00:53,485 : INFO : PROGRESS: at example #1400000, processed 379230638 words (549164/s), 768668 word types, 1399984 tags\n",
      "2017-04-09 03:01:11,678 : INFO : Loading new file for index: 360000\n",
      "2017-04-09 03:01:30,115 : INFO : Loading new file for index: 370000\n",
      "2017-04-09 03:01:39,537 : INFO : PROGRESS: at example #1500000, processed 406480119 words (591736/s), 807467 word types, 1499984 tags\n",
      "2017-04-09 03:01:47,836 : INFO : Loading new file for index: 380000\n",
      "2017-04-09 03:02:06,473 : INFO : Loading new file for index: 390000\n",
      "2017-04-09 03:02:24,517 : INFO : Loading new file for index: 400000\n",
      "2017-04-09 03:02:24,996 : INFO : PROGRESS: at example #1600000, processed 433642098 words (597513/s), 842268 word types, 1599984 tags\n",
      "2017-04-09 03:02:43,115 : INFO : Loading new file for index: 410000\n",
      "2017-04-09 03:03:01,621 : INFO : Loading new file for index: 420000\n",
      "2017-04-09 03:03:11,553 : INFO : PROGRESS: at example #1700000, processed 460752353 words (582320/s), 880186 word types, 1699984 tags\n",
      "2017-04-09 03:03:20,149 : INFO : Loading new file for index: 430000\n",
      "2017-04-09 03:03:40,767 : INFO : Loading new file for index: 440000\n",
      "2017-04-09 03:03:59,180 : INFO : Loading new file for index: 450000\n",
      "2017-04-09 03:03:59,705 : INFO : PROGRESS: at example #1800000, processed 487748645 words (560657/s), 916028 word types, 1799984 tags\n",
      "2017-04-09 03:04:17,820 : INFO : Loading new file for index: 460000\n",
      "2017-04-09 03:04:36,031 : INFO : Loading new file for index: 470000\n",
      "2017-04-09 03:04:45,759 : INFO : PROGRESS: at example #1900000, processed 514830039 words (588052/s), 950407 word types, 1899984 tags\n",
      "2017-04-09 03:04:54,657 : INFO : Loading new file for index: 480000\n",
      "2017-04-09 03:05:12,778 : INFO : Loading new file for index: 490000\n",
      "2017-04-09 03:05:31,205 : INFO : Loading new file for index: 500000\n",
      "2017-04-09 03:05:31,753 : INFO : PROGRESS: at example #2000000, processed 541953023 words (589721/s), 985717 word types, 1999984 tags\n",
      "2017-04-09 03:05:49,668 : INFO : Loading new file for index: 510000\n",
      "2017-04-09 03:06:08,162 : INFO : Loading new file for index: 520000\n",
      "2017-04-09 03:06:17,857 : INFO : PROGRESS: at example #2100000, processed 568863962 words (583721/s), 1019984 word types, 2099984 tags\n",
      "2017-04-09 03:06:26,342 : INFO : Loading new file for index: 530000\n",
      "2017-04-09 03:06:44,785 : INFO : Loading new file for index: 540000\n",
      "2017-04-09 03:07:03,110 : INFO : Loading new file for index: 550000\n",
      "2017-04-09 03:07:03,631 : INFO : PROGRESS: at example #2200000, processed 596070350 words (594373/s), 1056307 word types, 2199984 tags\n",
      "2017-04-09 03:07:21,187 : INFO : Loading new file for index: 560000\n",
      "2017-04-09 03:07:39,327 : INFO : Loading new file for index: 570000\n",
      "2017-04-09 03:07:48,986 : INFO : PROGRESS: at example #2300000, processed 623240326 words (599078/s), 1088443 word types, 2299984 tags\n",
      "2017-04-09 03:07:57,435 : INFO : Loading new file for index: 580000\n",
      "2017-04-09 03:08:15,096 : INFO : Loading new file for index: 590000\n",
      "2017-04-09 03:08:32,813 : INFO : Loading new file for index: 600000\n",
      "2017-04-09 03:08:33,320 : INFO : PROGRESS: at example #2400000, processed 650429090 words (613277/s), 1120047 word types, 2399984 tags\n",
      "2017-04-09 03:08:50,177 : INFO : Loading new file for index: 610000\n",
      "2017-04-09 03:09:07,681 : INFO : Loading new file for index: 620000\n",
      "2017-04-09 03:09:16,843 : INFO : PROGRESS: at example #2500000, processed 677642860 words (625291/s), 1150828 word types, 2499984 tags\n",
      "2017-04-09 03:09:24,933 : INFO : Loading new file for index: 630000\n",
      "2017-04-09 03:09:42,819 : INFO : Loading new file for index: 640000\n",
      "2017-04-09 03:10:00,582 : INFO : Loading new file for index: 650000\n",
      "2017-04-09 03:10:01,119 : INFO : PROGRESS: at example #2600000, processed 704890858 words (615429/s), 1182477 word types, 2599984 tags\n",
      "2017-04-09 03:10:18,355 : INFO : Loading new file for index: 660000\n",
      "2017-04-09 03:10:35,891 : INFO : Loading new file for index: 670000\n",
      "2017-04-09 03:10:47,333 : INFO : PROGRESS: at example #2700000, processed 732093693 words (588645/s), 1214175 word types, 2699984 tags\n",
      "2017-04-09 03:10:55,787 : INFO : Loading new file for index: 680000\n",
      "2017-04-09 03:11:13,716 : INFO : Loading new file for index: 690000\n",
      "2017-04-09 03:11:31,590 : INFO : Loading new file for index: 700000\n",
      "2017-04-09 03:11:32,101 : INFO : PROGRESS: at example #2800000, processed 759169172 words (604800/s), 1246289 word types, 2799984 tags\n",
      "2017-04-09 03:11:49,280 : INFO : Loading new file for index: 710000\n",
      "2017-04-09 03:12:07,073 : INFO : Loading new file for index: 720000\n",
      "2017-04-09 03:12:16,497 : INFO : PROGRESS: at example #2900000, processed 786061694 words (605764/s), 1276119 word types, 2899984 tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-09 03:12:24,658 : INFO : Loading new file for index: 730000\n",
      "2017-04-09 03:12:42,426 : INFO : Loading new file for index: 740000\n",
      "2017-04-09 03:13:00,258 : INFO : Loading new file for index: 750000\n",
      "2017-04-09 03:13:00,788 : INFO : PROGRESS: at example #3000000, processed 812967872 words (607500/s), 1305395 word types, 2999984 tags\n",
      "2017-04-09 03:13:18,143 : INFO : Loading new file for index: 760000\n",
      "2017-04-09 03:13:35,986 : INFO : Loading new file for index: 770000\n",
      "2017-04-09 03:13:45,314 : INFO : PROGRESS: at example #3100000, processed 840044953 words (608129/s), 1334965 word types, 3099984 tags\n",
      "2017-04-09 03:13:53,659 : INFO : Loading new file for index: 780000\n",
      "2017-04-09 03:14:11,709 : INFO : Loading new file for index: 790000\n",
      "2017-04-09 03:14:29,307 : INFO : Loading new file for index: 800000\n",
      "2017-04-09 03:14:29,836 : INFO : PROGRESS: at example #3200000, processed 867301368 words (612212/s), 1364123 word types, 3199984 tags\n",
      "2017-04-09 03:14:47,474 : INFO : Loading new file for index: 810000\n",
      "2017-04-09 03:15:05,542 : INFO : Loading new file for index: 820000\n",
      "2017-04-09 03:15:14,843 : INFO : PROGRESS: at example #3300000, processed 894274958 words (599347/s), 1397920 word types, 3299984 tags\n",
      "2017-04-09 03:15:23,103 : INFO : Loading new file for index: 830000\n",
      "2017-04-09 03:15:41,169 : INFO : Loading new file for index: 840000\n",
      "2017-04-09 03:15:59,211 : INFO : Loading new file for index: 850000\n",
      "2017-04-09 03:15:59,783 : INFO : PROGRESS: at example #3400000, processed 921140321 words (597806/s), 1427266 word types, 3399984 tags\n",
      "2017-04-09 03:16:17,105 : INFO : Loading new file for index: 860000\n",
      "2017-04-09 03:16:35,227 : INFO : Loading new file for index: 870000\n",
      "2017-04-09 03:16:44,765 : INFO : PROGRESS: at example #3500000, processed 948218785 words (602009/s), 1455982 word types, 3499984 tags\n",
      "2017-04-09 03:16:53,172 : INFO : Loading new file for index: 880000\n",
      "2017-04-09 03:17:10,694 : INFO : Loading new file for index: 890000\n",
      "2017-04-09 03:17:28,642 : INFO : Loading new file for index: 900000\n",
      "2017-04-09 03:17:29,155 : INFO : PROGRESS: at example #3600000, processed 975211021 words (608091/s), 1483832 word types, 3599980 tags\n",
      "2017-04-09 03:17:46,464 : INFO : Loading new file for index: 910000\n",
      "2017-04-09 03:18:04,195 : INFO : Loading new file for index: 920000\n",
      "2017-04-09 03:18:13,451 : INFO : PROGRESS: at example #3700000, processed 1002208062 words (609471/s), 1511785 word types, 3699980 tags\n",
      "2017-04-09 03:18:21,838 : INFO : Loading new file for index: 930000\n",
      "2017-04-09 03:18:39,591 : INFO : Loading new file for index: 940000\n",
      "2017-04-09 03:18:57,011 : INFO : Loading new file for index: 950000\n",
      "2017-04-09 03:18:57,597 : INFO : PROGRESS: at example #3800000, processed 1029312026 words (613982/s), 1540373 word types, 3799980 tags\n",
      "2017-04-09 03:19:14,767 : INFO : Loading new file for index: 960000\n",
      "2017-04-09 03:19:32,740 : INFO : Loading new file for index: 970000\n",
      "2017-04-09 03:19:42,114 : INFO : PROGRESS: at example #3900000, processed 1056473679 words (610167/s), 1568038 word types, 3899972 tags\n",
      "2017-04-09 03:19:53,153 : INFO : Loading new file for index: 980000\n",
      "2017-04-09 03:20:10,831 : INFO : Loading new file for index: 990000\n",
      "2017-04-09 03:20:28,745 : INFO : Loading new file for index: 1000000\n",
      "2017-04-09 03:20:29,259 : INFO : PROGRESS: at example #4000000, processed 1083398462 words (571113/s), 1594851 word types, 3999968 tags\n",
      "2017-04-09 03:20:46,560 : INFO : Loading new file for index: 1010000\n",
      "2017-04-09 03:21:04,985 : INFO : Loading new file for index: 1020000\n",
      "2017-04-09 03:21:14,522 : INFO : PROGRESS: at example #4100000, processed 1110663263 words (602386/s), 1621242 word types, 4099968 tags\n",
      "2017-04-09 03:21:23,047 : INFO : Loading new file for index: 1030000\n",
      "2017-04-09 03:21:41,349 : INFO : Loading new file for index: 1040000\n",
      "2017-04-09 03:22:00,154 : INFO : Loading new file for index: 1050000\n",
      "2017-04-09 03:22:00,680 : INFO : PROGRESS: at example #4200000, processed 1137638293 words (584422/s), 1648381 word types, 4199968 tags\n",
      "2017-04-09 03:22:17,824 : INFO : Loading new file for index: 1060000\n",
      "2017-04-09 03:22:35,417 : INFO : Loading new file for index: 1070000\n",
      "2017-04-09 03:22:44,545 : INFO : PROGRESS: at example #4300000, processed 1164578173 words (614157/s), 1675425 word types, 4299964 tags\n",
      "2017-04-09 03:22:52,933 : INFO : Loading new file for index: 1080000\n",
      "2017-04-09 03:23:10,677 : INFO : Loading new file for index: 1090000\n",
      "2017-04-09 03:23:28,893 : INFO : Loading new file for index: 1100000\n",
      "2017-04-09 03:23:29,405 : INFO : PROGRESS: at example #4400000, processed 1191533286 words (600897/s), 1702122 word types, 4399964 tags\n",
      "2017-04-09 03:23:46,435 : INFO : Loading new file for index: 1110000\n",
      "2017-04-09 03:24:04,082 : INFO : Loading new file for index: 1120000\n",
      "2017-04-09 03:24:13,516 : INFO : PROGRESS: at example #4500000, processed 1218374640 words (608513/s), 1728438 word types, 4499964 tags\n",
      "2017-04-09 03:24:22,095 : INFO : Loading new file for index: 1130000\n",
      "2017-04-09 03:24:39,803 : INFO : Loading new file for index: 1140000\n",
      "2017-04-09 03:24:57,351 : INFO : Loading new file for index: 1150000\n",
      "2017-04-09 03:24:57,949 : INFO : PROGRESS: at example #4600000, processed 1245382974 words (607854/s), 1754064 word types, 4599964 tags\n",
      "2017-04-09 03:25:15,528 : INFO : Loading new file for index: 1160000\n",
      "2017-04-09 03:25:33,456 : INFO : Loading new file for index: 1170000\n",
      "2017-04-09 03:25:42,850 : INFO : PROGRESS: at example #4700000, processed 1272510718 words (604183/s), 1780551 word types, 4699964 tags\n",
      "2017-04-09 03:25:51,067 : INFO : Loading new file for index: 1180000\n",
      "2017-04-09 03:26:09,238 : INFO : Loading new file for index: 1190000\n",
      "2017-04-09 03:26:27,095 : INFO : Loading new file for index: 1200000\n",
      "2017-04-09 03:26:27,653 : INFO : PROGRESS: at example #4800000, processed 1299495116 words (602302/s), 1806108 word types, 4799964 tags\n",
      "2017-04-09 03:26:45,127 : INFO : Loading new file for index: 1210000\n",
      "2017-04-09 03:27:03,280 : INFO : Loading new file for index: 1220000\n",
      "2017-04-09 03:27:12,998 : INFO : PROGRESS: at example #4900000, processed 1326612892 words (598053/s), 1832060 word types, 4899964 tags\n",
      "2017-04-09 03:27:21,452 : INFO : Loading new file for index: 1230000\n",
      "2017-04-09 03:27:39,619 : INFO : Loading new file for index: 1240000\n",
      "2017-04-09 03:27:57,814 : INFO : Loading new file for index: 1250000\n",
      "2017-04-09 03:27:58,398 : INFO : PROGRESS: at example #5000000, processed 1353842042 words (599788/s), 1861544 word types, 4999964 tags\n",
      "2017-04-09 03:28:15,915 : INFO : Loading new file for index: 1260000\n",
      "2017-04-09 03:28:33,934 : INFO : Loading new file for index: 1270000\n",
      "2017-04-09 03:28:43,159 : INFO : PROGRESS: at example #5100000, processed 1380796602 words (602212/s), 1885414 word types, 5099964 tags\n",
      "2017-04-09 03:28:51,581 : INFO : Loading new file for index: 1280000\n",
      "2017-04-09 03:29:03,185 : INFO : Loading new file for index: 1290000\n",
      "2017-04-09 03:29:03,186 : INFO : All files are loaded - last file: 1290000\n",
      "2017-04-09 03:29:03,703 : INFO : collected 1897069 word types and 5145255 unique tags from a corpus of 5145291 examples and 1393295375 words\n",
      "2017-04-09 03:29:04,755 : INFO : min_count=100 retains 67011 unique words (drops 1830058)\n",
      "2017-04-09 03:29:04,757 : INFO : min_count leaves 1382268769 word corpus (99% of original 1393295375)\n",
      "2017-04-09 03:29:04,912 : INFO : deleting the raw counts dictionary of 1897069 items\n",
      "2017-04-09 03:29:05,095 : INFO : sample=0.001 downsamples 42 most-common words\n",
      "2017-04-09 03:29:05,096 : INFO : downsampling leaves estimated 894341231 word corpus (64.7% of prior 1382268769)\n",
      "2017-04-09 03:29:05,097 : INFO : estimated required memory for 67011 words and 200 dimensions: 5285978100 bytes\n",
      "2017-04-09 03:29:05,313 : INFO : resetting layer weights\n",
      "2017-04-09 03:30:03,321 : INFO : saving Doc2Vec object under /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_claims/full/vocab_model/model, separately None\n",
      "2017-04-09 03:30:03,323 : INFO : storing numpy array 'doctag_syn0' to /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_claims/full/vocab_model/model.docvecs.doctag_syn0.npy\n",
      "2017-04-09 03:30:06,184 : INFO : storing numpy array 'syn1neg' to /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_claims/full/vocab_model/model.syn1neg.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-09 03:30:06,220 : INFO : not storing attribute syn0norm\n",
      "2017-04-09 03:30:06,221 : INFO : storing numpy array 'syn0' to /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_claims/full/vocab_model/model.syn0.npy\n",
      "2017-04-09 03:30:06,267 : INFO : not storing attribute cum_table\n"
     ]
    }
   ],
   "source": [
    "for level, model_name in models:\n",
    "    info(\"creating vocabulary for \" + str(level) + ' ' + model_name + ' in ')\n",
    "    doc2vec_model_save_location = os.path.join(root_location,\n",
    "                                               \"parameter_search_doc2vec_models_\" + str(level) + '_' + model_name,\n",
    "                                               \"full\")\n",
    "    if not os.path.exists(doc2vec_model_save_location):\n",
    "        os.makedirs(doc2vec_model_save_location)\n",
    "    if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL)):\n",
    "        os.makedirs(os.path.join(doc2vec_model_save_location, VOCAB_MODEL))\n",
    "\n",
    "    placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}_model_{}'.format(DOC2VEC_SIZE,\n",
    "                                                                    DOC2VEC_WINDOW,\n",
    "                                                                    'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                    DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                                    DOC2VEC_TRAIN_WORDS,\n",
    "                                                                    DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                                    str(DOC2VEC_MAX_VOCAB_SIZE),\n",
    "                                                                    str(level) + '_' + model_name\n",
    "                                                                    )\n",
    "    GLOBAL_VARS.DOC2VEC_MODEL_NAME = placeholder_model_name\n",
    "    placeholder_model_name = os.path.join(placeholder_model_name, \"epoch_{}\")\n",
    "    info(\"FILE \" + os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "    doc2vec_model = Doc2Vec(size=DOC2VEC_SIZE, window=DOC2VEC_WINDOW, min_count=MIN_WORD_COUNT,\n",
    "                    max_vocab_size= DOC2VEC_MAX_VOCAB_SIZE,\n",
    "                    sample=DOC2VEC_SAMPLE, seed=DOC2VEC_SEED, workers=NUM_CORES,\n",
    "                    # doc2vec algorithm dm=1 => PV-DM, dm=2 => PV-DBOW, PV-DM dictates CBOW for words\n",
    "                    dm=DOC2VEC_TYPE,\n",
    "                    # hs=0 => negative sampling, hs=1 => hierarchical softmax\n",
    "                    hs=DOC2VEC_HIERARCHICAL_SAMPLE, negative=DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                    dm_concat=DOC2VEC_CONCAT,\n",
    "                    # would train words with skip-gram on top of cbow, we don't need that for now\n",
    "                    dbow_words=DOC2VEC_TRAIN_WORDS,\n",
    "                    iter=DOC2VEC_EPOCHS)\n",
    "\n",
    "    GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "\n",
    "    training_docs_iterator = BatchWrapper(training_preprocessed_files_prefix, batch_size=10000, level=level,\n",
    "                                          level_type=model_name)\n",
    "    if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX)):\n",
    "        doc2vec_model.build_vocab(sentences=training_docs_iterator, progress_per=REPORT_VOCAB_PROGRESS)\n",
    "        doc2vec_model.save(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "    else:\n",
    "        doc2vec_model_vocab_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "        doc2vec_model.reset_from(doc2vec_model_vocab_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2vec_model = Doc2Vec(size=DOC2VEC_SIZE , window=DOC2VEC_WINDOW, min_count=MIN_WORD_COUNT, \n",
    "                max_vocab_size= DOC2VEC_MAX_VOCAB_SIZE,\n",
    "                sample=DOC2VEC_SAMPLE, seed=DOC2VEC_SEED, workers=NUM_CORES,\n",
    "                # doc2vec algorithm dm=1 => PV-DM, dm=2 => PV-DBOW, PV-DM dictates CBOW for words\n",
    "                dm=DOC2VEC_TYPE,\n",
    "                # hs=0 => negative sampling, hs=1 => hierarchical softmax\n",
    "                hs=DOC2VEC_HIERARCHICAL_SAMPLE, negative=DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                dm_concat=DOC2VEC_CONCAT,\n",
    "                # would train words with skip-gram on top of cbow, we don't need that for now\n",
    "                dbow_words=DOC2VEC_TRAIN_WORDS,\n",
    "                iter=DOC2VEC_EPOCHS)\n",
    "\n",
    "GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First: construct (or load) the vocabulary\n",
    "Only needed to be run if you dont already haave at least one epoch computed, otherwise, just set the start_from (below) to the epoch you want to restart from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-06 05:57:37,601 : INFO : loading Doc2Vec object from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/vocab_model/model\n",
      "2017-04-06 05:57:37,602 : INFO : Loading new file for index: 0\n",
      "2017-04-06 06:00:49,254 : INFO : loading docvecs recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/vocab_model/model.docvecs.* with mmap=None\n",
      "2017-04-06 06:00:49,255 : INFO : loading doctag_syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/vocab_model/model.docvecs.doctag_syn0.npy with mmap=None\n",
      "2017-04-06 06:01:07,168 : INFO : loading doctag_syn0_lockf from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/vocab_model/model.docvecs.doctag_syn0_lockf.npy with mmap=None\n",
      "2017-04-06 06:01:07,265 : INFO : loading syn1neg from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/vocab_model/model.syn1neg.npy with mmap=None\n",
      "2017-04-06 06:01:07,441 : INFO : loading syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/vocab_model/model.syn0.npy with mmap=None\n",
      "2017-04-06 06:01:07,617 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-06 06:01:07,618 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-06 06:01:08,595 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 50s, sys: 1min 9s, total: 14min\n",
      "Wall time: 14min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# training_docs_iterator = ExtendedPVDocumentBatchGenerator(training_preprocessed_files_prefix, batch_size=10000)\n",
    "training_docs_iterator = BatchClass(training_preprocessed_files_prefix, batch_size=10000)\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX)):\n",
    "    doc2vec_model.build_vocab(sentences=training_docs_iterator, progress_per=REPORT_VOCAB_PROGRESS)\n",
    "    doc2vec_model.save(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "else:\n",
    "    doc2vec_model_vocab_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "    doc2vec_model.reset_from(doc2vec_model_vocab_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocab_counts = {k:doc2vec_model.vocab[k].count for k in doc2vec_model.vocab.keys()}\n",
    "# dd = sorted(vocab_counts, key=vocab_counts.get)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
