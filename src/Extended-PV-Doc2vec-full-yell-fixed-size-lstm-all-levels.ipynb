{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of fixed size paragraph vectors using LSTM\n",
    "should be able to deal with all levels using the PARTS_LEVEL param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: TITAN X (Pascal) (CNMeM is disabled, cuDNN 5105)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple, defaultdict\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "from functools import partial\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Dropout, Masking\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Masking\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "from thesis.utils.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables used throughout the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_SEED = 1234\n",
    "DOC2VEC_SEED = 1234\n",
    "WORD2VEC_SEED = 1234\n",
    "NN_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CORES = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GLOBAL_VARS = namedtuple('GLOBAL_VARS', ['MODEL_NAME', 'DOC2VEC_MODEL_NAME', 'DOC2VEC_MODEL', \n",
    "                                         'SVM_MODEL_NAME', 'NN_MODEL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_MODEL = \"vocab_model\"\n",
    "MODEL_PREFIX = \"model\"\n",
    "VALIDATION_MATRIX = \"validation_matrix.pkl\"\n",
    "VALIDATION_DICT = \"validation_dict.pkl\"\n",
    "TEST_MATRIX = \"test_matrix.pkl\"\n",
    "TEST_DICT = \"test_dict.pkl\"\n",
    "METRICS = \"metrics.pkl\"\n",
    "CLASSIFIER = \"classifier.pkl\"\n",
    "TYPE_CLASSIFIER= \"{}_classifier.pkl\"\n",
    "\n",
    "TRAINING_DATA_MATRIX = \"X_level_{}.npy\"\n",
    "TRAINING_LABELS_MATRIX = \"y_{}.npy\"\n",
    "VALIDATION_DATA_MATRIX = \"Xv_level_{}.npy\"\n",
    "VALIDATION_LABELS_MATRIX = \"yv_{}.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_PARAMETER_SEARCH_PREFIX = \"lstm_{}_level_{}_batch_{}_nn_parameter_searches.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_location = \"/mnt/data2/shalaby/\"\n",
    "big_data_location = \"/mnt/data/shalaby/\"\n",
    "\n",
    "doc_vec_types = \"extended_abs_desc_claims_all_levels_3\"\n",
    "doc_vec_preprocessed_data_types = \"extended_pv_abs_desc_claims_all_levels_3\"\n",
    "\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "doc2vec_model_save_location = os.path.join(big_data_location, \"parameter_search_doc2vec_models_\" + doc_vec_types, \"full\")\n",
    "nn_parameter_search_location = os.path.join(root_location, \"nn_parameter_search_\" + doc_vec_types)\n",
    "if not os.path.exists(doc2vec_model_save_location):\n",
    "    os.makedirs(doc2vec_model_save_location)\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL)):\n",
    "    os.makedirs(os.path.join(doc2vec_model_save_location, VOCAB_MODEL))\n",
    "\n",
    "#training_file = root_location + \"docs_output.json\"\n",
    "training_file = root_location + 'docs_output.json'\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "valid_classes_file = exports_location + \"valid_classes.pkl\"\n",
    "valid_subclasses_file = exports_location + \"valid_subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "training_docs_list_file = exports_location + \"extended_pv_training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"extended_pv_validation_docs_list.pkl\"\n",
    "test_docs_list_file = exports_location + \"extended_pv_test_docs_list.pkl\"\n",
    "\n",
    "preprocessed_location = os.path.join(big_data_location, \"preprocessed_data\", doc_vec_preprocessed_data_types) + \"/\"\n",
    "\n",
    "training_preprocessed_files_prefix = preprocessed_location + \"extended_pv_training_docs_data_preprocessed-\"\n",
    "validation_preprocessed_files_prefix = preprocessed_location + \"extended_pv_validation_docs_data_preprocessed-\"\n",
    "test_preprocessed_files_prefix = preprocessed_location + \"extended_pv_test_docs_data_preprocessed-\"\n",
    "\n",
    "word2vec_questions_file = result = root_location + 'tensorflow/word2vec/questions-words.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load general data required for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.7 s, sys: 1.96 s, total: 19.7 s\n",
      "Wall time: 19.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "valid_classes = pickle.load(open(valid_classes_file))\n",
    "valid_subclasses = pickle.load(open(valid_subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120156"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29675"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37771"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OneHotEncoder():\n",
    "    \n",
    "    def __init__(self, classifications):\n",
    "        self.classifications = classifications\n",
    "        self.one_hot_indices = {}\n",
    "\n",
    "        # convert character classifications to bit vectors\n",
    "        for i, clssf in enumerate(classifications):\n",
    "            bits = [0] * len(classifications)\n",
    "            bits[i] = 1\n",
    "            self.one_hot_indices[clssf] = i\n",
    "    \n",
    "    def get_label_vector(self, labels):\n",
    "        \"\"\"\n",
    "        classes: array of string with the classes assigned to the instance\n",
    "        \"\"\"\n",
    "        output_vector = [0] * len(self.classifications)\n",
    "        for label in labels:\n",
    "            index = self.one_hot_indices[label]\n",
    "            output_vector[index] = 1\n",
    "            \n",
    "        return output_vector\n",
    "    \n",
    "def ensure_disk_location_exists(location):\n",
    "    if not os.path.exists(location):\n",
    "        os.makedirs(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FixedDocumentsStatsGenerator(object):\n",
    "    def __init__(self, filename_prefix):\n",
    "        self.filename_prefix = filename_prefix\n",
    "        self.docids = []\n",
    "        self.doc_parts = defaultdict(list)\n",
    "        self.doc_part_chunks = defaultdict(list)\n",
    "        self.curr_doc_index = 0\n",
    "        self.batch_end = -1\n",
    "    def load_new_batch_in_memory(self):\n",
    "        info(\"Loading new batch for index: {}\".format(self.curr_doc_index))\n",
    "        true_docs_count = 0\n",
    "        try:\n",
    "            with open(self.filename_prefix + str(self.curr_doc_index)) as preproc_file:\n",
    "                for line in preproc_file:\n",
    "                    line_array = line.split(\" \", 1)\n",
    "                    entity_id = line_array[0].strip()\n",
    "                    if self.is_doc(entity_id):\n",
    "                        self.docids.append(entity_id)\n",
    "                        true_docs_count+= 1\n",
    "                    elif self.is_doc_part(entity_id):\n",
    "                        self.doc_parts[self.get_doc_id(entity_id)].append(entity_id)\n",
    "                    elif self.is_doc_part_chunk(entity_id):\n",
    "                        self.doc_part_chunks[self.get_doc_id(entity_id)].append(entity_id)\n",
    "            self.batch_end = self.curr_doc_index + true_docs_count - 1 \n",
    "            info(\"Finished loading new batch of {} documents\".format(true_docs_count))\n",
    "        except IOError:\n",
    "            info(\"No more batches to load, exiting at index: {}\".format(self.curr_doc_index))\n",
    "            raise StopIteration()\n",
    "    def get_stats(self):\n",
    "        try:\n",
    "            while True:\n",
    "                if self.curr_doc_index > self.batch_end:\n",
    "                    self.load_new_batch_in_memory()\n",
    "                self.curr_doc_index = self.batch_end + 1\n",
    "        except StopIteration:\n",
    "            pass\n",
    "            \n",
    "    def get_doc_id(self, entity_id):\n",
    "        return entity_id.split(\"_\")[0]\n",
    "    def get_entity_parts(self, entity_id):\n",
    "        return entity_id.split(\"_\")\n",
    "    def is_doc(self, entity_id):\n",
    "        parts = self.get_entity_parts(entity_id)\n",
    "        if len(parts) == 1:\n",
    "            return True\n",
    "        return False\n",
    "    def is_doc_part(self, entity_id):\n",
    "        parts = self.get_entity_parts(entity_id)\n",
    "        if len(parts) == 2:\n",
    "            return True\n",
    "        return False\n",
    "    def is_doc_part_chunk(self, entity_id):\n",
    "        parts = self.get_entity_parts(entity_id)\n",
    "        if len(parts) == 3:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_doc_vector(entity_id):\n",
    "    if entity_id in doc2vec_model.docvecs:\n",
    "        if DOC2VEC_MMAP:\n",
    "            normal_array = []\n",
    "            normal_array[:] = doc2vec_model.docvecs[entity_id][:]\n",
    "            return normal_array\n",
    "        else:\n",
    "            return doc2vec_model.docvecs[entity_id]\n",
    "    else:\n",
    "        # some claims have low token count, so they cant fill out the whole 16 spots\n",
    "        return ZERO_VECTOR\n",
    "\n",
    "def data_generator(doc_stats, doc_id):\n",
    "    yield get_doc_vector(doc_id)\n",
    "    if PARTS_LEVEL >= LEVEL_DIVISIONS:\n",
    "        for part_id in doc_stats.doc_parts[doc_id]:\n",
    "            yield get_doc_vector(part_id)\n",
    "    if PARTS_LEVEL >= LEVEL_CHUNKS:\n",
    "        for part_id in doc_stats.doc_part_chunks[doc_id]:\n",
    "            yield get_doc_vector(part_id)\n",
    "    yield ZERO_VECTOR\n",
    "\n",
    "def validation_data_generator(doc_stats, validation_dict, doc_id):\n",
    "    yield validation_dict[doc_id]\n",
    "    if PARTS_LEVEL >= LEVEL_DIVISIONS:\n",
    "        for part_id in doc_stats.doc_parts[doc_id]:\n",
    "            yield validation_dict[part_id]\n",
    "    if PARTS_LEVEL >= LEVEL_CHUNKS:\n",
    "        for part_id in doc_stats.doc_part_chunks[doc_id]:\n",
    "            yield get_doc_vector(part_id)\n",
    "    yield ZERO_VECTOR\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_training_data(doc2vec_model, classifications, classifications_type, doc_stats, sequence_size, embedding_size):\n",
    "    \"\"\"\n",
    "    Creates or loads the X and y matrices used for training\n",
    "    \"\"\"\n",
    "    def get_training_y_labels():\n",
    "        \"\"\"\n",
    "        Creates or loads the y matrix used for training\n",
    "        \"\"\"\n",
    "        if not os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                       TRAINING_LABELS_MATRIX.format(classifications_type))):\n",
    "            info(\"Creating Training Labels\")\n",
    "            one_hot_encoder = OneHotEncoder(classifications)\n",
    "            classifications_set = set(classifications)\n",
    "            training_labels_mat = np.zeros((len(training_docs_list), len(classifications)), dtype=np.int8)\n",
    "            for i, doc_id in enumerate(training_docs_list):\n",
    "                eligible_classifications = set(doc_classification_map[doc_id]) & classifications_set\n",
    "                training_labels_mat[i][:] = one_hot_encoder.get_label_vector(eligible_classifications)\n",
    "        else:    \n",
    "            training_labels_mat = np.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                            TRAINING_LABELS_MATRIX.format(classifications_type))))\n",
    "        return training_labels_mat\n",
    "\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                       TRAINING_DATA_MATRIX.format(PARTS_LEVEL))):\n",
    "        info(\"Creating Training Data\")\n",
    "        one_hot_encoder = OneHotEncoder(classifications)\n",
    "        classifications_set = set(classifications)\n",
    "        # 1st level: document level\n",
    "        training_data = np.ndarray((len(training_docs_list), sequence_size, embedding_size), dtype=np.float32)\n",
    "        info(\"Training Data shape: {}\".format(training_data.shape))\n",
    "        training_labels_mat = np.zeros((len(training_docs_list), len(classifications)), dtype=np.int8)\n",
    "        for i, doc_id in enumerate(training_docs_list):\n",
    "            data_gen = data_generator(doc_stats, doc_id)\n",
    "            # 2nd level: constituents\n",
    "            for j in range(sequence_size):\n",
    "                #3rd level: feature vectors\n",
    "                training_data[i][j] = data_gen.next()\n",
    "            eligible_classifications = set(doc_classification_map[doc_id]) & classifications_set\n",
    "            training_labels_mat[i][:] = one_hot_encoder.get_label_vector(eligible_classifications)\n",
    "            if i % 10000 == 0:\n",
    "                info(\"Finished {} in training\".format(i))\n",
    "        \n",
    "        info(\"Saving Training Data to file...\")\n",
    "        np.save(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                  TRAINING_DATA_MATRIX.format(PARTS_LEVEL)), \"w\"), training_data)\n",
    "        np.save(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                  TRAINING_LABELS_MATRIX.format(classifications_type)), \"w\"), training_labels_mat)\n",
    "    else:\n",
    "        info(\"Loading Training Data from file\")\n",
    "        training_data = np.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                  TRAINING_DATA_MATRIX.format(PARTS_LEVEL))))\n",
    "        training_labels_mat = get_training_y_labels()\n",
    "        \n",
    "    return training_data, training_labels_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_data(validation_dict, classifications, classifications_type, doc_stats, sequence_size, embedding_size):\n",
    "    \"\"\"\n",
    "    Creates or loads the X and y matrices used for validation\n",
    "    \"\"\"\n",
    "    def get_validation_y_labels():\n",
    "        \"\"\"\n",
    "        Creates or loads the y matrix used for validation\n",
    "        \"\"\"\n",
    "        if not os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                       VALIDATION_LABELS_MATRIX.format(classifications_type))):\n",
    "            info(\"Creating Validation Labels\")\n",
    "            one_hot_encoder = OneHotEncoder(classifications)\n",
    "            classifications_set = set(classifications)\n",
    "            validation_labels_mat = np.zeros((len(validation_docs_list), len(classifications)), dtype=np.int8)\n",
    "            for i, doc_id in enumerate(validation_docs_list):\n",
    "                eligible_classifications = set(doc_classification_map[doc_id]) & classifications_set\n",
    "                validation_labels_mat[i][:] = one_hot_encoder.get_label_vector(eligible_classifications)\n",
    "        else:    \n",
    "            info(\"Loading Validation Labels\")\n",
    "            validation_labels_mat = np.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                        VALIDATION_LABELS_MATRIX.format(classifications_type))))\n",
    "        return validation_labels_mat\n",
    "\n",
    "    \n",
    "    if not os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                       VALIDATION_DATA_MATRIX.format(PARTS_LEVEL))):\n",
    "        info(\"Creating Validation Data\")\n",
    "        one_hot_encoder = OneHotEncoder(classifications)\n",
    "        classifications_set = set(classifications)\n",
    "        # 1st level: document level\n",
    "        validation_data = np.ndarray((len(validation_docs_list), sequence_size, embedding_size), dtype=np.float32)\n",
    "        info(\"Validation Data shape: {}\".format(validation_data.shape))\n",
    "        validation_labels_mat = np.zeros((len(validation_docs_list), len(classifications)), dtype=np.int8)\n",
    "        for i, doc_id in enumerate(validation_docs_list):\n",
    "            data_gen = validation_data_generator(doc_stats, validation_dict, doc_id)\n",
    "            # 2nd level: constituents\n",
    "            for j in range(sequence_size):\n",
    "                #3d level: feature vectors\n",
    "                validation_data[i][j] = data_gen.next()\n",
    "            eligible_classifications = set(doc_classification_map[doc_id]) & classifications_set\n",
    "            validation_labels_mat[i][:] = one_hot_encoder.get_label_vector(eligible_classifications)\n",
    "            if i % 10000 == 0:\n",
    "                info(\"Finished {} in validation\".format(i))\n",
    "        \n",
    "        info(\"Saving Validation Data to file...\")\n",
    "        np.save(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                  VALIDATION_DATA_MATRIX.format(PARTS_LEVEL)), \"w\"), validation_data)\n",
    "        np.save(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                  VALIDATION_LABELS_MATRIX.format(classifications_type)), \"w\"), validation_labels_mat)\n",
    "    else:\n",
    "        info(\"Loading Validation Data from file\")\n",
    "        validation_data = np.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                  VALIDATION_DATA_MATRIX.format(PARTS_LEVEL))))\n",
    "        validation_labels_mat = get_validation_y_labels()\n",
    "        \n",
    "    return validation_data, validation_labels_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set what we want to train for (classification type and level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifications = valid_classes\n",
    "classifications_type = 'classes'\n",
    "classifier_file = TYPE_CLASSIFIER.format(classifications_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is where we set which level we want to train for\n",
    "0 -> Use only the document vector  \n",
    "1 -> Use the document vector and the vectors for abstract, description, claims  \n",
    "2 -> Use the document vector and the vectors for abstract, description, claims plus the chunk vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEVEL_DOC = 0\n",
    "LEVEL_DIVISIONS = 1\n",
    "LEVEL_CHUNKS = 2\n",
    "\n",
    "PARTS_LEVEL = LEVEL_CHUNKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOC2VEC_SIZE = 200\n",
    "DOC2VEC_WINDOW = 2\n",
    "DOC2VEC_MAX_VOCAB_SIZE = None\n",
    "DOC2VEC_SAMPLE = 1e-3\n",
    "DOC2VEC_TYPE = 1\n",
    "DOC2VEC_HIERARCHICAL_SAMPLE = 0\n",
    "DOC2VEC_NEGATIVE_SAMPLE_SIZE = 10\n",
    "DOC2VEC_CONCAT = 0\n",
    "DOC2VEC_MEAN = 1\n",
    "DOC2VEC_TRAIN_WORDS = 0\n",
    "DOC2VEC_EPOCHS = 1 # we do our training manually one epoch at a time\n",
    "DOC2VEC_MAX_EPOCHS = 8\n",
    "REPORT_DELAY = 20 # report the progress every x seconds\n",
    "REPORT_VOCAB_PROGRESS = 100000 # report vocab progress every x documents\n",
    "\n",
    "DOC2VEC_MMAP = 'r'\n",
    "# DOC2VEC_MMAP = None\n",
    "\n",
    "ZERO_VECTOR = [0] * DOC2VEC_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8\n"
     ]
    }
   ],
   "source": [
    "placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}'.format(DOC2VEC_SIZE, \n",
    "                                                                DOC2VEC_WINDOW, \n",
    "                                                                'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                                DOC2VEC_TRAIN_WORDS,\n",
    "                                                                DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                                str(DOC2VEC_MAX_VOCAB_SIZE))\n",
    "GLOBAL_VARS.DOC2VEC_MODEL_NAME = placeholder_model_name\n",
    "placeholder_model_name = os.path.join(placeholder_model_name, \"epoch_{}\")\n",
    "\n",
    "epoch = 8\n",
    "\n",
    "GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "doc2vec_model = None\n",
    "print GLOBAL_VARS.MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 08:38:56,134 : INFO : loading Doc2Vec object from /mnt/data/shalaby/parameter_search_doc2vec_models_extended_abs_desc_claims_all_levels_3/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/shalaby/parameter_search_doc2vec_models_extended_abs_desc_claims_all_levels_3/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 08:39:11,270 : INFO : loading docvecs recursively from /mnt/data/shalaby/parameter_search_doc2vec_models_extended_abs_desc_claims_all_levels_3/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.docvecs.* with mmap=r\n",
      "2017-03-21 08:39:11,272 : INFO : loading doctag_syn0 from /mnt/data/shalaby/parameter_search_doc2vec_models_extended_abs_desc_claims_all_levels_3/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.docvecs.doctag_syn0.npy with mmap=r\n",
      "2017-03-21 08:39:11,286 : INFO : loading syn1neg from /mnt/data/shalaby/parameter_search_doc2vec_models_extended_abs_desc_claims_all_levels_3/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.syn1neg.npy with mmap=r\n",
      "2017-03-21 08:39:11,287 : INFO : loading syn0 from /mnt/data/shalaby/parameter_search_doc2vec_models_extended_abs_desc_claims_all_levels_3/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.syn0.npy with mmap=r\n",
      "2017-03-21 08:39:11,289 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-03-21 08:39:11,290 : INFO : setting ignored attribute cum_table to None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 s, sys: 1.72 s, total: 14.4 s\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)\n",
    "if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)):\n",
    "    doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX), mmap=DOC2VEC_MMAP)\n",
    "    doc2vec_model.workers = NUM_CORES\n",
    "    GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "else:\n",
    "    info(\"Couldnt find the doc2vec model with epoch {}\".format(epoch))\n",
    "    raise Exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_doc_stats_file = os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \"extended_pv_training_doc_stats.pkl\")\n",
    "validation_doc_stats_file = os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \"extended_pv_validation_doc_stats.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Data to use for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create/Load Training Document Stats \n",
    "these contain references to the ids of the parts of each document \n",
    "\n",
    "(ex. 059884 -> [\"059884_abstract\", \"059884_abstract\", \"059884_abstract\", \"059884_abstract_part-1\",...]) \n",
    "\n",
    "so we know what to load when constructing the training and validation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 08:41:53,923 : INFO : Loading Training Document Stats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.37 s, sys: 880 ms, total: 10.3 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.exists(training_doc_stats_file):\n",
    "    info(\"Creating Training Document Stats\")\n",
    "    doc_stats = FixedDocumentsStatsGenerator(training_preprocessed_files_prefix)\n",
    "    doc_stats.get_stats()\n",
    "    pickle.dump(doc_stats, open(training_doc_stats_file, \"w\"))\n",
    "else:\n",
    "    info(\"Loading Training Document Stats\")\n",
    "    doc_stats = pickle.load(open(training_doc_stats_file, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Size: 34\n"
     ]
    }
   ],
   "source": [
    "MAX_SIZE = 1 # for document vector\n",
    "if PARTS_LEVEL >= LEVEL_DIVISIONS:\n",
    "    MAX_PARTS = int(np.max([len(doc_stats.doc_parts[d]) for d in doc_stats.docids]))\n",
    "    MAX_SIZE += MAX_PARTS\n",
    "\n",
    "if PARTS_LEVEL >= LEVEL_CHUNKS:\n",
    "    MAX_PART_CHUNKS = int(np.max([len(doc_stats.doc_part_chunks[d]) for d in doc_stats.docids]))\n",
    "    MAX_SIZE += MAX_PART_CHUNKS\n",
    "    \n",
    "print \"Max Size: {}\".format(MAX_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Training Data Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 08:42:04,309 : INFO : Creating Training Data\n",
      "2017-03-21 08:42:04,311 : INFO : Training Data shape: (120156, 34, 200)\n",
      "2017-03-21 08:42:04,331 : INFO : Finished 0 in training\n",
      "2017-03-21 08:42:19,429 : INFO : Finished 10000 in training\n",
      "2017-03-21 08:42:34,349 : INFO : Finished 20000 in training\n",
      "2017-03-21 08:42:49,231 : INFO : Finished 30000 in training\n",
      "2017-03-21 08:43:04,074 : INFO : Finished 40000 in training\n",
      "2017-03-21 08:43:18,887 : INFO : Finished 50000 in training\n",
      "2017-03-21 08:43:34,022 : INFO : Finished 60000 in training\n",
      "2017-03-21 08:43:49,037 : INFO : Finished 70000 in training\n",
      "2017-03-21 08:44:04,075 : INFO : Finished 80000 in training\n",
      "2017-03-21 08:44:19,101 : INFO : Finished 90000 in training\n",
      "2017-03-21 08:44:34,227 : INFO : Finished 100000 in training\n",
      "2017-03-21 08:44:49,349 : INFO : Finished 110000 in training\n",
      "2017-03-21 08:45:12,145 : INFO : Finished 120000 in training\n",
      "2017-03-21 08:45:12,456 : INFO : Saving Training Data to file...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 51s, sys: 17.7 s, total: 3min 9s\n",
      "Wall time: 3min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X, y = get_training_data(doc2vec_model, classifications, classifications_type, doc_stats, MAX_SIZE, DOC2VEC_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3268243328\n",
      "(120156, 34, 200)\n",
      "(120156, 244)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print sys.getsizeof(X)\n",
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create/Load Validation Doc Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_dict = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Validation Dict. This is the dictionary that contains the precomputed doc2vec vectors for each document, document part and chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 27s, sys: 5.87 s, total: 1min 33s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "validation_dict = pickle.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_DICT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 08:54:02,147 : INFO : Loading new batch for index: 0\n",
      "2017-03-21 08:54:13,670 : INFO : Finished loading new batch of 10000 documents\n",
      "2017-03-21 08:54:13,672 : INFO : Loading new batch for index: 10000\n",
      "2017-03-21 08:54:24,641 : INFO : Finished loading new batch of 10000 documents\n",
      "2017-03-21 08:54:24,646 : INFO : Loading new batch for index: 20000\n",
      "2017-03-21 08:54:34,957 : INFO : Finished loading new batch of 9675 documents\n",
      "2017-03-21 08:54:34,959 : INFO : Loading new batch for index: 29675\n",
      "2017-03-21 08:54:34,960 : INFO : No more batches to load, exiting at index: 29675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.3 s, sys: 4.86 s, total: 17.1 s\n",
      "Wall time: 35.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.exists(validation_doc_stats_file):\n",
    "    validation_doc_stats = FixedDocumentsStatsGenerator(validation_preprocessed_files_prefix)\n",
    "    validation_doc_stats.get_stats()\n",
    "    pickle.dump(validation_doc_stats, open(validation_doc_stats_file, \"w\"))\n",
    "else:\n",
    "    info(\"Loading Validation Document Stats\")\n",
    "    validation_doc_stats = pickle.load(open(validation_doc_stats_file, \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Validation Data Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 08:54:37,574 : INFO : Creating Validation Data\n",
      "2017-03-21 08:54:37,576 : INFO : Validation Data shape: (29675, 34, 200)\n",
      "2017-03-21 08:54:37,578 : INFO : Finished 0 in validation\n",
      "2017-03-21 08:54:47,866 : INFO : Finished 10000 in validation\n",
      "2017-03-21 08:54:57,755 : INFO : Finished 20000 in validation\n",
      "2017-03-21 08:55:07,112 : INFO : Saving Validation Data to file...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.46 s, sys: 24.6 s, total: 30.1 s\n",
      "Wall time: 30.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Xv, yv = get_validation_data(validation_dict, classifications, classifications_type, validation_doc_stats, \n",
    "                             MAX_SIZE, DOC2VEC_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del validation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29675, 34, 200)\n",
      "(29675, 244)\n"
     ]
    }
   ],
   "source": [
    "print Xv.shape\n",
    "print yv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Parameters and training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_keras_rnn_model(input_size, output_size, lstm_output_size, w_dropout_do, u_dropout_do, \n",
    "                           stack_layers=1, conv_size=None):\n",
    "    \n",
    "    model= Sequential()\n",
    "#     model.add(Masking(mask_value=0., input_shape=(MAX_SIZE, input_size)))\n",
    "    if conv_size:\n",
    "        model.add(Convolution1D(nb_filter=conv_size, input_shape=(MAX_SIZE, input_size), filter_length=3, \n",
    "                                border_mode='same', activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_length=2))\n",
    "    for i in range(stack_layers):\n",
    "        model.add(LSTM(lstm_output_size, input_dim=input_size, dropout_W=w_dropout_do, dropout_U=u_dropout_do,\n",
    "                       return_sequences=False if i+1 == stack_layers else True,\n",
    "                  name='lstm_{}_u-drop_{}_w-drop_{}_layer_{}'.format(lstm_output_size, str(u_dropout_do), str(w_dropout_do), str(i+1))))\n",
    "    model.add(Dense(output_size, activation='sigmoid', name='sigmoid_output'))\n",
    "    model.compile(optimizer=NN_OPTIMIZER, loss='binary_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# minimum change in val_loss from previous epoch to register as a decrease\n",
    "early_stopper_deltas = {\n",
    "    'sections': 0.0001,\n",
    "    'classes': 0.00001,\n",
    "    'subclasses': 0.00001\n",
    "}\n",
    "# how many epochs to wait when there is no decrease in val_loss before early stopping\n",
    "early_stopper_patience = {\n",
    "    'sections': 10,\n",
    "    'classes': 10,\n",
    "    'subclasses': 10\n",
    "}\n",
    "# number of epochs after which we do periodic evaluation of validation metrics\n",
    "epochs_before_validation = {\n",
    "    'sections': 20,\n",
    "    'classes': 20,\n",
    "    'subclasses': 20\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN_OUTPUT_NEURONS = len(classifications)\n",
    "\n",
    "EARLY_STOPPER_MIN_DELTA = early_stopper_deltas[classifications_type]\n",
    "EARLY_STOPPER_PATIENCE = early_stopper_patience[classifications_type]\n",
    "\n",
    "NN_MAX_EPOCHS = 200\n",
    "NN_RANDOM_SEARCH_BUDGET = 20\n",
    "NN_PARAM_SAMPLE_SEED = 1234\n",
    "\n",
    "NN_BATCH_SIZE = 2048\n",
    "\n",
    "MODEL_VERBOSITY = 1\n",
    "\n",
    "NN_OPTIMIZER = 'rmsprop'\n",
    "# NN_OPTIMIZER = 'adam'\n",
    "\n",
    "to_skip = []\n",
    "\n",
    "load_existing_results = True\n",
    "save_results = True\n",
    "\n",
    "# parameters to use when doing random hyperparameter search\n",
    "lstm_output_sizes = [200,500]\n",
    "w_dropout_options = [0.4,0.6]\n",
    "u_dropout_options = [0.4,0.6]\n",
    "stack_layers_options = [1,2,3]\n",
    "conv_size_options = [None]\n",
    "\n",
    "\n",
    "# # Uncomment for Specific Configuration\n",
    "# NN_RANDOM_SEARCH_BUDGET = 1\n",
    "# lstm_output_sizes = [500]\n",
    "# w_dropout_options = [0.6]\n",
    "# u_dropout_options = [0.4]\n",
    "# stack_layers_options = [1]\n",
    "# conv_size_options = [None]\n",
    "# conv_size_options = [None, 32,100,200,300]\n",
    "\n",
    "np.random.seed(NN_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MetricsCallback(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback called by keras after each epoch. Records the best validation loss and periodically checks the \n",
    "    validation metrics\n",
    "    \"\"\"\n",
    "    EPOCHS_BEFORE_VALIDATION = epochs_before_validation[classifications_type]\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.epoch_index = 0\n",
    "        self.val_loss_reductions = 0\n",
    "        self.metrics_dict = {}\n",
    "        self.best_val_loss = np.iinfo(np.int32).max\n",
    "        self.best_weights = None\n",
    "        self.best_validation_metrics = None\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epoch_index += 1\n",
    "        if logs['val_loss'] < self.best_val_loss:\n",
    "            self.val_loss_reductions += 1\n",
    "            self.best_val_loss = logs['val_loss']\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            #print '\\r    \\r' # to remove the previous line of verbose output of model fit\n",
    "            #time.sleep(0.1)\n",
    "            info('Found lower val loss for epoch {} => {}'.format(self.epoch_index, round(logs['val_loss'], 5)))\n",
    "            if self.val_loss_reductions % MetricsCallback.EPOCHS_BEFORE_VALIDATION == 0:\n",
    "                \n",
    "                info('Validation Loss Reduced {} times'.format(self.val_loss_reductions))\n",
    "                info('Evaluating on Validation Data')\n",
    "                yvp = self.model.predict(Xv)\n",
    "                yvp_binary = get_binary_0_5(yvp)\n",
    "                info('Generating Validation Metrics')\n",
    "                validation_metrics = get_metrics(yv, yvp, yvp_binary)\n",
    "                print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "                    validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "                    validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "                self.metrics_dict[self.epoch_index] = validation_metrics\n",
    "#                 self.best_validation_metrics = validation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:11:22,452 : INFO : No Previous results exist in /mnt/data2/shalaby/nn_parameter_search_extended_abs_desc_claims_all_levels_3/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/lstm_classes_level_2_batch_2048_nn_parameter_searches.pkl\n",
      "2017-03-21 11:11:22,455 : INFO : ***************************************************************************************\n",
      "2017-03-21 11:11:22,456 : INFO : lstm_optimizer_rmsprop_size_200_w-drop_0.6_u-drop_0.6_stack_2_conv_None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_200_u-drop_0.6_w-drop_0.6_l (None, None, 200)     320800      lstm_input_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "lstm_200_u-drop_0.6_w-drop_0.6_l (None, 200)           320800      lstm_200_u-drop_0.6_w-drop_0.6_la\n",
      "____________________________________________________________________________________________________\n",
      "sigmoid_output (Dense)           (None, 244)           49044       lstm_200_u-drop_0.6_w-drop_0.6_la\n",
      "====================================================================================================\n",
      "Total params: 690644\n",
      "____________________________________________________________________________________________________\n",
      "Train on 120156 samples, validate on 29675 samples\n",
      "Epoch 1/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0974"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:12:51,553 : INFO : Found lower val loss for epoch 1 => 0.02288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 56s - loss: 0.0966 - val_loss: 0.0229\n",
      "Epoch 2/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0263"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:13:25,834 : INFO : Found lower val loss for epoch 2 => 0.02247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 34s - loss: 0.0263 - val_loss: 0.0225\n",
      "Epoch 3/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0261"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:14:00,133 : INFO : Found lower val loss for epoch 3 => 0.02239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 34s - loss: 0.0261 - val_loss: 0.0224\n",
      "Epoch 4/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0261"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:14:35,684 : INFO : Found lower val loss for epoch 4 => 0.02238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 35s - loss: 0.0261 - val_loss: 0.0224\n",
      "Epoch 5/200\n",
      "120156/120156 [==============================] - 36s - loss: 0.0260 - val_loss: 0.0224\n",
      "Epoch 6/200\n",
      "120156/120156 [==============================] - 37s - loss: 0.0261 - val_loss: 0.0224\n",
      "Epoch 7/200\n",
      "120156/120156 [==============================] - 40s - loss: 0.0261 - val_loss: 0.0224\n",
      "Epoch 8/200\n",
      "120156/120156 [==============================] - 41s - loss: 0.0260 - val_loss: 0.0224\n",
      "Epoch 9/200\n",
      "120156/120156 [==============================] - 39s - loss: 0.0260 - val_loss: 0.0224\n",
      "Epoch 10/200\n",
      "120156/120156 [==============================] - 40s - loss: 0.0260 - val_loss: 0.0225\n",
      "Epoch 11/200\n",
      "120156/120156 [==============================] - 39s - loss: 0.0260 - val_loss: 0.0224\n",
      "Epoch 12/200\n",
      "120156/120156 [==============================] - 120s - loss: 0.0260 - val_loss: 0.0224\n",
      "Epoch 13/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0259"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:21:47,134 : INFO : Found lower val loss for epoch 13 => 0.02229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 36s - loss: 0.0259 - val_loss: 0.0223\n",
      "Epoch 14/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0259"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:22:27,782 : INFO : Found lower val loss for epoch 14 => 0.02215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 40s - loss: 0.0259 - val_loss: 0.0221\n",
      "Epoch 15/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0258"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:23:07,460 : INFO : Found lower val loss for epoch 15 => 0.02201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 39s - loss: 0.0258 - val_loss: 0.0220\n",
      "Epoch 16/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0257"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:23:47,811 : INFO : Found lower val loss for epoch 16 => 0.02186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 40s - loss: 0.0257 - val_loss: 0.0219\n",
      "Epoch 17/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0256"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:25:42,153 : INFO : Found lower val loss for epoch 17 => 0.02171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 114s - loss: 0.0256 - val_loss: 0.0217\n",
      "Epoch 18/200\n",
      "120156/120156 [==============================] - 117s - loss: 0.0255 - val_loss: 0.0217\n",
      "Epoch 19/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0255"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:29:38,566 : INFO : Found lower val loss for epoch 19 => 0.02146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 119s - loss: 0.0255 - val_loss: 0.0215\n",
      "Epoch 20/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0253"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:31:37,282 : INFO : Found lower val loss for epoch 20 => 0.02123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 118s - loss: 0.0253 - val_loss: 0.0212\n",
      "Epoch 21/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0252"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:33:36,077 : INFO : Found lower val loss for epoch 21 => 0.02107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 118s - loss: 0.0252 - val_loss: 0.0211\n",
      "Epoch 22/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:35:33,865 : INFO : Found lower val loss for epoch 22 => 0.02091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 117s - loss: 0.0250 - val_loss: 0.0209\n",
      "Epoch 23/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0248"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:36:42,242 : INFO : Found lower val loss for epoch 23 => 0.02068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 68s - loss: 0.0248 - val_loss: 0.0207\n",
      "Epoch 24/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0246"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:37:20,407 : INFO : Found lower val loss for epoch 24 => 0.02053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 38s - loss: 0.0246 - val_loss: 0.0205\n",
      "Epoch 25/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0244"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:37:59,251 : INFO : Found lower val loss for epoch 25 => 0.0203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 38s - loss: 0.0244 - val_loss: 0.0203\n",
      "Epoch 26/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0242"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:38:39,587 : INFO : Found lower val loss for epoch 26 => 0.02003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 40s - loss: 0.0242 - val_loss: 0.0200\n",
      "Epoch 27/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0239"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:39:37,385 : INFO : Found lower val loss for epoch 27 => 0.01996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 57s - loss: 0.0239 - val_loss: 0.0200\n",
      "Epoch 28/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0237"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:40:49,727 : INFO : Found lower val loss for epoch 28 => 0.01987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 72s - loss: 0.0237 - val_loss: 0.0199\n",
      "Epoch 29/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0235"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:41:29,828 : INFO : Found lower val loss for epoch 29 => 0.01953\n",
      "2017-03-21 11:41:29,830 : INFO : Validation Loss Reduced 20 times\n",
      "2017-03-21 11:41:29,831 : INFO : Evaluating on Validation Data\n",
      "2017-03-21 11:41:57,878 : INFO : Generating Validation Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Validation Metrics: Cov Err: 13.555 | Top 3: 0.465 | Top 5: 0.568 | F1 Micro: 0.058 | F1 Macro: 0.002\n",
      "120156/120156 [==============================] - 71s - loss: 0.0235 - val_loss: 0.0195\n",
      "Epoch 30/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0232"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:42:37,689 : INFO : Found lower val loss for epoch 30 => 0.0194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 36s - loss: 0.0232 - val_loss: 0.0194\n",
      "Epoch 31/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0230"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:43:17,643 : INFO : Found lower val loss for epoch 31 => 0.01932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 39s - loss: 0.0230 - val_loss: 0.0193\n",
      "Epoch 32/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0228"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:44:13,392 : INFO : Found lower val loss for epoch 32 => 0.01892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 55s - loss: 0.0228 - val_loss: 0.0189\n",
      "Epoch 33/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0225"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:46:13,865 : INFO : Found lower val loss for epoch 33 => 0.01878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 120s - loss: 0.0226 - val_loss: 0.0188\n",
      "Epoch 34/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0223"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:48:12,376 : INFO : Found lower val loss for epoch 34 => 0.0186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 118s - loss: 0.0223 - val_loss: 0.0186\n",
      "Epoch 35/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0220"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:50:12,017 : INFO : Found lower val loss for epoch 35 => 0.01859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 119s - loss: 0.0220 - val_loss: 0.0186\n",
      "Epoch 36/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0218"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:52:14,654 : INFO : Found lower val loss for epoch 36 => 0.01826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 122s - loss: 0.0218 - val_loss: 0.0183\n",
      "Epoch 37/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0215"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:54:15,953 : INFO : Found lower val loss for epoch 37 => 0.01819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 121s - loss: 0.0215 - val_loss: 0.0182\n",
      "Epoch 38/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0212"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 11:56:16,422 : INFO : Found lower val loss for epoch 38 => 0.01813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 120s - loss: 0.0213 - val_loss: 0.0181\n",
      "Epoch 39/200\n",
      "120156/120156 [==============================] - 121s - loss: 0.0211 - val_loss: 0.0182\n",
      "Epoch 40/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0208"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 12:00:19,445 : INFO : Found lower val loss for epoch 40 => 0.018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 121s - loss: 0.0208 - val_loss: 0.0180\n",
      "Epoch 41/200\n",
      "120156/120156 [==============================] - 117s - loss: 0.0206 - val_loss: 0.0181\n",
      "Epoch 42/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0203"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 12:04:18,572 : INFO : Found lower val loss for epoch 42 => 0.01771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 121s - loss: 0.0203 - val_loss: 0.0177\n",
      "Epoch 43/200\n",
      "120156/120156 [==============================] - 118s - loss: 0.0202 - val_loss: 0.0179\n",
      "Epoch 44/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0199"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 12:08:48,403 : INFO : Found lower val loss for epoch 44 => 0.01727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 151s - loss: 0.0199 - val_loss: 0.0173\n",
      "Epoch 45/200\n",
      "120156/120156 [==============================] - 156s - loss: 0.0198 - val_loss: 0.0176\n",
      "Epoch 46/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0196"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 12:14:01,409 : INFO : Found lower val loss for epoch 46 => 0.01664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 156s - loss: 0.0196 - val_loss: 0.0166\n",
      "Epoch 47/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0194"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 12:16:39,468 : INFO : Found lower val loss for epoch 47 => 0.01657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 158s - loss: 0.0194 - val_loss: 0.0166\n",
      "Epoch 48/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0193"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 12:19:14,847 : INFO : Found lower val loss for epoch 48 => 0.01624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 155s - loss: 0.0193 - val_loss: 0.0162\n",
      "Epoch 49/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0192"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 12:21:29,626 : INFO : Found lower val loss for epoch 49 => 0.01619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 134s - loss: 0.0192 - val_loss: 0.0162\n",
      "Epoch 50/200\n",
      "120156/120156 [==============================] - 122s - loss: 0.0190 - val_loss: 0.0164\n",
      "Epoch 51/200\n",
      "120156/120156 [==============================] - 118s - loss: 0.0189 - val_loss: 0.0162\n",
      "Epoch 52/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 12:27:50,697 : INFO : Found lower val loss for epoch 52 => 0.01606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 139s - loss: 0.0188 - val_loss: 0.0161\n",
      "Epoch 53/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0187"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 12:30:28,548 : INFO : Found lower val loss for epoch 53 => 0.01604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 157s - loss: 0.0186 - val_loss: 0.0160\n",
      "Epoch 54/200\n",
      "120156/120156 [==============================] - 155s - loss: 0.0185 - val_loss: 0.0161\n",
      "Epoch 55/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0184"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 12:35:42,824 : INFO : Found lower val loss for epoch 55 => 0.01574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 158s - loss: 0.0184 - val_loss: 0.0157\n",
      "Epoch 56/200\n",
      "120156/120156 [==============================] - 157s - loss: 0.0183 - val_loss: 0.0158\n",
      "Epoch 57/200\n",
      "120156/120156 [==============================] - 157s - loss: 0.0182 - val_loss: 0.0158\n",
      "Epoch 58/200\n",
      "120156/120156 [==============================] - 157s - loss: 0.0181 - val_loss: 0.0157\n",
      "Epoch 59/200\n",
      "120156/120156 [==============================] - 161s - loss: 0.0180 - val_loss: 0.0159\n",
      "Epoch 60/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0180"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 12:48:55,695 : INFO : Found lower val loss for epoch 60 => 0.01562\n",
      "2017-03-21 12:48:55,696 : INFO : Validation Loss Reduced 40 times\n",
      "2017-03-21 12:48:55,697 : INFO : Evaluating on Validation Data\n",
      "2017-03-21 12:51:06,042 : INFO : Generating Validation Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Validation Metrics: Cov Err: 7.995 | Top 3: 0.639 | Top 5: 0.730 | F1 Micro: 0.356 | F1 Macro: 0.018\n",
      "120156/120156 [==============================] - 300s - loss: 0.0180 - val_loss: 0.0156\n",
      "Epoch 61/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0179"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 12:53:55,247 : INFO : Found lower val loss for epoch 61 => 0.0154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 157s - loss: 0.0179 - val_loss: 0.0154\n",
      "Epoch 62/200\n",
      "120156/120156 [==============================] - 144s - loss: 0.0178 - val_loss: 0.0155\n",
      "Epoch 63/200\n",
      "120156/120156 [==============================] - 125s - loss: 0.0177 - val_loss: 0.0156\n",
      "Epoch 64/200\n",
      "120156/120156 [==============================] - 119s - loss: 0.0176 - val_loss: 0.0157\n",
      "Epoch 65/200\n",
      "120156/120156 [==============================] - 119s - loss: 0.0176 - val_loss: 0.0156\n",
      "Epoch 66/200\n",
      "120156/120156 [==============================] - 121s - loss: 0.0175 - val_loss: 0.0157\n",
      "Epoch 67/200\n",
      "120156/120156 [==============================] - 119s - loss: 0.0175 - val_loss: 0.0155\n",
      "Epoch 68/200\n",
      "120156/120156 [==============================] - 121s - loss: 0.0174 - val_loss: 0.0155\n",
      "Epoch 69/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0173"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 13:10:24,698 : INFO : Found lower val loss for epoch 69 => 0.01523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 119s - loss: 0.0173 - val_loss: 0.0152\n",
      "Epoch 70/200\n",
      "118784/120156 [============================>.] - ETA: 1s - loss: 0.0173"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 13:12:10,970 : INFO : Found lower val loss for epoch 70 => 0.01521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 106s - loss: 0.0173 - val_loss: 0.0152\n",
      "Epoch 71/200\n",
      "120156/120156 [==============================] - 36s - loss: 0.0172 - val_loss: 0.0154\n",
      "Epoch 72/200\n",
      "120156/120156 [==============================] - 37s - loss: 0.0171 - val_loss: 0.0152\n",
      "Epoch 73/200\n",
      "120156/120156 [==============================] - 34s - loss: 0.0171 - val_loss: 0.0153\n",
      "Epoch 74/200\n",
      "120156/120156 [==============================] - 35s - loss: 0.0171 - val_loss: 0.0153\n",
      "Epoch 75/200\n",
      "120156/120156 [==============================] - 35s - loss: 0.0170 - val_loss: 0.0155\n",
      "Epoch 76/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0170"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 13:15:46,496 : INFO : Found lower val loss for epoch 76 => 0.01508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 35s - loss: 0.0170 - val_loss: 0.0151\n",
      "Epoch 77/200\n",
      "120156/120156 [==============================] - 35s - loss: 0.0169 - val_loss: 0.0153\n",
      "Epoch 78/200\n",
      "120156/120156 [==============================] - 35s - loss: 0.0169 - val_loss: 0.0157\n",
      "Epoch 79/200\n",
      "120156/120156 [==============================] - 39s - loss: 0.0168 - val_loss: 0.0151\n",
      "Epoch 80/200\n",
      "120156/120156 [==============================] - 40s - loss: 0.0168 - val_loss: 0.0151\n",
      "Epoch 81/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0168"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 13:18:55,917 : INFO : Found lower val loss for epoch 81 => 0.0149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 38s - loss: 0.0167 - val_loss: 0.0149\n",
      "Epoch 82/200\n",
      "120156/120156 [==============================] - 40s - loss: 0.0167 - val_loss: 0.0151\n",
      "Epoch 83/200\n",
      "120156/120156 [==============================] - 42s - loss: 0.0167 - val_loss: 0.0153\n",
      "Epoch 84/200\n",
      "120156/120156 [==============================] - 49s - loss: 0.0166 - val_loss: 0.0152\n",
      "Epoch 85/200\n",
      "120156/120156 [==============================] - 44s - loss: 0.0166 - val_loss: 0.0155\n",
      "Epoch 86/200\n",
      "120156/120156 [==============================] - 43s - loss: 0.0165 - val_loss: 0.0149\n",
      "Epoch 87/200\n",
      "120156/120156 [==============================] - 42s - loss: 0.0165 - val_loss: 0.0152\n",
      "Epoch 88/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0165"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 13:24:02,984 : INFO : Found lower val loss for epoch 88 => 0.01482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 43s - loss: 0.0165 - val_loss: 0.0148\n",
      "Epoch 89/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0164"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 13:24:44,117 : INFO : Found lower val loss for epoch 89 => 0.01465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 41s - loss: 0.0164 - val_loss: 0.0147\n",
      "Epoch 90/200\n",
      "120156/120156 [==============================] - 42s - loss: 0.0164 - val_loss: 0.0152\n",
      "Epoch 91/200\n",
      "120156/120156 [==============================] - 41s - loss: 0.0164 - val_loss: 0.0147\n",
      "Epoch 92/200\n",
      "120156/120156 [==============================] - 40s - loss: 0.0163 - val_loss: 0.0148\n",
      "Epoch 93/200\n",
      "120156/120156 [==============================] - 44s - loss: 0.0163 - val_loss: 0.0148\n",
      "Epoch 94/200\n",
      "120156/120156 [==============================] - 43s - loss: 0.0163 - val_loss: 0.0149\n",
      "Epoch 95/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0163"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 13:28:57,219 : INFO : Found lower val loss for epoch 95 => 0.01463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 40s - loss: 0.0163 - val_loss: 0.0146\n",
      "Epoch 96/200\n",
      "120156/120156 [==============================] - 41s - loss: 0.0162 - val_loss: 0.0148\n",
      "Epoch 97/200\n",
      "120156/120156 [==============================] - 43s - loss: 0.0162 - val_loss: 0.0147\n",
      "Epoch 98/200\n",
      "120156/120156 [==============================] - 44s - loss: 0.0162 - val_loss: 0.0148\n",
      "Epoch 99/200\n",
      "120156/120156 [==============================] - 43s - loss: 0.0161 - val_loss: 0.0147\n",
      "Epoch 100/200\n",
      "120156/120156 [==============================] - 45s - loss: 0.0161 - val_loss: 0.0148\n",
      "Epoch 101/200\n",
      "120156/120156 [==============================] - 44s - loss: 0.0161 - val_loss: 0.0150\n",
      "Epoch 102/200\n",
      "120156/120156 [==============================] - 46s - loss: 0.0161 - val_loss: 0.0149\n",
      "Epoch 103/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0161"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 13:34:48,253 : INFO : Found lower val loss for epoch 103 => 0.01459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 41s - loss: 0.0161 - val_loss: 0.0146\n",
      "Epoch 104/200\n",
      "120156/120156 [==============================] - 42s - loss: 0.0160 - val_loss: 0.0148\n",
      "Epoch 105/200\n",
      "120156/120156 [==============================] - 45s - loss: 0.0160 - val_loss: 0.0150\n",
      "Epoch 106/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0160"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 13:36:59,854 : INFO : Found lower val loss for epoch 106 => 0.01456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 43s - loss: 0.0160 - val_loss: 0.0146\n",
      "Epoch 107/200\n",
      "120156/120156 [==============================] - 43s - loss: 0.0160 - val_loss: 0.0147\n",
      "Epoch 108/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0159"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 13:38:27,505 : INFO : Found lower val loss for epoch 108 => 0.01454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 44s - loss: 0.0159 - val_loss: 0.0145\n",
      "Epoch 109/200\n",
      "120156/120156 [==============================] - 43s - loss: 0.0159 - val_loss: 0.0147\n",
      "Epoch 110/200\n",
      "120156/120156 [==============================] - 44s - loss: 0.0159 - val_loss: 0.0148\n",
      "Epoch 111/200\n",
      "120156/120156 [==============================] - 44s - loss: 0.0159 - val_loss: 0.0150\n",
      "Epoch 112/200\n",
      "120156/120156 [==============================] - 44s - loss: 0.0159 - val_loss: 0.0148\n",
      "Epoch 113/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0159"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 13:42:08,588 : INFO : Found lower val loss for epoch 113 => 0.01449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 43s - loss: 0.0159 - val_loss: 0.0145\n",
      "Epoch 114/200\n",
      "120156/120156 [==============================] - 37s - loss: 0.0158 - val_loss: 0.0147\n",
      "Epoch 115/200\n",
      "120156/120156 [==============================] - 38s - loss: 0.0158 - val_loss: 0.0149\n",
      "Epoch 116/200\n",
      "120156/120156 [==============================] - 38s - loss: 0.0158 - val_loss: 0.0147\n",
      "Epoch 117/200\n",
      "118784/120156 [============================>.] - ETA: 0s - loss: 0.0158"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 13:44:38,931 : INFO : Found lower val loss for epoch 117 => 0.01448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 36s - loss: 0.0158 - val_loss: 0.0145\n",
      "Epoch 118/200\n",
      "120156/120156 [==============================] - 40s - loss: 0.0158 - val_loss: 0.0146\n",
      "Epoch 119/200\n",
      "120156/120156 [==============================] - 38s - loss: 0.0157 - val_loss: 0.0146\n",
      "Epoch 120/200\n",
      "120156/120156 [==============================] - 38s - loss: 0.0157 - val_loss: 0.0147\n",
      "Epoch 121/200\n",
      "120156/120156 [==============================] - 38s - loss: 0.0157 - val_loss: 0.0150\n",
      "Epoch 122/200\n",
      "120156/120156 [==============================] - 41s - loss: 0.0157 - val_loss: 0.0146\n",
      "Epoch 123/200\n",
      "120156/120156 [==============================] - 62s - loss: 0.0157 - val_loss: 0.0145\n",
      "Epoch 124/200\n",
      "120156/120156 [==============================] - 120s - loss: 0.0157 - val_loss: 0.0149"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 13:50:58,695 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00123: early stopping\n",
      "CPU times: user 30min 44s, sys: 2h 6min 1s, total: 2h 36min 46s\n",
      "Wall time: 2h 39min 34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 13:54:57,719 : INFO : Generating Training Metrics\n",
      "2017-03-21 13:56:06,592 : INFO : Evaluating on Validation Data using saved best weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Training Metrics: Cov Err: 5.576 | Top 3: 0.739 | Top 5: 0.819 | F1 Micro: 0.558 | F1 Macro: 0.069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 13:57:29,931 : INFO : Generating Validation Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Validation Metrics: Cov Err: 6.991 | Top 3: 0.676 | Top 5: 0.767 | F1 Micro: 0.384 | F1 Macro: 0.030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 13:57:41,847 : INFO : ***************************************************************************************\n",
      "2017-03-21 13:57:41,850 : INFO : lstm_optimizer_rmsprop_size_500_w-drop_0.4_u-drop_0.4_stack_2_conv_None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_500_u-drop_0.4_w-drop_0.4_l (None, None, 500)     1402000     lstm_input_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "lstm_500_u-drop_0.4_w-drop_0.4_l (None, 500)           2002000     lstm_500_u-drop_0.4_w-drop_0.4_la\n",
      "____________________________________________________________________________________________________\n",
      "sigmoid_output (Dense)           (None, 244)           122244      lstm_500_u-drop_0.4_w-drop_0.4_la\n",
      "====================================================================================================\n",
      "Total params: 3526244\n",
      "____________________________________________________________________________________________________\n",
      "Train on 120156 samples, validate on 29675 samples\n",
      "Epoch 1/200\n",
      "  2048/120156 [..............................] - ETA: 102s - loss: 0.7045"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Error allocating 139264000 bytes of device memory (out of memory).\nApply node that caused the error: GpuSplit{4}(GpuDimShuffle{1,0,2}.0, TensorConstant{2}, TensorConstant{(4,) of 500})\nToposort index: 701\nInputs types: [CudaNdarrayType(float32, 3D), TensorType(int8, scalar), TensorType(int64, vector)]\nInputs shapes: [(2048, 34, 2000), (), (4,)]\nInputs strides: [(2000, 4096000, 1), (), (8,)]\nInputs values: ['not shown', array(2, dtype=int8), array([500, 500, 500, 500])]\nOutputs clients: [[GpuReshape{2}(GpuSplit{4}.0, MakeVector{dtype='int64'}.0)], [GpuReshape{2}(GpuSplit{4}.1, MakeVector{dtype='int64'}.0)], [GpuReshape{2}(GpuSplit{4}.2, MakeVector{dtype='int64'}.0)], [GpuReshape{2}(GpuSplit{4}.3, MakeVector{dtype='int64'}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-0cfca66371f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# Model Fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time history = model.fit(x=X, y=y, validation_data=(Xv,yv), batch_size=NN_BATCH_SIZE,                               nb_epoch=NN_MAX_EPOCHS, verbose=MODEL_VERBOSITY,                               callbacks=[early_stopper, metrics_callback])'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Evaluating on Training Data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[1;34m(self, arg_s)\u001b[0m\n\u001b[0;32m   2161\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2162\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2165\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line)\u001b[0m\n\u001b[0;32m   2082\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2083\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2084\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2085\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2086\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    650\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 652\u001b[1;33m                               sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[0;32m   1109\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1111\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m    824\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 826\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    827\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    809\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    869\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    872\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    910\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/theano/tensor/basic.pyc\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self, node, inputs, outputs)\u001b[0m\n\u001b[0;32m   3557\u001b[0m             \u001b[0mupper_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlower_idx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msplits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3558\u001b[0m             \u001b[0mgeneral_key\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlower_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3559\u001b[1;33m             \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeneral_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3560\u001b[0m             \u001b[0mlower_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupper_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Error allocating 139264000 bytes of device memory (out of memory).\nApply node that caused the error: GpuSplit{4}(GpuDimShuffle{1,0,2}.0, TensorConstant{2}, TensorConstant{(4,) of 500})\nToposort index: 701\nInputs types: [CudaNdarrayType(float32, 3D), TensorType(int8, scalar), TensorType(int64, vector)]\nInputs shapes: [(2048, 34, 2000), (), (4,)]\nInputs strides: [(2000, 4096000, 1), (), (8,)]\nInputs values: ['not shown', array(2, dtype=int8), array([500, 500, 500, 500])]\nOutputs clients: [[GpuReshape{2}(GpuSplit{4}.0, MakeVector{dtype='int64'}.0)], [GpuReshape{2}(GpuSplit{4}.1, MakeVector{dtype='int64'}.0)], [GpuReshape{2}(GpuSplit{4}.2, MakeVector{dtype='int64'}.0)], [GpuReshape{2}(GpuSplit{4}.3, MakeVector{dtype='int64'}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "# random search for parameters\n",
    "param_sampler = ParameterSampler({\n",
    "    'lstm_output_size':lstm_output_sizes,\n",
    "    'w_dropout':w_dropout_options,\n",
    "    'u_dropout':u_dropout_options,\n",
    "    'stack_layers':stack_layers_options,\n",
    "    'conv_size':conv_size_options,\n",
    "}, n_iter=NN_RANDOM_SEARCH_BUDGET, random_state=NN_PARAM_SAMPLE_SEED)\n",
    "\n",
    "# load previous finshed results so we dont redo them\n",
    "param_results_dict = {}\n",
    "if load_existing_results:\n",
    "    param_results_path = os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                       NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, PARTS_LEVEL, NN_BATCH_SIZE)))\n",
    "    if os.path.exists(param_results_path):\n",
    "        param_results_dict = pickle.load(open(param_results_path))\n",
    "    else:\n",
    "        info('No Previous results exist in {}'.format(param_results_path))\n",
    "        \n",
    "\n",
    "# create nn parameter search directory\n",
    "if not os.path.exists(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME)):\n",
    "    os.makedirs(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME))\n",
    "\n",
    "# for every parameter set picked by random search, use it to train the model and output the metrics\n",
    "for parameters in param_sampler:\n",
    "    start_time = time.time()\n",
    "    lstm_output_size = parameters['lstm_output_size']\n",
    "    w_dropout_do = parameters['w_dropout']\n",
    "    u_dropout_do = parameters['u_dropout']\n",
    "    stack_layers = parameters['stack_layers']\n",
    "    conv_size = parameters['conv_size']\n",
    "\n",
    "    GLOBAL_VARS.NN_MODEL_NAME = 'lstm_optimizer_{}_size_{}_w-drop_{}_u-drop_{}_stack_{}_conv_{}'.format(NN_OPTIMIZER,\n",
    "        lstm_output_size,  w_dropout_do, u_dropout_do, stack_layers, str(conv_size)\n",
    "    )\n",
    "\n",
    "    if GLOBAL_VARS.NN_MODEL_NAME in param_results_dict.keys() or GLOBAL_VARS.NN_MODEL_NAME in to_skip:\n",
    "        print \"skipping: {}\".format(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "        continue\n",
    "\n",
    "    info('***************************************************************************************')\n",
    "    info(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "\n",
    "    # creating the actual keras model\n",
    "    model = create_keras_rnn_model(DOC2VEC_SIZE, NN_OUTPUT_NEURONS, \n",
    "                                  lstm_output_size, w_dropout_do, u_dropout_do, stack_layers, conv_size)\n",
    "    model.summary()\n",
    "\n",
    "    # callbacks for early stopping and for generating validation metrics\n",
    "    early_stopper = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=EARLY_STOPPER_MIN_DELTA, \\\n",
    "                                                  patience=EARLY_STOPPER_PATIENCE, verbose=1, mode='auto')\n",
    "    metrics_callback = MetricsCallback()\n",
    "\n",
    "\n",
    "    # Model Fitting\n",
    "    %time history = model.fit(x=X, y=y, validation_data=(Xv,yv), batch_size=NN_BATCH_SIZE, \\\n",
    "                              nb_epoch=NN_MAX_EPOCHS, verbose=MODEL_VERBOSITY, \\\n",
    "                              callbacks=[early_stopper, metrics_callback])\n",
    "    \n",
    "    \n",
    "    info('Evaluating on Training Data')\n",
    "    yp = model.predict(X) # get raw probability for predicted labels\n",
    "    yp_binary = get_binary_0_5(yp) # use 0.5 as threshold for setting labels to 0 or 1\n",
    "    #print yvp\n",
    "    info('Generating Training Metrics')\n",
    "    training_metrics = get_metrics(y, yp, yp_binary)\n",
    "    print \"****** Training Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    training_metrics['coverage_error'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "    training_metrics['f1_micro'], training_metrics['f1_macro'])\n",
    "    \n",
    "    \n",
    "    info('Evaluating on Validation Data using saved best weights')\n",
    "    model.set_weights(metrics_callback.best_weights)\n",
    "    yvp = model.predict(Xv) # get raw probability for predicted labels\n",
    "    yvp_binary = get_binary_0_5(yvp) # use 0.5 as threshold for setting labels to 0 or 1\n",
    "    #print yvp\n",
    "    info('Generating Validation Metrics')\n",
    "    validation_metrics = get_metrics(yv, yvp, yvp_binary)\n",
    "    print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "        validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "        validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "    best_validation_metrics = validation_metrics\n",
    "    \n",
    "    time.sleep(0.2)\n",
    "\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME] = dict()\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['best_validation_metrics'] = best_validation_metrics\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['epochs'] = len(history.history['val_loss'])\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['best_weights'] = metrics_callback.best_weights\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['best_val_loss'] = metrics_callback.best_val_loss\n",
    "#         param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['last_weights'] = last_model_weights\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['duration'] =  duration\n",
    "\n",
    "    del history, metrics_callback, model\n",
    "\n",
    "if save_results:\n",
    "    pickle.dump(param_results_dict, open(os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                                   NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, PARTS_LEVEL, NN_BATCH_SIZE))), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NameError: name 'model' is not defined\n"
     ]
    }
   ],
   "source": [
    "%xdel model\n",
    "import gc\n",
    "for i in range(3): gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lstm_optimizer_rmsprop_size_200_w-drop_0.4_u-drop_0.2',\n",
       " 'lstm_optimizer_adam_size_300_w-drop_0.2_u-drop_0.3',\n",
       " 'lstm_optimizer_adam_size_200_w-drop_0.3_u-drop_0.2',\n",
       " 'lstm_optimizer_adam_size_200_w-drop_0.2_u-drop_0.3',\n",
       " 'lstm_optimizer_adam_size_200_w-drop_0.2_u-drop_0.2',\n",
       " 'lstm_optimizer_rmsprop_size_200_w-drop_0.2_u-drop_0.4']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_results_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(param_results_dict, open(os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                                   NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE))), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # create nn parameter search directory\n",
    "    if not os.path.exists(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME)):\n",
    "        os.makedirs(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
