{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates the doc2vec vector embeddings for a specific configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "from functools import partial\n",
    "\n",
    "from thesis.utils.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables used throughout the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_SEED = 1234\n",
    "DOC2VEC_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_WORD_COUNT = 300\n",
    "NUM_CORES = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GLOBAL_VARS = namedtuple('GLOBAL_VARS', ['MODEL_NAME', 'DOC2VEC_MODEL_NAME', 'DOC2VEC_MODEL', \n",
    "                                         'SVM_MODEL_NAME', 'NN_MODEL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_MODEL = \"vocab_model\"\n",
    "MODEL_PREFIX = \"model\"\n",
    "VALIDATION_DICT = \"validation_dict.pkl\"\n",
    "TEST_MATRIX = \"test_matrix.pkl\"\n",
    "TEST_DICT = \"test_dict.pkl\"\n",
    "METRICS = \"metrics.pkl\"\n",
    "CLASSIFIER = \"classifier.pkl\"\n",
    "GZIP_EXTENSION = \".gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_location = \"/mnt/virtual-machines/data/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "doc2vec_model_save_location = os.path.join(root_location, \"parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks\", \"full\")\n",
    "if not os.path.exists(doc2vec_model_save_location):\n",
    "    os.makedirs(doc2vec_model_save_location)\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL)):\n",
    "    os.makedirs(os.path.join(doc2vec_model_save_location, VOCAB_MODEL))\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "training_docs_list_file = exports_location + \"training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"validation_docs_list.pkl\"\n",
    "test_docs_list_file = exports_location + \"test_docs_list.pkl\"\n",
    "\n",
    "preprocessed_location = root_location + \"preprocessed_data/extended_pv_abs_desc_claims_full_chunks/\"\n",
    "\n",
    "training_preprocessed_files_prefix = preprocessed_location + \"extended_pv_training_docs_data_preprocessed-\"\n",
    "validation_preprocessed_files_prefix = preprocessed_location + \"extended_pv_validation_docs_data_preprocessed-\"\n",
    "test_preprocessed_files_prefix = preprocessed_location + \"extended_pv_test_docs_data_preprocessed-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load general data required for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.3 s, sys: 1.24 s, total: 23.5 s\n",
      "Wall time: 23.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286325"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321473"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401877"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensure_disk_location_exists(location):\n",
    "    if not os.path.exists(location):\n",
    "        os.makedirs(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VALIDATION_MINI_BATCH_SIZE = 10000\n",
    "def get_extended_docs_with_inference_data_only(doc2vec_model, file_to_write, preprocessed_files_prefix):\n",
    "    \"\"\"\n",
    "    Use the trained doc2vec model to get the paragraph vector representations of the validation or test documents\n",
    "    \"\"\"\n",
    "\n",
    "    def infer_one_doc(doc_tuple):\n",
    "        # doc2vec_model.random = np.random.RandomState(DOC2VEC_SEED)\n",
    "        doc_id, doc_tokens = doc_tuple\n",
    "        rep = doc2vec_model.infer_vector(doc_tokens)\n",
    "        return (doc_id, rep)\n",
    "\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write)):\n",
    "        info(\"===== Loading inference vectors\")\n",
    "        inference_documents_reps = pickle.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write)))\n",
    "        info(\"Loaded inference vectors matrix\")\n",
    "    else:\n",
    "        inference_documents_reps = {}\n",
    "        info(\"===== Getting vectors with inference\")\n",
    "\n",
    "        # Multi-threaded inference\n",
    "#         inference_docs_iterator = ExtendedPVDocumentBatchGenerator(preprocessed_files_prefix, batch_size=None)\n",
    "        inference_docs_iterator = BatchClass(preprocessed_files_prefix, batch_size=None)\n",
    "        generator_func = inference_docs_iterator.__iter__()\n",
    "        # map consumes the whole iterator on the spot, so we have to use itertools.islice to fake mini-batching\n",
    "        mini_batch_size = VALIDATION_MINI_BATCH_SIZE\n",
    "        batches_run = 1\n",
    "        pool = ThreadPool(NUM_CORES)\n",
    "        while True:\n",
    "            threaded_reps_partial = pool.map(infer_one_doc, itertools.islice(generator_func, mini_batch_size))\n",
    "            info(\"Finished: {} tags\".format(batches_run * mini_batch_size))\n",
    "            batches_run += 1\n",
    "            if threaded_reps_partial:\n",
    "                # threaded_reps.extend(threaded_reps_partial)\n",
    "                inference_documents_reps.update(threaded_reps_partial)\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        pool.close()\n",
    "        pool.terminate()\n",
    "\n",
    "        pickle.dump(inference_documents_reps,\n",
    "                    open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write), 'w'))\n",
    "\n",
    "    return inference_documents_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExtendedPVDocumentBatchGeneratorMine(object):\n",
    "    def __init__(self, filename_prefix, batch_size=10000 ):\n",
    "        \"\"\"\n",
    "        batch_size cant be > 10,000 due to a limitation in doc2vec training, \n",
    "        None means no batching (only use for inference)\n",
    "        \"\"\"\n",
    "        assert batch_size <= 10000 or batch_size is None\n",
    "        self.filename_prefix = filename_prefix\n",
    "        self.curr_lines = []\n",
    "        self.curr_docids = []\n",
    "        self.batch_size = batch_size\n",
    "        self.curr_index = 0\n",
    "        self.curr_doc_index = 0\n",
    "        self.batch_end = -1\n",
    "    def load_new_batch_in_memory(self):\n",
    "        del self.curr_lines, self.curr_docids\n",
    "        self.curr_lines, self.curr_docids = [], []\n",
    "        info(\"Loading new batch for index: {}\".format(self.curr_doc_index))\n",
    "        #if self.curr_doc_index > 0: self.curr_doc_index -= 1\n",
    "        true_docs_count = 0\n",
    "        try:\n",
    "            with open(self.filename_prefix + str(self.curr_doc_index)) as preproc_file:\n",
    "                for line in preproc_file:\n",
    "                    line_array = line.split(\" \")\n",
    "                    doc_id = line_array[0]\n",
    "                    doc_tokens = line_array[1:]\n",
    "                    self.curr_lines.append(doc_tokens)\n",
    "                    self.curr_docids.append(doc_id)\n",
    "                    if is_real_doc(doc_id):\n",
    "                        true_docs_count+= 1\n",
    "            self.batch_end = self.curr_doc_index + true_docs_count - 1 \n",
    "            info(\"Finished loading new batch of {} documents\".format(true_docs_count))\n",
    "        except IOError:\n",
    "            info(\"No more batches to load, exiting at index: {}\".format(self.curr_index))\n",
    "            raise StopIteration()\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            if self.curr_doc_index > self.batch_end:\n",
    "                self.load_new_batch_in_memory()\n",
    "            for (doc_id, tokens) in zip(self.curr_docids, self.curr_lines):\n",
    "                if self.batch_size is not None:\n",
    "                    curr_batch_iter = 0\n",
    "                    # divide the document to batches according to the batch size\n",
    "                    while curr_batch_iter < len(tokens):\n",
    "                        yield LabeledSentence(words=tokens[curr_batch_iter: curr_batch_iter + self.batch_size], tags=[doc_id])\n",
    "                        curr_batch_iter += self.batch_size\n",
    "                else:\n",
    "                    yield doc_id, tokens\n",
    "                self.curr_index += 1\n",
    "                # increment only for full docs\n",
    "                if is_real_doc(doc_id):\n",
    "                    self.curr_doc_index += 1\n",
    "                    if self.curr_doc_index % 1000 == 0:\n",
    "                        info(\"New Doc: {:5}, Batch End: {:5}\".format(self.curr_doc_index, self.batch_end))\n",
    "\n",
    "def is_real_doc(doc_id):\n",
    "    return doc_id.find(\"_\") == -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ExtendedPVDocumentBatchGenerator(Process):\n",
    "    def __init__(self, filename_prefix, queue, batch_size=10000, start_file=0, offset=10000):\n",
    "        \"\"\"\n",
    "        batch_size cant be > 10,000 due to a limitation in doc2vec training, \n",
    "        None means no batching (only use for inference)\n",
    "        \"\"\"\n",
    "        assert batch_size <= 10000 or batch_size is None\n",
    "        super(ExtendedPVDocumentBatchGenerator, self).__init__()\n",
    "        self.queue = queue\n",
    "        self.offset = offset\n",
    "        self.filename_prefix = filename_prefix\n",
    "        self.batch_size = batch_size\n",
    "        self.cur_file = None\n",
    "        self.files_loaded = start_file - offset\n",
    "\n",
    "    def line_processing(self, line, batch_size):\n",
    "        line_array = tuple(line.split(\" \"))\n",
    "        doc_id = line_array[0]\n",
    "        line_array = line_array[1:]\n",
    "        len_line_array = len(line_array)\n",
    "        # divide the document to batches according to the batch size\n",
    "        if batch_size is not None:\n",
    "            curr_batch_iter = 0\n",
    "            while curr_batch_iter < len_line_array:\n",
    "                self.queue.put(LabeledSentence(words=line_array[curr_batch_iter: curr_batch_iter + batch_size], tags=[doc_id]), block=True, timeout=None)\n",
    "                curr_batch_iter += batch_size\n",
    "        else:\n",
    "            self.queue.put((doc_id, line_array), block=True, timeout=None)\n",
    "        return\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            try:\n",
    "                if self.cur_file is None:\n",
    "                    info(\"Loading new file for index: {}\".format(str(self.files_loaded + self.offset)))\n",
    "                    self.cur_file = gzip.open(self.filename_prefix + str(self.files_loaded + self.offset) + GZIP_EXTENSION)\n",
    "                    self.files_loaded += self.offset\n",
    "                for line in self.cur_file:\n",
    "                    self.line_processing(line, self.batch_size)\n",
    "                self.cur_file.close()\n",
    "                self.cur_file = None\n",
    "            except IOError:\n",
    "                self.queue.put(False, block=True, timeout=None)\n",
    "                info(\"All files are loaded - last file: {}\".format(str(self.files_loaded + self.offset)))\n",
    "                return\n",
    "\n",
    "\n",
    "class BatchWrapper(object):\n",
    "    def __init__(self, training_preprocessed_files_prefix, buffer_size=10000, batch_size=10000):\n",
    "        self.q = Queue(maxsize=buffer_size)\n",
    "        self.p = ExtendedPVDocumentBatchGenerator(training_preprocessed_files_prefix, queue=self.q,\n",
    "                                                  batch_size=batch_size, start_file=0, offset=10000)\n",
    "        self.p.start()\n",
    "        self.cur_data = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            item = self.q.get(block=True)\n",
    "            if item is False:\n",
    "                raise StopIteration()\n",
    "            else:\n",
    "                yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2vec and SVM Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOC2VEC_SIZE = 200\n",
    "DOC2VEC_WINDOW = 2\n",
    "DOC2VEC_MAX_VOCAB_SIZE = None\n",
    "DOC2VEC_SAMPLE = 1e-3\n",
    "DOC2VEC_TYPE = 1\n",
    "DOC2VEC_HIERARCHICAL_SAMPLE = 0\n",
    "DOC2VEC_NEGATIVE_SAMPLE_SIZE = 10\n",
    "DOC2VEC_CONCAT = 0\n",
    "DOC2VEC_MEAN = 1\n",
    "DOC2VEC_TRAIN_WORDS = 0\n",
    "DOC2VEC_EPOCHS = 1 # we do our training manually one epoch at a time\n",
    "DOC2VEC_MAX_EPOCHS = 8\n",
    "REPORT_DELAY = 20 # report the progress every x seconds\n",
    "REPORT_VOCAB_PROGRESS = 100000 # report vocab progress every x documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_{}'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}'.format(DOC2VEC_SIZE, \n",
    "                                                                DOC2VEC_WINDOW, \n",
    "                                                                'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                                DOC2VEC_TRAIN_WORDS,\n",
    "                                                                DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                                str(DOC2VEC_MAX_VOCAB_SIZE))\n",
    "GLOBAL_VARS.DOC2VEC_MODEL_NAME = placeholder_model_name\n",
    "placeholder_model_name = os.path.join(placeholder_model_name, \"epoch_{}\")\n",
    "placeholder_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2vec_model = Doc2Vec(size=DOC2VEC_SIZE , window=DOC2VEC_WINDOW, min_count=MIN_WORD_COUNT, \n",
    "                max_vocab_size= DOC2VEC_MAX_VOCAB_SIZE,\n",
    "                sample=DOC2VEC_SAMPLE, seed=DOC2VEC_SEED, workers=NUM_CORES,\n",
    "                # doc2vec algorithm dm=1 => PV-DM, dm=2 => PV-DBOW, PV-DM dictates CBOW for words\n",
    "                dm=DOC2VEC_TYPE,\n",
    "                # hs=0 => negative sampling, hs=1 => hierarchical softmax\n",
    "                hs=DOC2VEC_HIERARCHICAL_SAMPLE, negative=DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                dm_concat=DOC2VEC_CONCAT,\n",
    "                # would train words with skip-gram on top of cbow, we don't need that for now\n",
    "                dbow_words=DOC2VEC_TRAIN_WORDS,\n",
    "                iter=DOC2VEC_EPOCHS)\n",
    "\n",
    "GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First: construct (or load) the vocabulary\n",
    "Only needed to be run if you dont already haave at least one epoch computed, otherwise, just set the start_from (below) to the epoch you want to restart from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-06 05:57:37,601 : INFO : loading Doc2Vec object from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/vocab_model/model\n",
      "2017-04-06 05:57:37,602 : INFO : Loading new file for index: 0\n",
      "2017-04-06 06:00:49,254 : INFO : loading docvecs recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/vocab_model/model.docvecs.* with mmap=None\n",
      "2017-04-06 06:00:49,255 : INFO : loading doctag_syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/vocab_model/model.docvecs.doctag_syn0.npy with mmap=None\n",
      "2017-04-06 06:01:07,168 : INFO : loading doctag_syn0_lockf from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/vocab_model/model.docvecs.doctag_syn0_lockf.npy with mmap=None\n",
      "2017-04-06 06:01:07,265 : INFO : loading syn1neg from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/vocab_model/model.syn1neg.npy with mmap=None\n",
      "2017-04-06 06:01:07,441 : INFO : loading syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/vocab_model/model.syn0.npy with mmap=None\n",
      "2017-04-06 06:01:07,617 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-06 06:01:07,618 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-06 06:01:08,595 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 50s, sys: 1min 9s, total: 14min\n",
      "Wall time: 14min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# training_docs_iterator = ExtendedPVDocumentBatchGenerator(training_preprocessed_files_prefix, batch_size=10000)\n",
    "training_docs_iterator = BatchClass(training_preprocessed_files_prefix, batch_size=10000)\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX)):\n",
    "    doc2vec_model.build_vocab(sentences=training_docs_iterator, progress_per=REPORT_VOCAB_PROGRESS)\n",
    "    doc2vec_model.save(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "else:\n",
    "    doc2vec_model_vocab_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "    doc2vec_model.reset_from(doc2vec_model_vocab_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocab_counts = {k:doc2vec_model.vocab[k].count for k in doc2vec_model.vocab.keys()}\n",
    "# dd = sorted(vocab_counts, key=vocab_counts.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Training, validation and Metrics Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2vec_model.min_alpha = 0.025\n",
    "DOC2VEC_ALPHA_DECREASE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_model.workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2vec_model.workers = NUM_CORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BatchClass = BatchWrapper\n",
    "# BatchClass = ExtendedPVDocumentBatchGeneratorMine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-10 12:28:09,227 : INFO : ****************** Epoch 8 --- Working on doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8 *******************\n",
      "2017-04-10 12:28:09,228 : INFO : loading Doc2Vec object from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model\n",
      "2017-04-10 12:31:51,008 : INFO : loading docvecs recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.docvecs.* with mmap=None\n",
      "2017-04-10 12:31:51,009 : INFO : loading doctag_syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.docvecs.doctag_syn0.npy with mmap=None\n",
      "2017-04-10 12:32:49,817 : INFO : loading doctag_syn0_lockf from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.docvecs.doctag_syn0_lockf.npy with mmap=None\n",
      "2017-04-10 12:32:50,882 : INFO : loading syn1neg from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.syn1neg.npy with mmap=None\n",
      "2017-04-10 12:32:52,992 : INFO : loading syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.syn0.npy with mmap=None\n",
      "2017-04-10 12:32:55,115 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-10 12:32:55,115 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-10 12:32:55,116 : INFO : loaded /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 56s, sys: 55.9 s, total: 3min 52s\n",
      "Wall time: 4min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# when resuming, resume from an epoch with a previously created doc2vec model to get the learning rate right\n",
    "start_from = 8\n",
    "for epoch in range(start_from, DOC2VEC_MAX_EPOCHS+1):\n",
    "    GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "    info(\"****************** Epoch {} --- Working on {} *******************\".format(epoch, GLOBAL_VARS.MODEL_NAME))\n",
    "    \n",
    "    # if we have the model, just load it, otherwise train the previous model\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)):\n",
    "        doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "        doc2vec_model.workers = NUM_CORES\n",
    "        GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "    else:\n",
    "        # train the doc2vec model\n",
    "#         training_docs_iterator = ExtendedPVDocumentBatchGenerator(training_preprocessed_files_prefix, batch_size=10000)\n",
    "        training_docs_iterator = BatchClass(training_preprocessed_files_prefix, batch_size=10000)\n",
    "        %time doc2vec_model.train(sentences=training_docs_iterator, report_delay=REPORT_DELAY)\n",
    "        doc2vec_model.alpha -= DOC2VEC_ALPHA_DECREASE  # decrease the learning rate\n",
    "        doc2vec_model.min_alpha = doc2vec_model.alpha  # fix the learning rate, no decay\n",
    "        ensure_disk_location_exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME))\n",
    "        doc2vec_model.save(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "        GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "        \n",
    "    # only do the inference for higher epochs, as inference usually takes as much time as the actual training\n",
    "    if epoch > 7:\n",
    "        # Validation Embeddings\n",
    "        info('Getting Validation Embeddings')\n",
    "        Xv = get_extended_docs_with_inference_data_only(doc2vec_model, VALIDATION_DICT, \n",
    "                                         validation_preprocessed_files_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Only (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CORES = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BatchClass = BatchWrapper\n",
    "# BatchClass = ExtendedPVDocumentBatchGeneratorMine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-10 19:36:00,967 : INFO : loading Doc2Vec object from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model\n",
      "2017-04-10 19:38:45,963 : INFO : loading docvecs recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.docvecs.* with mmap=None\n",
      "2017-04-10 19:38:45,964 : INFO : loading doctag_syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.docvecs.doctag_syn0.npy with mmap=None\n",
      "2017-04-10 19:39:04,256 : INFO : loading doctag_syn0_lockf from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.docvecs.doctag_syn0_lockf.npy with mmap=None\n",
      "2017-04-10 19:39:04,359 : INFO : loading syn1neg from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.syn1neg.npy with mmap=None\n",
      "2017-04-10 19:39:04,556 : INFO : loading syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.syn0.npy with mmap=None\n",
      "2017-04-10 19:39:04,746 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-10 19:39:04,747 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-10 19:39:04,747 : INFO : loaded /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model\n"
     ]
    }
   ],
   "source": [
    "epoch = 8\n",
    "GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "\n",
    "if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)):\n",
    "    doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "    doc2vec_model.workers = NUM_CORES\n",
    "    GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-10 19:40:15,701 : INFO : ===== Getting vectors with inference\n",
      "2017-04-10 19:40:16,443 : INFO : Loading new file for index: 0\n",
      "2017-04-10 19:40:46,137 : INFO : Finished: 10000 tags\n",
      "2017-04-10 19:41:13,788 : INFO : Finished: 20000 tags\n",
      "2017-04-10 19:41:42,912 : INFO : Finished: 30000 tags\n",
      "2017-04-10 19:42:10,934 : INFO : Finished: 40000 tags\n",
      "2017-04-10 19:42:37,678 : INFO : Finished: 50000 tags\n",
      "2017-04-10 19:43:05,127 : INFO : Finished: 60000 tags\n",
      "2017-04-10 19:43:31,967 : INFO : Finished: 70000 tags\n",
      "2017-04-10 19:43:59,439 : INFO : Finished: 80000 tags\n",
      "2017-04-10 19:44:30,121 : INFO : Finished: 90000 tags\n",
      "2017-04-10 19:44:57,802 : INFO : Finished: 100000 tags\n",
      "2017-04-10 19:45:24,656 : INFO : Finished: 110000 tags\n",
      "2017-04-10 19:45:51,828 : INFO : Finished: 120000 tags\n",
      "2017-04-10 19:46:19,612 : INFO : Finished: 130000 tags\n",
      "2017-04-10 19:46:47,529 : INFO : Finished: 140000 tags\n",
      "2017-04-10 19:47:14,191 : INFO : Finished: 150000 tags\n",
      "2017-04-10 19:47:39,468 : INFO : Finished: 160000 tags\n",
      "2017-04-10 19:48:07,949 : INFO : Finished: 170000 tags\n",
      "2017-04-10 19:48:35,654 : INFO : Finished: 180000 tags\n",
      "2017-04-10 19:49:02,931 : INFO : Finished: 190000 tags\n",
      "2017-04-10 19:49:31,070 : INFO : Finished: 200000 tags\n",
      "2017-04-10 19:49:56,942 : INFO : Finished: 210000 tags\n",
      "2017-04-10 19:50:24,536 : INFO : Finished: 220000 tags\n",
      "2017-04-10 19:50:51,729 : INFO : Finished: 230000 tags\n",
      "2017-04-10 19:51:23,430 : INFO : Finished: 240000 tags\n",
      "2017-04-10 19:51:48,967 : INFO : Finished: 250000 tags\n",
      "2017-04-10 19:52:14,713 : INFO : Finished: 260000 tags\n",
      "2017-04-10 19:52:41,068 : INFO : Finished: 270000 tags\n",
      "2017-04-10 19:53:07,030 : INFO : Finished: 280000 tags\n",
      "2017-04-10 19:53:33,366 : INFO : Finished: 290000 tags\n",
      "2017-04-10 19:54:00,247 : INFO : Finished: 300000 tags\n",
      "2017-04-10 19:54:26,502 : INFO : Finished: 310000 tags\n",
      "2017-04-10 19:54:52,745 : INFO : Finished: 320000 tags\n",
      "2017-04-10 19:54:54,931 : INFO : Loading new file for index: 10000\n",
      "2017-04-10 19:55:21,529 : INFO : Finished: 330000 tags\n",
      "2017-04-10 19:55:48,687 : INFO : Finished: 340000 tags\n",
      "2017-04-10 19:56:16,461 : INFO : Finished: 350000 tags\n",
      "2017-04-10 19:56:43,649 : INFO : Finished: 360000 tags\n",
      "2017-04-10 19:57:11,136 : INFO : Finished: 370000 tags\n",
      "2017-04-10 19:57:38,325 : INFO : Finished: 380000 tags\n",
      "2017-04-10 19:58:06,122 : INFO : Finished: 390000 tags\n",
      "2017-04-10 19:58:32,734 : INFO : Finished: 400000 tags\n",
      "2017-04-10 19:59:00,724 : INFO : Finished: 410000 tags\n",
      "2017-04-10 19:59:28,329 : INFO : Finished: 420000 tags\n",
      "2017-04-10 19:59:55,320 : INFO : Finished: 430000 tags\n",
      "2017-04-10 20:00:23,715 : INFO : Finished: 440000 tags\n",
      "2017-04-10 20:00:53,014 : INFO : Finished: 450000 tags\n",
      "2017-04-10 20:01:19,634 : INFO : Finished: 460000 tags\n",
      "2017-04-10 20:01:47,059 : INFO : Finished: 470000 tags\n",
      "2017-04-10 20:02:13,433 : INFO : Finished: 480000 tags\n",
      "2017-04-10 20:02:42,783 : INFO : Finished: 490000 tags\n",
      "2017-04-10 20:03:11,466 : INFO : Finished: 500000 tags\n",
      "2017-04-10 20:03:38,462 : INFO : Finished: 510000 tags\n",
      "2017-04-10 20:04:04,738 : INFO : Finished: 520000 tags\n",
      "2017-04-10 20:04:33,213 : INFO : Finished: 530000 tags\n",
      "2017-04-10 20:04:58,746 : INFO : Finished: 540000 tags\n",
      "2017-04-10 20:05:27,004 : INFO : Finished: 550000 tags\n",
      "2017-04-10 20:05:54,514 : INFO : Finished: 560000 tags\n",
      "2017-04-10 20:06:22,014 : INFO : Finished: 570000 tags\n",
      "2017-04-10 20:06:49,508 : INFO : Finished: 580000 tags\n",
      "2017-04-10 20:07:15,670 : INFO : Finished: 590000 tags\n",
      "2017-04-10 20:07:44,144 : INFO : Finished: 600000 tags\n",
      "2017-04-10 20:08:10,256 : INFO : Finished: 610000 tags\n",
      "2017-04-10 20:08:38,381 : INFO : Finished: 620000 tags\n",
      "2017-04-10 20:09:06,096 : INFO : Finished: 630000 tags\n",
      "2017-04-10 20:09:33,588 : INFO : Finished: 640000 tags\n",
      "2017-04-10 20:10:01,812 : INFO : Finished: 650000 tags\n",
      "2017-04-10 20:10:29,543 : INFO : Finished: 660000 tags\n",
      "2017-04-10 20:10:31,568 : INFO : Loading new file for index: 20000\n",
      "2017-04-10 20:10:56,462 : INFO : Finished: 670000 tags\n",
      "2017-04-10 20:11:23,013 : INFO : Finished: 680000 tags\n",
      "2017-04-10 20:11:50,103 : INFO : Finished: 690000 tags\n",
      "2017-04-10 20:12:16,807 : INFO : Finished: 700000 tags\n",
      "2017-04-10 20:12:42,998 : INFO : Finished: 710000 tags\n",
      "2017-04-10 20:13:09,692 : INFO : Finished: 720000 tags\n",
      "2017-04-10 20:13:37,795 : INFO : Finished: 730000 tags\n",
      "2017-04-10 20:14:05,285 : INFO : Finished: 740000 tags\n",
      "2017-04-10 20:14:32,412 : INFO : Finished: 750000 tags\n",
      "2017-04-10 20:14:59,671 : INFO : Finished: 760000 tags\n",
      "2017-04-10 20:15:26,651 : INFO : Finished: 770000 tags\n",
      "2017-04-10 20:15:53,265 : INFO : Finished: 780000 tags\n",
      "2017-04-10 20:16:19,122 : INFO : Finished: 790000 tags\n",
      "2017-04-10 20:16:45,859 : INFO : Finished: 800000 tags\n",
      "2017-04-10 20:17:12,618 : INFO : Finished: 810000 tags\n",
      "2017-04-10 20:17:39,427 : INFO : Finished: 820000 tags\n",
      "2017-04-10 20:18:05,996 : INFO : Finished: 830000 tags\n",
      "2017-04-10 20:18:34,384 : INFO : Finished: 840000 tags\n",
      "2017-04-10 20:19:00,606 : INFO : Finished: 850000 tags\n",
      "2017-04-10 20:19:27,233 : INFO : Finished: 860000 tags\n",
      "2017-04-10 20:19:54,290 : INFO : Finished: 870000 tags\n",
      "2017-04-10 20:20:21,249 : INFO : Finished: 880000 tags\n",
      "2017-04-10 20:20:49,260 : INFO : Finished: 890000 tags\n",
      "2017-04-10 20:21:16,091 : INFO : Finished: 900000 tags\n",
      "2017-04-10 20:21:42,470 : INFO : Finished: 910000 tags\n",
      "2017-04-10 20:22:06,220 : INFO : Finished: 920000 tags\n",
      "2017-04-10 20:22:31,855 : INFO : Finished: 930000 tags\n",
      "2017-04-10 20:22:55,999 : INFO : Finished: 940000 tags\n",
      "2017-04-10 20:23:20,186 : INFO : Finished: 950000 tags\n",
      "2017-04-10 20:23:45,453 : INFO : Finished: 960000 tags\n",
      "2017-04-10 20:24:11,817 : INFO : Finished: 970000 tags\n",
      "2017-04-10 20:24:36,857 : INFO : Finished: 980000 tags\n",
      "2017-04-10 20:25:02,220 : INFO : Finished: 990000 tags\n",
      "2017-04-10 20:25:25,325 : INFO : Finished: 1000000 tags\n",
      "2017-04-10 20:25:27,216 : INFO : Loading new file for index: 30000\n",
      "2017-04-10 20:25:49,834 : INFO : Finished: 1010000 tags\n",
      "2017-04-10 20:26:15,434 : INFO : Finished: 1020000 tags\n",
      "2017-04-10 20:26:39,592 : INFO : Finished: 1030000 tags\n",
      "2017-04-10 20:27:05,026 : INFO : Finished: 1040000 tags\n",
      "2017-04-10 20:27:32,014 : INFO : Finished: 1050000 tags\n",
      "2017-04-10 20:27:58,779 : INFO : Finished: 1060000 tags\n",
      "2017-04-10 20:28:28,082 : INFO : Finished: 1070000 tags\n",
      "2017-04-10 20:28:55,371 : INFO : Finished: 1080000 tags\n",
      "2017-04-10 20:29:23,704 : INFO : Finished: 1090000 tags\n",
      "2017-04-10 20:29:51,536 : INFO : Finished: 1100000 tags\n",
      "2017-04-10 20:30:18,965 : INFO : Finished: 1110000 tags\n",
      "2017-04-10 20:30:46,386 : INFO : Finished: 1120000 tags\n",
      "2017-04-10 20:31:14,440 : INFO : Finished: 1130000 tags\n",
      "2017-04-10 20:31:41,683 : INFO : Finished: 1140000 tags\n",
      "2017-04-10 20:32:07,618 : INFO : Finished: 1150000 tags\n",
      "2017-04-10 20:32:35,214 : INFO : Finished: 1160000 tags\n",
      "2017-04-10 20:33:01,351 : INFO : Finished: 1170000 tags\n",
      "2017-04-10 20:33:29,318 : INFO : Finished: 1180000 tags\n",
      "2017-04-10 20:33:56,999 : INFO : Finished: 1190000 tags\n",
      "2017-04-10 20:34:24,385 : INFO : Finished: 1200000 tags\n",
      "2017-04-10 20:34:53,915 : INFO : Finished: 1210000 tags\n",
      "2017-04-10 20:35:21,826 : INFO : Finished: 1220000 tags\n",
      "2017-04-10 20:35:48,496 : INFO : Finished: 1230000 tags\n",
      "2017-04-10 20:36:15,263 : INFO : Finished: 1240000 tags\n",
      "2017-04-10 20:36:42,103 : INFO : Finished: 1250000 tags\n",
      "2017-04-10 20:37:09,365 : INFO : Finished: 1260000 tags\n",
      "2017-04-10 20:37:35,856 : INFO : Finished: 1270000 tags\n",
      "2017-04-10 20:38:01,991 : INFO : Finished: 1280000 tags\n",
      "2017-04-10 20:38:29,533 : INFO : Finished: 1290000 tags\n",
      "2017-04-10 20:38:57,850 : INFO : Finished: 1300000 tags\n",
      "2017-04-10 20:39:24,974 : INFO : Finished: 1310000 tags\n",
      "2017-04-10 20:39:51,566 : INFO : Finished: 1320000 tags\n",
      "2017-04-10 20:40:21,468 : INFO : Finished: 1330000 tags\n",
      "2017-04-10 20:40:49,257 : INFO : Finished: 1340000 tags\n",
      "2017-04-10 20:40:51,106 : INFO : Loading new file for index: 40000\n",
      "2017-04-10 20:41:16,589 : INFO : Finished: 1350000 tags\n",
      "2017-04-10 20:41:41,926 : INFO : Finished: 1360000 tags\n",
      "2017-04-10 20:42:10,053 : INFO : Finished: 1370000 tags\n",
      "2017-04-10 20:42:36,608 : INFO : Finished: 1380000 tags\n",
      "2017-04-10 20:43:03,277 : INFO : Finished: 1390000 tags\n",
      "2017-04-10 20:43:29,759 : INFO : Finished: 1400000 tags\n",
      "2017-04-10 20:43:56,738 : INFO : Finished: 1410000 tags\n",
      "2017-04-10 20:44:24,730 : INFO : Finished: 1420000 tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-10 20:44:50,627 : INFO : Finished: 1430000 tags\n",
      "2017-04-10 20:45:18,500 : INFO : Finished: 1440000 tags\n",
      "2017-04-10 20:45:45,679 : INFO : Finished: 1450000 tags\n",
      "2017-04-10 20:46:14,395 : INFO : Finished: 1460000 tags\n",
      "2017-04-10 20:46:42,538 : INFO : Finished: 1470000 tags\n",
      "2017-04-10 20:47:10,947 : INFO : Finished: 1480000 tags\n",
      "2017-04-10 20:47:39,123 : INFO : Finished: 1490000 tags\n",
      "2017-04-10 20:48:07,554 : INFO : Finished: 1500000 tags\n",
      "2017-04-10 20:48:33,342 : INFO : Finished: 1510000 tags\n",
      "2017-04-10 20:49:01,627 : INFO : Finished: 1520000 tags\n",
      "2017-04-10 20:49:29,052 : INFO : Finished: 1530000 tags\n",
      "2017-04-10 20:49:55,962 : INFO : Finished: 1540000 tags\n",
      "2017-04-10 20:50:24,346 : INFO : Finished: 1550000 tags\n",
      "2017-04-10 20:50:50,843 : INFO : Finished: 1560000 tags\n",
      "2017-04-10 20:51:18,118 : INFO : Finished: 1570000 tags\n",
      "2017-04-10 20:51:44,450 : INFO : Finished: 1580000 tags\n",
      "2017-04-10 20:52:11,854 : INFO : Finished: 1590000 tags\n",
      "2017-04-10 20:52:38,998 : INFO : Finished: 1600000 tags\n",
      "2017-04-10 20:53:05,830 : INFO : Finished: 1610000 tags\n",
      "2017-04-10 20:53:31,749 : INFO : Finished: 1620000 tags\n",
      "2017-04-10 20:53:58,969 : INFO : Finished: 1630000 tags\n",
      "2017-04-10 20:54:26,160 : INFO : Finished: 1640000 tags\n",
      "2017-04-10 20:54:53,703 : INFO : Finished: 1650000 tags\n",
      "2017-04-10 20:55:21,300 : INFO : Finished: 1660000 tags\n",
      "2017-04-10 20:55:47,790 : INFO : Finished: 1670000 tags\n",
      "2017-04-10 20:56:14,949 : INFO : Finished: 1680000 tags\n",
      "2017-04-10 20:56:16,983 : INFO : Loading new file for index: 50000\n",
      "2017-04-10 20:56:41,816 : INFO : Finished: 1690000 tags\n",
      "2017-04-10 20:57:08,842 : INFO : Finished: 1700000 tags\n",
      "2017-04-10 20:57:35,932 : INFO : Finished: 1710000 tags\n",
      "2017-04-10 20:58:02,687 : INFO : Finished: 1720000 tags\n",
      "2017-04-10 20:58:30,405 : INFO : Finished: 1730000 tags\n",
      "2017-04-10 20:58:58,236 : INFO : Finished: 1740000 tags\n",
      "2017-04-10 20:59:27,565 : INFO : Finished: 1750000 tags\n",
      "2017-04-10 20:59:54,243 : INFO : Finished: 1760000 tags\n",
      "2017-04-10 21:00:21,116 : INFO : Finished: 1770000 tags\n",
      "2017-04-10 21:00:49,760 : INFO : Finished: 1780000 tags\n",
      "2017-04-10 21:01:16,675 : INFO : Finished: 1790000 tags\n",
      "2017-04-10 21:01:43,856 : INFO : Finished: 1800000 tags\n",
      "2017-04-10 21:02:11,413 : INFO : Finished: 1810000 tags\n",
      "2017-04-10 21:02:39,194 : INFO : Finished: 1820000 tags\n",
      "2017-04-10 21:03:06,521 : INFO : Finished: 1830000 tags\n",
      "2017-04-10 21:03:35,713 : INFO : Finished: 1840000 tags\n",
      "2017-04-10 21:04:02,608 : INFO : Finished: 1850000 tags\n",
      "2017-04-10 21:04:28,460 : INFO : Finished: 1860000 tags\n",
      "2017-04-10 21:04:56,302 : INFO : Finished: 1870000 tags\n",
      "2017-04-10 21:05:23,188 : INFO : Finished: 1880000 tags\n",
      "2017-04-10 21:05:52,260 : INFO : Finished: 1890000 tags\n",
      "2017-04-10 21:06:18,929 : INFO : Finished: 1900000 tags\n",
      "2017-04-10 21:06:45,810 : INFO : Finished: 1910000 tags\n",
      "2017-04-10 21:07:12,657 : INFO : Finished: 1920000 tags\n",
      "2017-04-10 21:07:40,067 : INFO : Finished: 1930000 tags\n",
      "2017-04-10 21:08:06,435 : INFO : Finished: 1940000 tags\n",
      "2017-04-10 21:08:35,601 : INFO : Finished: 1950000 tags\n",
      "2017-04-10 21:09:02,714 : INFO : Finished: 1960000 tags\n",
      "2017-04-10 21:09:30,092 : INFO : Finished: 1970000 tags\n",
      "2017-04-10 21:09:58,697 : INFO : Finished: 1980000 tags\n",
      "2017-04-10 21:10:27,770 : INFO : Finished: 1990000 tags\n",
      "2017-04-10 21:10:54,638 : INFO : Finished: 2000000 tags\n",
      "2017-04-10 21:11:21,085 : INFO : Finished: 2010000 tags\n",
      "2017-04-10 21:11:48,628 : INFO : Finished: 2020000 tags\n",
      "2017-04-10 21:11:50,688 : INFO : Loading new file for index: 60000\n",
      "2017-04-10 21:12:15,639 : INFO : Finished: 2030000 tags\n",
      "2017-04-10 21:12:43,208 : INFO : Finished: 2040000 tags\n",
      "2017-04-10 21:13:10,976 : INFO : Finished: 2050000 tags\n",
      "2017-04-10 21:13:38,125 : INFO : Finished: 2060000 tags\n",
      "2017-04-10 21:14:04,814 : INFO : Finished: 2070000 tags\n",
      "2017-04-10 21:14:33,119 : INFO : Finished: 2080000 tags\n",
      "2017-04-10 21:14:59,422 : INFO : Finished: 2090000 tags\n",
      "2017-04-10 21:15:25,761 : INFO : Finished: 2100000 tags\n",
      "2017-04-10 21:15:53,196 : INFO : Finished: 2110000 tags\n",
      "2017-04-10 21:16:20,875 : INFO : Finished: 2120000 tags\n",
      "2017-04-10 21:16:46,516 : INFO : Finished: 2130000 tags\n",
      "2017-04-10 21:17:14,455 : INFO : Finished: 2140000 tags\n",
      "2017-04-10 21:17:42,838 : INFO : Finished: 2150000 tags\n",
      "2017-04-10 21:18:09,488 : INFO : Finished: 2160000 tags\n",
      "2017-04-10 21:18:37,358 : INFO : Finished: 2170000 tags\n",
      "2017-04-10 21:19:05,314 : INFO : Finished: 2180000 tags\n",
      "2017-04-10 21:19:33,525 : INFO : Finished: 2190000 tags\n",
      "2017-04-10 21:20:02,506 : INFO : Finished: 2200000 tags\n",
      "2017-04-10 21:20:31,380 : INFO : Finished: 2210000 tags\n",
      "2017-04-10 21:21:00,764 : INFO : Finished: 2220000 tags\n",
      "2017-04-10 21:21:28,373 : INFO : Finished: 2230000 tags\n",
      "2017-04-10 21:21:57,875 : INFO : Finished: 2240000 tags\n",
      "2017-04-10 21:22:25,402 : INFO : Finished: 2250000 tags\n",
      "2017-04-10 21:22:52,367 : INFO : Finished: 2260000 tags\n",
      "2017-04-10 21:23:22,084 : INFO : Finished: 2270000 tags\n",
      "2017-04-10 21:23:50,734 : INFO : Finished: 2280000 tags\n",
      "2017-04-10 21:24:19,107 : INFO : Finished: 2290000 tags\n",
      "2017-04-10 21:24:48,699 : INFO : Finished: 2300000 tags\n",
      "2017-04-10 21:25:17,290 : INFO : Finished: 2310000 tags\n",
      "2017-04-10 21:25:46,693 : INFO : Finished: 2320000 tags\n",
      "2017-04-10 21:26:14,602 : INFO : Finished: 2330000 tags\n",
      "2017-04-10 21:26:41,055 : INFO : Finished: 2340000 tags\n",
      "2017-04-10 21:27:07,931 : INFO : Finished: 2350000 tags\n",
      "2017-04-10 21:27:35,257 : INFO : Finished: 2360000 tags\n",
      "2017-04-10 21:27:41,995 : INFO : Loading new file for index: 70000\n",
      "2017-04-10 21:28:03,408 : INFO : Finished: 2370000 tags\n",
      "2017-04-10 21:28:37,989 : INFO : Finished: 2380000 tags\n",
      "2017-04-10 21:29:41,238 : INFO : Finished: 2390000 tags\n",
      "2017-04-10 21:30:08,040 : INFO : Finished: 2400000 tags\n",
      "2017-04-10 21:30:35,375 : INFO : Finished: 2410000 tags\n",
      "2017-04-10 21:31:03,143 : INFO : Finished: 2420000 tags\n",
      "2017-04-10 21:31:30,927 : INFO : Finished: 2430000 tags\n",
      "2017-04-10 21:31:58,581 : INFO : Finished: 2440000 tags\n",
      "2017-04-10 21:32:25,296 : INFO : Finished: 2450000 tags\n",
      "2017-04-10 21:32:53,142 : INFO : Finished: 2460000 tags\n",
      "2017-04-10 21:33:21,605 : INFO : Finished: 2470000 tags\n",
      "2017-04-10 21:33:48,405 : INFO : Finished: 2480000 tags\n",
      "2017-04-10 21:34:15,707 : INFO : Finished: 2490000 tags\n",
      "2017-04-10 21:34:41,584 : INFO : Finished: 2500000 tags\n",
      "2017-04-10 21:35:09,555 : INFO : Finished: 2510000 tags\n",
      "2017-04-10 21:35:36,101 : INFO : Finished: 2520000 tags\n",
      "2017-04-10 21:36:04,635 : INFO : Finished: 2530000 tags\n",
      "2017-04-10 21:36:31,633 : INFO : Finished: 2540000 tags\n",
      "2017-04-10 21:36:58,241 : INFO : Finished: 2550000 tags\n",
      "2017-04-10 21:37:25,440 : INFO : Finished: 2560000 tags\n",
      "2017-04-10 21:37:53,091 : INFO : Finished: 2570000 tags\n",
      "2017-04-10 21:38:19,052 : INFO : Finished: 2580000 tags\n",
      "2017-04-10 21:38:45,737 : INFO : Finished: 2590000 tags\n",
      "2017-04-10 21:39:13,026 : INFO : Finished: 2600000 tags\n",
      "2017-04-10 21:39:40,082 : INFO : Finished: 2610000 tags\n",
      "2017-04-10 21:40:09,359 : INFO : Finished: 2620000 tags\n",
      "2017-04-10 21:40:39,649 : INFO : Finished: 2630000 tags\n",
      "2017-04-10 21:41:07,427 : INFO : Finished: 2640000 tags\n",
      "2017-04-10 21:41:34,828 : INFO : Finished: 2650000 tags\n",
      "2017-04-10 21:42:01,802 : INFO : Finished: 2660000 tags\n",
      "2017-04-10 21:42:28,053 : INFO : Finished: 2670000 tags\n",
      "2017-04-10 21:42:54,493 : INFO : Finished: 2680000 tags\n",
      "2017-04-10 21:43:21,533 : INFO : Finished: 2690000 tags\n",
      "2017-04-10 21:43:48,195 : INFO : Finished: 2700000 tags\n",
      "2017-04-10 21:43:50,576 : INFO : Loading new file for index: 80000\n",
      "2017-04-10 21:44:15,275 : INFO : Finished: 2710000 tags\n",
      "2017-04-10 21:44:45,321 : INFO : Finished: 2720000 tags\n",
      "2017-04-10 21:45:13,416 : INFO : Finished: 2730000 tags\n",
      "2017-04-10 21:45:40,578 : INFO : Finished: 2740000 tags\n",
      "2017-04-10 21:46:07,513 : INFO : Finished: 2750000 tags\n",
      "2017-04-10 21:46:34,557 : INFO : Finished: 2760000 tags\n",
      "2017-04-10 21:47:03,511 : INFO : Finished: 2770000 tags\n",
      "2017-04-10 21:47:29,709 : INFO : Finished: 2780000 tags\n",
      "2017-04-10 21:47:55,938 : INFO : Finished: 2790000 tags\n",
      "2017-04-10 21:48:23,851 : INFO : Finished: 2800000 tags\n",
      "2017-04-10 21:48:50,456 : INFO : Finished: 2810000 tags\n",
      "2017-04-10 21:49:17,281 : INFO : Finished: 2820000 tags\n",
      "2017-04-10 21:49:42,821 : INFO : Finished: 2830000 tags\n",
      "2017-04-10 21:50:08,673 : INFO : Finished: 2840000 tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-10 21:50:34,592 : INFO : Finished: 2850000 tags\n",
      "2017-04-10 21:51:02,377 : INFO : Finished: 2860000 tags\n",
      "2017-04-10 21:51:29,194 : INFO : Finished: 2870000 tags\n",
      "2017-04-10 21:51:55,758 : INFO : Finished: 2880000 tags\n",
      "2017-04-10 21:52:22,824 : INFO : Finished: 2890000 tags\n",
      "2017-04-10 21:52:51,268 : INFO : Finished: 2900000 tags\n",
      "2017-04-10 21:53:19,621 : INFO : Finished: 2910000 tags\n",
      "2017-04-10 21:53:49,377 : INFO : Finished: 2920000 tags\n",
      "2017-04-10 21:54:15,058 : INFO : Finished: 2930000 tags\n",
      "2017-04-10 21:54:41,435 : INFO : Finished: 2940000 tags\n",
      "2017-04-10 21:55:08,195 : INFO : Finished: 2950000 tags\n",
      "2017-04-10 21:55:35,521 : INFO : Finished: 2960000 tags\n",
      "2017-04-10 21:56:04,848 : INFO : Finished: 2970000 tags\n",
      "2017-04-10 21:56:31,892 : INFO : Finished: 2980000 tags\n",
      "2017-04-10 21:56:59,543 : INFO : Finished: 2990000 tags\n",
      "2017-04-10 21:57:27,077 : INFO : Finished: 3000000 tags\n",
      "2017-04-10 21:57:54,105 : INFO : Finished: 3010000 tags\n",
      "2017-04-10 21:58:20,286 : INFO : Finished: 3020000 tags\n",
      "2017-04-10 21:58:49,397 : INFO : Finished: 3030000 tags\n",
      "2017-04-10 21:59:16,920 : INFO : Finished: 3040000 tags\n",
      "2017-04-10 21:59:18,891 : INFO : Loading new file for index: 90000\n",
      "2017-04-10 21:59:43,328 : INFO : Finished: 3050000 tags\n",
      "2017-04-10 22:00:10,685 : INFO : Finished: 3060000 tags\n",
      "2017-04-10 22:00:40,902 : INFO : Finished: 3070000 tags\n",
      "2017-04-10 22:01:07,756 : INFO : Finished: 3080000 tags\n",
      "2017-04-10 22:01:34,682 : INFO : Finished: 3090000 tags\n",
      "2017-04-10 22:02:00,628 : INFO : Finished: 3100000 tags\n",
      "2017-04-10 22:02:27,054 : INFO : Finished: 3110000 tags\n",
      "2017-04-10 22:02:53,838 : INFO : Finished: 3120000 tags\n",
      "2017-04-10 22:03:22,269 : INFO : Finished: 3130000 tags\n",
      "2017-04-10 22:03:48,732 : INFO : Finished: 3140000 tags\n",
      "2017-04-10 22:04:16,112 : INFO : Finished: 3150000 tags\n",
      "2017-04-10 22:04:43,943 : INFO : Finished: 3160000 tags\n",
      "2017-04-10 22:05:10,578 : INFO : Finished: 3170000 tags\n",
      "2017-04-10 22:05:38,393 : INFO : Finished: 3180000 tags\n",
      "2017-04-10 22:06:06,642 : INFO : Finished: 3190000 tags\n",
      "2017-04-10 22:06:34,955 : INFO : Finished: 3200000 tags\n",
      "2017-04-10 22:07:01,934 : INFO : Finished: 3210000 tags\n",
      "2017-04-10 22:07:30,190 : INFO : Finished: 3220000 tags\n",
      "2017-04-10 22:07:58,698 : INFO : Finished: 3230000 tags\n",
      "2017-04-10 22:08:28,304 : INFO : Finished: 3240000 tags\n",
      "2017-04-10 22:08:56,368 : INFO : Finished: 3250000 tags\n",
      "2017-04-10 22:09:24,354 : INFO : Finished: 3260000 tags\n",
      "2017-04-10 22:09:52,716 : INFO : Finished: 3270000 tags\n",
      "2017-04-10 22:10:20,446 : INFO : Finished: 3280000 tags\n",
      "2017-04-10 22:10:47,319 : INFO : Finished: 3290000 tags\n",
      "2017-04-10 22:11:14,402 : INFO : Finished: 3300000 tags\n",
      "2017-04-10 22:11:41,121 : INFO : Finished: 3310000 tags\n",
      "2017-04-10 22:12:09,598 : INFO : Finished: 3320000 tags\n",
      "2017-04-10 22:12:36,732 : INFO : Finished: 3330000 tags\n",
      "2017-04-10 22:13:04,408 : INFO : Finished: 3340000 tags\n",
      "2017-04-10 22:13:32,789 : INFO : Finished: 3350000 tags\n",
      "2017-04-10 22:14:01,202 : INFO : Finished: 3360000 tags\n",
      "2017-04-10 22:14:29,586 : INFO : Finished: 3370000 tags\n",
      "2017-04-10 22:14:57,130 : INFO : Finished: 3380000 tags\n",
      "2017-04-10 22:14:59,156 : INFO : Loading new file for index: 100000\n",
      "2017-04-10 22:15:24,326 : INFO : Finished: 3390000 tags\n",
      "2017-04-10 22:15:50,028 : INFO : Finished: 3400000 tags\n",
      "2017-04-10 22:16:17,082 : INFO : Finished: 3410000 tags\n",
      "2017-04-10 22:16:45,427 : INFO : Finished: 3420000 tags\n",
      "2017-04-10 22:17:13,527 : INFO : Finished: 3430000 tags\n",
      "2017-04-10 22:17:41,445 : INFO : Finished: 3440000 tags\n",
      "2017-04-10 22:18:08,142 : INFO : Finished: 3450000 tags\n",
      "2017-04-10 22:18:35,188 : INFO : Finished: 3460000 tags\n",
      "2017-04-10 22:19:02,177 : INFO : Finished: 3470000 tags\n",
      "2017-04-10 22:19:30,203 : INFO : Finished: 3480000 tags\n",
      "2017-04-10 22:19:59,214 : INFO : Finished: 3490000 tags\n",
      "2017-04-10 22:20:25,992 : INFO : Finished: 3500000 tags\n",
      "2017-04-10 22:20:53,350 : INFO : Finished: 3510000 tags\n",
      "2017-04-10 22:21:21,044 : INFO : Finished: 3520000 tags\n",
      "2017-04-10 22:21:47,722 : INFO : Finished: 3530000 tags\n",
      "2017-04-10 22:22:13,796 : INFO : Finished: 3540000 tags\n",
      "2017-04-10 22:22:42,033 : INFO : Finished: 3550000 tags\n",
      "2017-04-10 22:23:08,977 : INFO : Finished: 3560000 tags\n",
      "2017-04-10 22:23:36,596 : INFO : Finished: 3570000 tags\n",
      "2017-04-10 22:24:02,494 : INFO : Finished: 3580000 tags\n",
      "2017-04-10 22:24:31,184 : INFO : Finished: 3590000 tags\n",
      "2017-04-10 22:24:57,940 : INFO : Finished: 3600000 tags\n",
      "2017-04-10 22:25:23,589 : INFO : Finished: 3610000 tags\n",
      "2017-04-10 22:25:49,296 : INFO : Finished: 3620000 tags\n",
      "2017-04-10 22:26:17,089 : INFO : Finished: 3630000 tags\n",
      "2017-04-10 22:26:45,633 : INFO : Finished: 3640000 tags\n",
      "2017-04-10 22:27:12,665 : INFO : Finished: 3650000 tags\n",
      "2017-04-10 22:27:38,711 : INFO : Finished: 3660000 tags\n",
      "2017-04-10 22:28:04,934 : INFO : Finished: 3670000 tags\n",
      "2017-04-10 22:28:33,191 : INFO : Finished: 3680000 tags\n",
      "2017-04-10 22:29:00,564 : INFO : Finished: 3690000 tags\n",
      "2017-04-10 22:29:30,458 : INFO : Finished: 3700000 tags\n",
      "2017-04-10 22:30:04,983 : INFO : Finished: 3710000 tags\n",
      "2017-04-10 22:30:31,781 : INFO : Finished: 3720000 tags\n",
      "2017-04-10 22:30:33,846 : INFO : Loading new file for index: 110000\n",
      "2017-04-10 22:30:59,281 : INFO : Finished: 3730000 tags\n",
      "2017-04-10 22:31:27,399 : INFO : Finished: 3740000 tags\n",
      "2017-04-10 22:31:55,546 : INFO : Finished: 3750000 tags\n",
      "2017-04-10 22:32:22,258 : INFO : Finished: 3760000 tags\n",
      "2017-04-10 22:32:52,147 : INFO : Finished: 3770000 tags\n",
      "2017-04-10 22:33:18,748 : INFO : Finished: 3780000 tags\n",
      "2017-04-10 22:33:46,620 : INFO : Finished: 3790000 tags\n",
      "2017-04-10 22:34:13,422 : INFO : Finished: 3800000 tags\n",
      "2017-04-10 22:34:40,849 : INFO : Finished: 3810000 tags\n",
      "2017-04-10 22:35:09,884 : INFO : Finished: 3820000 tags\n",
      "2017-04-10 22:35:37,714 : INFO : Finished: 3830000 tags\n",
      "2017-04-10 22:36:06,036 : INFO : Finished: 3840000 tags\n",
      "2017-04-10 22:36:34,070 : INFO : Finished: 3850000 tags\n",
      "2017-04-10 22:37:00,713 : INFO : Finished: 3860000 tags\n",
      "2017-04-10 22:37:28,362 : INFO : Finished: 3870000 tags\n",
      "2017-04-10 22:37:56,039 : INFO : Finished: 3880000 tags\n",
      "2017-04-10 22:38:21,711 : INFO : Finished: 3890000 tags\n",
      "2017-04-10 22:38:49,092 : INFO : Finished: 3900000 tags\n",
      "2017-04-10 22:39:15,997 : INFO : Finished: 3910000 tags\n",
      "2017-04-10 22:39:43,453 : INFO : Finished: 3920000 tags\n",
      "2017-04-10 22:40:10,688 : INFO : Finished: 3930000 tags\n",
      "2017-04-10 22:40:39,118 : INFO : Finished: 3940000 tags\n",
      "2017-04-10 22:41:06,007 : INFO : Finished: 3950000 tags\n",
      "2017-04-10 22:41:34,242 : INFO : Finished: 3960000 tags\n",
      "2017-04-10 22:42:01,993 : INFO : Finished: 3970000 tags\n",
      "2017-04-10 22:42:28,767 : INFO : Finished: 3980000 tags\n",
      "2017-04-10 22:42:56,159 : INFO : Finished: 3990000 tags\n",
      "2017-04-10 22:43:26,434 : INFO : Finished: 4000000 tags\n",
      "2017-04-10 22:43:53,928 : INFO : Finished: 4010000 tags\n",
      "2017-04-10 22:44:22,177 : INFO : Finished: 4020000 tags\n",
      "2017-04-10 22:44:50,423 : INFO : Finished: 4030000 tags\n",
      "2017-04-10 22:45:18,583 : INFO : Finished: 4040000 tags\n",
      "2017-04-10 22:45:47,246 : INFO : Finished: 4050000 tags\n",
      "2017-04-10 22:46:15,022 : INFO : Finished: 4060000 tags\n",
      "2017-04-10 22:46:17,004 : INFO : Loading new file for index: 120000\n",
      "2017-04-10 22:46:42,068 : INFO : Finished: 4070000 tags\n",
      "2017-04-10 22:47:10,228 : INFO : Finished: 4080000 tags\n",
      "2017-04-10 22:47:37,623 : INFO : Finished: 4090000 tags\n",
      "2017-04-10 22:48:04,704 : INFO : Finished: 4100000 tags\n",
      "2017-04-10 22:48:31,406 : INFO : Finished: 4110000 tags\n",
      "2017-04-10 22:48:58,322 : INFO : Finished: 4120000 tags\n",
      "2017-04-10 22:49:26,067 : INFO : Finished: 4130000 tags\n",
      "2017-04-10 22:49:53,485 : INFO : Finished: 4140000 tags\n",
      "2017-04-10 22:50:20,379 : INFO : Finished: 4150000 tags\n",
      "2017-04-10 22:50:48,293 : INFO : Finished: 4160000 tags\n",
      "2017-04-10 22:51:15,467 : INFO : Finished: 4170000 tags\n",
      "2017-04-10 22:51:40,793 : INFO : Finished: 4180000 tags\n",
      "2017-04-10 22:52:08,429 : INFO : Finished: 4190000 tags\n",
      "2017-04-10 22:52:34,771 : INFO : Finished: 4200000 tags\n",
      "2017-04-10 22:53:00,089 : INFO : Finished: 4210000 tags\n",
      "2017-04-10 22:53:28,612 : INFO : Finished: 4220000 tags\n",
      "2017-04-10 22:53:56,715 : INFO : Finished: 4230000 tags\n",
      "2017-04-10 22:54:25,676 : INFO : Finished: 4240000 tags\n",
      "2017-04-10 22:54:54,828 : INFO : Finished: 4250000 tags\n",
      "2017-04-10 22:55:21,702 : INFO : Finished: 4260000 tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-10 22:55:48,533 : INFO : Finished: 4270000 tags\n",
      "2017-04-10 22:56:18,776 : INFO : Finished: 4280000 tags\n",
      "2017-04-10 22:56:47,182 : INFO : Finished: 4290000 tags\n",
      "2017-04-10 22:57:15,601 : INFO : Finished: 4300000 tags\n",
      "2017-04-10 22:57:43,475 : INFO : Finished: 4310000 tags\n",
      "2017-04-10 22:58:10,593 : INFO : Finished: 4320000 tags\n",
      "2017-04-10 22:58:39,663 : INFO : Finished: 4330000 tags\n",
      "2017-04-10 22:59:06,996 : INFO : Finished: 4340000 tags\n"
     ]
    }
   ],
   "source": [
    "Xv = get_extended_docs_with_inference_data_only(doc2vec_model, VALIDATION_DICT, \n",
    "                                         validation_preprocessed_files_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
