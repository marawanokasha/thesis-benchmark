{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import cPickle as pickle\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import urllib2\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "\n",
    "from multiprocessing import Pool as ThreadPool\n",
    "import itertools\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import nltk\n",
    "\n",
    "from thesis.utils.text import get_sentences, sentence_wordtokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_location = \"/mnt/data2/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "# training_file = root_location + 'docs_output_training_validation_documents_' + str(SAMPLE_RATIO)\n",
    "training_file = root_location + 'docs_output.json'\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "# training_docs_list_file = exports_location + \"training_documents_\" + str(SAMPLE_RATIO) + \"_sample.pkl\"\n",
    "# validation_docs_list_file = exports_location + \"validation_documents_\" + str(SAMPLE_RATIO) + \"_sample.pkl\"\n",
    "training_docs_list_file = exports_location + \"extended_pv_training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"extended_pv_validation_docs_list.pkl\"\n",
    "test_docs_list_file = exports_location + \"extended_pv_test_docs_list.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 s, sys: 748 ms, total: 17.8 s\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120156"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ES_URL = 'http://localhost:9200/patents/patent/{}'\n",
    "HEADING_TAG = 'heading'\n",
    "PARAGRAPH_TAG = 'p'\n",
    "UL_TAG = 'ul'\n",
    "LI_TAG = 'li'\n",
    "OL_TAG = 'ol'\n",
    "DESC_OF_DRAWINGS_TAG = 'description-of-drawings'\n",
    "MIN_PARAGRAPH_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_with_previous(curr_node_tag, previous_node_tag, previous_node_text):\n",
    "    if curr_node_tag == PARAGRAPH_TAG and previous_node_tag == HEADING_TAG:\n",
    "        return True\n",
    "    if previous_node_text and len(previous_node_text) < MIN_PARAGRAPH_LENGTH:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def get_paragraphs(root):\n",
    "    paragraphs = []\n",
    "    previous_node_text = None\n",
    "    previous_tag = None\n",
    "    for child in root:\n",
    "        node_text = None\n",
    "        if child.tag != DESC_OF_DRAWINGS_TAG:\n",
    "            node_text = get_node_text(child)\n",
    "            if node_text.strip():\n",
    "                if merge_with_previous(child.tag, previous_tag, previous_node_text) and len(paragraphs) > 0:\n",
    "                    paragraphs[-1] += ' ' + node_text\n",
    "                else:\n",
    "                    paragraphs.append(node_text)\n",
    "        else:\n",
    "            node_text = extract_desc_of_drawings_paragraph(child)\n",
    "            paragraphs.append(node_text)\n",
    "            \n",
    "        previous_tag = child.tag\n",
    "        previous_node_text = node_text\n",
    "    return paragraphs\n",
    "    \n",
    "def extract_desc_of_drawings_paragraph(node):\n",
    "    previous_tag = None\n",
    "    sentences = []\n",
    "    for child in node:\n",
    "        node_text = get_node_text(child)\n",
    "        if child.tag == PARAGRAPH_TAG and previous_tag == HEADING_TAG:\n",
    "            sentences[-1] += ' ' + node_text\n",
    "        else:\n",
    "            # a paragraph in drawings descriptions is treated as a sentence\n",
    "            if child.tag == PARAGRAPH_TAG:\n",
    "                node_text = apply_sentence_end(node_text)\n",
    "            sentences.append(node_text)\n",
    "        previous_tag = child.tag\n",
    "    \n",
    "    return ' '.join(sentences)\n",
    "\n",
    "def apply_sentence_end(text):\n",
    "    if text and text.strip():\n",
    "        text = text.strip().strip(';.')\n",
    "        text += '. '\n",
    "    return text\n",
    "\n",
    "def itertext_custom(self):\n",
    "    tag = self.tag\n",
    "    if not isinstance(tag, basestring) and tag is not None:\n",
    "        return\n",
    "    if self.text:\n",
    "        if tag == LI_TAG:\n",
    "            yield apply_sentence_end(self.text)\n",
    "        else:\n",
    "            yield self.text.replace('\\n',' ')\n",
    "    for e in self:\n",
    "        for s in e.itertext_custom():\n",
    "            yield s\n",
    "        if e.tail:\n",
    "            yield e.tail\n",
    "\n",
    "ET.Element.itertext_custom = itertext_custom\n",
    "# def get_node_text(node):\n",
    "#     node_text = ''\n",
    "#     for child in node:\n",
    "#         # for ul tags, get li tags as sentences\n",
    "#         if child.tag == UL_TAG:\n",
    "#             li_sentences = [apply_sentence_end(get_node_text_iterative(c)) for c in child]\n",
    "#             child_text = ' '.join(li_sentences)\n",
    "#         else:\n",
    "#             child_text = get_node_text_iterative(child)\n",
    "#         node_text += child_text\n",
    "#     return node_text\n",
    "        \n",
    "get_node_text = lambda node: ''.join(node.itertext_custom()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conc_paragraphs(parag1, parag2):\n",
    "    return parag1.strip('.') + '.' + ' ' + parag2\n",
    "\n",
    "def concatenate_sentences_to_paragraphs(paragraphs):\n",
    "    \"\"\"\n",
    "    for 1 sentence paragraphs, concatenate them to the next or previous paragraph depending on context\n",
    "    \"\"\"\n",
    "    for i in range(len(paragraphs)):\n",
    "        if i >= len((paragraphs)): break\n",
    "        parag = paragraphs[i]\n",
    "        sentences = get_sentences(parag)\n",
    "        \n",
    "        if len(sentences) == 1:\n",
    "            prev_paragraph = paragraphs[i-1] if i-1 >= 0 else None\n",
    "            next_paragraph = paragraphs[i+1] if i+1 < len(paragraphs) else None\n",
    "\n",
    "            if (next_paragraph and len(get_sentences(next_paragraph)) == 1):\n",
    "                # If a series of 1 sentence length paragraphs exist, conc all of them in one paragraph\n",
    "                while True:\n",
    "                    if next_paragraph and len(get_sentences(next_paragraph)) == 1:\n",
    "                        parag = conc_paragraphs(parag, next_paragraph)\n",
    "                        paragraphs[i] = parag\n",
    "                        del paragraphs[i+1]\n",
    "\n",
    "                        # reinitialize for loop\n",
    "                        next_paragraph = paragraphs[i+1] if i+1 < len(paragraphs) else None\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "            # otherwise, just concatenate the 1 sentence paragraph to the previous paragraph\n",
    "            elif prev_paragraph:\n",
    "#                 print '============== Found prev eligible paragraph'\n",
    "                prev_paragraph = conc_paragraphs(prev_paragraph, parag)\n",
    "                paragraphs[i-1] = prev_paragraph\n",
    "                del paragraphs[i]\n",
    "\n",
    "            # if this is the first paragraph, then just concatenate it with the next one\n",
    "            elif next_paragraph:\n",
    "                parag = conc_paragraphs(parag, next_paragraph)\n",
    "                paragraphs[i] = parag\n",
    "                del paragraphs[i+1]\n",
    "\n",
    "def get_adjusted_paragraphs(root):\n",
    "    paragraphs = get_paragraphs(root)\n",
    "    concatenate_sentences_to_paragraphs(paragraphs)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_patent_description(doc_id):\n",
    "    url_to_fetch = ES_URL.format(doc_id)\n",
    "\n",
    "    response = urllib2.urlopen(url_to_fetch)\n",
    "    patent_content = response.read()\n",
    "\n",
    "    patent_object = json.loads(patent_content)['_source']\n",
    "    desc = patent_object['description'][0]\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SENTENCE_ID = \"{}_p{}_s{}\"\n",
    "PARAGRAPH_ID = \"{}_p{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000\n",
    "\n",
    "preprocessed_location = \"/mnt/data/shalaby/\" + \"preprocessed_data/\"\n",
    "TRAINING_PREPROCESSED_FILES_PREFIX = preprocessed_location + \"extended_pv_training_docs_data_preprocessed-\"\n",
    "VALIDATION_PREPROCESSED_FILES_PREFIX = preprocessed_location + \"extended_pv_validation_docs_data_preprocessed-\"\n",
    "TEST_PREPROCESSED_FILES_PREFIX = preprocessed_location + \"extended_pv_test_docs_data_preprocessed-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multithreaded_extended_batch_creation(start_index):\n",
    "\n",
    "    if os.path.exists(FILE_PREFIX + str(start_index)):\n",
    "        info(\"Batch {} already exists, skipping..\".format(start_index))\n",
    "        return\n",
    "    \n",
    "    info(\"Batch creation working on {}\\n\".format(start_index))\n",
    "    token_lines = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for doc_index, doc_id in enumerate(DOCS_LIST[start_index:]):\n",
    "        desc = get_patent_description(doc_id)\n",
    "        root = ET.fromstring(desc.encode('utf-8'))\n",
    "\n",
    "        # lists of list of tokens in sentence, paragraph\n",
    "        all_sentences_tokens_list = []\n",
    "        all_paragraphs_tokens_list = []\n",
    "        \n",
    "        # just one list of all tokens in the doc\n",
    "        doc_tokens_list = [doc_id] \n",
    "        \n",
    "        # get paragraphs\n",
    "        paragraphs = get_adjusted_paragraphs(root)\n",
    "\n",
    "        for parag_index, parag in enumerate(paragraphs):\n",
    "            paragraph_id = PARAGRAPH_ID.format(doc_id, parag_index+1)\n",
    "            curr_paragraph_tokens = [paragraph_id]\n",
    "            \n",
    "            # get sentences in paragraphs\n",
    "            parag_sentences = get_sentences(parag)\n",
    "            for sentence_index, parag_sent in enumerate(parag_sentences):\n",
    "                sentence_id = SENTENCE_ID.format(doc_id, parag_index+1, sentence_index+1)\n",
    "                curr_sentence_tokens = sentence_wordtokenizer(parag_sent)\n",
    "                \n",
    "                if len(curr_sentence_tokens) > 0:\n",
    "                    all_sentences_tokens_list.append([sentence_id] + curr_sentence_tokens)\n",
    "\n",
    "                    # we do this incrementally instead of tokenizing the whole paragraph/doc again\n",
    "                    curr_paragraph_tokens.extend(curr_sentence_tokens)\n",
    "                    doc_tokens_list.extend(curr_sentence_tokens)\n",
    "                \n",
    "            all_paragraphs_tokens_list.append(curr_paragraph_tokens)\n",
    "            del parag_sentences\n",
    "\n",
    "        # now add the document tokens and the sentence tokens to the list that will be written to the file\n",
    "        token_lines.append(doc_tokens_list)\n",
    "        token_lines.extend(all_paragraphs_tokens_list)\n",
    "        token_lines.extend(all_sentences_tokens_list)\n",
    "\n",
    "        del paragraphs, desc\n",
    "        if doc_index % 1000 == 0: info(\"Doc: {:6} -> Total Lines to write: {:8}\".format(start_index + doc_index, len(token_lines)))\n",
    "        if doc_index >= BATCH_SIZE - 1:\n",
    "            break\n",
    "    duration = time.time() - start_time\n",
    "    info(\"Finished batch {} of size {:d} in {:.0f}m {:.0f}s\".format(start_index, BATCH_SIZE, * divmod(duration, 60)))\n",
    "    info(\"For index {}, the actual number of lines written is: {}\".format(start_index, len(token_lines)))\n",
    "    \n",
    "    write_batch(FILE_PREFIX, token_lines, start_index)\n",
    "    del token_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_batch(file_prefix, batch_lines, batch_start):\n",
    "    if len(batch_lines):\n",
    "        print \"writing batch %d\" % batch_start\n",
    "        with open(file_prefix + str(batch_start), 'w') as batch_file:\n",
    "            for line in batch_lines:\n",
    "                batch_file.write((u\" \".join(line) + \"\\n\").encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOCS_LIST = training_docs_list\n",
    "FILE_PREFIX = TRAINING_PREPROCESSED_FILES_PREFIX\n",
    "SAMPLE_SIZE = len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-08 02:09:14,865 : INFO : Batch creation working on 10000\n",
      "\n",
      "2017-03-08 02:09:14,865 : INFO : Batch creation working on 0\n",
      "\n",
      "2017-03-08 02:09:14,867 : INFO : Batch creation working on 50000\n",
      "\n",
      "2017-03-08 02:09:14,865 : INFO : Batch creation working on 40000\n",
      "\n",
      "2017-03-08 02:09:14,865 : INFO : Batch creation working on 30000\n",
      "\n",
      "2017-03-08 02:09:14,865 : INFO : Batch creation working on 20000\n",
      "\n",
      "2017-03-08 02:09:14,974 : INFO : Doc:  50000 -> Total Lines to write:       75\n",
      "2017-03-08 02:09:15,030 : INFO : Doc:  20000 -> Total Lines to write:      267\n",
      "2017-03-08 02:09:15,033 : INFO : Doc:      0 -> Total Lines to write:      174\n",
      "2017-03-08 02:09:15,039 : INFO : Doc:  30000 -> Total Lines to write:      327\n",
      "2017-03-08 02:09:15,077 : INFO : Doc:  40000 -> Total Lines to write:      263\n",
      "2017-03-08 02:09:15,131 : INFO : Doc:  10000 -> Total Lines to write:      283\n",
      "2017-03-08 02:10:52,977 : INFO : Doc:  21000 -> Total Lines to write:   329036\n",
      "2017-03-08 02:10:53,302 : INFO : Doc:  51000 -> Total Lines to write:   328643\n",
      "2017-03-08 02:10:59,477 : INFO : Doc:   1000 -> Total Lines to write:   293103\n",
      "2017-03-08 02:11:10,468 : INFO : Doc:  41000 -> Total Lines to write:   323631\n",
      "2017-03-08 02:11:10,864 : INFO : Doc:  31000 -> Total Lines to write:   336742\n",
      "2017-03-08 02:11:14,801 : INFO : Doc:  11000 -> Total Lines to write:   336877\n",
      "2017-03-08 02:12:30,591 : INFO : Doc:   2000 -> Total Lines to write:   567809\n",
      "2017-03-08 02:12:32,743 : INFO : Doc:  22000 -> Total Lines to write:   656265\n",
      "2017-03-08 02:12:36,687 : INFO : Doc:  52000 -> Total Lines to write:   662266\n",
      "2017-03-08 02:12:59,013 : INFO : Doc:  32000 -> Total Lines to write:   649030\n",
      "2017-03-08 02:13:07,813 : INFO : Doc:  12000 -> Total Lines to write:   648026\n",
      "2017-03-08 02:13:24,488 : INFO : Doc:  42000 -> Total Lines to write:   696896\n",
      "2017-03-08 02:14:10,624 : INFO : Doc:  23000 -> Total Lines to write:   965969\n",
      "2017-03-08 02:14:16,294 : INFO : Doc:   3000 -> Total Lines to write:   863524\n",
      "2017-03-08 02:14:17,820 : INFO : Doc:  53000 -> Total Lines to write:   988208\n",
      "2017-03-08 02:14:51,761 : INFO : Doc:  33000 -> Total Lines to write:   974585\n",
      "2017-03-08 02:15:04,324 : INFO : Doc:  13000 -> Total Lines to write:   974145\n",
      "2017-03-08 02:15:21,632 : INFO : Doc:  43000 -> Total Lines to write:  1017212\n",
      "2017-03-08 02:15:58,776 : INFO : Doc:  24000 -> Total Lines to write:  1299616\n",
      "2017-03-08 02:16:06,178 : INFO : Doc:   4000 -> Total Lines to write:  1185313\n",
      "2017-03-08 02:16:13,084 : INFO : Doc:  54000 -> Total Lines to write:  1345885\n",
      "2017-03-08 02:16:51,769 : INFO : Doc:  34000 -> Total Lines to write:  1314041\n",
      "2017-03-08 02:16:52,816 : INFO : Doc:  14000 -> Total Lines to write:  1277711\n",
      "2017-03-08 02:17:26,644 : INFO : Doc:  44000 -> Total Lines to write:  1358034\n",
      "2017-03-08 02:17:40,811 : INFO : Doc:  25000 -> Total Lines to write:  1610550\n",
      "2017-03-08 02:17:59,253 : INFO : Doc:   5000 -> Total Lines to write:  1503055\n",
      "2017-03-08 02:17:59,268 : INFO : Doc:  55000 -> Total Lines to write:  1686392\n",
      "2017-03-08 02:18:34,537 : INFO : Doc:  15000 -> Total Lines to write:  1568371\n",
      "2017-03-08 02:18:41,688 : INFO : Doc:  35000 -> Total Lines to write:  1635523\n",
      "2017-03-08 02:19:30,296 : INFO : Doc:  26000 -> Total Lines to write:  1936436\n",
      "2017-03-08 02:19:33,409 : INFO : Doc:  45000 -> Total Lines to write:  1704161\n",
      "2017-03-08 02:20:03,999 : INFO : Doc:  56000 -> Total Lines to write:  2032579\n",
      "2017-03-08 02:20:23,160 : INFO : Doc:   6000 -> Total Lines to write:  1881074\n",
      "2017-03-08 02:20:49,941 : INFO : Doc:  16000 -> Total Lines to write:  1909735\n",
      "2017-03-08 02:21:11,440 : INFO : Doc:  36000 -> Total Lines to write:  1993846\n",
      "2017-03-08 02:21:40,196 : INFO : Doc:  27000 -> Total Lines to write:  2250591\n",
      "2017-03-08 02:21:55,216 : INFO : Doc:  46000 -> Total Lines to write:  2045259\n",
      "2017-03-08 02:22:35,827 : INFO : Doc:  57000 -> Total Lines to write:  2399380\n",
      "2017-03-08 02:22:44,484 : INFO : Doc:   7000 -> Total Lines to write:  2201328\n",
      "2017-03-08 02:23:11,331 : INFO : Doc:  17000 -> Total Lines to write:  2244499\n",
      "2017-03-08 02:23:38,546 : INFO : Doc:  37000 -> Total Lines to write:  2320694\n",
      "2017-03-08 02:23:54,144 : INFO : Doc:  28000 -> Total Lines to write:  2570440\n",
      "2017-03-08 02:24:26,047 : INFO : Doc:  47000 -> Total Lines to write:  2397387\n",
      "2017-03-08 02:24:52,957 : INFO : Doc:   8000 -> Total Lines to write:  2497529\n",
      "2017-03-08 02:24:58,195 : INFO : Doc:  58000 -> Total Lines to write:  2746268\n",
      "2017-03-08 02:25:40,265 : INFO : Doc:  18000 -> Total Lines to write:  2579741\n",
      "2017-03-08 02:26:00,947 : INFO : Doc:  29000 -> Total Lines to write:  2875848\n",
      "2017-03-08 02:26:03,552 : INFO : Doc:  38000 -> Total Lines to write:  2651942\n",
      "2017-03-08 02:26:48,933 : INFO : Doc:  48000 -> Total Lines to write:  2743574\n",
      "2017-03-08 02:27:16,254 : INFO : Doc:  59000 -> Total Lines to write:  3079768\n",
      "2017-03-08 02:27:26,554 : INFO : Doc:   9000 -> Total Lines to write:  2856080\n",
      "2017-03-08 02:28:00,356 : INFO : Doc:  19000 -> Total Lines to write:  2892843\n",
      "2017-03-08 02:28:17,501 : INFO : Doc:  39000 -> Total Lines to write:  2966662\n",
      "2017-03-08 02:28:21,573 : INFO : Finished batch 20000 of size 10000 in 19m 7s\n",
      "2017-03-08 02:28:21,578 : INFO : For index 20000, the actual number of lines written is: 3200378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-08 02:28:45,406 : INFO : Batch creation working on 60000\n",
      "\n",
      "2017-03-08 02:28:45,809 : INFO : Doc:  60000 -> Total Lines to write:      157\n",
      "2017-03-08 02:29:24,680 : INFO : Doc:  49000 -> Total Lines to write:  3099990\n",
      "2017-03-08 02:29:51,038 : INFO : Finished batch 50000 of size 10000 in 20m 36s\n",
      "2017-03-08 02:29:51,041 : INFO : For index 50000, the actual number of lines written is: 3432568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-08 02:29:53,004 : INFO : Finished batch 0 of size 10000 in 20m 38s\n",
      "2017-03-08 02:29:53,006 : INFO : For index 0, the actual number of lines written is: 3173898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-08 02:30:15,113 : INFO : Batch creation working on 70000\n",
      "\n",
      "2017-03-08 02:30:15,132 : INFO : Batch creation working on 80000\n",
      "\n",
      "2017-03-08 02:30:15,823 : INFO : Doc:  70000 -> Total Lines to write:      346\n",
      "2017-03-08 02:30:16,160 : INFO : Doc:  80000 -> Total Lines to write:     1184\n",
      "2017-03-08 02:30:43,971 : INFO : Finished batch 10000 of size 10000 in 21m 29s\n",
      "2017-03-08 02:30:43,976 : INFO : For index 10000, the actual number of lines written is: 3195411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-08 02:31:05,493 : INFO : Batch creation working on 90000\n",
      "\n",
      "2017-03-08 02:31:06,562 : INFO : Doc:  90000 -> Total Lines to write:      172\n",
      "2017-03-08 02:31:21,877 : INFO : Finished batch 30000 of size 10000 in 22m 7s\n",
      "2017-03-08 02:31:21,881 : INFO : For index 30000, the actual number of lines written is: 3310449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-08 02:31:43,766 : INFO : Batch creation working on 100000\n",
      "\n",
      "2017-03-08 02:31:44,433 : INFO : Doc: 100000 -> Total Lines to write:      236\n",
      "2017-03-08 02:31:50,654 : INFO : Doc:  61000 -> Total Lines to write:   367284\n",
      "2017-03-08 02:32:41,747 : INFO : Finished batch 40000 of size 10000 in 23m 27s\n",
      "2017-03-08 02:32:41,751 : INFO : For index 40000, the actual number of lines written is: 3440447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-08 02:33:04,569 : INFO : Batch creation working on 110000\n",
      "\n",
      "2017-03-08 02:33:07,535 : INFO : Doc: 110000 -> Total Lines to write:      839\n",
      "2017-03-08 02:33:31,181 : INFO : Doc:  81000 -> Total Lines to write:   344219\n",
      "2017-03-08 02:33:36,290 : INFO : Doc:  71000 -> Total Lines to write:   349699\n",
      "2017-03-08 02:34:22,483 : INFO : Doc:  91000 -> Total Lines to write:   343071\n",
      "2017-03-08 02:34:42,892 : INFO : Doc:  62000 -> Total Lines to write:   683732\n",
      "2017-03-08 02:35:04,733 : INFO : Doc: 101000 -> Total Lines to write:   355971\n",
      "2017-03-08 02:36:00,055 : INFO : Doc: 111000 -> Total Lines to write:   385464\n",
      "2017-03-08 02:36:23,209 : INFO : Doc:  72000 -> Total Lines to write:   687879\n",
      "2017-03-08 02:36:31,062 : INFO : Doc:  82000 -> Total Lines to write:   712707\n",
      "2017-03-08 02:37:24,486 : INFO : Doc:  92000 -> Total Lines to write:   698973\n",
      "2017-03-08 02:37:28,270 : INFO : Doc:  63000 -> Total Lines to write:  1037330\n",
      "2017-03-08 02:37:48,410 : INFO : Doc: 102000 -> Total Lines to write:   706878\n",
      "2017-03-08 02:38:55,320 : INFO : Doc: 112000 -> Total Lines to write:   766457\n",
      "2017-03-08 02:39:17,198 : INFO : Doc:  73000 -> Total Lines to write:  1040330\n",
      "2017-03-08 02:39:23,066 : INFO : Doc:  83000 -> Total Lines to write:  1047836\n",
      "2017-03-08 02:40:14,205 : INFO : Doc:  64000 -> Total Lines to write:  1383288\n",
      "2017-03-08 02:40:21,183 : INFO : Doc:  93000 -> Total Lines to write:  1062070\n",
      "2017-03-08 02:40:43,570 : INFO : Doc: 103000 -> Total Lines to write:  1090683\n",
      "2017-03-08 02:41:50,835 : INFO : Doc: 113000 -> Total Lines to write:  1159456\n",
      "2017-03-08 02:41:57,537 : INFO : Doc:  74000 -> Total Lines to write:  1373663\n",
      "2017-03-08 02:42:34,394 : INFO : Doc:  84000 -> Total Lines to write:  1425371\n",
      "2017-03-08 02:42:56,725 : INFO : Doc:  65000 -> Total Lines to write:  1718135\n",
      "2017-03-08 02:43:27,195 : INFO : Doc:  94000 -> Total Lines to write:  1444716\n",
      "2017-03-08 02:43:29,477 : INFO : Doc: 104000 -> Total Lines to write:  1446364\n",
      "2017-03-08 02:44:36,811 : INFO : Doc: 114000 -> Total Lines to write:  1515406\n",
      "2017-03-08 02:44:49,587 : INFO : Doc:  75000 -> Total Lines to write:  1739221\n",
      "2017-03-08 02:45:29,171 : INFO : Doc:  85000 -> Total Lines to write:  1761224\n",
      "2017-03-08 02:45:38,689 : INFO : Doc:  66000 -> Total Lines to write:  2057932\n",
      "2017-03-08 02:46:13,572 : INFO : Doc: 105000 -> Total Lines to write:  1801946\n",
      "2017-03-08 02:46:19,490 : INFO : Doc:  95000 -> Total Lines to write:  1821055\n",
      "2017-03-08 02:47:24,367 : INFO : Doc: 115000 -> Total Lines to write:  1894712\n",
      "2017-03-08 02:47:24,777 : INFO : Doc:  76000 -> Total Lines to write:  2078732\n",
      "2017-03-08 02:48:17,306 : INFO : Doc:  67000 -> Total Lines to write:  2386418\n",
      "2017-03-08 02:48:20,812 : INFO : Doc:  86000 -> Total Lines to write:  2108355\n",
      "2017-03-08 02:48:58,691 : INFO : Doc: 106000 -> Total Lines to write:  2143727\n",
      "2017-03-08 02:49:06,969 : INFO : Doc:  96000 -> Total Lines to write:  2173101\n",
      "2017-03-08 02:50:03,518 : INFO : Doc:  77000 -> Total Lines to write:  2419119\n",
      "2017-03-08 02:50:31,599 : INFO : Doc: 116000 -> Total Lines to write:  2301783\n",
      "2017-03-08 02:51:21,306 : INFO : Doc:  68000 -> Total Lines to write:  2771510\n",
      "2017-03-08 02:51:27,226 : INFO : Doc:  87000 -> Total Lines to write:  2477633\n",
      "2017-03-08 02:51:43,824 : INFO : Doc: 107000 -> Total Lines to write:  2503836\n",
      "2017-03-08 02:51:47,022 : INFO : Doc:  97000 -> Total Lines to write:  2522317\n",
      "2017-03-08 02:52:58,804 : INFO : Doc:  78000 -> Total Lines to write:  2782299\n",
      "2017-03-08 02:53:43,849 : INFO : Doc: 117000 -> Total Lines to write:  2723874\n",
      "2017-03-08 02:53:45,536 : INFO : Doc:  69000 -> Total Lines to write:  3092289\n",
      "2017-03-08 02:54:11,894 : INFO : Doc:  88000 -> Total Lines to write:  2830428\n",
      "2017-03-08 02:54:29,207 : INFO : Doc: 108000 -> Total Lines to write:  2876160\n",
      "2017-03-08 02:54:30,101 : INFO : Doc:  98000 -> Total Lines to write:  2880190\n",
      "2017-03-08 02:55:52,504 : INFO : Doc:  79000 -> Total Lines to write:  3145046\n",
      "2017-03-08 02:56:46,398 : INFO : Finished batch 60000 of size 10000 in 28m 1s\n",
      "2017-03-08 02:56:46,401 : INFO : For index 60000, the actual number of lines written is: 3447934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-08 02:57:03,185 : INFO : Doc: 118000 -> Total Lines to write:  3116340\n",
      "2017-03-08 02:57:12,487 : INFO : Batch creation working on 120000\n",
      "\n",
      "2017-03-08 02:57:12,642 : INFO : Doc: 120000 -> Total Lines to write:      136\n",
      "2017-03-08 02:57:27,585 : INFO : Doc: 109000 -> Total Lines to write:  3235133\n",
      "2017-03-08 02:57:32,701 : INFO : Doc:  99000 -> Total Lines to write:  3232649\n",
      "2017-03-08 02:57:34,690 : INFO : Doc:  89000 -> Total Lines to write:  3181306\n",
      "2017-03-08 02:57:42,056 : INFO : Finished batch 120000 of size 10000 in 0m 30s\n",
      "2017-03-08 02:57:42,059 : INFO : For index 120000, the actual number of lines written is: 56442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 120000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-08 02:58:44,958 : INFO : Finished batch 70000 of size 10000 in 28m 30s\n",
      "2017-03-08 02:58:44,962 : INFO : For index 70000, the actual number of lines written is: 3481088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-08 02:59:38,516 : INFO : Doc: 119000 -> Total Lines to write:  3454499\n",
      "2017-03-08 03:00:18,424 : INFO : Finished batch 90000 of size 10000 in 29m 13s\n",
      "2017-03-08 03:00:18,429 : INFO : For index 90000, the actual number of lines written is: 3605520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-08 03:00:22,717 : INFO : Finished batch 100000 of size 10000 in 28m 39s\n",
      "2017-03-08 03:00:22,721 : INFO : For index 100000, the actual number of lines written is: 3594460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-08 03:00:58,523 : INFO : Finished batch 80000 of size 10000 in 30m 43s\n",
      "2017-03-08 03:00:58,528 : INFO : For index 80000, the actual number of lines written is: 3561721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 80000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-08 03:01:46,356 : INFO : Finished batch 110000 of size 10000 in 28m 42s\n",
      "2017-03-08 03:01:46,361 : INFO : For index 110000, the actual number of lines written is: 3696658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 110000\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pool = ThreadPool(6) # use just 6 because every batch requires a lot of memory\n",
    "    # +1 since range is end-exclusive\n",
    "    batches = range(0, (divmod(SAMPLE_SIZE, BATCH_SIZE)[0]+1) * BATCH_SIZE, BATCH_SIZE )\n",
    "    indices = pool.map(multithreaded_extended_batch_creation, batches)\n",
    "    pool.close()\n",
    "    pool.terminate()\n",
    "finally:\n",
    "    pool.close()\n",
    "    pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multithreaded_extended_batch_creation(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOCS_LIST = validation_docs_list\n",
    "FILE_PREFIX = VALIDATION_PREPROCESSED_FILES_PREFIX\n",
    "SAMPLE_SIZE = len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-08 03:21:11,761 : INFO : Batch creation working on 20000\n",
      "\n",
      "2017-03-08 03:21:11,761 : INFO : Batch creation working on 10000\n",
      "\n",
      "2017-03-08 03:21:11,760 : INFO : Batch creation working on 0\n",
      "\n",
      "2017-03-08 03:21:12,077 : INFO : Doc:      0 -> Total Lines to write:       72\n",
      "2017-03-08 03:21:12,111 : INFO : Doc:  20000 -> Total Lines to write:      116\n",
      "2017-03-08 03:21:12,166 : INFO : Doc:  10000 -> Total Lines to write:      372\n",
      "2017-03-08 03:23:09,621 : INFO : Doc:   1000 -> Total Lines to write:   307968\n",
      "2017-03-08 03:23:25,492 : INFO : Doc:  11000 -> Total Lines to write:   343452\n",
      "2017-03-08 03:23:33,229 : INFO : Doc:  21000 -> Total Lines to write:   358265\n",
      "2017-03-08 03:25:04,002 : INFO : Doc:   2000 -> Total Lines to write:   607301\n",
      "2017-03-08 03:25:40,539 : INFO : Doc:  12000 -> Total Lines to write:   696745\n",
      "2017-03-08 03:25:52,468 : INFO : Doc:  22000 -> Total Lines to write:   710605\n",
      "2017-03-08 03:27:12,268 : INFO : Doc:   3000 -> Total Lines to write:   910284\n",
      "2017-03-08 03:27:44,207 : INFO : Doc:  13000 -> Total Lines to write:  1028846\n",
      "2017-03-08 03:28:09,545 : INFO : Doc:  23000 -> Total Lines to write:  1062100\n",
      "2017-03-08 03:29:07,062 : INFO : Doc:   4000 -> Total Lines to write:  1205158\n",
      "2017-03-08 03:29:51,091 : INFO : Doc:  14000 -> Total Lines to write:  1344071\n",
      "2017-03-08 03:30:30,329 : INFO : Doc:  24000 -> Total Lines to write:  1420197\n",
      "2017-03-08 03:31:02,304 : INFO : Doc:   5000 -> Total Lines to write:  1505328\n",
      "2017-03-08 03:32:01,863 : INFO : Doc:  15000 -> Total Lines to write:  1674865\n",
      "2017-03-08 03:32:59,542 : INFO : Doc:  25000 -> Total Lines to write:  1783985\n",
      "2017-03-08 03:33:10,287 : INFO : Doc:   6000 -> Total Lines to write:  1794896\n",
      "2017-03-08 03:34:09,680 : INFO : Doc:  16000 -> Total Lines to write:  1983800\n",
      "2017-03-08 03:35:20,069 : INFO : Doc:   7000 -> Total Lines to write:  2100817\n",
      "2017-03-08 03:35:23,112 : INFO : Doc:  26000 -> Total Lines to write:  2129002\n",
      "2017-03-08 03:36:19,871 : INFO : Doc:  17000 -> Total Lines to write:  2310357\n",
      "2017-03-08 03:37:37,911 : INFO : Doc:   8000 -> Total Lines to write:  2412205\n",
      "2017-03-08 03:37:54,102 : INFO : Doc:  27000 -> Total Lines to write:  2498024\n",
      "2017-03-08 03:38:41,697 : INFO : Doc:  18000 -> Total Lines to write:  2650451\n",
      "2017-03-08 03:40:08,819 : INFO : Doc:   9000 -> Total Lines to write:  2717843\n",
      "2017-03-08 03:40:33,517 : INFO : Doc:  28000 -> Total Lines to write:  2846927\n",
      "2017-03-08 03:41:11,792 : INFO : Doc:  19000 -> Total Lines to write:  2970880\n",
      "2017-03-08 03:42:38,592 : INFO : Finished batch 0 of size 10000 in 21m 27s\n",
      "2017-03-08 03:42:38,597 : INFO : For index 0, the actual number of lines written is: 3044053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-08 03:42:57,921 : INFO : Doc:  29000 -> Total Lines to write:  3205532\n",
      "2017-03-08 03:43:31,627 : INFO : Finished batch 10000 of size 10000 in 22m 20s\n",
      "2017-03-08 03:43:31,632 : INFO : For index 10000, the actual number of lines written is: 3300118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-08 03:44:18,162 : INFO : Finished batch 20000 of size 10000 in 23m 6s\n",
      "2017-03-08 03:44:18,167 : INFO : For index 20000, the actual number of lines written is: 3405385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 20000\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pool = ThreadPool(16)\n",
    "    # +1 since range is end-exclusive\n",
    "    batches = range(0, (divmod(SAMPLE_SIZE, BATCH_SIZE)[0]+1) * BATCH_SIZE, BATCH_SIZE )\n",
    "    indices = pool.map(multithreaded_extended_batch_creation, batches)\n",
    "    pool.close()\n",
    "    pool.terminate()\n",
    "finally:\n",
    "    pool.close()\n",
    "    pool.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOCS_LIST = test_docs_list\n",
    "FILE_PREFIX = TEST_PREPROCESSED_FILES_PREFIX\n",
    "SAMPLE_SIZE = len(test_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-08 04:30:26,423 : INFO : Batch creation working on 0\n",
      "\n",
      "2017-03-08 04:30:26,423 : INFO : Batch creation working on 20000\n",
      "\n",
      "2017-03-08 04:30:26,423 : INFO : Batch creation working on 10000\n",
      "\n",
      "2017-03-08 04:30:26,423 : INFO : Batch creation working on 30000\n",
      "\n",
      "2017-03-08 04:30:48,394 : INFO : Doc:  10000 -> Total Lines to write:      105\n",
      "2017-03-08 04:30:48,491 : INFO : Doc:      0 -> Total Lines to write:       86\n",
      "2017-03-08 04:30:48,639 : INFO : Doc:  20000 -> Total Lines to write:      188\n",
      "2017-03-08 04:30:53,858 : INFO : Doc:  30000 -> Total Lines to write:      514\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pool = ThreadPool(16)\n",
    "    # +1 since range is end-exclusive\n",
    "    batches = range(0, (divmod(SAMPLE_SIZE, BATCH_SIZE)[0]+1) * BATCH_SIZE, BATCH_SIZE )\n",
    "    indices = pool.map(multithreaded_extended_batch_creation, batches)\n",
    "    pool.close()\n",
    "    pool.terminate()\n",
    "finally:\n",
    "    pool.close()\n",
    "    pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
