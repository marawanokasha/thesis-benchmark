{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates the doc2vec vector embeddings for a specific configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "import gzip\n",
    "\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "from functools import partial\n",
    "\n",
    "from thesis.utils.metrics import *\n",
    "from thesis.utils.file import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables used throughout the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_SEED = 1234\n",
    "DOC2VEC_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_WORD_COUNT = 100\n",
    "NUM_CORES = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GLOBAL_VARS = namedtuple('GLOBAL_VARS', ['MODEL_NAME', 'DOC2VEC_MODEL_NAME', 'DOC2VEC_MODEL', \n",
    "                                         'SVM_MODEL_NAME', 'NN_MODEL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_MODEL = \"vocab_model\"\n",
    "MODEL_PREFIX = \"model\"\n",
    "VALIDATION_DICT = \"validation_dict.pkl\"\n",
    "TEST_MATRIX = \"test_matrix.pkl\"\n",
    "TEST_DICT = \"test_dict.pkl\"\n",
    "METRICS = \"metrics.pkl\"\n",
    "CLASSIFIER = \"classifier.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_location = \"/mnt/virtual-machines/data/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "training_docs_list_file = exports_location + \"training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"validation_docs_list.pkl\"\n",
    "test_docs_list_file = exports_location + \"test_docs_list.pkl\"\n",
    "\n",
    "preprocessed_location = root_location + \"preprocessed_data/extended_pv_abs_desc_claims_full_chunks/\"\n",
    "\n",
    "training_preprocessed_files_prefix = preprocessed_location + \"extended_pv_training_docs_data_preprocessed-\"\n",
    "validation_preprocessed_files_prefix = preprocessed_location + \"extended_pv_validation_docs_data_preprocessed-\"\n",
    "test_preprocessed_files_prefix = preprocessed_location + \"extended_pv_test_docs_data_preprocessed-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load general data required for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.1 s, sys: 1.22 s, total: 19.3 s\n",
      "Wall time: 19.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286325"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321473"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401877"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VALIDATION_MINI_BATCH_SIZE = 100000\n",
    "def get_extended_docs_with_inference_data_only(doc2vec_model, file_to_write, preprocessed_files_prefix):\n",
    "    \"\"\"\n",
    "    Use the trained doc2vec model to get the paragraph vector representations of the validation or test documents\n",
    "    \"\"\"\n",
    "\n",
    "    def infer_one_doc(doc_tuple):\n",
    "        # doc2vec_model.random = np.random.RandomState(DOC2VEC_SEED)\n",
    "        doc_id, doc_tokens = doc_tuple\n",
    "        rep = doc2vec_model.infer_vector(doc_tokens)\n",
    "        return (doc_id, rep)\n",
    "\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write)):\n",
    "        info(\"===== Loading inference vectors\")\n",
    "        inference_documents_reps = pickle.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write)))\n",
    "        info(\"Loaded inference vectors matrix\")\n",
    "    else:\n",
    "        inference_documents_reps = {}\n",
    "        info(\"===== Getting vectors with inference\")\n",
    "\n",
    "        # Multi-threaded inference\n",
    "#         inference_docs_iterator = ExtendedPVDocumentBatchGenerator(preprocessed_files_prefix, batch_size=None)\n",
    "        inference_docs_iterator = BatchClass(preprocessed_files_prefix, batch_size=None)\n",
    "        generator_func = inference_docs_iterator.__iter__()\n",
    "        pool = ThreadPool(NUM_CORES)\n",
    "        # map consumes the whole iterator on the spot, so we have to use itertools.islice to fake mini-batching\n",
    "        mini_batch_size = VALIDATION_MINI_BATCH_SIZE\n",
    "        batches_run = 1\n",
    "        while True:\n",
    "            threaded_reps_partial = pool.map(infer_one_doc, itertools.islice(generator_func, mini_batch_size))\n",
    "            info(\"Finished: {} tags\".format(batches_run * mini_batch_size))\n",
    "            batches_run += 1\n",
    "            if threaded_reps_partial:\n",
    "                # threaded_reps.extend(threaded_reps_partial)\n",
    "                inference_documents_reps.update(threaded_reps_partial)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        pickle.dump(inference_documents_reps,\n",
    "                    open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write), 'w'))\n",
    "\n",
    "    return inference_documents_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExtendedPVDocumentBatchGenerator(Process):\n",
    "    def __init__(self, filename_prefix, queue, batch_size=10000, start_file=0, offset=10000):\n",
    "        super(ExtendedPVDocumentBatchGenerator, self).__init__()\n",
    "        self.queue = queue\n",
    "        self.offset = offset\n",
    "        self.filename_prefix = filename_prefix\n",
    "        self.files_loaded = start_file - offset\n",
    "\n",
    "    def run(self):\n",
    "        cur_file = None\n",
    "        while True:\n",
    "            try:\n",
    "                if cur_file is None:\n",
    "                    info(\"Loading new file for index: {}\".format(str(self.files_loaded + self.offset)))\n",
    "                    cur_file = gzip.open(self.filename_prefix + str(self.files_loaded + self.offset) + '.gz')\n",
    "#                     cur_file = open(self.filename_prefix + str(self.files_loaded + self.offset))\n",
    "                    self.files_loaded += self.offset\n",
    "                for line in cur_file:\n",
    "                    self.queue.put(line)\n",
    "                cur_file.close()\n",
    "                cur_file = None\n",
    "            except IOError:\n",
    "                self.queue.put(False, block=True, timeout=None)\n",
    "                info(\"All files are loaded - last file: {}\".format(str(self.files_loaded + self.offset)))\n",
    "                return\n",
    "\n",
    "\n",
    "class BatchWrapper(object):\n",
    "    def __init__(self, training_preprocessed_files_prefix, buffer_size=10000, batch_size=10000, level=1, level_type=None):\n",
    "        assert batch_size <= 10000 or batch_size is None\n",
    "        self.level = level\n",
    "        self.level_type = level_type[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.q = Queue(maxsize=buffer_size)\n",
    "        self.p = ExtendedPVDocumentBatchGenerator(training_preprocessed_files_prefix, queue=self.q,\n",
    "                                                  batch_size=batch_size, start_file=0, offset=10000)\n",
    "        self.p.start()\n",
    "        self.cur_data = []\n",
    "\n",
    "    def is_correct_type(self, doc_id):\n",
    "        parts = doc_id.split(\"_\")\n",
    "        len_parts = len(parts)\n",
    "        if len_parts == self.level:\n",
    "            if len_parts == 1:\n",
    "                return True\n",
    "            if len_parts == self.level and (parts[1][0] == self.level_type or self.level_type is None):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def return_sentences(self, line):\n",
    "        line_array = tuple(line.split(\" \"))\n",
    "        doc_id = line_array[0]\n",
    "        if not self.is_correct_type(doc_id):\n",
    "            return False\n",
    "        line_array = line_array[1:]\n",
    "        len_line_array = len(line_array)\n",
    "        curr_batch_iter = 0\n",
    "        # divide the document to batches according to the batch size\n",
    "        sentences = []\n",
    "        while curr_batch_iter < len_line_array:\n",
    "            sentences.append(LabeledSentence(words=line_array[curr_batch_iter: curr_batch_iter + self.batch_size], tags=[doc_id]))\n",
    "            curr_batch_iter += self.batch_size\n",
    "        return tuple(sentences)\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            item = self.q.get(block=True)\n",
    "            if item is False:\n",
    "                raise StopIteration()\n",
    "            else:\n",
    "                sentences = self.return_sentences(item)\n",
    "                if not sentences:\n",
    "                    None\n",
    "                else:\n",
    "                    for sentence in sentences:\n",
    "                        yield sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2vec and SVM Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOC2VEC_SIZE = 200\n",
    "DOC2VEC_WINDOW = 2\n",
    "DOC2VEC_MAX_VOCAB_SIZE = None\n",
    "DOC2VEC_SAMPLE = 1e-3\n",
    "DOC2VEC_TYPE = 1\n",
    "DOC2VEC_HIERARCHICAL_SAMPLE = 0\n",
    "DOC2VEC_NEGATIVE_SAMPLE_SIZE = 10\n",
    "DOC2VEC_CONCAT = 0\n",
    "DOC2VEC_MEAN = 1\n",
    "DOC2VEC_TRAIN_WORDS = 0\n",
    "DOC2VEC_EPOCHS = 1 # we do our training manually one epoch at a time\n",
    "DOC2VEC_MAX_EPOCHS = 8\n",
    "REPORT_DELAY = 20 # report the progress every x seconds\n",
    "REPORT_VOCAB_PROGRESS = 100000 # report vocab progress every x documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Doc2vec model and create/load the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    (1, 'document')\n",
    "]\n",
    "level, model_name = models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-09 16:55:19,180 : INFO : creating/loading vocabulary for 1 document in \n",
      "2017-04-09 16:55:19,181 : INFO : FILE /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/vocab_model/model\n",
      "2017-04-09 16:55:19,182 : INFO : Loading vocab model\n",
      "2017-04-09 16:55:19,183 : INFO : loading Doc2Vec object from /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/vocab_model/model\n",
      "2017-04-09 16:55:24,363 : INFO : loading docvecs recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/vocab_model/model.docvecs.* with mmap=None\n",
      "2017-04-09 16:55:24,364 : INFO : loading doctag_syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/vocab_model/model.docvecs.doctag_syn0.npy with mmap=None\n",
      "2017-04-09 16:55:24,911 : INFO : loading wv recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/vocab_model/model.wv.* with mmap=None\n",
      "2017-04-09 16:55:24,912 : INFO : loading syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/vocab_model/model.wv.syn0.npy with mmap=None\n",
      "2017-04-09 16:55:25,100 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-09 16:55:25,101 : INFO : loading syn1neg from /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/vocab_model/model.syn1neg.npy with mmap=None\n",
      "2017-04-09 16:55:25,288 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-09 16:55:25,289 : INFO : loaded /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/vocab_model/model\n",
      "2017-04-09 16:55:26,225 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "info(\"creating/loading vocabulary for \" + str(level) + ' ' + model_name + ' in ')\n",
    "doc2vec_model_save_location = os.path.join(root_location,\n",
    "                                           \"parameter_search_doc2vec_models_\" + str(level) + '_' + model_name,\n",
    "                                           \"full\")\n",
    "if not os.path.exists(doc2vec_model_save_location):\n",
    "    os.makedirs(doc2vec_model_save_location)\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL)):\n",
    "    os.makedirs(os.path.join(doc2vec_model_save_location, VOCAB_MODEL))\n",
    "\n",
    "placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}_model_{}'.format(DOC2VEC_SIZE,\n",
    "                                                                DOC2VEC_WINDOW,\n",
    "                                                                'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                                DOC2VEC_TRAIN_WORDS,\n",
    "                                                                DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                                str(DOC2VEC_MAX_VOCAB_SIZE),\n",
    "                                                                str(level) + '_' + model_name\n",
    "                                                                )\n",
    "GLOBAL_VARS.DOC2VEC_MODEL_NAME = placeholder_model_name\n",
    "placeholder_model_name = os.path.join(placeholder_model_name, \"epoch_{}\")\n",
    "info(\"FILE \" + os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "doc2vec_model = Doc2Vec(size=DOC2VEC_SIZE, window=DOC2VEC_WINDOW, min_count=MIN_WORD_COUNT,\n",
    "                max_vocab_size= DOC2VEC_MAX_VOCAB_SIZE,\n",
    "                sample=DOC2VEC_SAMPLE, seed=DOC2VEC_SEED, workers=NUM_CORES,\n",
    "                # doc2vec algorithm dm=1 => PV-DM, dm=2 => PV-DBOW, PV-DM dictates CBOW for words\n",
    "                dm=DOC2VEC_TYPE,\n",
    "                # hs=0 => negative sampling, hs=1 => hierarchical softmax\n",
    "                hs=DOC2VEC_HIERARCHICAL_SAMPLE, negative=DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                dm_concat=DOC2VEC_CONCAT,\n",
    "                # would train words with skip-gram on top of cbow, we don't need that for now\n",
    "                dbow_words=DOC2VEC_TRAIN_WORDS,\n",
    "                iter=DOC2VEC_EPOCHS)\n",
    "\n",
    "GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX)):\n",
    "    \n",
    "    info(\"Creating vocab model\")\n",
    "    training_docs_iterator = BatchWrapper(training_preprocessed_files_prefix, batch_size=10000, level=level,\n",
    "                                          level_type=model_name)\n",
    "    doc2vec_model.build_vocab(sentences=training_docs_iterator, progress_per=REPORT_VOCAB_PROGRESS)\n",
    "    doc2vec_model.save(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "else:\n",
    "    info(\"Loading vocab model\")\n",
    "    doc2vec_model_vocab_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "    doc2vec_model.reset_from(doc2vec_model_vocab_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Training, validation and Metrics Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2vec_model.min_alpha = 0.025\n",
    "DOC2VEC_ALPHA_DECREASE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2vec_model.workers = NUM_CORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-09 16:56:40,606 : INFO : ****************** Epoch 1 --- Working on doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_1_document/epoch_1 *******************\n",
      "2017-04-09 16:56:40,735 : INFO : training model with 16 workers on 446814 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=10 window=2\n",
      "2017-04-09 16:56:40,735 : INFO : Loading new file for index: 0\n",
      "2017-04-09 16:56:40,736 : INFO : expecting 1879865 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-09 16:56:41,782 : INFO : PROGRESS: at 0.01% examples, 610582 words/s, in_qsize 0, out_qsize 2\n",
      "2017-04-09 16:57:01,783 : INFO : PROGRESS: at 0.20% examples, 837420 words/s, in_qsize 0, out_qsize 2\n",
      "2017-04-09 16:57:21,803 : INFO : PROGRESS: at 0.41% examples, 854427 words/s, in_qsize 0, out_qsize 2\n",
      "2017-04-09 16:57:41,806 : INFO : PROGRESS: at 0.61% examples, 854564 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 16:57:55,704 : INFO : Loading new file for index: 10000\n",
      "2017-04-09 16:58:01,810 : INFO : PROGRESS: at 0.82% examples, 860230 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 16:58:21,827 : INFO : PROGRESS: at 1.03% examples, 864500 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 16:58:41,834 : INFO : PROGRESS: at 1.23% examples, 868730 words/s, in_qsize 0, out_qsize 1\n",
      "2017-04-09 16:59:01,832 : INFO : PROGRESS: at 1.44% examples, 870761 words/s, in_qsize 0, out_qsize 1\n",
      "2017-04-09 16:59:09,577 : INFO : Loading new file for index: 20000\n",
      "2017-04-09 16:59:21,835 : INFO : PROGRESS: at 1.65% examples, 872536 words/s, in_qsize 0, out_qsize 1\n",
      "2017-04-09 16:59:41,835 : INFO : PROGRESS: at 1.85% examples, 873516 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:00:01,858 : INFO : PROGRESS: at 2.05% examples, 873553 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:00:21,866 : INFO : PROGRESS: at 2.25% examples, 870498 words/s, in_qsize 0, out_qsize 2\n",
      "2017-04-09 17:00:28,802 : INFO : Loading new file for index: 30000\n",
      "2017-04-09 17:00:41,880 : INFO : PROGRESS: at 2.44% examples, 866762 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:01:01,886 : INFO : PROGRESS: at 2.64% examples, 868653 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:01:21,905 : INFO : PROGRESS: at 2.84% examples, 868239 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:01:41,911 : INFO : PROGRESS: at 3.03% examples, 865038 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:01:48,322 : INFO : Loading new file for index: 40000\n",
      "2017-04-09 17:02:01,912 : INFO : PROGRESS: at 3.23% examples, 862762 words/s, in_qsize 0, out_qsize 1\n",
      "2017-04-09 17:02:21,917 : INFO : PROGRESS: at 3.42% examples, 860861 words/s, in_qsize 0, out_qsize 1\n",
      "2017-04-09 17:02:41,925 : INFO : PROGRESS: at 3.61% examples, 858968 words/s, in_qsize 0, out_qsize 6\n",
      "2017-04-09 17:03:01,932 : INFO : PROGRESS: at 3.80% examples, 857467 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:03:09,816 : INFO : Loading new file for index: 50000\n",
      "2017-04-09 17:03:21,934 : INFO : PROGRESS: at 4.00% examples, 856158 words/s, in_qsize 0, out_qsize 2\n",
      "2017-04-09 17:03:41,946 : INFO : PROGRESS: at 4.19% examples, 854755 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:04:01,944 : INFO : PROGRESS: at 4.37% examples, 853401 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:04:21,946 : INFO : PROGRESS: at 4.57% examples, 852494 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:04:32,468 : INFO : Loading new file for index: 60000\n",
      "2017-04-09 17:04:41,946 : INFO : PROGRESS: at 4.76% examples, 851440 words/s, in_qsize 0, out_qsize 2\n",
      "2017-04-09 17:05:01,948 : INFO : PROGRESS: at 4.96% examples, 850852 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:05:21,951 : INFO : PROGRESS: at 5.15% examples, 849839 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:05:41,951 : INFO : PROGRESS: at 5.35% examples, 849343 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:05:51,867 : INFO : Loading new file for index: 70000\n",
      "2017-04-09 17:06:01,954 : INFO : PROGRESS: at 5.54% examples, 849321 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:06:21,956 : INFO : PROGRESS: at 5.74% examples, 849032 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:06:41,972 : INFO : PROGRESS: at 5.93% examples, 848161 words/s, in_qsize 0, out_qsize 1\n",
      "2017-04-09 17:07:01,993 : INFO : PROGRESS: at 6.12% examples, 848060 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:07:13,017 : INFO : Loading new file for index: 80000\n",
      "2017-04-09 17:07:21,995 : INFO : PROGRESS: at 6.32% examples, 847548 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:07:41,996 : INFO : PROGRESS: at 6.51% examples, 847419 words/s, in_qsize 0, out_qsize 1\n",
      "2017-04-09 17:08:02,006 : INFO : PROGRESS: at 6.71% examples, 846949 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:08:22,010 : INFO : PROGRESS: at 6.90% examples, 846821 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:08:33,833 : INFO : Loading new file for index: 90000\n",
      "2017-04-09 17:08:42,013 : INFO : PROGRESS: at 7.09% examples, 846472 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:09:02,028 : INFO : PROGRESS: at 7.29% examples, 845774 words/s, in_qsize 0, out_qsize 1\n",
      "2017-04-09 17:09:22,073 : INFO : PROGRESS: at 7.48% examples, 845378 words/s, in_qsize 0, out_qsize 4\n",
      "2017-04-09 17:09:42,053 : INFO : PROGRESS: at 7.67% examples, 845019 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:09:54,353 : INFO : Loading new file for index: 100000\n",
      "2017-04-09 17:10:02,065 : INFO : PROGRESS: at 7.86% examples, 844927 words/s, in_qsize 0, out_qsize 1\n",
      "2017-04-09 17:10:22,076 : INFO : PROGRESS: at 8.06% examples, 844687 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:10:42,077 : INFO : PROGRESS: at 8.25% examples, 844385 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:11:02,089 : INFO : PROGRESS: at 8.44% examples, 843964 words/s, in_qsize 0, out_qsize 3\n",
      "2017-04-09 17:11:15,549 : INFO : Loading new file for index: 110000\n",
      "2017-04-09 17:11:22,085 : INFO : PROGRESS: at 8.64% examples, 843739 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:11:42,104 : INFO : PROGRESS: at 8.83% examples, 843280 words/s, in_qsize 0, out_qsize 1\n",
      "2017-04-09 17:12:02,095 : INFO : PROGRESS: at 9.03% examples, 843085 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:12:22,098 : INFO : PROGRESS: at 9.21% examples, 842861 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:12:37,612 : INFO : Loading new file for index: 120000\n",
      "2017-04-09 17:12:42,114 : INFO : PROGRESS: at 9.41% examples, 842630 words/s, in_qsize 1, out_qsize 0\n",
      "2017-04-09 17:13:02,114 : INFO : PROGRESS: at 9.60% examples, 842569 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:13:22,121 : INFO : PROGRESS: at 9.79% examples, 842428 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:13:42,131 : INFO : PROGRESS: at 9.99% examples, 842531 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:13:57,088 : INFO : Loading new file for index: 130000\n",
      "2017-04-09 17:14:02,136 : INFO : PROGRESS: at 10.19% examples, 842687 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:14:22,145 : INFO : PROGRESS: at 10.39% examples, 842780 words/s, in_qsize 0, out_qsize 1\n",
      "2017-04-09 17:14:42,144 : INFO : PROGRESS: at 10.59% examples, 842729 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:15:02,150 : INFO : PROGRESS: at 10.78% examples, 842488 words/s, in_qsize 0, out_qsize 2\n",
      "2017-04-09 17:15:16,097 : INFO : Loading new file for index: 140000\n",
      "2017-04-09 17:15:22,151 : INFO : PROGRESS: at 10.97% examples, 842383 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:15:42,158 : INFO : PROGRESS: at 11.16% examples, 842132 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:16:02,159 : INFO : PROGRESS: at 11.36% examples, 842213 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:16:22,160 : INFO : PROGRESS: at 11.55% examples, 842156 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:16:35,662 : INFO : Loading new file for index: 150000\n",
      "2017-04-09 17:16:42,163 : INFO : PROGRESS: at 11.75% examples, 842117 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:17:02,166 : INFO : PROGRESS: at 11.95% examples, 842322 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:17:22,167 : INFO : PROGRESS: at 12.14% examples, 842199 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:17:42,186 : INFO : PROGRESS: at 12.34% examples, 842229 words/s, in_qsize 0, out_qsize 2\n",
      "2017-04-09 17:17:55,633 : INFO : Loading new file for index: 160000\n",
      "2017-04-09 17:18:02,185 : INFO : PROGRESS: at 12.54% examples, 842291 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-09 17:18:22,190 : INFO : PROGRESS: at 12.73% examples, 842020 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:18:42,196 : INFO : PROGRESS: at 12.92% examples, 841905 words/s, in_qsize 0, out_qsize 2\n",
      "2017-04-09 17:19:02,200 : INFO : PROGRESS: at 13.12% examples, 841955 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:19:15,216 : INFO : Loading new file for index: 170000\n",
      "2017-04-09 17:19:22,211 : INFO : PROGRESS: at 13.32% examples, 841940 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:19:42,221 : INFO : PROGRESS: at 13.51% examples, 841777 words/s, in_qsize 0, out_qsize 2\n",
      "2017-04-09 17:20:02,225 : INFO : PROGRESS: at 13.71% examples, 841798 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:20:22,230 : INFO : PROGRESS: at 13.90% examples, 841741 words/s, in_qsize 0, out_qsize 0\n",
      "2017-04-09 17:20:36,235 : INFO : Loading new file for index: 180000\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# when resuming, resume from an epoch with a previously created doc2vec model to get the learning rate right\n",
    "start_from = 1\n",
    "for epoch in range(start_from, DOC2VEC_MAX_EPOCHS+1):\n",
    "    GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "    info(\"****************** Epoch {} --- Working on {} *******************\".format(epoch, GLOBAL_VARS.MODEL_NAME))\n",
    "    \n",
    "    # if we have the model, just load it, otherwise train the previous model\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)):\n",
    "        doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "        doc2vec_model.workers = NUM_CORES\n",
    "        GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "    else:\n",
    "        # train the doc2vec model\n",
    "        training_docs_iterator = BatchWrapper(training_preprocessed_files_prefix, batch_size=10000, level=level,\n",
    "                                          level_type=model_name)\n",
    "        %time doc2vec_model.train(sentences=training_docs_iterator, report_delay=REPORT_DELAY)\n",
    "        doc2vec_model.alpha -= DOC2VEC_ALPHA_DECREASE  # decrease the learning rate\n",
    "        doc2vec_model.min_alpha = doc2vec_model.alpha  # fix the learning rate, no decay\n",
    "        ensure_disk_location_exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME))\n",
    "        doc2vec_model.save(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "        GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "        \n",
    "    # only do the inference for higher epochs, as inference usually takes as much time as the actual training\n",
    "    if epoch > 7:\n",
    "        # Validation Embeddings\n",
    "        info('Getting Validation Embeddings')\n",
    "        Xv = get_extended_docs_with_inference_data_only(doc2vec_model, VALIDATION_DICT, \n",
    "                                         validation_preprocessed_files_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CORES = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-09 00:47:30,381 : INFO : ===== Getting vectors with inference\n",
      "2017-04-09 00:47:32,316 : INFO : Loading new file for index: 0\n",
      "2017-04-09 00:48:09,470 : INFO : Finished: 100000 tags\n",
      "2017-04-09 00:48:46,768 : INFO : Finished: 200000 tags\n",
      "2017-04-09 00:49:24,072 : INFO : Finished: 300000 tags\n",
      "2017-04-09 00:49:30,546 : INFO : Loading new file for index: 10000\n",
      "2017-04-09 00:50:01,360 : INFO : Finished: 400000 tags\n",
      "2017-04-09 00:50:39,518 : INFO : Finished: 500000 tags\n",
      "2017-04-09 00:51:17,982 : INFO : Finished: 600000 tags\n",
      "2017-04-09 00:51:31,647 : INFO : Loading new file for index: 20000\n",
      "2017-04-09 00:51:56,195 : INFO : Finished: 700000 tags\n",
      "2017-04-09 00:52:32,945 : INFO : Finished: 800000 tags\n",
      "2017-04-09 00:53:10,328 : INFO : Finished: 900000 tags\n",
      "2017-04-09 00:53:47,587 : INFO : Finished: 1000000 tags\n",
      "2017-04-09 00:53:51,325 : INFO : Loading new file for index: 30000\n",
      "2017-04-09 00:54:25,132 : INFO : Finished: 1100000 tags\n",
      "2017-04-09 00:55:03,058 : INFO : Finished: 1200000 tags\n",
      "2017-04-09 00:55:39,845 : INFO : Finished: 1300000 tags\n",
      "2017-04-09 00:55:49,827 : INFO : Loading new file for index: 40000\n",
      "2017-04-09 00:56:17,719 : INFO : Finished: 1400000 tags\n",
      "2017-04-09 00:56:56,656 : INFO : Finished: 1500000 tags\n",
      "2017-04-09 00:57:33,701 : INFO : Finished: 1600000 tags\n",
      "2017-04-09 00:57:50,133 : INFO : Loading new file for index: 50000\n",
      "2017-04-09 00:58:11,085 : INFO : Finished: 1700000 tags\n",
      "2017-04-09 00:58:47,924 : INFO : Finished: 1800000 tags\n",
      "2017-04-09 00:59:25,367 : INFO : Finished: 1900000 tags\n",
      "2017-04-09 01:00:02,766 : INFO : Finished: 2000000 tags\n",
      "2017-04-09 01:00:09,417 : INFO : Loading new file for index: 60000\n",
      "2017-04-09 01:00:09,419 : INFO : All files are loaded - last file: 60000\n",
      "2017-04-09 01:00:17,815 : INFO : Finished: 2100000 tags\n",
      "2017-04-09 01:00:18,398 : INFO : Finished: 2200000 tags\n"
     ]
    }
   ],
   "source": [
    "Xv = get_extended_docs_with_inference_data_only(doc2vec_model, VALIDATION_DICT, \n",
    "                                         validation_preprocessed_files_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
