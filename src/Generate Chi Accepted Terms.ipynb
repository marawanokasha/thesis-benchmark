{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import coverage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IS_SAMPLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "STOP_WORDS = nltk.corpus.stopwords.words('english')\n",
    "NUMBER_INDICATOR = \"number_inidicator\"\n",
    "CURRENCY_INDICATOR = \"currency_inidicator\"\n",
    "CHEMICAL_INDICATOR = \"chemical_inidicator\"\n",
    "MIN_SIZE = 3\n",
    "MIN_DOCUMENTS = 5\n",
    "TOP_N_FEATURES = 10000\n",
    "\n",
    "TEST_SET_PERCENTAGE = 0.2\n",
    "VALIDATION_IN_TRAINING_PERCENTAGE = 0.2\n",
    "MIN_DOCUMENTS_FOR_TEST = 1\n",
    "MIN_DOCUMENTS_FOR_VALIDATION = 1\n",
    "\n",
    "MIN_DOCUMENTS_FOR_TRAINING_SAMPLE = 10\n",
    "MIN_DOCUMENTS_FOR_TEST_SAMPLE = 1\n",
    "MIN_DOCUMENTS_FOR_VALIDATION_SAMPLE = 1\n",
    "\n",
    "SVM_ITERATIONS = 1000\n",
    "SVM_CONVERGENCE = 0.1\n",
    "SVM_REG = 0.01\n",
    "\n",
    "BM25_K = 1.5  # controls power of tf component\n",
    "BM25_b = 0.75  # controls the BM25 length normalization\n",
    "\n",
    "RANDOM_SEED = 10000\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer().stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Manipulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stemtokenizer(text, doc_id):\n",
    "    \"\"\" MAIN FUNCTION to get clean stems out of a text. A list of clean stems are returned \"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\s+', gaps=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stems = []  # result\n",
    "    previous_unigram = None\n",
    "    for token in tokens:\n",
    "        stem = token.lower()\n",
    "        stem = stem.strip(string.punctuation)\n",
    "        if stem:\n",
    "            if is_number(stem):\n",
    "                stem = NUMBER_INDICATOR\n",
    "            elif is_currency(stem):\n",
    "                stem = CURRENCY_INDICATOR\n",
    "            elif is_chemical(stem):\n",
    "                stem = CHEMICAL_INDICATOR\n",
    "            elif is_stopword(stem):\n",
    "                stem = None\n",
    "            else:\n",
    "                stem = stemmer(token)\n",
    "                stem = stem.strip(string.punctuation)\n",
    "            if stem and len(stem) >= MIN_SIZE:\n",
    "                # extract uni-grams\n",
    "                stems.append((stem,{doc_id: 1}))\n",
    "                # extract bi-grams\n",
    "                if previous_unigram: stems.append((previous_unigram + \" \" + stem,{doc_id: 1}))\n",
    "                previous_unigram = stem\n",
    "    del tokens\n",
    "    return stems\n",
    "\n",
    "def is_stopword(word):\n",
    "  return word in STOP_WORDS\n",
    "\n",
    "def is_number(str):\n",
    "    \"\"\" Returns true if given string is a number (float or int)\"\"\"\n",
    "    try:\n",
    "        float(str.replace(\",\", \"\"))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_currency(str):\n",
    "    return str[0] == \"$\"\n",
    "\n",
    "def is_chemical(str):\n",
    "    return str.count(\"-\") > 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_postings(postings_list1, postings_list2):\n",
    "    # key could be either a doc id or a term\n",
    "    for key in postings_list2:\n",
    "        if postings_list1.get(key):\n",
    "            postings_list1[key] += postings_list2[key]\n",
    "        else:\n",
    "            postings_list1[key] = postings_list2[key]\n",
    "    return postings_list1\n",
    "\n",
    "def get_term_dictionary(terms):\n",
    "    \"\"\"\n",
    "    Maps string terms to indexes in an array\n",
    "    \"\"\"\n",
    "    term_dictionary = {}\n",
    "    term_array = [None] * len(terms)\n",
    "    def put(key):\n",
    "        hashvalue = hashfunction(key, len(term_array))\n",
    "        if term_array[hashvalue] == None:\n",
    "            term_array[hashvalue] = key\n",
    "            return hashvalue\n",
    "        else:\n",
    "            nextslot = rehash(hashvalue, len(term_array))\n",
    "            while term_array[nextslot] != None:\n",
    "                nextslot = rehash(nextslot, len(term_array))\n",
    "            if term_array[nextslot] == None:\n",
    "                term_array[nextslot] = key\n",
    "                return nextslot\n",
    "    def hashfunction(key, size):\n",
    "        return hash(key) % size\n",
    "    def rehash(oldhash, size):\n",
    "        return (oldhash + 1) % size\n",
    "    i = 0\n",
    "    for term in terms:\n",
    "        corresponding_index = put(term)\n",
    "        term_dictionary[term] = corresponding_index\n",
    "        i+=1\n",
    "        if i%10000 == 0: print \"finished \" + str(i)\n",
    "    return term_dictionary\n",
    "\n",
    "def get_doc_index(term, postings_list, term_dictionary):\n",
    "    #return [(doc_id, {term: postings_list[doc_id]}) for doc_id in postings_list]\n",
    "    return [(doc_id, {term_dictionary[term]: postings_list[doc_id]}) for doc_id in postings_list]\n",
    "\n",
    "def get_classes(ipc_classification):\n",
    "    sections = []\n",
    "    classes = []\n",
    "    subclasses = []\n",
    "    for classification in ipc_classification:\n",
    "        # we do the check because some documents have repetitions\n",
    "        section_name = classification['section']\n",
    "        class_name = classification['section'] + \"-\" + classification['class']\n",
    "        subclass_name = classification['section'] + \"-\" + classification['class'] + \"-\" + classification['subclass']\n",
    "        if section_name not in sections:\n",
    "            sections.append(section_name)\n",
    "        if class_name not in classes:\n",
    "            classes.append(class_name)\n",
    "        if subclass_name not in subclasses:\n",
    "            subclasses.append(subclass_name)\n",
    "    return {\"sections\": sections, \"classes\": classes, \"subclasses\": subclasses}\n",
    "\n",
    "\n",
    "def get_training_vector_old(classification, term_list, classifications, classification_key_name, number_of_terms):\n",
    "    clss = 1 if classification in classifications[classification_key_name] else 0\n",
    "    return LabeledPoint(clss, SparseVector(number_of_terms, term_list))\n",
    "\n",
    "def get_training_vector(classification, term_list, classifications, number_of_terms):\n",
    "    clss = 1 if classification in classifications else 0\n",
    "    return LabeledPoint(clss, SparseVector(number_of_terms, term_list))\n",
    "\n",
    "\n",
    "def calculate_sublinear_tf(tf):\n",
    "    # laplace smoothing with +1 in case of term with no documents (useful during testing)\n",
    "    return math.log10(1 + tf)\n",
    "\n",
    "\n",
    "def calculate_tf_idf(tf, df, N):\n",
    "    # laplace smoothing with +1 in case of term with no documents (useful during testing)\n",
    "    return tf * math.log10((N+1) / (df + 1))\n",
    "\n",
    "\n",
    "def calculate_bm25(tf, df, N, d_len, d_avg):\n",
    "    idf = max(0, math.log10((N-df + 0.5)/(df+0.5))) # in rare cases where the df is over 50% of N, this could become -ve, so we guard against that\n",
    "    tf_comp = float(((BM25_K + 1) * tf)) / ( BM25_K * ((1-BM25_b) + BM25_b*(float(d_len)/d_avg)) + tf)\n",
    "    return tf_comp * idf\n",
    "\n",
    "\n",
    "def calculate_rf(df_relevant, df_non_relevant):\n",
    "    return math.log( (2 + (float(df_relevant)/max(1, df_non_relevant))), 2)\n",
    "\n",
    "\n",
    "def calculate_tf_rf(tf, df_relevant, df_non_relevant):\n",
    "    return tf * calculate_rf(df_relevant, df_non_relevant)\n",
    "\n",
    "\n",
    "def compare_classifications(x,y):\n",
    "    len_comp = cmp(len(x), len(y))\n",
    "    if len_comp == 0:\n",
    "        return cmp(x,y)\n",
    "    return len_comp\n",
    "\n",
    "\n",
    "def create_doc_index(term_index, term_dictionary):\n",
    "    return term_index \\\n",
    "        .flatMap(lambda (term, postings_list): get_doc_index(term, postings_list, term_dictionary)) \\\n",
    "        .reduceByKey(lambda x, y: merge_postings(x, y))\n",
    "\n",
    "\n",
    "def get_rf_stats(postings, classification):\n",
    "    a_plus_c = set(postings.keys())\n",
    "    a_plus_b = set(classifications_index[classification])\n",
    "    # first intersection is to get (a), second difference is to get (c) (checkout tf-rf paper for reference)\n",
    "    a = a_plus_c.intersection(a_plus_b)\n",
    "    c = a_plus_c.difference(a_plus_b)\n",
    "    size_a = len(a)\n",
    "    size_c = len(c)\n",
    "    return size_a, size_c\n",
    "\n",
    "\n",
    "def get_rf_postings(classification):\n",
    "    def get_rf_postings_internal(postings):\n",
    "        size_a, size_c = get_rf_stats(postings, classification)\n",
    "        return {docId: calculate_rf(size_a, size_c)\n",
    "                for docId, tf in postings.items()}\n",
    "    return get_rf_postings_internal\n",
    "\n",
    "\n",
    "def get_tf_rf_postings(classification):\n",
    "    def get_tf_rf_postings_internal(postings):\n",
    "        size_a, size_c = get_rf_stats(postings, classification)\n",
    "        return {docId: calculate_tf_rf(tf, size_a, size_c)\n",
    "                for docId, tf in postings.items()}\n",
    "    return get_tf_rf_postings_internal\n",
    "\n",
    "\n",
    "def train_level_old(docs_with_classes, classification, classification_label):\n",
    "    training_vectors = docs_with_classes.map(\n",
    "        lambda (doc_id, (term_list, classifications)): get_training_vector_old(classification, term_list, classifications,\n",
    "                                                                           classification_label, number_of_terms))\n",
    "    svm = SVMWithSGD.train(training_vectors, iterations=SVM_ITERATIONS, convergenceTol=SVM_CONVERGENCE)\n",
    "    return training_vectors, svm\n",
    "\n",
    "\n",
    "def train_level(docs_with_classes, classification, number_of_terms):\n",
    "    training_vectors = docs_with_classes.map(\n",
    "        lambda (doc_id, (term_list, classifications)): get_training_vector(classification, term_list,\n",
    "                                                                           classifications, number_of_terms))\n",
    "    svm = SVMWithSGD.train(training_vectors, iterations=SVM_ITERATIONS, convergenceTol=SVM_CONVERGENCE, regParam=SVM_REG)\n",
    "    return training_vectors, svm\n",
    "\n",
    "\n",
    "def get_error(svm, test_vectors):\n",
    "    labelsAndPreds = test_vectors.map(lambda p: (p.label, svm.predict(p.features)))\n",
    "    trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(test_vectors.count())\n",
    "    return trainErr\n",
    "\n",
    "\n",
    "def train_all(docs_with_classes):\n",
    "    training_errors = {}\n",
    "    for section in sections:\n",
    "        training_vectors, svm = train_level(docs_with_classes, section, \"sections\")\n",
    "        train_err = get_error(svm, training_vectors)\n",
    "        training_errors[section] = train_err\n",
    "    #\n",
    "    with open(training_errors_output, 'w') as file:\n",
    "        file.write(json.dumps(training_errors))\n",
    "    #\n",
    "    for clss in classes:\n",
    "        training_vectors, svm = train_level(docs_with_classes, clss, \"classes\")\n",
    "        train_err = get_error(svm, training_vectors)\n",
    "        training_errors[clss] = train_err\n",
    "    \n",
    "    with open(training_errors_output, 'w') as file:\n",
    "        file.write(json.dumps(training_errors))\n",
    "    \n",
    "    for subclass in subclasses:\n",
    "        training_vectors, svm = train_level(docs_with_classes, subclass, \"subclasses\")\n",
    "        train_err = get_error(svm, training_vectors)\n",
    "        training_errors[subclass] = train_err\n",
    "    return training_errors\n",
    "\n",
    "\n",
    "def get_labeled_points_from_doc_index(doc_index, doc_classification_map, number_of_terms):\n",
    "    docs_with_classes = doc_index.map(lambda (doc_id, terms): (doc_id, (terms, doc_classification_map[doc_id])))\n",
    "    training_vectors = docs_with_classes.map(\n",
    "        lambda (doc_id, (term_list, classifications)): get_training_vector(classification, term_list,\n",
    "                                                                           classifications, number_of_terms))\n",
    "    return training_vectors\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    \n",
    "    def __init__(self, labels, scores, threshold=0.5):\n",
    "        self.threshold = 0\n",
    "        self.count = len(self.labels)\n",
    "        \n",
    "        self.tp = 0\n",
    "        self.fp = 0\n",
    "        self.fn = 0\n",
    "        self.tn = 0\n",
    "        \n",
    "        for (l,s) in zip(labels,scores):\n",
    "            if self.is_true(l) and self.is_true(s):\n",
    "                self.tp += 1\n",
    "            if self.is_true(l) and not self.is_true(s):\n",
    "                self.fn += 1\n",
    "            if not self.is_true(l) and self.is_true(s):\n",
    "                self.fp += 1\n",
    "            if not self.is_true(l) and not self.is_true(s):\n",
    "                self.tn += 1\n",
    "        self.precision = self.get_precision()\n",
    "        self.recall = self.get_precision()\n",
    "        self.fa = self.get_f1()\n",
    "        self.error_rate = self.get_error_rate()\n",
    "        \n",
    "    def calculate_contingency(self, label, contingency):\n",
    "        \n",
    "        self.tp = 0\n",
    "        self.fp = 0\n",
    "        self.fn = 0\n",
    "        self.tn = 0\n",
    "        \n",
    "        for (l,s) in zip(labels,scores):\n",
    "            if self.is_true(l) and self.is_true(s):\n",
    "                self.tp += 1\n",
    "            if self.is_true(l) and not self.is_true(s):\n",
    "                self.fn += 1\n",
    "            if not self.is_true(l) and self.is_true(s):\n",
    "                self.fp += 1\n",
    "            if not self.is_true(l) and not self.is_true(s):\n",
    "                self.tn += 1\n",
    "    \n",
    "    def is_true(self, label):\n",
    "        return label > self.threshold\n",
    "    \n",
    "    def get_error_rate(self):\n",
    "        return float(self.tp + self.tn) / len(labels)\n",
    "    \n",
    "    def get_precision(self):\n",
    "        # self.calculate_contingency()\n",
    "        if self.tp == 0: return 0\n",
    "        return float(self.tp) / (self.tp + self.fp)\n",
    "        \n",
    "    def get_recall(self):\n",
    "        # self.calculate_contingency()\n",
    "        if self.tp == 0: return 0\n",
    "        return float(self.tp) / (self.tp + self.fn)\n",
    "    \n",
    "    def get_f1(self):\n",
    "        return 2 * (self.get_precision() * self.get_recall()) / (self.get_precision() + self.get_recall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input/Output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sc = SparkContext(\"\", \"Generate Inverted Index Job\")\n",
    "es_server = \"deka.cip.ifi.lmu.de\"\n",
    "es_port = \"9200\"\n",
    "\n",
    "save_parent_location = \"hdfs://deka.cip.ifi.lmu.de/svm/new/\"\n",
    "if IS_SAMPLE: \n",
    "    save_parent_location = save_parent_location + \"sample/\"\n",
    "\n",
    "file_name = \"sample.json\"\n",
    "test_file_name = \"sample.json\"\n",
    "#url = \"/media/Work/workspace/thesis/benchmark/output/\" + file_name\n",
    "sample_location = save_parent_location + file_name\n",
    "sample_test_location = save_parent_location + test_file_name\n",
    "docs_output = save_parent_location + \"docs_output\"\n",
    "postings_list_output = save_parent_location + \"postings_list_full.json\"\n",
    "accepted_terms_list_output = save_parent_location + \"accepted_terms_list.pkl\"\n",
    "postings_list_chi_selected_output = save_parent_location + \"postings_list_{}.json\"\n",
    "classification_index_output = save_parent_location + \"classification_index.pkl\"\n",
    "doc_classification_map_output = save_parent_location + \"doc_classification_map.pkl\"\n",
    "sections_output = save_parent_location + \"sections.pkl\"\n",
    "classes_output = save_parent_location + \"classes.pkl\"\n",
    "subclasses_output = save_parent_location + \"subclasses.pkl\"\n",
    "classifications_output = save_parent_location + \"classifications.pkl\"\n",
    "doc_lengths_map_output = save_parent_location + \"doc_lengths_map.pkl\"\n",
    "term_dictionary_output = save_parent_location + \"term_dictionary.pkl\"\n",
    "# training, validation and test set lists\n",
    "training_docs_list_output = save_parent_location + \"training_docs_list.pkl\"\n",
    "validation_docs_list_output = save_parent_location + \"validation_docs_list.pkl\"\n",
    "test_docs_list_output = save_parent_location + \"test_docs_list.pkl\"\n",
    "test_postings_list_output = save_parent_location + \"test_postings_list_50000.json\"\n",
    "training_errors_output = save_parent_location + \"training_errors.json\"\n",
    "model_output = save_parent_location + \"models/\" + \"iter_\" + str(SVM_ITERATIONS) + \"_reg_\" + str(SVM_REG) + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_name(method, classification, reg=SVM_REG, no_of_features=TOP_N_FEATURES, iterations=SVM_ITERATIONS):\n",
    "    return save_parent_location + \"models/\" + \"iter_\" + str(iterations) + \"_reg_\" + str(reg) + \"/\" + method + \"_\" + classification + \"_model.svm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Classification Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_classification_map = dict(sc.pickleFile(doc_classification_map_output).collect())\n",
    "doc_count = len(doc_classification_map)\n",
    "classifications_index = dict(sc.pickleFile(classification_index_output).collect())\n",
    "sections = sc.pickleFile(sections_output).collect()\n",
    "classes = sc.pickleFile(classes_output).collect()\n",
    "subclasses = sc.pickleFile(subclasses_output).collect()\n",
    "classifications = sc.pickleFile(classifications_output).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifications_index_set = {k:set(docs) for k,docs in classifications_index.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2009750"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'G-20-B', [u'07433566', u'07896523', u'06985663', u'07116477', u'07218441'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications_index.items()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'G-20-B', {u'06985663', u'07116477', u'07218441', u'07433566', u'07896523'})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications_index_set.items()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'07007598', [u'B', u'B-30', u'B-30-B'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_classification_map.items()[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'A', u'B', u'C', u'D', u'E', u'F', u'G', u'H']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the training, validation and test document lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_documents = sc.pickleFile(training_docs_list_output).collect()\n",
    "validation_documents = sc.pickleFile(validation_docs_list_output).collect()\n",
    "test_documents = sc.pickleFile(test_docs_list_output).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401877"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(test_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_chi_index(term_index, classifications_index_set, subclasses, number_of_docs):\n",
    "    return term_index.map(lambda (term, postings_list): (term, calculate_chi_squared(postings_list.keys(), classifications_index_set, subclasses, number_of_docs)))\n",
    "\n",
    "def calculate_chi_squared(document_list, classifications_index_set, subclasses, number_of_docs):\n",
    "    \"\"\"\n",
    "    Chi squared is the ratio of the difference between actual frequency and expected frequency of a term relative to the expected frequency\n",
    "    summed up across all classes and whether the term appears or not\n",
    "    Here we calculate the average chi squared score which is one of two options in multi-lable classification (the other being max)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    chi_score = 0\n",
    "    N = len(document_list)\n",
    "    doc_set = set(document_list)\n",
    "    Nt1 = N # actual collection frequency of having the word\n",
    "    Nt0 = number_of_docs - N # actual collection frequency of not having the word\n",
    "    Pt1 = float(N)/ number_of_docs # probability of the term happening\n",
    "    Pt0 = float(number_of_docs - N)/ number_of_docs # probablility of the term not happening\n",
    "    #print \"Docs Stats: Term present in %d (%.7f), Not Present in %d (%.7f) \" % (Nt1, Pt1, Nt0, Pt0)\n",
    "    end_time = time.time()\n",
    "    #print \"Pre loop: %.4f\" % (end_time - start_time)\n",
    "    start_time = time.time()\n",
    "    for subclass in subclasses:\n",
    "        Pc1 = float(len(classifications_index_set[subclass]))/ number_of_docs # probability of the class happening\n",
    "        Pc0 = 1 - Pc1\n",
    "        Pt1c1 = float(len(doc_set & classifications_index_set[subclass])) / number_of_docs\n",
    "        Pt1c0 = Pt1 - Pt1c1\n",
    "        Pt0c1 = Pc1 - Pt1c1\n",
    "        Pt0c0 = 1 - Pt1c0 - Pt0c1 - Pt1c1\n",
    "        \n",
    "        cat_chi_score = (number_of_docs * math.pow(Pt1c1 * Pt0c0 - Pt1c0 * Pt0c1, 2))/(Pt1 * Pt0 * Pc1 * Pc0)\n",
    "        #cat_chi_score = (number_of_docs * Pt1c1 * Pt0c0 - Pt1c0 * Pt0c1)/(Pt1 * Pt0 * Pc1 * Pc0)\n",
    "        # calculate average chi score\n",
    "        chi_score += Pc1 * cat_chi_score\n",
    "        #print \"subclass %s: %.7f, %.7f, %.7f, %.7f, %.7f, %.7f\" % (subclass, Pc1, Pt1c1, Pt1c0, Pt0c1, Pt0c0, chi_score)\n",
    "    end_time = time.time()\n",
    "    print \"Per call: %.3f\" % (end_time - start_time)\n",
    "    return chi_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Queue as Q\n",
    "import time\n",
    "import cProfile\n",
    "from line_profiler import LineProfiler\n",
    "min_doc_postings_lists_file = \"/big/s/shalaby/postings_list_full.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1467900248.512972"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_chi():\n",
    "    start_time = time.time()\n",
    "    top_terms_pq = Q.PriorityQueue()\n",
    "    i = 0\n",
    "    for line in open(min_doc_postings_lists_file, 'r'):\n",
    "        (term, document_list) = json.loads(line)\n",
    "        #print \"%s: %d\" % (term, len(document_list))\n",
    "        chi_score = calculate_chi_squared(document_list.keys(), classifications_index_set, subclasses, doc_count)\n",
    "        top_terms_pq.put(-chi_score, term)\n",
    "        if i % 10 == 0:\n",
    "            curr_time = time.time()\n",
    "            print \"Duration: %.3f\" % (curr_time - start_time)\n",
    "            start_time = curr_time\n",
    "        i+=1\n",
    "        if i> 100: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per call: 0.029\n",
      "Duration: 0.030\n",
      "Per call: 0.013\n",
      "Per call: 0.009\n",
      "Per call: 0.012\n",
      "Per call: 0.009\n",
      "Per call: 0.009\n",
      "Per call: 0.010\n",
      "Per call: 0.009\n",
      "Per call: 0.008\n",
      "Per call: 0.007\n",
      "Per call: 0.007\n",
      "Duration: 0.096\n",
      "Per call: 0.038\n",
      "Per call: 0.007\n",
      "Per call: 0.008\n",
      "Per call: 0.013\n",
      "Per call: 0.008\n",
      "Per call: 0.034\n",
      "Per call: 0.008\n",
      "Per call: 0.011\n",
      "Per call: 0.007\n",
      "Per call: 0.009\n",
      "Duration: 0.146\n",
      "Per call: 0.008\n",
      "Per call: 0.007\n",
      "Per call: 0.007\n",
      "Per call: 0.009\n",
      "Per call: 0.008\n",
      "Per call: 0.010\n",
      "Per call: 0.009\n",
      "Per call: 0.008\n",
      "Per call: 0.008\n",
      "Per call: 0.021\n",
      "Duration: 0.097\n",
      "Per call: 0.009\n",
      "Per call: 0.008\n",
      "Per call: 0.008\n",
      "Per call: 0.007\n",
      "Per call: 0.006\n",
      "Per call: 0.007\n",
      "Per call: 0.007\n",
      "Per call: 0.009\n",
      "Per call: 0.007\n",
      "Per call: 0.011\n",
      "Duration: 0.080\n",
      "Per call: 0.014\n",
      "Per call: 0.009\n",
      "Per call: 0.007\n",
      "Per call: 0.012\n",
      "Per call: 0.008\n",
      "Per call: 0.007\n",
      "Per call: 0.128\n",
      "Per call: 0.008\n",
      "Per call: 0.008\n",
      "Per call: 0.008\n",
      "Duration: 0.230\n",
      "Per call: 0.009\n",
      "Per call: 0.009\n",
      "Per call: 0.062\n",
      "Per call: 0.007\n",
      "Per call: 0.007\n",
      "Per call: 0.008\n",
      "Per call: 0.007\n",
      "Per call: 0.007\n",
      "Per call: 0.019\n",
      "Per call: 0.013\n",
      "Duration: 0.154\n",
      "Per call: 0.007\n",
      "Per call: 0.007\n",
      "Per call: 0.007\n",
      "Per call: 0.009\n",
      "Per call: 0.008\n",
      "Per call: 0.014\n",
      "Per call: 0.007\n",
      "Per call: 0.020\n",
      "Per call: 0.008\n",
      "Per call: 0.008\n",
      "Duration: 0.097\n",
      "Per call: 0.011\n",
      "Per call: 0.008\n",
      "Per call: 0.010\n",
      "Per call: 0.010\n",
      "Per call: 0.011\n",
      "Per call: 0.048\n",
      "Per call: 0.021\n",
      "Per call: 0.012\n",
      "Per call: 0.011\n",
      "Per call: 0.014\n",
      "Duration: 0.158\n",
      "Per call: 0.012\n",
      "Per call: 0.012\n",
      "Per call: 0.012\n",
      "Per call: 0.011\n",
      "Per call: 0.011\n",
      "Per call: 0.011\n",
      "Per call: 0.011\n",
      "Per call: 0.007\n",
      "Per call: 0.013\n",
      "Per call: 0.007\n",
      "Duration: 0.106\n",
      "Per call: 0.007\n",
      "Per call: 0.009\n",
      "Per call: 0.007\n",
      "Per call: 0.007\n",
      "Per call: 0.008\n",
      "Per call: 0.007\n",
      "Per call: 0.008\n",
      "Per call: 0.008\n",
      "Per call: 0.026\n",
      "Per call: 0.014\n",
      "Duration: 0.102\n"
     ]
    }
   ],
   "source": [
    "test_chi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "profile = LineProfiler(test_chi, calculate_chi_squared )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per call: 0.076\n",
      "Duration: 0.078\n",
      "Per call: 0.078\n",
      "Per call: 0.071\n",
      "Per call: 0.068\n",
      "Per call: 0.065\n",
      "Per call: 0.063\n",
      "Per call: 0.089\n",
      "Per call: 0.071\n",
      "Per call: 0.054\n",
      "Per call: 0.066\n",
      "Per call: 0.053\n",
      "Duration: 0.682\n",
      "Per call: 0.080\n",
      "Per call: 0.053\n",
      "Per call: 0.054\n",
      "Per call: 0.059\n",
      "Per call: 0.054\n",
      "Per call: 0.078\n",
      "Per call: 0.105\n",
      "Per call: 0.056\n",
      "Per call: 0.057\n",
      "Per call: 0.051\n",
      "Duration: 0.655\n",
      "Per call: 0.056\n",
      "Per call: 0.049\n",
      "Per call: 0.055\n",
      "Per call: 0.054\n",
      "Per call: 0.049\n",
      "Per call: 0.056\n",
      "Per call: 0.049\n",
      "Per call: 0.055\n",
      "Per call: 0.054\n",
      "Per call: 0.064\n",
      "Duration: 0.546\n",
      "Per call: 0.055\n",
      "Per call: 0.047\n",
      "Per call: 0.053\n",
      "Per call: 0.051\n",
      "Per call: 0.052\n",
      "Per call: 0.053\n",
      "Per call: 0.048\n",
      "Per call: 0.056\n",
      "Per call: 0.050\n",
      "Per call: 0.058\n",
      "Duration: 0.527\n",
      "Per call: 0.059\n",
      "Per call: 0.053\n",
      "Per call: 0.053\n",
      "Per call: 0.054\n",
      "Per call: 0.054\n",
      "Per call: 0.053\n",
      "Per call: 0.163\n",
      "Per call: 0.054\n",
      "Per call: 0.082\n",
      "Per call: 0.057\n",
      "Duration: 0.702\n",
      "Per call: 0.055\n",
      "Per call: 0.046\n",
      "Per call: 0.114\n",
      "Per call: 0.051\n",
      "Per call: 0.058\n",
      "Per call: 0.049\n",
      "Per call: 0.054\n",
      "Per call: 0.054\n",
      "Per call: 0.064\n",
      "Per call: 0.060\n",
      "Duration: 0.611\n",
      "Per call: 0.047\n",
      "Per call: 0.055\n",
      "Per call: 0.050\n",
      "Per call: 0.057\n",
      "Per call: 0.053\n",
      "Per call: 0.058\n",
      "Per call: 0.054\n",
      "Per call: 0.062\n",
      "Per call: 0.056\n",
      "Per call: 0.050\n",
      "Duration: 0.545\n",
      "Per call: 0.060\n",
      "Per call: 0.052\n",
      "Per call: 0.049\n",
      "Per call: 0.054\n",
      "Per call: 0.048\n",
      "Per call: 0.063\n",
      "Per call: 0.060\n",
      "Per call: 0.053\n",
      "Per call: 0.054\n",
      "Per call: 0.049\n",
      "Duration: 0.544\n",
      "Per call: 0.056\n",
      "Per call: 0.051\n",
      "Per call: 0.050\n",
      "Per call: 0.054\n",
      "Per call: 0.049\n",
      "Per call: 0.054\n",
      "Per call: 0.053\n",
      "Per call: 0.049\n",
      "Per call: 0.062\n",
      "Per call: 0.046\n",
      "Duration: 0.527\n",
      "Per call: 0.054\n",
      "Per call: 0.054\n",
      "Per call: 0.051\n",
      "Per call: 0.085\n",
      "Per call: 0.052\n",
      "Per call: 0.054\n",
      "Per call: 0.050\n",
      "Per call: 0.074\n",
      "Per call: 0.081\n",
      "Per call: 0.063\n",
      "Duration: 0.623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<line_profiler.LineProfiler at 0x7f8a25661ae0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile.run('test_chi()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 3.75098 s\n",
      "File: <ipython-input-60-c5a7d60d2ab4>\n",
      "Function: calculate_chi_squared at line 4\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     4                                           def calculate_chi_squared(document_list, classifications_index_set, subclasses, number_of_docs):\n",
      "     5                                               \"\"\"\n",
      "     6                                               Chi squared is the ratio of the difference between actual frequency and expected frequency of a term relative to the expected frequency\n",
      "     7                                               summed up across all classes and whether the term appears or not\n",
      "     8                                               Here we calculate the average chi squared score which is one of two options in multi-lable classification (the other being max)\n",
      "     9                                               \"\"\"\n",
      "    10       101          199      2.0      0.0      start_time = time.time()\n",
      "    11       101          118      1.2      0.0      chi_score = 0\n",
      "    12       101          138      1.4      0.0      N = len(document_list)\n",
      "    13       101         1756     17.4      0.0      doc_set = set(document_list)\n",
      "    14       101          126      1.2      0.0      Nt1 = N # actual collection frequency of having the word\n",
      "    15       101          124      1.2      0.0      Nt0 = number_of_docs - N # actual collection frequency of not having the word\n",
      "    16       101          300      3.0      0.0      Pt1 = float(N)/ number_of_docs # probability of the term happening\n",
      "    17       101          199      2.0      0.0      Pt0 = float(number_of_docs - N)/ number_of_docs # probablility of the term not happening\n",
      "    18                                               #print \"Docs Stats: Term present in %d (%.7f), Not Present in %d (%.7f) \" % (Nt1, Pt1, Nt0, Pt0)\n",
      "    19       101          153      1.5      0.0      end_time = time.time()\n",
      "    20                                               #print \"Pre loop: %.4f\" % (end_time - start_time)\n",
      "    21       101          149      1.5      0.0      start_time = time.time()\n",
      "    22    185941       231728      1.2      6.2      for subclass in subclasses:\n",
      "    23    185840       518710      2.8     13.8          Pc1 = float(len(classifications_index_set[subclass]))/ number_of_docs # probability of the class happening\n",
      "    24    185840       258347      1.4      6.9          Pc0 = 1 - Pc1\n",
      "    25    185840      1057833      5.7     28.2          Pt1c1 = float(len(doc_set & classifications_index_set[subclass])) / number_of_docs\n",
      "    26    185840       261066      1.4      7.0          Pt1c0 = Pt1 - Pt1c1\n",
      "    27    185840       243282      1.3      6.5          Pt0c1 = Pc1 - Pt1c1\n",
      "    28    185840       320826      1.7      8.6          Pt0c0 = 1 - Pt1c0 - Pt0c1 - Pt1c1\n",
      "    29                                                   \n",
      "    30    185840       530014      2.9     14.1          cat_chi_score = (number_of_docs * math.pow(Pt1c1 * Pt0c0 - Pt1c0 * Pt0c1, 2))/(Pt1 * Pt0 * Pc1 * Pc0)\n",
      "    31                                                   #cat_chi_score = (number_of_docs * Pt1c1 * Pt0c0 - Pt1c0 * Pt0c1)/(Pt1 * Pt0 * Pc1 * Pc0)\n",
      "    32                                                   # calculate average chi score\n",
      "    33    185840       309562      1.7      8.3          chi_score += Pc1 * cat_chi_score\n",
      "    34                                                   #print \"subclass %s: %.7f, %.7f, %.7f, %.7f, %.7f, %.7f\" % (subclass, Pc1, Pt1c1, Pt1c0, Pt0c1, Pt0c0, chi_score)\n",
      "    35       101          268      2.7      0.0      end_time = time.time()\n",
      "    36       101        15930    157.7      0.4      print \"Per call: %.3f\" % (end_time - start_time)\n",
      "    37       101          154      1.5      0.0      return chi_score\n",
      "\n",
      "Total time: 6.04054 s\n",
      "File: <ipython-input-61-6103db293b12>\n",
      "Function: test_chi at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def test_chi():\n",
      "     2         1            3      3.0      0.0      start_time = time.time()\n",
      "     3         1          106    106.0      0.0      top_terms_pq = Q.PriorityQueue()\n",
      "     4         1            2      2.0      0.0      i = 0\n",
      "     5       101         1730     17.1      0.0      for line in open(min_doc_postings_lists_file, 'r'):\n",
      "     6       101        28720    284.4      0.5          (term, document_list) = json.loads(line)\n",
      "     7                                                   #print \"%s: %d\" % (term, len(document_list))\n",
      "     8       101      6003326  59438.9     99.4          chi_score = calculate_chi_squared(document_list.keys(), classifications_index_set, subclasses, doc_count)\n",
      "     9       101         3729     36.9      0.1          top_terms_pq.put(-chi_score, term)\n",
      "    10       101          208      2.1      0.0          if i % 10 == 0:\n",
      "    11        11           20      1.8      0.0              curr_time = time.time()\n",
      "    12        11          552     50.2      0.0              print \"Duration: %.3f\" % (curr_time - start_time)\n",
      "    13        11           17      1.5      0.0              start_time = curr_time\n",
      "    14       101          126      1.2      0.0          i+=1\n",
      "    15       101         1997     19.8      0.0          if i> 100: break\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0.42\n",
      "Duration: 2.51\n",
      "Duration: 2.37\n",
      "Duration: 2.30\n",
      "Duration: 2.19\n",
      "Duration: 2.30\n",
      "Duration: 2.49\n",
      "Duration: 2.45\n",
      "Duration: 2.44\n",
      "Duration: 2.39\n",
      "Duration: 2.13\n",
      "         374003 function calls in 24.009 seconds\n",
      "\n",
      "   Ordered by: primitive call count\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "   371882    0.097    0.000    0.097    0.000 {len}\n",
      "      202    0.003    0.000    0.003    0.000 {method 'acquire' of 'thread.lock' objects}\n",
      "      202    0.000    0.000    0.000    0.000 {method 'end' of '_sre.SRE_Match' objects}\n",
      "      202    0.001    0.000    0.001    0.000 {method 'match' of '_sre.SRE_Pattern' objects}\n",
      "      101    0.001    0.000    0.007    0.000 Queue.py:107(put)\n",
      "      101   23.863    0.236   23.961    0.237 <ipython-input-16-444c60ee5bf9>:4(calculate_chi_squared)\n",
      "      101    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
      "      101    0.000    0.000    0.000    0.000 {_heapq.heappush}\n",
      "      101    0.000    0.000    0.031    0.000 __init__.py:294(loads)\n",
      "      101    0.001    0.000    0.030    0.000 decoder.py:359(decode)\n",
      "      101    0.029    0.000    0.029    0.000 decoder.py:370(raw_decode)\n",
      "      101    0.001    0.000    0.002    0.000 threading.py:373(notify)\n",
      "      101    0.000    0.000    0.000    0.000 threading.py:64(_note)\n",
      "      101    0.000    0.000    0.001    0.000 Queue.py:224(_put)\n",
      "      101    0.000    0.000    0.001    0.000 threading.py:300(_is_owned)\n",
      "      101    0.000    0.000    0.000    0.000 {method 'release' of 'thread.lock' objects}\n",
      "       22    0.000    0.000    0.000    0.000 {_codecs.utf_8_decode}\n",
      "       22    0.000    0.000    0.003    0.000 iostream.py:308(write)\n",
      "       22    0.000    0.000    0.000    0.000 {method 'decode' of 'str' objects}\n",
      "       22    0.000    0.000    0.000    0.000 {isinstance}\n",
      "       22    0.000    0.000    0.000    0.000 iostream.py:227(_is_master_process)\n",
      "       22    0.000    0.000    0.000    0.000 {posix.getpid}\n",
      "       22    0.000    0.000    0.000    0.000 utf_8.py:15(decode)\n",
      "       22    0.000    0.000    0.002    0.000 iostream.py:240(_schedule_flush)\n",
      "       22    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
      "       12    0.000    0.000    0.000    0.000 {time.time}\n",
      "       11    0.000    0.000    0.000    0.000 {hasattr}\n",
      "       11    0.000    0.000    0.000    0.000 {thread.get_ident}\n",
      "       11    0.000    0.000    0.001    0.000 posix.py:53(wake)\n",
      "       11    0.001    0.000    0.001    0.000 {method 'write' of 'file' objects}\n",
      "       11    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "       11    0.000    0.000    0.001    0.000 ioloop.py:928(add_callback)\n",
      "       11    0.000    0.000    0.000    0.000 stack_context.py:253(wrap)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:242(Condition)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:59(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:260(__init__)\n",
      "        1    0.001    0.001    0.001    0.001 {open}\n",
      "        1    0.000    0.000   24.009   24.009 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.000    0.000 {thread.allocate_lock}\n",
      "        1    0.007    0.007   24.009   24.009 <ipython-input-14-c5aa0e479310>:1(test_chi)\n",
      "        1    0.000    0.000    0.000    0.000 Queue.py:26(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 Queue.py:218(_init)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cProfile.run('test_chi()', sort='pcalls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         2 function calls in 0.000 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.000    0.000 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cProfile.run(\"2+2\", sort=\"cumulative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44846888"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_doc_postings_lists.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# min_doc_postings_lists = sc.parallelize(min_doc_postings_lists.take(10000))\n",
    "\n",
    "# term_accepted_chi_list_with_scores = get_chi_index(min_doc_postings_lists, classifications_index, subclasses, doc_count).takeOrdered(TOP_N_FEATURES, lambda (term,score): -score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture myout\n",
    "\n",
    "term_accepted_chi_list_with_scores = min_doc_postings_lists.map(lambda (term, postings_list): (term, calculate_chi_squared(postings_list.keys(), classifications_index, subclasses, doc_count))).takeOrdered(TOP_N_FEATURES, lambda (term,score): -score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term_accepted_chi_list_with_scores = get_chi_index(min_doc_postings_lists, classifications_index, subclasses, doc_count).takeOrdered(TOP_N_FEATURES, lambda (term,score): -score)\n",
    "term_accepted_chi_list = map(lambda (x,y): x, term_accepted_chi_list_with_scores)\n",
    "# gets a bit slower at the end but finishes eventually \n",
    "term_dictionary = get_term_dictionary(term_accepted_chi_list)\n",
    "min_doc_postings_lists = min_doc_postings_lists.filter(lambda (term, postings): term in term_accepted_chi_list).cache()\n",
    "# Save Postings List\n",
    "# min_doc_postings_lists.map(lambda (term, postings_list): \",\".join([term, json.dumps(postings_list)])).repartition(1).saveAsTextFile(postings_list_output)\n",
    "min_doc_postings_lists.map(lambda postings: json.dumps(postings)).saveAsTextFile(postings_list_chi_selected_output.format(str(TOP_N_FEATURES)))\n",
    "sc.parallelize(term_dictionary.items()).saveAsPickleFile(term_dictionary_output)\n",
    "sc.parallelize(term_accepted_chi_list).saveAsPickleFile(accepted_terms_list_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "term_accepted_chi_list_with_scores = get_chi_index(min_doc_postings_lists, classifications_index, subclasses, doc_count).takeOrdered(TOP_N_FEATURES, lambda (term,score): -score)\n",
    "term_accepted_chi_list = map(lambda (x,y): x, term_accepted_chi_list_with_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'result tire', 120.00464625703135),\n",
       " (u'PLIAs describ', 27.38847101545022),\n",
       " (u'11.AP.26.157', 24.99762059035747),\n",
       " (u'poly(glycoamidoamine may', 11.041338300811203),\n",
       " (u'2\\xd72\\xd72 form', 5.367248809371421),\n",
       " (u'number_inidicator fifth-stag', 3.532208801170213),\n",
       " (u'erect across', 3.357988592234342),\n",
       " (u'stresses depth', 3.2341600269691106),\n",
       " (u'ABS dimens', 2.6859473213716405),\n",
       " (u'inward workpiec', 1.8592295579347673)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample for testing\n",
    "# sample_no = 10\n",
    "# postings_sample = min_doc_postings_lists.take(sample_no)\n",
    "# terms_with_chi = get_chi_index(sc.parallelize(postings_sample), classifications_index, subclasses, doc_count).takeOrdered(sample_no, lambda (term,score): -score)\n",
    "# terms_with_chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'result tire', 120.00464625703135),\n",
       " (u'inward workpiec', 1.8592295579347673),\n",
       " (u'2\\xd72\\xd72 form', 5.367248809371421),\n",
       " (u'PLIAs describ', 27.38847101545022),\n",
       " (u'ABS dimens', 2.6859473213716405),\n",
       " (u'11.AP.26.157', 24.99762059035747),\n",
       " (u'number_inidicator fifth-stag', 3.532208801170213),\n",
       " (u'erect across', 3.357988592234342),\n",
       " (u'stresses depth', 3.2341600269691106),\n",
       " (u'poly(glycoamidoamine may', 11.041338300811203)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#map(lambda (term, postings_list): (term, calculate_chi_squared(postings_list.keys(), classifications_index, subclasses, doc_count)), postings_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'also set',\n",
       " u'bodi',\n",
       " u'milliseconds',\n",
       " u'Societi',\n",
       " u'5-7',\n",
       " u'jacket',\n",
       " u'promoters',\n",
       " u'number_inidicator dynam',\n",
       " u'partially',\n",
       " u'determin process',\n",
       " u'Sci number_inidicator',\n",
       " u'applic file',\n",
       " u'however embodi',\n",
       " u'side end',\n",
       " u'assembl compris',\n",
       " u'image FIG',\n",
       " u'devic employ',\n",
       " u'appli second',\n",
       " u'includ case',\n",
       " u'miner oil',\n",
       " u'power requir',\n",
       " u'decreas',\n",
       " u'technic scientif',\n",
       " u'passed',\n",
       " u'frequent',\n",
       " u'invent effect',\n",
       " u'dens',\n",
       " u'scenarios',\n",
       " u'body',\n",
       " u'bidirect',\n",
       " u'provid complet',\n",
       " u'micro',\n",
       " u'embodi consid',\n",
       " u'radial outward',\n",
       " u'applic claim',\n",
       " u'clarity',\n",
       " u'administ patient',\n",
       " u'e.g first',\n",
       " u'apparatu accord',\n",
       " u'released',\n",
       " u'includ frame',\n",
       " u'use locat',\n",
       " u'system softwar',\n",
       " u'detail FIGS',\n",
       " u'program provid',\n",
       " u'important',\n",
       " u'ascorb',\n",
       " u'order enabl',\n",
       " u'system and/or',\n",
       " u'output data',\n",
       " u'number_inidicator case',\n",
       " u'illustr process',\n",
       " u'resin number_inidicator',\n",
       " u'slant',\n",
       " u'amount power',\n",
       " u'art embodi',\n",
       " u'infect',\n",
       " u'16b',\n",
       " u'number_inidicator gradual',\n",
       " u'thu gener',\n",
       " u'includ video',\n",
       " u'kinas',\n",
       " u'network node',\n",
       " u'posit upper',\n",
       " u'factors',\n",
       " u'said second',\n",
       " u'workstat',\n",
       " u'second type',\n",
       " u'need method',\n",
       " u'number_inidicator multiplex',\n",
       " u'Detail',\n",
       " u'variou modifications',\n",
       " u'Memori number_inidicator',\n",
       " u'includ circuit',\n",
       " u'second prefer',\n",
       " u'years',\n",
       " u'method identifi',\n",
       " u'wherein plural',\n",
       " u'ambient temperature',\n",
       " u'structur one',\n",
       " u'show step',\n",
       " u'figure',\n",
       " u'compon illustr',\n",
       " u'show portion',\n",
       " u'recognize',\n",
       " u'infrastructure',\n",
       " u'anywher',\n",
       " u'similar function',\n",
       " u'closer',\n",
       " u'cost-effect',\n",
       " u'transmit control',\n",
       " u'revers direct',\n",
       " u'similar element',\n",
       " u'preservatives',\n",
       " u'maximum number_inidicator',\n",
       " u'feeder',\n",
       " u'embodiment understood',\n",
       " u'Core',\n",
       " u'interfac includ',\n",
       " u'curv surfac']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_accepted_chi_list[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recreate term dictionary with just the accepted terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 10000\n"
     ]
    }
   ],
   "source": [
    "# gets a bit slower at the end but finishes eventually \n",
    "term_dictionary = get_term_dictionary(term_accepted_chi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_doc_postings_lists = min_doc_postings_lists.filter(lambda (term, postings): term in term_accepted_chi_list).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_of_terms = min_doc_postings_lists.count()\n",
    "number_of_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Reduced Postings List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save Postings List\n",
    "# min_doc_postings_lists.map(lambda (term, postings_list): \",\".join([term, json.dumps(postings_list)])).repartition(1).saveAsTextFile(postings_list_output)\n",
    "min_doc_postings_lists.map(lambda postings: json.dumps(postings)).saveAsTextFile(postings_list_chi_selected_output.format(str(TOP_N_FEATURES)))\n",
    "sc.parallelize(term_dictionary.items()).saveAsPickleFile(term_dictionary_output)\n",
    "sc.parallelize(term_accepted_chi_list).saveAsPickleFile(accepted_terms_list_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Reduced Postings List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_doc_postings_lists = sc.textFile(postings_list_chi_selected_output).map(lambda json_postings: json.loads(json_postings))\n",
    "term_dictionary = dict(sc.pickleFile(term_dictionary_output).collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.6.1)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
