{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: TITAN X (Pascal) (CNMeM is disabled, cuDNN 5105)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple, defaultdict\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "from functools import partial\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "from thesis.utils.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IS_SAMPLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_SEED = 1234\n",
    "DOC2VEC_SEED = 1234\n",
    "WORD2VEC_SEED = 1234\n",
    "NN_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_WORD_COUNT = 50\n",
    "MIN_SIZE = 0\n",
    "NUM_CORES = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GLOBAL_VARS = namedtuple('GLOBAL_VARS', ['MODEL_NAME', 'DOC2VEC_MODEL_NAME', 'DOC2VEC_MODEL', \n",
    "                                         'SVM_MODEL_NAME', 'NN_MODEL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_MODEL = \"vocab_model\"\n",
    "MODEL_PREFIX = \"model\"\n",
    "VALIDATION_MATRIX = \"validation_matrix.pkl\"\n",
    "VALIDATION_DICT = \"validation_dict.pkl\"\n",
    "TEST_MATRIX = \"test_matrix.pkl\"\n",
    "TEST_DICT = \"test_dict.pkl\"\n",
    "METRICS = \"metrics.pkl\"\n",
    "CLASSIFIER = \"classifier.pkl\"\n",
    "TYPE_CLASSIFIER= \"{}_classifier.pkl\"\n",
    "\n",
    "TRAINING_DATA_MATRIX = \"X.npy\"\n",
    "TRAINING_LABELS_MATRIX = \"y_{}.npy\"\n",
    "VALIDATION_DATA_MATRIX = \"Xv.npy\"\n",
    "VALIDATION_LABELS_MATRIX = \"yv_{}.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_PARAMETER_SEARCH_PREFIX = \"lstm_{}_batch_{}_nn_parameter_searches.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_location = \"/mnt/data2/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "doc2vec_model_save_location = os.path.join(\"/mnt/data/shalaby/\", \"parameter_search_doc2vec_models_extended\", \"full\")\n",
    "nn_parameter_search_location = os.path.join(root_location, \"nn_parameter_search_extended\")\n",
    "if not os.path.exists(doc2vec_model_save_location):\n",
    "    os.makedirs(doc2vec_model_save_location)\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL)):\n",
    "    os.makedirs(os.path.join(doc2vec_model_save_location, VOCAB_MODEL))\n",
    "\n",
    "#training_file = root_location + \"docs_output.json\"\n",
    "training_file = root_location + 'docs_output.json'\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "valid_classes_file = exports_location + \"valid_classes.pkl\"\n",
    "valid_subclasses_file = exports_location + \"valid_subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "training_docs_list_file = exports_location + \"extended_pv_training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"extended_pv_validation_docs_list.pkl\"\n",
    "test_docs_list_file = exports_location + \"extended_pv_test_docs_list.pkl\"\n",
    "\n",
    "\n",
    "training_doc_stats_file = exports_location + \"extended_pv_training_doc_stats.pkl\"\n",
    "validation_doc_stats_file = exports_location + \"extended_pv_validation_doc_stats.pkl\"\n",
    "\n",
    "preprocessed_location = root_location + \"preprocessed_data/\"\n",
    "\n",
    "training_preprocessed_files_prefix = preprocessed_location + \"extended_pv_training_docs_data_preprocessed-\"\n",
    "validation_preprocessed_files_prefix = preprocessed_location + \"extended_pv_validation_docs_data_preprocessed-\"\n",
    "test_preprocessed_files_prefix = preprocessed_location + \"extended_pv_test_docs_data_preprocessed-\"\n",
    "\n",
    "word2vec_questions_file = result = root_location + 'tensorflow/word2vec/questions-words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.3 s, sys: 23.6 s, total: 39.8 s\n",
      "Wall time: 39.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "valid_classes = pickle.load(open(valid_classes_file))\n",
    "valid_subclasses = pickle.load(open(valid_subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120156"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29675"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37771"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ensure_disk_location_exists(location):\n",
    "    if not os.path.exists(location):\n",
    "        os.makedirs(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VALIDATION_MINI_BATCH_SIZE = 10000\n",
    "def get_extended_docs_with_inference(doc2vec_model, doc_classification_map, classifications,\n",
    "                                           docs_list, file_to_write, preprocessed_files_prefix):\n",
    "    \"\"\"\n",
    "    Use the trained doc2vec model to get the paragraph vector representations of the validation or test documents\n",
    "    \"\"\"\n",
    "\n",
    "    def infer_one_doc(doc_tuple):\n",
    "        # doc2vec_model.random = np.random.RandomState(DOC2VEC_SEED)\n",
    "        doc_id, doc_tokens = doc_tuple\n",
    "        rep = doc2vec_model.infer_vector(doc_tokens)\n",
    "        return (doc_id, rep)\n",
    "\n",
    "\n",
    "    one_hot_encoder = OneHotEncoder(classifications)\n",
    "    classifications_set = set(classifications)\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write)):\n",
    "        info(\"===== Loading inference vectors\")\n",
    "        inference_labels = []\n",
    "        inference_vectors_matrix = pickle.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write)))\n",
    "        info(\"Loaded inference vectors matrix\")\n",
    "        for i, doc_id in enumerate(docs_list):\n",
    "            curr_doc_labels = set(doc_classification_map[doc_id]) & classifications_set\n",
    "            inference_labels.append(one_hot_encoder.get_label_vector(curr_doc_labels))\n",
    "            if i % 100000 == 0:\n",
    "                info(\"Finished {} in validation loading\".format(i))\n",
    "        inference_labels = np.array(inference_labels, dtype=np.int8)\n",
    "    else:\n",
    "        inference_documents_reps = {}\n",
    "        inference_vectors = []\n",
    "        inference_labels = []\n",
    "        info(\"===== Getting vectors with inference\")\n",
    "\n",
    "        # Multi-threaded inference\n",
    "        inference_docs_iterator = ExtendedPVDocumentBatchGenerator(preprocessed_files_prefix, batch_size=None)\n",
    "        generator_func = inference_docs_iterator.__iter__()\n",
    "        pool = ThreadPool(NUM_CORES)\n",
    "        # map consumes the whole iterator on the spot, so we have to use itertools.islice to fake mini-batching\n",
    "        mini_batch_size = VALIDATION_MINI_BATCH_SIZE\n",
    "        while True:\n",
    "            threaded_reps_partial = pool.map(infer_one_doc, itertools.islice(generator_func, mini_batch_size))\n",
    "            info(\"Finished: {} tags\".format(str(inference_docs_iterator.curr_index + 1)))\n",
    "            if threaded_reps_partial:\n",
    "                # threaded_reps.extend(threaded_reps_partial)\n",
    "                inference_documents_reps.update(threaded_reps_partial)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # create matrix for the inferred vectors\n",
    "        for doc_id in docs_list:\n",
    "            inference_vectors.append(inference_documents_reps[doc_id])\n",
    "            curr_doc_labels = set(doc_classification_map[doc_id]) & classifications_set\n",
    "            inference_labels.append(one_hot_encoder.get_label_vector(curr_doc_labels))\n",
    "        inference_vectors_matrix = np.array(inference_vectors)\n",
    "        inference_labels = np.array(inference_labels, dtype=np.int8)\n",
    "        pickle.dump(inference_vectors_matrix,\n",
    "                    open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write), 'w'))\n",
    "\n",
    "    return inference_vectors_matrix, inference_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OneHotEncoder():\n",
    "    \n",
    "    def __init__(self, classifications):\n",
    "        self.classifications = classifications\n",
    "        self.one_hot_indices = {}\n",
    "\n",
    "        # convert character classifications to bit vectors\n",
    "        for i, clssf in enumerate(classifications):\n",
    "            bits = [0] * len(classifications)\n",
    "            bits[i] = 1\n",
    "            self.one_hot_indices[clssf] = i\n",
    "    \n",
    "    def get_label_vector(self, labels):\n",
    "        \"\"\"\n",
    "        classes: array of string with the classes assigned to the instance\n",
    "        \"\"\"\n",
    "        output_vector = [0] * len(self.classifications)\n",
    "        for label in labels:\n",
    "            index = self.one_hot_indices[label]\n",
    "            output_vector[index] = 1\n",
    "            \n",
    "        return output_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ExtendedPVDocumentBatchGenerator(object):\n",
    "    def __init__(self, filename_prefix, batch_size=10000 ):\n",
    "        \"\"\"\n",
    "        batch_size cant be > 10,000 due to a limitation in doc2vec training, \n",
    "        None means no batching (only use for inference)\n",
    "        \"\"\"\n",
    "        assert batch_size <= 10000 or batch_size is None\n",
    "        self.filename_prefix = filename_prefix\n",
    "        self.curr_lines = []\n",
    "        self.curr_docids = []\n",
    "        self.batch_size = batch_size\n",
    "        self.curr_index = 0\n",
    "        self.curr_doc_index = 0\n",
    "        self.batch_end = -1\n",
    "    def load_new_batch_in_memory(self):\n",
    "        del self.curr_lines, self.curr_docids\n",
    "        self.curr_lines, self.curr_docids = [], []\n",
    "        info(\"Loading new batch for index: {}\".format(self.curr_doc_index))\n",
    "        #if self.curr_doc_index > 0: self.curr_doc_index -= 1\n",
    "        true_docs_count = 0\n",
    "        try:\n",
    "            with open(self.filename_prefix + str(self.curr_doc_index)) as preproc_file:\n",
    "                for line in preproc_file:\n",
    "                    line_array = line.split(\" \")\n",
    "                    doc_id = line_array[0]\n",
    "                    doc_tokens = line_array[1:]\n",
    "                    self.curr_lines.append(doc_tokens)\n",
    "                    self.curr_docids.append(doc_id)\n",
    "                    if is_real_doc(doc_id):\n",
    "                        true_docs_count+= 1\n",
    "            self.batch_end = self.curr_doc_index + true_docs_count - 1 \n",
    "            info(true_docs_count)\n",
    "            info(\"Finished loading new batch\")\n",
    "        except IOError:\n",
    "            info(\"No more batches to load, exiting at index: {}\".format(self.curr_index))\n",
    "            raise StopIteration()\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            if self.curr_doc_index > self.batch_end:\n",
    "                self.load_new_batch_in_memory()\n",
    "            for (doc_id, tokens) in zip(self.curr_docids, self.curr_lines):\n",
    "                if self.batch_size is not None:\n",
    "                    curr_batch_iter = 0\n",
    "                    # divide the document to batches according to the batch size\n",
    "                    while curr_batch_iter < len(tokens):\n",
    "                        yield LabeledSentence(words=tokens[curr_batch_iter: curr_batch_iter + self.batch_size], tags=[doc_id])\n",
    "                        curr_batch_iter += self.batch_size\n",
    "                else:\n",
    "                    yield doc_id, tokens\n",
    "                self.curr_index += 1\n",
    "                # increment only for full docs\n",
    "                if is_real_doc(doc_id):\n",
    "                    self.curr_doc_index += 1\n",
    "                    if self.curr_doc_index % 1000 == 0:\n",
    "                        info(\"New Doc: {:5}, Batch End: {:5}\".format(self.curr_doc_index, self.batch_end))\n",
    "\n",
    "def is_real_doc(doc_id):\n",
    "    return doc_id.find(\"_\") == -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Document, Paragraph and Sentence Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DocumentsStatsGenerator(object):\n",
    "    def __init__(self, filename_prefix):\n",
    "        self.filename_prefix = filename_prefix\n",
    "        self.docids = []\n",
    "        self.doc_paragraphs = defaultdict(list)\n",
    "        self.doc_sentences = defaultdict(list)\n",
    "        self.curr_doc_index = 0\n",
    "        self.batch_end = -1\n",
    "    def load_new_batch_in_memory(self):\n",
    "        info(\"Loading new batch for index: {}\".format(self.curr_doc_index))\n",
    "        true_docs_count = 0\n",
    "        try:\n",
    "            with open(self.filename_prefix + str(self.curr_doc_index)) as preproc_file:\n",
    "                for line in preproc_file:\n",
    "                    line_array = line.split(\" \", 1)\n",
    "                    entity_id = line_array[0].strip()\n",
    "                    if self.is_doc(entity_id):\n",
    "                        self.docids.append(entity_id)\n",
    "                        true_docs_count+= 1\n",
    "                    elif self.is_paragraph(entity_id):\n",
    "                        self.doc_paragraphs[self.get_doc_id(entity_id)].append(entity_id)\n",
    "                    elif self.is_sentence(entity_id):\n",
    "                        self.doc_sentences[self.get_doc_id(entity_id)].append(entity_id)\n",
    "            self.batch_end = self.curr_doc_index + true_docs_count - 1 \n",
    "            info(\"Finished loading new batch of {} documents\".format(true_docs_count))\n",
    "        except IOError:\n",
    "            info(\"No more batches to load, exiting at index: {}\".format(self.curr_doc_index))\n",
    "            raise StopIteration()\n",
    "    def get_stats(self):\n",
    "        try:\n",
    "            while True:\n",
    "                if self.curr_doc_index > self.batch_end:\n",
    "                    self.load_new_batch_in_memory()\n",
    "                self.curr_doc_index = self.batch_end + 1\n",
    "        except StopIteration:\n",
    "            pass\n",
    "            \n",
    "    def get_doc_id(self, entity_id):\n",
    "        return entity_id.split(\"_\")[0]\n",
    "    def get_entity_parts(self, entity_id):\n",
    "        return entity_id.split(\"_\")\n",
    "    def is_doc(self, entity_id):\n",
    "        parts = self.get_entity_parts(entity_id)\n",
    "        if len(parts) == 1:\n",
    "            return True\n",
    "        return False\n",
    "    def is_paragraph(self, entity_id):\n",
    "        parts = self.get_entity_parts(entity_id)\n",
    "        if len(parts) == 2:\n",
    "            return True\n",
    "        return False\n",
    "    def is_sentence(self, entity_id):\n",
    "        parts = self.get_entity_parts(entity_id)\n",
    "        if len(parts) == 3:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_doc_vector(entity_id):\n",
    "    if DOC2VEC_MMAP:\n",
    "        normal_array = []\n",
    "        normal_array[:] = doc2vec_model.docvecs[entity_id][:]\n",
    "        return normal_array\n",
    "    else:\n",
    "        return doc2vec_model.docvecs[entity_id]\n",
    "\n",
    "def data_generator(doc_stats, doc_id):\n",
    "    yield get_doc_vector(doc_id)\n",
    "    for paragraph_id in doc_stats.doc_paragraphs[doc_id]:\n",
    "        yield get_doc_vector(paragraph_id)\n",
    "    for sentence_id in doc_stats.doc_sentences[doc_id]:\n",
    "        yield get_doc_vector(sentence_id)\n",
    "    while True:\n",
    "        yield ZERO_VECTOR\n",
    "\n",
    "\n",
    "def validation_data_generator(doc_stats, validation_dict, doc_id):\n",
    "    yield validation_dict[doc_id]\n",
    "    for paragraph_id in doc_stats.doc_paragraphs[doc_id]:\n",
    "        yield validation_dict[paragraph_id]\n",
    "    for sentence_id in doc_stats.doc_sentences[doc_id]:\n",
    "        yield validation_dict[sentence_id]\n",
    "    while True:\n",
    "        yield ZERO_VECTOR\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_training_data(doc2vec_model, classifications, classifications_type, doc_stats, sequence_size, embedding_size):\n",
    "    if not os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, TRAINING_DATA_MATRIX)):\n",
    "        info(\"Creating Training Data\")\n",
    "        one_hot_encoder = OneHotEncoder(classifications)\n",
    "        classifications_set = set(classifications)\n",
    "        # 1st level: document level\n",
    "        training_data = np.ndarray((len(training_docs_list), sequence_size, embedding_size), dtype=np.float32)\n",
    "        info(\"Training Data shape: {}\".format(training_data.shape))\n",
    "        training_labels_mat = np.zeros((len(training_docs_list), len(classifications)), dtype=np.int8)\n",
    "        for i, doc_id in enumerate(training_docs_list):\n",
    "            data_gen = data_generator(doc_stats, doc_id)\n",
    "            # 2nd level: constituents\n",
    "            for j in range(sequence_size):\n",
    "                #3d level: feature vectors\n",
    "                training_data[i][j]= data_gen.next()\n",
    "            eligible_classifications = set(doc_classification_map[doc_id]) & classifications_set\n",
    "            training_labels_mat[i][:] = one_hot_encoder.get_label_vector(eligible_classifications)\n",
    "            if i % 10000 == 0:\n",
    "                info(\"Finished {} in training\".format(i))\n",
    "        \n",
    "        info(\"Saving Training Data to file...\")\n",
    "        np.save(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                  TRAINING_DATA_MATRIX), \"w\"), training_data)\n",
    "        np.save(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                  TRAINING_LABELS_MATRIX.format(classifications_type)), \"w\"), training_labels_mat)\n",
    "    else:\n",
    "        info(\"Loading Training Data from file\")\n",
    "        training_data = np.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                  TRAINING_DATA_MATRIX)))\n",
    "        training_labels_mat = np.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                        TRAINING_LABELS_MATRIX.format(classifications_type))))\n",
    "    return training_data, training_labels_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_data(validation_dict, classifications, classifications_type, doc_stats, sequence_size, embedding_size):\n",
    "    if not os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_DATA_MATRIX)):\n",
    "        info(\"Creating Validation Data\")\n",
    "        one_hot_encoder = OneHotEncoder(classifications)\n",
    "        classifications_set = set(classifications)\n",
    "        # 1st level: document level\n",
    "        validation_data = np.ndarray((len(validation_docs_list), sequence_size, embedding_size), dtype=np.float32)\n",
    "        info(\"Validation Data shape: {}\".format(validation_data.shape))\n",
    "        validation_labels_mat = np.zeros((len(validation_docs_list), len(classifications)), dtype=np.int8)\n",
    "        for i, doc_id in enumerate(validation_docs_list):\n",
    "            data_gen = validation_data_generator(doc_stats, validation_dict, doc_id)\n",
    "            # 2nd level: constituents\n",
    "            for j in range(sequence_size):\n",
    "                #3d level: feature vectors\n",
    "                validation_data[i][j]= data_gen.next()\n",
    "            eligible_classifications = set(doc_classification_map[doc_id]) & classifications_set\n",
    "            validation_labels_mat[i][:] = one_hot_encoder.get_label_vector(eligible_classifications)\n",
    "            if i % 10000 == 0:\n",
    "                info(\"Finished {} in validation\".format(i))\n",
    "        \n",
    "        info(\"Saving Validation Data to file...\")\n",
    "        np.save(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                  VALIDATION_DATA_MATRIX), \"w\"), validation_data)\n",
    "        np.save(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                  VALIDATION_LABELS_MATRIX.format(classifications_type)), \"w\"), validation_labels_mat)\n",
    "    else:\n",
    "        info(\"Loading Validation Data from file\")\n",
    "        validation_data = np.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                  VALIDATION_DATA_MATRIX)))\n",
    "        validation_labels_mat = np.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                        VALIDATION_LABELS_MATRIX.format(classifications_type))))\n",
    "    return validation_data, validation_labels_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Training, validation and Metrics Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Masking\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifications = sections\n",
    "classifications_type = 'sections'\n",
    "classifier_file = TYPE_CLASSIFIER.format(classifications_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VALIDATION_METRICS_FILENAME= '{}_validation_metrics.pkl'.format(classifications_type)\n",
    "TRAINING_METRICS_FILENAME = '{}_training_metrics.pkl'.format(classifications_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOC2VEC_SIZE = 200\n",
    "DOC2VEC_WINDOW = 2\n",
    "DOC2VEC_MAX_VOCAB_SIZE = None\n",
    "DOC2VEC_SAMPLE = 1e-3\n",
    "DOC2VEC_TYPE = 1\n",
    "DOC2VEC_HIERARCHICAL_SAMPLE = 0\n",
    "DOC2VEC_NEGATIVE_SAMPLE_SIZE = 10\n",
    "DOC2VEC_CONCAT = 0\n",
    "DOC2VEC_MEAN = 1\n",
    "DOC2VEC_TRAIN_WORDS = 0\n",
    "DOC2VEC_EPOCHS = 1 # we do our training manually one epoch at a time\n",
    "DOC2VEC_MAX_EPOCHS = 8\n",
    "REPORT_DELAY = 20 # report the progress every x seconds\n",
    "REPORT_VOCAB_PROGRESS = 100000 # report vocab progress every x documents\n",
    "\n",
    "# DOC2VEC_MMAP = 'r'\n",
    "DOC2VEC_MMAP = None\n",
    "\n",
    "ZERO_VECTOR = [0] * DOC2VEC_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8\n"
     ]
    }
   ],
   "source": [
    "placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}'.format(DOC2VEC_SIZE, \n",
    "                                                                DOC2VEC_WINDOW, \n",
    "                                                                'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                                DOC2VEC_TRAIN_WORDS,\n",
    "                                                                DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                                str(DOC2VEC_MAX_VOCAB_SIZE))\n",
    "GLOBAL_VARS.DOC2VEC_MODEL_NAME = placeholder_model_name\n",
    "placeholder_model_name = os.path.join(placeholder_model_name, \"epoch_{}\")\n",
    "\n",
    "epoch = 8\n",
    "\n",
    "GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "doc2vec_model = None\n",
    "print GLOBAL_VARS.MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 16:36:29,548 : INFO : loading Doc2Vec object from /mnt/data/shalaby/parameter_search_doc2vec_models_extended/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_5/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/shalaby/parameter_search_doc2vec_models_extended/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_5/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 16:38:44,710 : INFO : loading docvecs recursively from /mnt/data/shalaby/parameter_search_doc2vec_models_extended/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_5/model.docvecs.* with mmap=None\n",
      "2017-03-09 16:38:44,712 : INFO : loading doctag_syn0 from /mnt/data/shalaby/parameter_search_doc2vec_models_extended/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_5/model.docvecs.doctag_syn0.npy with mmap=None\n",
      "2017-03-09 16:43:14,235 : INFO : loading doctag_syn0_lockf from /mnt/data/shalaby/parameter_search_doc2vec_models_extended/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_5/model.docvecs.doctag_syn0_lockf.npy with mmap=None\n",
      "2017-03-09 16:43:17,087 : INFO : loading syn1neg from /mnt/data/shalaby/parameter_search_doc2vec_models_extended/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_5/model.syn1neg.npy with mmap=None\n",
      "2017-03-09 16:43:20,678 : INFO : loading syn0 from /mnt/data/shalaby/parameter_search_doc2vec_models_extended/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_5/model.syn0.npy with mmap=None\n",
      "2017-03-09 16:43:22,701 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-03-09 16:43:22,722 : INFO : setting ignored attribute cum_table to None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 3s, sys: 1min 14s, total: 3min 18s\n",
      "Wall time: 7min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)\n",
    "if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)):\n",
    "    doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX), mmap=DOC2VEC_MMAP)\n",
    "    doc2vec_model.workers = NUM_CORES\n",
    "    GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "else:\n",
    "    info(\"Couldnt find the doc2vec model with epoch {}\".format(epoch))\n",
    "    raise Exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create/Load Training Document Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-16 11:04:12,248 : INFO : Loading Training Document Stats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 1min 19s, total: 2min 22s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.exists(training_doc_stats_file):\n",
    "    info(\"Creating Training Document Stats\")\n",
    "    doc_stats = DocumentsStatsGenerator(training_preprocessed_files_prefix)\n",
    "    doc_stats.get_stats()\n",
    "    pickle.dump(doc_stats, open(training_doc_stats_file, \"w\"))\n",
    "else:\n",
    "    info(\"Loading Training Document Stats\")\n",
    "    doc_stats = pickle.load(open(training_doc_stats_file, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraphs: 47\n",
      "Sentences: 196\n",
      "Max Size: 244\n"
     ]
    }
   ],
   "source": [
    "MAX_PARAGRAPHS = int(np.percentile([len(doc_stats.doc_paragraphs[d]) for d in doc_stats.docids], 50))\n",
    "print \"Paragraphs: {}\".format(MAX_PARAGRAPHS)\n",
    "MAX_SENTENCES = int(np.percentile([len(doc_stats.doc_sentences[d]) for d in doc_stats.docids], 50))\n",
    "print \"Sentences: {}\".format(MAX_SENTENCES)\n",
    "# +1 is for the document vector\n",
    "MAX_SIZE = MAX_PARAGRAPHS + MAX_SENTENCES + 1\n",
    "print \"Max Size: {}\".format(MAX_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20368"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(doc_stats.doc_sentences[d]) for d in doc_stats.docids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4686"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(doc_stats.doc_paragraphs[d]) for d in doc_stats.docids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Training Data Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-16 11:06:35,060 : INFO : Loading Training Data from file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 3min 33s, total: 3min 33s\n",
      "Wall time: 6min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X, y = get_training_data(doc2vec_model, classifications, classifications_type, doc_stats, MAX_SIZE, DOC2VEC_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23454451328\n",
      "(120156, 244, 200)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print sys.getsizeof(X)\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Validation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_dict = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create/Load Validation Doc Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Validation Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 52s, sys: 13.7 s, total: 14min 6s\n",
      "Wall time: 14min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "validation_dict = pickle.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_DICT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.1 s, sys: 14.4 s, total: 31.5 s\n",
      "Wall time: 31.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.exists(validation_doc_stats_file):\n",
    "    validation_doc_stats = DocumentsStatsGenerator(validation_preprocessed_files_prefix)\n",
    "    validation_doc_stats.get_stats()\n",
    "    pickle.dump(validation_doc_stats, open(validation_doc_stats_file, \"w\"))\n",
    "else:\n",
    "    validation_doc_stats = pickle.load(open(validation_doc_stats_file, \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Validation Data Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-16 11:13:59,093 : INFO : Loading Validation Data from file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 1min 54s, total: 1min 54s\n",
      "Wall time: 2min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Xv, yv = get_validation_data(validation_dict, classifications, classifications_type, validation_doc_stats, \n",
    "                             MAX_SIZE, DOC2VEC_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del validation_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_keras_rnn_model(input_size, output_size, lstm_output_size, w_dropout_do, u_dropout_do):\n",
    "    \n",
    "    model= Sequential()\n",
    "    #model.add(Masking(mask_value=0., input_shape=(MAX_SIZE, input_size)))\n",
    "    model.add(LSTM(lstm_output_size, input_dim=input_size, dropout_W=w_dropout_do, dropout_U=u_dropout_do))\n",
    "    model.add(Dense(output_size, activation='sigmoid', name='sigmoid_output'))\n",
    "    model.compile(optimizer=NN_OPTIMIZER, loss='binary_crossentropy')\n",
    "    return model\n",
    "    \n",
    "#     lstm = LSTM(lstm_output_size, input_dim=input_size, dropout_W=w_dropout_do, dropout_U=u_dropout_do)\n",
    "    \n",
    "#     pooling = GlobalAveragePooling1D()(lstm)\n",
    "    \n",
    "#     sigmoid_output = Dense(output_size, activation='sigmoid', name='sigmoid_output')(pooling)\n",
    "\n",
    "#     model = Model(input=doc_input, output=sigmoid_output)\n",
    "#     model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stopper_deltas = {\n",
    "    'sections': 0.0001,\n",
    "    'classes': 0.00001,\n",
    "    'subclasses': 0.00001\n",
    "}\n",
    "early_stopper_patience = {\n",
    "    'sections': 10,\n",
    "    'classes': 10,\n",
    "    'subclasses': 10\n",
    "}\n",
    "epochs_before_validation = {\n",
    "    'sections': 10,\n",
    "    'classes': 50,\n",
    "    'subclasses': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_OUTPUT_NEURONS = len(classifications)\n",
    "\n",
    "EARLY_STOPPER_MIN_DELTA = early_stopper_deltas[classifications_type]\n",
    "EARLY_STOPPER_PATIENCE = early_stopper_patience[classifications_type]\n",
    "\n",
    "NN_MAX_EPOCHS = 100\n",
    "NN_RANDOM_SEARCH_BUDGET = 2\n",
    "NN_PARAM_SAMPLE_SEED = 1234\n",
    "\n",
    "NN_BATCH_SIZE = 1024\n",
    "\n",
    "MODEL_VERBOSITY = 1\n",
    "\n",
    "NN_OPTIMIZER = 'rmsprop'\n",
    "# NN_OPTIMIZER = 'adam'\n",
    "\n",
    "to_skip = []\n",
    "\n",
    "load_existing_results = True\n",
    "save_results = True\n",
    "\n",
    "\n",
    "lstm_output_sizes = [300,400]\n",
    "w_dropout_options = [0.4]\n",
    "u_dropout_options = [0.2]\n",
    "\n",
    "\n",
    "# Uncomment for Specific Configuration\n",
    "NN_RANDOM_SEARCH_BUDGET = 1\n",
    "lstm_output_sizes = [500]\n",
    "w_dropout_options = [0.6]\n",
    "u_dropout_options = [0.4]\n",
    "\n",
    "np.random.seed(NN_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MetricsCallback(keras.callbacks.Callback):\n",
    "    \n",
    "    EPOCHS_BEFORE_VALIDATION = epochs_before_validation[classifications_type]\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.epoch_index = 0\n",
    "        self.val_loss_reductions = 0\n",
    "        self.metrics_dict = {}\n",
    "        self.best_val_loss = np.iinfo(np.int32).max\n",
    "        self.best_weights = None\n",
    "        self.best_validation_metrics = None\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epoch_index += 1\n",
    "        if logs['val_loss'] < self.best_val_loss:\n",
    "            self.val_loss_reductions += 1\n",
    "            self.best_val_loss = logs['val_loss']\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            print '\\r    \\r' # to remove the previous line of verbose output of model fit\n",
    "            time.sleep(0.2)\n",
    "            info('Found lower val loss for epoch {} => {}'.format(self.epoch_index, round(logs['val_loss'], 5)))\n",
    "            if self.val_loss_reductions % MetricsCallback.EPOCHS_BEFORE_VALIDATION == 0:\n",
    "                \n",
    "                info('Validation Loss Reduced {} times'.format(self.val_loss_reductions))\n",
    "                info('Evaluating on Validation Data')\n",
    "                yvp = self.model.predict(Xv)\n",
    "                yvp_binary = get_binary_0_5(yvp)\n",
    "                info('Generating Validation Metrics')\n",
    "                validation_metrics = get_metrics(yv, yvp, yvp_binary)\n",
    "                print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "                    validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "                    validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "                self.metrics_dict[self.epoch_index] = validation_metrics\n",
    "#                 self.best_validation_metrics = validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 01:11:01,019 : INFO : ***************************************************************************************\n",
      "2017-03-14 01:11:01,020 : INFO : lstm_optimizer_rmsprop_size_300_w-drop_0.4_u-drop_0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_1 (LSTM)                    (None, 300)           601200      lstm_input_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "sigmoid_output (Dense)           (None, 8)             2408        lstm_1[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 603608\n",
      "____________________________________________________________________________________________________\n",
      "Train on 120156 samples, validate on 29675 samples\n",
      "Epoch 1/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 01:13:59,341 : INFO : Found lower val loss for epoch 1 => 0.35809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 156s - loss: 0.4378 - val_loss: 0.3581\n",
      "Epoch 2/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 01:16:38,505 : INFO : Found lower val loss for epoch 2 => 0.35705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 159s - loss: 0.3868 - val_loss: 0.3570\n",
      "Epoch 3/100\n",
      "120156/120156 [==============================] - 163s - loss: 0.3835 - val_loss: 0.3609\n",
      "Epoch 4/100\n",
      "120156/120156 [==============================] - 165s - loss: 0.3802 - val_loss: 0.3641\n",
      "Epoch 5/100\n",
      "120156/120156 [==============================] - 165s - loss: 0.3777 - val_loss: 0.3601\n",
      "Epoch 6/100\n",
      "120156/120156 [==============================] - 166s - loss: 0.3716 - val_loss: 0.3760\n",
      "Epoch 7/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 01:30:25,533 : INFO : Found lower val loss for epoch 7 => 0.3456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 166s - loss: 0.3669 - val_loss: 0.3456\n",
      "Epoch 8/100\n",
      "120156/120156 [==============================] - 165s - loss: 0.3599 - val_loss: 0.3498\n",
      "Epoch 9/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 01:35:59,991 : INFO : Found lower val loss for epoch 9 => 0.33401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 168s - loss: 0.3523 - val_loss: 0.3340\n",
      "Epoch 10/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 01:38:47,510 : INFO : Found lower val loss for epoch 10 => 0.315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 167s - loss: 0.3468 - val_loss: 0.3150\n",
      "Epoch 11/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 01:41:35,122 : INFO : Found lower val loss for epoch 11 => 0.29927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 167s - loss: 0.3424 - val_loss: 0.2993\n",
      "Epoch 12/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 01:44:20,629 : INFO : Found lower val loss for epoch 12 => 0.2849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 165s - loss: 0.3383 - val_loss: 0.2849\n",
      "Epoch 13/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 01:47:06,672 : INFO : Found lower val loss for epoch 13 => 0.28289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 166s - loss: 0.3341 - val_loss: 0.2829\n",
      "Epoch 14/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 01:49:53,087 : INFO : Found lower val loss for epoch 14 => 0.26881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 166s - loss: 0.3300 - val_loss: 0.2688\n",
      "Epoch 15/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 01:52:39,292 : INFO : Found lower val loss for epoch 15 => 0.25667\n",
      "2017-03-14 01:52:39,295 : INFO : Validation Loss Reduced 10 times\n",
      "2017-03-14 01:52:39,296 : INFO : Evaluating on Validation Data\n",
      "2017-03-14 01:54:06,483 : INFO : Generating Validation Metrics\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Validation Metrics: Cov Err: 1.972 | Top 3: 0.887 | Top 5: 0.969 | F1 Micro: 0.538 | F1 Macro: 0.363\n",
      "120156/120156 [==============================] - 254s - loss: 0.3271 - val_loss: 0.2567\n",
      "Epoch 16/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 01:56:57,067 : INFO : Found lower val loss for epoch 16 => 0.25227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 169s - loss: 0.3244 - val_loss: 0.2523\n",
      "Epoch 17/100\n",
      "120156/120156 [==============================] - 172s - loss: 0.3216 - val_loss: 0.2550\n",
      "Epoch 18/100\n",
      "120156/120156 [==============================] - 171s - loss: 0.3197 - val_loss: 0.2549\n",
      "Epoch 19/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 02:05:32,257 : INFO : Found lower val loss for epoch 19 => 0.24029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 171s - loss: 0.3179 - val_loss: 0.2403\n",
      "Epoch 20/100\n",
      "120156/120156 [==============================] - 170s - loss: 0.3156 - val_loss: 0.2422\n",
      "Epoch 21/100\n",
      "120156/120156 [==============================] - 171s - loss: 0.3138 - val_loss: 0.2407\n",
      "Epoch 22/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 02:14:06,749 : INFO : Found lower val loss for epoch 22 => 0.23641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 172s - loss: 0.3119 - val_loss: 0.2364\n",
      "Epoch 23/100\n",
      "120156/120156 [==============================] - 172s - loss: 0.3106 - val_loss: 0.2464\n",
      "Epoch 24/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 02:19:49,470 : INFO : Found lower val loss for epoch 24 => 0.23246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 170s - loss: 0.3087 - val_loss: 0.2325\n",
      "Epoch 25/100\n",
      "120156/120156 [==============================] - 171s - loss: 0.3078 - val_loss: 0.2387\n",
      "Epoch 26/100\n",
      "120156/120156 [==============================] - 170s - loss: 0.3061 - val_loss: 0.2357\n",
      "Epoch 27/100\n",
      "120156/120156 [==============================] - 170s - loss: 0.3048 - val_loss: 0.2397\n",
      "Epoch 28/100\n",
      "120156/120156 [==============================] - 172s - loss: 0.3035 - val_loss: 0.2329\n",
      "Epoch 29/100\n",
      "120156/120156 [==============================] - 171s - loss: 0.3028 - val_loss: 0.2375\n",
      "Epoch 30/100\n",
      "120156/120156 [==============================] - 171s - loss: 0.3012 - val_loss: 0.2362\n",
      "Epoch 31/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 02:39:49,085 : INFO : Found lower val loss for epoch 31 => 0.23216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 171s - loss: 0.3007 - val_loss: 0.2322\n",
      "Epoch 32/100\n",
      "120156/120156 [==============================] - 172s - loss: 0.2994 - val_loss: 0.2433\n",
      "Epoch 33/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 02:45:33,135 : INFO : Found lower val loss for epoch 33 => 0.22867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 171s - loss: 0.2986 - val_loss: 0.2287\n",
      "Epoch 34/100\n",
      "120156/120156 [==============================] - 171s - loss: 0.2977 - val_loss: 0.2437\n",
      "Epoch 35/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 02:51:16,537 : INFO : Found lower val loss for epoch 35 => 0.22537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 171s - loss: 0.2972 - val_loss: 0.2254\n",
      "Epoch 36/100\n",
      "120156/120156 [==============================] - 169s - loss: 0.2961 - val_loss: 0.2384\n",
      "Epoch 37/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 02:56:57,199 : INFO : Found lower val loss for epoch 37 => 0.22491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 170s - loss: 0.2954 - val_loss: 0.2249\n",
      "Epoch 38/100\n",
      "120156/120156 [==============================] - 170s - loss: 0.2946 - val_loss: 0.2397\n",
      "Epoch 39/100\n",
      "120156/120156 [==============================] - 170s - loss: 0.2933 - val_loss: 0.2276\n",
      "Epoch 40/100\n",
      "120156/120156 [==============================] - 170s - loss: 0.2931 - val_loss: 0.2295\n",
      "Epoch 41/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 03:08:19,321 : INFO : Found lower val loss for epoch 41 => 0.22251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 170s - loss: 0.2922 - val_loss: 0.2225\n",
      "Epoch 42/100\n",
      "120156/120156 [==============================] - 170s - loss: 0.2915 - val_loss: 0.2434\n",
      "Epoch 43/100\n",
      "120156/120156 [==============================] - 170s - loss: 0.2911 - val_loss: 0.2241\n",
      "Epoch 44/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 03:16:52,743 : INFO : Found lower val loss for epoch 44 => 0.22234\n",
      "2017-03-14 03:16:52,746 : INFO : Validation Loss Reduced 20 times\n",
      "2017-03-14 03:16:52,747 : INFO : Evaluating on Validation Data\n",
      "2017-03-14 03:18:13,723 : INFO : Generating Validation Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Validation Metrics: Cov Err: 1.752 | Top 3: 0.923 | Top 5: 0.982 | F1 Micro: 0.665 | F1 Macro: 0.609\n",
      "120156/120156 [==============================] - 254s - loss: 0.2903 - val_loss: 0.2223\n",
      "Epoch 45/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 03:21:06,620 : INFO : Found lower val loss for epoch 45 => 0.21895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 171s - loss: 0.2896 - val_loss: 0.2189\n",
      "Epoch 46/100\n",
      "120156/120156 [==============================] - 171s - loss: 0.2888 - val_loss: 0.2210\n",
      "Epoch 47/100\n",
      "120156/120156 [==============================] - 171s - loss: 0.2885 - val_loss: 0.2286\n",
      "Epoch 48/100\n",
      "120156/120156 [==============================] - 170s - loss: 0.2879 - val_loss: 0.2277\n",
      "Epoch 49/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 03:32:31,585 : INFO : Found lower val loss for epoch 49 => 0.21744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 171s - loss: 0.2873 - val_loss: 0.2174\n",
      "Epoch 50/100\n",
      "120156/120156 [==============================] - 171s - loss: 0.2870 - val_loss: 0.2181\n",
      "Epoch 51/100\n",
      "120156/120156 [==============================] - 171s - loss: 0.2866 - val_loss: 0.2277\n",
      "Epoch 52/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 03:41:06,316 : INFO : Found lower val loss for epoch 52 => 0.21055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 172s - loss: 0.2858 - val_loss: 0.2105\n",
      "Epoch 53/100\n",
      "120156/120156 [==============================] - 170s - loss: 0.2854 - val_loss: 0.2215\n",
      "Epoch 54/100\n",
      "120156/120156 [==============================] - 170s - loss: 0.2847 - val_loss: 0.2228\n",
      "Epoch 55/100\n",
      "120156/120156 [==============================] - 171s - loss: 0.2844 - val_loss: 0.2118\n",
      "Epoch 56/100\n",
      "120156/120156 [==============================] - 171s - loss: 0.2838 - val_loss: 0.2253\n",
      "Epoch 57/100\n",
      "120156/120156 [==============================] - 170s - loss: 0.2833 - val_loss: 0.2107\n",
      "Epoch 58/100\n",
      "120156/120156 [==============================] - 171s - loss: 0.2830 - val_loss: 0.2185\n",
      "Epoch 59/100\n",
      "120156/120156 [==============================] - 169s - loss: 0.2825 - val_loss: 0.2172\n",
      "Epoch 60/100\n",
      "120156/120156 [==============================] - 171s - loss: 0.2823 - val_loss: 0.2125\n",
      "Epoch 61/100\n",
      "120156/120156 [==============================] - 169s - loss: 0.2818 - val_loss: 0.2112\n",
      "Epoch 62/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 04:09:33,101 : INFO : Found lower val loss for epoch 62 => 0.20774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 170s - loss: 0.2812 - val_loss: 0.2077\n",
      "Epoch 63/100\n",
      "120156/120156 [==============================] - 170s - loss: 0.2808 - val_loss: 0.2158\n",
      "Epoch 64/100\n",
      "120156/120156 [==============================] - 170s - loss: 0.2808 - val_loss: 0.2229\n",
      "Epoch 65/100\n",
      "120156/120156 [==============================] - 171s - loss: 0.2803 - val_loss: 0.2172\n",
      "Epoch 66/100\n",
      "120156/120156 [==============================] - 169s - loss: 0.2798 - val_loss: 0.2117\n",
      "Epoch 67/100\n",
      "120156/120156 [==============================] - 170s - loss: 0.2792 - val_loss: 0.2080\n",
      "Epoch 68/100\n",
      "120156/120156 [==============================] - 171s - loss: 0.2792 - val_loss: 0.2205\n",
      "Epoch 69/100\n",
      "120156/120156 [==============================] - 169s - loss: 0.2788 - val_loss: 0.2196\n",
      "Epoch 70/100\n",
      "120156/120156 [==============================] - 170s - loss: 0.2785 - val_loss: 0.2111\n",
      "Epoch 71/100\n",
      "120156/120156 [==============================] - 170s - loss: 0.2783 - val_loss: 0.2081\n",
      "Epoch 72/100\n",
      "120156/120156 [==============================] - 170s - loss: 0.2778 - val_loss: 0.2142\n",
      "Epoch 73/100\n",
      "120156/120156 [==============================] - 170s - loss: 0.2771 - val_loss: 0.2160"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 04:40:48,571 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00072: early stopping\n",
      "CPU times: user 1h 23min 58s, sys: 2h 3min 45s, total: 3h 27min 43s\n",
      "Wall time: 3h 29min 43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 04:46:08,035 : INFO : Generating Training Metrics\n",
      "2017-03-14 04:46:11,325 : INFO : Evaluating on Validation Data using saved best weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Training Metrics: Cov Err: 2.242 | Top 3: 0.818 | Top 5: 0.953 | F1 Micro: 0.535 | F1 Macro: 0.465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-14 04:47:36,037 : INFO : Generating Validation Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Validation Metrics: Cov Err: 1.668 | Top 3: 0.941 | Top 5: 0.986 | F1 Micro: 0.691 | F1 Macro: 0.631\n"
     ]
    }
   ],
   "source": [
    "param_sampler = ParameterSampler({\n",
    "    'lstm_output_size':lstm_output_sizes,\n",
    "    'w_dropout':w_dropout_options,\n",
    "    'u_dropout':u_dropout_options,\n",
    "}, n_iter=NN_RANDOM_SEARCH_BUDGET, random_state=NN_PARAM_SAMPLE_SEED)\n",
    "\n",
    "param_results_dict = {}\n",
    "if load_existing_results:\n",
    "    param_results_path = os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                       NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE)))\n",
    "    if os.path.exists(param_results_path):\n",
    "        param_results_dict = pickle.load(open(param_results_path))\n",
    "    else:\n",
    "        info('No Previous results exist in {}'.format(param_results_path))\n",
    "        \n",
    "# create nn parameter search directory\n",
    "if not os.path.exists(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME)):\n",
    "    os.makedirs(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME))\n",
    "        \n",
    "for parameters in param_sampler:\n",
    "    start_time = time.time()\n",
    "    lstm_output_size = parameters['lstm_output_size']\n",
    "    w_dropout_do = parameters['w_dropout']\n",
    "    u_dropout_do = parameters['u_dropout']\n",
    "\n",
    "    GLOBAL_VARS.NN_MODEL_NAME = 'lstm_optimizer_{}_size_{}_w-drop_{}_u-drop_{}'.format(NN_OPTIMIZER,\n",
    "        lstm_output_size,  w_dropout_do, u_dropout_do\n",
    "    )\n",
    "\n",
    "    if GLOBAL_VARS.NN_MODEL_NAME in param_results_dict.keys() or GLOBAL_VARS.NN_MODEL_NAME in to_skip:\n",
    "        print \"skipping: {}\".format(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "        continue\n",
    "#         if first_hidden_layer_size < DOC2VEC_SIZE or second_hidden_layer_size < NN_OUTPUT_NEURONS:\n",
    "#             print \"skipping: {} due to 1st layer size {} < {} or 2nd layer size {} < {}\".format(GLOBAL_VARS.NN_MODEL_NAME,\n",
    "#                                                                                                 first_hidden_layer_size, DOC2VEC_SIZE, \n",
    "#                                                                                                 second_hidden_layer_size, NN_OUTPUT_NEURONS)\n",
    "#             continue\n",
    "\n",
    "\n",
    "    info('***************************************************************************************')\n",
    "    info(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "\n",
    "    model = create_keras_rnn_model(DOC2VEC_SIZE, NN_OUTPUT_NEURONS, \n",
    "                                  lstm_output_size, w_dropout_do, u_dropout_do)\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    early_stopper = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=EARLY_STOPPER_MIN_DELTA, \\\n",
    "                                                  patience=EARLY_STOPPER_PATIENCE, verbose=1, mode='auto')\n",
    "    metrics_callback = MetricsCallback()\n",
    "\n",
    "\n",
    "\n",
    "    # Model Fitting\n",
    "    %time history = model.fit(x=X, y=y, validation_data=(Xv,yv), batch_size=NN_BATCH_SIZE, \\\n",
    "                              nb_epoch=NN_MAX_EPOCHS, verbose=MODEL_VERBOSITY, \\\n",
    "                              callbacks=[early_stopper, metrics_callback])\n",
    "    \n",
    "    info('Evaluating on Training Data')\n",
    "    yp = model.predict(X)\n",
    "    yp_binary = get_binary_0_5(yp)\n",
    "    #print yvp\n",
    "    info('Generating Training Metrics')\n",
    "    training_metrics = get_metrics(y, yp, yp_binary)\n",
    "    print \"****** Training Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    training_metrics['coverage_error'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "    training_metrics['f1_micro'], training_metrics['f1_macro'])\n",
    "    \n",
    "    \n",
    "    info('Evaluating on Validation Data using saved best weights')\n",
    "    model.set_weights(metrics_callback.best_weights)\n",
    "    yvp = model.predict(Xv)\n",
    "    yvp_binary = get_binary_0_5(yvp)\n",
    "    #print yvp\n",
    "    info('Generating Validation Metrics')\n",
    "    validation_metrics = get_metrics(yv, yvp, yvp_binary)\n",
    "    print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "        validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "        validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "    best_validation_metrics = validation_metrics\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME] = dict()\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['best_validation_metrics'] = best_validation_metrics\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['epochs'] = len(history.history['val_loss'])\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['best_weights'] = metrics_callback.best_weights\n",
    "#         param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['last_weights'] = last_model_weights\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['duration'] =  duration\n",
    "\n",
    "    del history, metrics_callback, model\n",
    "\n",
    "if save_results:\n",
    "    pickle.dump(param_results_dict, open(os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                                   NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE))), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NameError: name 'model' is not defined\n"
     ]
    }
   ],
   "source": [
    "%xdel model\n",
    "import gc\n",
    "for i in range(3): gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lstm_optimizer_rmsprop_size_200_w-drop_0.4_u-drop_0.2',\n",
       " 'lstm_optimizer_adam_size_300_w-drop_0.2_u-drop_0.3',\n",
       " 'lstm_optimizer_adam_size_200_w-drop_0.3_u-drop_0.2',\n",
       " 'lstm_optimizer_adam_size_200_w-drop_0.2_u-drop_0.3',\n",
       " 'lstm_optimizer_adam_size_200_w-drop_0.2_u-drop_0.2',\n",
       " 'lstm_optimizer_rmsprop_size_200_w-drop_0.2_u-drop_0.4']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_results_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(param_results_dict, open(os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                                   NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE))), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # create nn parameter search directory\n",
    "    if not os.path.exists(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME)):\n",
    "        os.makedirs(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(param_results_dict, open(os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                                   NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE))), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 03:51:31,120 : INFO : Generating Training Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 6s, sys: 1min 35s, total: 5min 41s\n",
      "Wall time: 5min 41s\n",
      "CPU times: user 136 ms, sys: 40 ms, total: 176 ms\n",
      "Wall time: 172 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Training Metrics: Cov Err: 2.940 | Top 3: 0.705 | Top 5: 0.910 | F1 Micro: 0.000 | F1 Macro: 0.000\n",
      "CPU times: user 4min 9s, sys: 1min 35s, total: 5min 44s\n",
      "Wall time: 5min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%time yp = model.predict(X)\n",
    "%time yp_binary = get_binary_0_5(yp)\n",
    "#print yvp\n",
    "info('Generating Training Metrics')\n",
    "training_metrics = get_metrics(y, yp, yp_binary)\n",
    "print \"****** Training Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    training_metrics['coverage_error'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "    training_metrics['f1_micro'], training_metrics['f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2937    \n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2882    \n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2843    \n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2808    \n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2779    \n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2743    \n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2729    \n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2684    \n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2687    \n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2713    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f253497d250>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X, y=y, batch_size=NN_BATCH_SIZE, \\\n",
    "                              nb_epoch=10, verbose=MODEL_VERBOSITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-07 15:30:31,828 : INFO : Generating Training Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.9 s, sys: 17.3 s, total: 56.2 s\n",
      "Wall time: 56.2 s\n",
      "CPU times: user 20 ms, sys: 4 ms, total: 24 ms\n",
      "Wall time: 23.7 ms\n",
      "****** Validation Metrics: Cov Err: 2.181 | Top 3: 0.853 | Top 5: 0.954 | F1 Micro: 0.541 | F1 Macro: 0.403\n",
      "CPU times: user 39.3 s, sys: 17.3 s, total: 56.7 s\n",
      "Wall time: 56.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%time yp = model.predict(X)\n",
    "%time yp_binary = get_binary_0_5(yp)\n",
    "#print yvp\n",
    "info('Generating Training Metrics')\n",
    "training_metrics = get_metrics(y, yp, yp_binary)\n",
    "print \"****** Training Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    training_metrics['coverage_error'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "    training_metrics['f1_micro'], training_metrics['f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2643    \n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2611    \n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2612    \n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2650    \n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2597    \n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2572    \n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2551    \n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2519    \n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2484    \n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2497    \n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2501    \n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2461    \n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2460    \n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2423    \n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2408    \n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2403    \n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2477    \n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2427    \n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2396    \n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2371    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2534933d90>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X, y=y, batch_size=NN_BATCH_SIZE, \\\n",
    "                              nb_epoch=20, verbose=MODEL_VERBOSITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-07 16:05:49,006 : INFO : Generating Training Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38 s, sys: 18.4 s, total: 56.4 s\n",
      "Wall time: 56.4 s\n",
      "CPU times: user 24 ms, sys: 0 ns, total: 24 ms\n",
      "Wall time: 23.7 ms\n",
      "****** Validation Metrics: Cov Err: 1.980 | Top 3: 0.887 | Top 5: 0.963 | F1 Micro: 0.628 | F1 Macro: 0.467\n",
      "CPU times: user 38.4 s, sys: 18.4 s, total: 56.9 s\n",
      "Wall time: 56.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%time yp = model.predict(X)\n",
    "%time yp_binary = get_binary_0_5(yp)\n",
    "#print yvp\n",
    "info('Generating Training Metrics')\n",
    "training_metrics = get_metrics(y, yp, yp_binary)\n",
    "print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    training_metrics['coverage_error'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "    training_metrics['f1_micro'], training_metrics['f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
