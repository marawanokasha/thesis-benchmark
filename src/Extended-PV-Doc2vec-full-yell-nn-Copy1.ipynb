{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K40m (CNMeM is disabled, cuDNN 5105)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple, defaultdict\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "from functools import partial\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "from thesis.utils.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IS_SAMPLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_SEED = 1234\n",
    "DOC2VEC_SEED = 1234\n",
    "WORD2VEC_SEED = 1234\n",
    "NN_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_WORD_COUNT = 50\n",
    "MIN_SIZE = 0\n",
    "NUM_CORES = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GLOBAL_VARS = namedtuple('GLOBAL_VARS', ['MODEL_NAME', 'DOC2VEC_MODEL_NAME', 'DOC2VEC_MODEL', \n",
    "                                         'SVM_MODEL_NAME', 'NN_MODEL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_MODEL = \"vocab_model\"\n",
    "MODEL_PREFIX = \"model\"\n",
    "VALIDATION_MATRIX = \"validation_matrix.pkl\"\n",
    "VALIDATION_DICT = \"validation_dict.pkl\"\n",
    "TEST_MATRIX = \"test_matrix.pkl\"\n",
    "TEST_DICT = \"test_dict.pkl\"\n",
    "METRICS = \"metrics.pkl\"\n",
    "CLASSIFIER = \"classifier.pkl\"\n",
    "TYPE_CLASSIFIER= \"{}_classifier.pkl\"\n",
    "\n",
    "TRAINING_DATA_MATRIX = \"X.npy\"\n",
    "TRAINING_LABELS_MATRIX = \"y_{}.npy\"\n",
    "VALIDATION_DATA_MATRIX = \"Xv.npy\"\n",
    "VALIDATION_LABELS_MATRIX = \"yv_{}.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_PARAMETER_SEARCH_PREFIX = \"lstm_{}_batch_{}_nn_parameter_searches.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_location = \"/mnt/data2/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "doc2vec_model_save_location = os.path.join(\"/mnt/data/shalaby/\", \"parameter_search_doc2vec_models_extended\", \"full\")\n",
    "nn_parameter_search_location = os.path.join(root_location, \"nn_parameter_search_extended\")\n",
    "if not os.path.exists(doc2vec_model_save_location):\n",
    "    os.makedirs(doc2vec_model_save_location)\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL)):\n",
    "    os.makedirs(os.path.join(doc2vec_model_save_location, VOCAB_MODEL))\n",
    "\n",
    "#training_file = root_location + \"docs_output.json\"\n",
    "training_file = root_location + 'docs_output.json'\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "valid_classes_file = exports_location + \"valid_classes.pkl\"\n",
    "valid_subclasses_file = exports_location + \"valid_subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "training_docs_list_file = exports_location + \"extended_pv_training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"extended_pv_validation_docs_list.pkl\"\n",
    "test_docs_list_file = exports_location + \"extended_pv_test_docs_list.pkl\"\n",
    "\n",
    "\n",
    "training_doc_stats_file = exports_location + \"extended_pv_training_doc_stats.pkl\"\n",
    "validation_doc_stats_file = exports_location + \"extended_pv_validation_doc_stats.pkl\"\n",
    "\n",
    "preprocessed_location = root_location + \"preprocessed_data/\"\n",
    "\n",
    "training_preprocessed_files_prefix = preprocessed_location + \"extended_pv_training_docs_data_preprocessed-\"\n",
    "validation_preprocessed_files_prefix = preprocessed_location + \"extended_pv_validation_docs_data_preprocessed-\"\n",
    "test_preprocessed_files_prefix = preprocessed_location + \"extended_pv_test_docs_data_preprocessed-\"\n",
    "\n",
    "word2vec_questions_file = result = root_location + 'tensorflow/word2vec/questions-words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.6 s, sys: 760 ms, total: 16.4 s\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "valid_classes = pickle.load(open(valid_classes_file))\n",
    "valid_subclasses = pickle.load(open(valid_subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120156"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29675"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37771"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ensure_disk_location_exists(location):\n",
    "    if not os.path.exists(location):\n",
    "        os.makedirs(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VALIDATION_MINI_BATCH_SIZE = 10000\n",
    "def get_extended_docs_with_inference(doc2vec_model, doc_classification_map, classifications,\n",
    "                                           docs_list, file_to_write, preprocessed_files_prefix):\n",
    "    \"\"\"\n",
    "    Use the trained doc2vec model to get the paragraph vector representations of the validation or test documents\n",
    "    \"\"\"\n",
    "\n",
    "    def infer_one_doc(doc_tuple):\n",
    "        # doc2vec_model.random = np.random.RandomState(DOC2VEC_SEED)\n",
    "        doc_id, doc_tokens = doc_tuple\n",
    "        rep = doc2vec_model.infer_vector(doc_tokens)\n",
    "        return (doc_id, rep)\n",
    "\n",
    "\n",
    "    one_hot_encoder = OneHotEncoder(classifications)\n",
    "    classifications_set = set(classifications)\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write)):\n",
    "        info(\"===== Loading inference vectors\")\n",
    "        inference_labels = []\n",
    "        inference_vectors_matrix = pickle.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write)))\n",
    "        info(\"Loaded inference vectors matrix\")\n",
    "        for i, doc_id in enumerate(docs_list):\n",
    "            curr_doc_labels = set(doc_classification_map[doc_id]) & classifications_set\n",
    "            inference_labels.append(one_hot_encoder.get_label_vector(curr_doc_labels))\n",
    "            if i % 100000 == 0:\n",
    "                info(\"Finished {} in validation loading\".format(i))\n",
    "        inference_labels = np.array(inference_labels, dtype=np.int8)\n",
    "    else:\n",
    "        inference_documents_reps = {}\n",
    "        inference_vectors = []\n",
    "        inference_labels = []\n",
    "        info(\"===== Getting vectors with inference\")\n",
    "\n",
    "        # Multi-threaded inference\n",
    "        inference_docs_iterator = ExtendedPVDocumentBatchGenerator(preprocessed_files_prefix, batch_size=None)\n",
    "        generator_func = inference_docs_iterator.__iter__()\n",
    "        pool = ThreadPool(NUM_CORES)\n",
    "        # map consumes the whole iterator on the spot, so we have to use itertools.islice to fake mini-batching\n",
    "        mini_batch_size = VALIDATION_MINI_BATCH_SIZE\n",
    "        while True:\n",
    "            threaded_reps_partial = pool.map(infer_one_doc, itertools.islice(generator_func, mini_batch_size))\n",
    "            info(\"Finished: {} tags\".format(str(inference_docs_iterator.curr_index + 1)))\n",
    "            if threaded_reps_partial:\n",
    "                # threaded_reps.extend(threaded_reps_partial)\n",
    "                inference_documents_reps.update(threaded_reps_partial)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # create matrix for the inferred vectors\n",
    "        for doc_id in docs_list:\n",
    "            inference_vectors.append(inference_documents_reps[doc_id])\n",
    "            curr_doc_labels = set(doc_classification_map[doc_id]) & classifications_set\n",
    "            inference_labels.append(one_hot_encoder.get_label_vector(curr_doc_labels))\n",
    "        inference_vectors_matrix = np.array(inference_vectors)\n",
    "        inference_labels = np.array(inference_labels, dtype=np.int8)\n",
    "        pickle.dump(inference_vectors_matrix,\n",
    "                    open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write), 'w'))\n",
    "\n",
    "    return inference_vectors_matrix, inference_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OneHotEncoder():\n",
    "    \n",
    "    def __init__(self, classifications):\n",
    "        self.classifications = classifications\n",
    "        self.one_hot_indices = {}\n",
    "\n",
    "        # convert character classifications to bit vectors\n",
    "        for i, clssf in enumerate(classifications):\n",
    "            bits = [0] * len(classifications)\n",
    "            bits[i] = 1\n",
    "            self.one_hot_indices[clssf] = i\n",
    "    \n",
    "    def get_label_vector(self, labels):\n",
    "        \"\"\"\n",
    "        classes: array of string with the classes assigned to the instance\n",
    "        \"\"\"\n",
    "        output_vector = [0] * len(self.classifications)\n",
    "        for label in labels:\n",
    "            index = self.one_hot_indices[label]\n",
    "            output_vector[index] = 1\n",
    "            \n",
    "        return output_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ExtendedPVDocumentBatchGenerator(object):\n",
    "    def __init__(self, filename_prefix, batch_size=10000 ):\n",
    "        \"\"\"\n",
    "        batch_size cant be > 10,000 due to a limitation in doc2vec training, \n",
    "        None means no batching (only use for inference)\n",
    "        \"\"\"\n",
    "        assert batch_size <= 10000 or batch_size is None\n",
    "        self.filename_prefix = filename_prefix\n",
    "        self.curr_lines = []\n",
    "        self.curr_docids = []\n",
    "        self.batch_size = batch_size\n",
    "        self.curr_index = 0\n",
    "        self.curr_doc_index = 0\n",
    "        self.batch_end = -1\n",
    "    def load_new_batch_in_memory(self):\n",
    "        del self.curr_lines, self.curr_docids\n",
    "        self.curr_lines, self.curr_docids = [], []\n",
    "        info(\"Loading new batch for index: {}\".format(self.curr_doc_index))\n",
    "        #if self.curr_doc_index > 0: self.curr_doc_index -= 1\n",
    "        true_docs_count = 0\n",
    "        try:\n",
    "            with open(self.filename_prefix + str(self.curr_doc_index)) as preproc_file:\n",
    "                for line in preproc_file:\n",
    "                    line_array = line.split(\" \")\n",
    "                    doc_id = line_array[0]\n",
    "                    doc_tokens = line_array[1:]\n",
    "                    self.curr_lines.append(doc_tokens)\n",
    "                    self.curr_docids.append(doc_id)\n",
    "                    if is_real_doc(doc_id):\n",
    "                        true_docs_count+= 1\n",
    "            self.batch_end = self.curr_doc_index + true_docs_count - 1 \n",
    "            info(true_docs_count)\n",
    "            info(\"Finished loading new batch\")\n",
    "        except IOError:\n",
    "            info(\"No more batches to load, exiting at index: {}\".format(self.curr_index))\n",
    "            raise StopIteration()\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            if self.curr_doc_index > self.batch_end:\n",
    "                self.load_new_batch_in_memory()\n",
    "            for (doc_id, tokens) in zip(self.curr_docids, self.curr_lines):\n",
    "                if self.batch_size is not None:\n",
    "                    curr_batch_iter = 0\n",
    "                    # divide the document to batches according to the batch size\n",
    "                    while curr_batch_iter < len(tokens):\n",
    "                        yield LabeledSentence(words=tokens[curr_batch_iter: curr_batch_iter + self.batch_size], tags=[doc_id])\n",
    "                        curr_batch_iter += self.batch_size\n",
    "                else:\n",
    "                    yield doc_id, tokens\n",
    "                self.curr_index += 1\n",
    "                # increment only for full docs\n",
    "                if is_real_doc(doc_id):\n",
    "                    self.curr_doc_index += 1\n",
    "                    if self.curr_doc_index % 1000 == 0:\n",
    "                        info(\"New Doc: {:5}, Batch End: {:5}\".format(self.curr_doc_index, self.batch_end))\n",
    "\n",
    "def is_real_doc(doc_id):\n",
    "    return doc_id.find(\"_\") == -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Document, Paragraph and Sentence Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DocumentsStatsGenerator(object):\n",
    "    def __init__(self, filename_prefix):\n",
    "        self.filename_prefix = filename_prefix\n",
    "        self.docids = []\n",
    "        self.doc_paragraphs = defaultdict(list)\n",
    "        self.doc_sentences = defaultdict(list)\n",
    "        self.curr_doc_index = 0\n",
    "        self.batch_end = -1\n",
    "    def load_new_batch_in_memory(self):\n",
    "        info(\"Loading new batch for index: {}\".format(self.curr_doc_index))\n",
    "        true_docs_count = 0\n",
    "        try:\n",
    "            with open(self.filename_prefix + str(self.curr_doc_index)) as preproc_file:\n",
    "                for line in preproc_file:\n",
    "                    line_array = line.split(\" \", 1)\n",
    "                    entity_id = line_array[0].strip()\n",
    "                    if self.is_doc(entity_id):\n",
    "                        self.docids.append(entity_id)\n",
    "                        true_docs_count+= 1\n",
    "                    elif self.is_paragraph(entity_id):\n",
    "                        self.doc_paragraphs[self.get_doc_id(entity_id)].append(entity_id)\n",
    "                    elif self.is_sentence(entity_id):\n",
    "                        self.doc_sentences[self.get_doc_id(entity_id)].append(entity_id)\n",
    "            self.batch_end = self.curr_doc_index + true_docs_count - 1 \n",
    "            info(\"Finished loading new batch of {} documents\".format(true_docs_count))\n",
    "        except IOError:\n",
    "            info(\"No more batches to load, exiting at index: {}\".format(self.curr_doc_index))\n",
    "            raise StopIteration()\n",
    "    def get_stats(self):\n",
    "        try:\n",
    "            while True:\n",
    "                if self.curr_doc_index > self.batch_end:\n",
    "                    self.load_new_batch_in_memory()\n",
    "                self.curr_doc_index = self.batch_end + 1\n",
    "        except StopIteration:\n",
    "            pass\n",
    "            \n",
    "    def get_doc_id(self, entity_id):\n",
    "        return entity_id.split(\"_\")[0]\n",
    "    def get_entity_parts(self, entity_id):\n",
    "        return entity_id.split(\"_\")\n",
    "    def is_doc(self, entity_id):\n",
    "        parts = self.get_entity_parts(entity_id)\n",
    "        if len(parts) == 1:\n",
    "            return True\n",
    "        return False\n",
    "    def is_paragraph(self, entity_id):\n",
    "        parts = self.get_entity_parts(entity_id)\n",
    "        if len(parts) == 2:\n",
    "            return True\n",
    "        return False\n",
    "    def is_sentence(self, entity_id):\n",
    "        parts = self.get_entity_parts(entity_id)\n",
    "        if len(parts) == 3:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_doc_vector(entity_id):\n",
    "    if DOC2VEC_MMAP:\n",
    "        normal_array = []\n",
    "        normal_array[:] = doc2vec_model.docvecs[entity_id][:]\n",
    "        return normal_array\n",
    "    else:\n",
    "        return doc2vec_model.docvecs[entity_id]\n",
    "\n",
    "def data_generator(doc_stats, doc_id):\n",
    "    yield get_doc_vector(doc_id)\n",
    "    for paragraph_id in doc_stats.doc_paragraphs[doc_id]:\n",
    "        yield get_doc_vector(paragraph_id)\n",
    "    for sentence_id in doc_stats.doc_sentences[doc_id]:\n",
    "        yield get_doc_vector(sentence_id)\n",
    "    while True:\n",
    "        yield ZERO_VECTOR\n",
    "\n",
    "\n",
    "def validation_data_generator(doc_stats, validation_dict, doc_id):\n",
    "    yield validation_dict[doc_id]\n",
    "    for paragraph_id in doc_stats.doc_paragraphs[doc_id]:\n",
    "        yield validation_dict[paragraph_id]\n",
    "    for sentence_id in doc_stats.doc_sentences[doc_id]:\n",
    "        yield validation_dict[sentence_id]\n",
    "    while True:\n",
    "        yield ZERO_VECTOR\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_training_data(doc2vec_model, classifications, classifications_type, doc_stats, sequence_size, embedding_size):\n",
    "    if not os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, TRAINING_DATA_MATRIX)):\n",
    "        info(\"Creating Training Data\")\n",
    "        one_hot_encoder = OneHotEncoder(classifications)\n",
    "        classifications_set = set(classifications)\n",
    "        # 1st level: document level\n",
    "        training_data = np.ndarray((len(training_docs_list), sequence_size, embedding_size), dtype=np.float32)\n",
    "        info(\"Training Data shape: {}\".format(training_data.shape))\n",
    "        training_labels_mat = np.zeros((len(training_docs_list), len(classifications)), dtype=np.int8)\n",
    "        for i, doc_id in enumerate(training_docs_list):\n",
    "            data_gen = data_generator(doc_stats, doc_id)\n",
    "            # 2nd level: constituents\n",
    "            for j in range(sequence_size):\n",
    "                #3d level: feature vectors\n",
    "                training_data[i][j]= data_gen.next()\n",
    "            eligible_classifications = set(doc_classification_map[doc_id]) & classifications_set\n",
    "            training_labels_mat[i][:] = one_hot_encoder.get_label_vector(eligible_classifications)\n",
    "            if i % 10000 == 0:\n",
    "                info(\"Finished {} in training\".format(i))\n",
    "        \n",
    "        info(\"Saving Training Data to file...\")\n",
    "        np.save(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                  TRAINING_DATA_MATRIX), \"w\"), training_data)\n",
    "        np.save(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                  TRAINING_LABELS_MATRIX.format(classifications_type)), \"w\"), training_labels_mat)\n",
    "    else:\n",
    "        info(\"Loading Training Data from file\")\n",
    "        training_data = np.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                  TRAINING_DATA_MATRIX)))\n",
    "        training_labels_mat = np.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                        TRAINING_LABELS_MATRIX.format(classifications_type))))\n",
    "    return training_data, training_labels_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_data(validation_dict, classifications, classifications_type, doc_stats, sequence_size, embedding_size):\n",
    "    if not os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_DATA_MATRIX)):\n",
    "        info(\"Creating Validation Data\")\n",
    "        one_hot_encoder = OneHotEncoder(classifications)\n",
    "        classifications_set = set(classifications)\n",
    "        # 1st level: document level\n",
    "        validation_data = np.ndarray((len(validation_docs_list), sequence_size, embedding_size), dtype=np.float32)\n",
    "        info(\"Validation Data shape: {}\".format(validation_data.shape))\n",
    "        validation_labels_mat = np.zeros((len(validation_docs_list), len(classifications)), dtype=np.int8)\n",
    "        for i, doc_id in enumerate(validation_docs_list):\n",
    "            data_gen = validation_data_generator(doc_stats, validation_dict, doc_id)\n",
    "            # 2nd level: constituents\n",
    "            for j in range(sequence_size):\n",
    "                #3d level: feature vectors\n",
    "                validation_data[i][j]= data_gen.next()\n",
    "            eligible_classifications = set(doc_classification_map[doc_id]) & classifications_set\n",
    "            validation_labels_mat[i][:] = one_hot_encoder.get_label_vector(eligible_classifications)\n",
    "            if i % 10000 == 0:\n",
    "                info(\"Finished {} in validation\".format(i))\n",
    "        \n",
    "        info(\"Saving Validation Data to file...\")\n",
    "        np.save(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                  VALIDATION_DATA_MATRIX), \"w\"), validation_data)\n",
    "        np.save(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                  VALIDATION_LABELS_MATRIX.format(classifications_type)), \"w\"), validation_labels_mat)\n",
    "    else:\n",
    "        info(\"Loading Validation Data from file\")\n",
    "        validation_data = np.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                  VALIDATION_DATA_MATRIX)))\n",
    "        validation_labels_mat = np.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                        VALIDATION_LABELS_MATRIX.format(classifications_type))))\n",
    "    return validation_data, validation_labels_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Training, validation and Metrics Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Masking\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifications = sections\n",
    "classifications_type = 'sections'\n",
    "classifier_file = TYPE_CLASSIFIER.format(classifications_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VALIDATION_METRICS_FILENAME= '{}_validation_metrics.pkl'.format(classifications_type)\n",
    "TRAINING_METRICS_FILENAME = '{}_training_metrics.pkl'.format(classifications_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOC2VEC_SIZE = 200\n",
    "DOC2VEC_WINDOW = 2\n",
    "DOC2VEC_MAX_VOCAB_SIZE = None\n",
    "DOC2VEC_SAMPLE = 1e-3\n",
    "DOC2VEC_TYPE = 1\n",
    "DOC2VEC_HIERARCHICAL_SAMPLE = 0\n",
    "DOC2VEC_NEGATIVE_SAMPLE_SIZE = 10\n",
    "DOC2VEC_CONCAT = 0\n",
    "DOC2VEC_MEAN = 1\n",
    "DOC2VEC_TRAIN_WORDS = 0\n",
    "DOC2VEC_EPOCHS = 1 # we do our training manually one epoch at a time\n",
    "DOC2VEC_MAX_EPOCHS = 8\n",
    "REPORT_DELAY = 20 # report the progress every x seconds\n",
    "REPORT_VOCAB_PROGRESS = 100000 # report vocab progress every x documents\n",
    "\n",
    "# DOC2VEC_MMAP = 'r'\n",
    "DOC2VEC_MMAP = None\n",
    "\n",
    "ZERO_VECTOR = [0] * DOC2VEC_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8\n"
     ]
    }
   ],
   "source": [
    "placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}'.format(DOC2VEC_SIZE, \n",
    "                                                                DOC2VEC_WINDOW, \n",
    "                                                                'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                                DOC2VEC_TRAIN_WORDS,\n",
    "                                                                DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                                str(DOC2VEC_MAX_VOCAB_SIZE))\n",
    "GLOBAL_VARS.DOC2VEC_MODEL_NAME = placeholder_model_name\n",
    "placeholder_model_name = os.path.join(placeholder_model_name, \"epoch_{}\")\n",
    "\n",
    "epoch = 8\n",
    "\n",
    "GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "doc2vec_model = None\n",
    "print GLOBAL_VARS.MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 16:36:29,548 : INFO : loading Doc2Vec object from /mnt/data/shalaby/parameter_search_doc2vec_models_extended/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_5/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/shalaby/parameter_search_doc2vec_models_extended/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_5/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 16:38:44,710 : INFO : loading docvecs recursively from /mnt/data/shalaby/parameter_search_doc2vec_models_extended/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_5/model.docvecs.* with mmap=None\n",
      "2017-03-09 16:38:44,712 : INFO : loading doctag_syn0 from /mnt/data/shalaby/parameter_search_doc2vec_models_extended/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_5/model.docvecs.doctag_syn0.npy with mmap=None\n",
      "2017-03-09 16:43:14,235 : INFO : loading doctag_syn0_lockf from /mnt/data/shalaby/parameter_search_doc2vec_models_extended/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_5/model.docvecs.doctag_syn0_lockf.npy with mmap=None\n",
      "2017-03-09 16:43:17,087 : INFO : loading syn1neg from /mnt/data/shalaby/parameter_search_doc2vec_models_extended/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_5/model.syn1neg.npy with mmap=None\n",
      "2017-03-09 16:43:20,678 : INFO : loading syn0 from /mnt/data/shalaby/parameter_search_doc2vec_models_extended/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_5/model.syn0.npy with mmap=None\n",
      "2017-03-09 16:43:22,701 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-03-09 16:43:22,722 : INFO : setting ignored attribute cum_table to None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 3s, sys: 1min 14s, total: 3min 18s\n",
      "Wall time: 7min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)\n",
    "if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)):\n",
    "    doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX), mmap=DOC2VEC_MMAP)\n",
    "    doc2vec_model.workers = NUM_CORES\n",
    "    GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "else:\n",
    "    info(\"Couldnt find the doc2vec model with epoch {}\".format(epoch))\n",
    "    raise Exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create/Load Training Document Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-11 04:17:40,851 : INFO : Loading Training Document Stats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.5 s, sys: 3.2 s, total: 1min 1s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.exists(training_doc_stats_file):\n",
    "    info(\"Creating Training Document Stats\")\n",
    "    doc_stats = DocumentsStatsGenerator(training_preprocessed_files_prefix)\n",
    "    doc_stats.get_stats()\n",
    "    pickle.dump(doc_stats, open(training_doc_stats_file, \"w\"))\n",
    "else:\n",
    "    info(\"Loading Training Document Stats\")\n",
    "    doc_stats = pickle.load(open(training_doc_stats_file, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraphs: 47\n",
      "Sentences: 196\n",
      "Max Size: 244\n"
     ]
    }
   ],
   "source": [
    "MAX_PARAGRAPHS = int(np.percentile([len(doc_stats.doc_paragraphs[d]) for d in doc_stats.docids], 50))\n",
    "print \"Paragraphs: {}\".format(MAX_PARAGRAPHS)\n",
    "MAX_SENTENCES = int(np.percentile([len(doc_stats.doc_sentences[d]) for d in doc_stats.docids], 50))\n",
    "print \"Sentences: {}\".format(MAX_SENTENCES)\n",
    "# +1 is for the document vector\n",
    "MAX_SIZE = MAX_PARAGRAPHS + MAX_SENTENCES + 1\n",
    "print \"Max Size: {}\".format(MAX_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20368"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(doc_stats.doc_sentences[d]) for d in doc_stats.docids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4686"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(doc_stats.doc_paragraphs[d]) for d in doc_stats.docids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Training Data Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-11 04:18:42,638 : INFO : Loading Training Data from file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 736 ms, sys: 1min 20s, total: 1min 21s\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X, y = get_training_data(doc2vec_model, classifications, classifications_type, doc_stats, MAX_SIZE, DOC2VEC_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23454451328\n",
      "(120156, 244, 200)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print sys.getsizeof(X)\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Validation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_dict = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create/Load Validation Doc Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Validation Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 52s, sys: 13.7 s, total: 14min 6s\n",
      "Wall time: 14min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "validation_dict = pickle.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_DICT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.1 s, sys: 1.74 s, total: 15.9 s\n",
      "Wall time: 15.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.exists(validation_doc_stats_file):\n",
    "    validation_doc_stats = DocumentsStatsGenerator(validation_preprocessed_files_prefix)\n",
    "    validation_doc_stats.get_stats()\n",
    "    pickle.dump(validation_doc_stats, open(validation_doc_stats_file, \"w\"))\n",
    "else:\n",
    "    validation_doc_stats = pickle.load(open(validation_doc_stats_file, \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Validation Data Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-11 04:22:46,121 : INFO : Loading Validation Data from file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 184 ms, sys: 20.4 s, total: 20.5 s\n",
      "Wall time: 45.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Xv, yv = get_validation_data(validation_dict, classifications, classifications_type, validation_doc_stats, \n",
    "                             MAX_SIZE, DOC2VEC_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del validation_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NN_OPTIMIZER = 'rmsprop'\n",
    "NN_OPTIMIZER = 'adam'\n",
    "def create_keras_rnn_model(input_size, output_size, lstm_output_size, w_dropout_do, u_dropout_do):\n",
    "    \n",
    "    model= Sequential()\n",
    "    #model.add(Masking(mask_value=0., input_shape=(MAX_SIZE, input_size)))\n",
    "    model.add(LSTM(lstm_output_size, input_dim=input_size, dropout_W=w_dropout_do, dropout_U=u_dropout_do))\n",
    "    model.add(Dense(output_size, activation='sigmoid', name='sigmoid_output'))\n",
    "    model.compile(optimizer=NN_OPTIMIZER, loss='binary_crossentropy')\n",
    "    return model\n",
    "    \n",
    "#     lstm = LSTM(lstm_output_size, input_dim=input_size, dropout_W=w_dropout_do, dropout_U=u_dropout_do)\n",
    "    \n",
    "#     pooling = GlobalAveragePooling1D()(lstm)\n",
    "    \n",
    "#     sigmoid_output = Dense(output_size, activation='sigmoid', name='sigmoid_output')(pooling)\n",
    "\n",
    "#     model = Model(input=doc_input, output=sigmoid_output)\n",
    "#     model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stopper_deltas = {\n",
    "    'sections': 0.0001,\n",
    "    'classes': 0.00001,\n",
    "    'subclasses': 0.00001\n",
    "}\n",
    "early_stopper_patience = {\n",
    "    'sections': 10,\n",
    "    'classes': 10,\n",
    "    'subclasses': 10\n",
    "}\n",
    "epochs_before_validation = {\n",
    "    'sections': 10,\n",
    "    'classes': 50,\n",
    "    'subclasses': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_OUTPUT_NEURONS = len(classifications)\n",
    "\n",
    "EARLY_STOPPER_MIN_DELTA = early_stopper_deltas[classifications_type]\n",
    "EARLY_STOPPER_PATIENCE = early_stopper_patience[classifications_type]\n",
    "\n",
    "NN_MAX_EPOCHS = 50\n",
    "NN_RANDOM_SEARCH_BUDGET = 4\n",
    "NN_PARAM_SAMPLE_SEED = 1234\n",
    "\n",
    "NN_BATCH_SIZE = 512\n",
    "\n",
    "MODEL_VERBOSITY = 1\n",
    "\n",
    "to_skip = []\n",
    "\n",
    "load_existing_results = False\n",
    "save_results = True\n",
    "\n",
    "\n",
    "lstm_output_sizes = [200,300]\n",
    "w_dropout_options = [0.2,0.3]\n",
    "u_dropout_options = [0.2,0.3]\n",
    "\n",
    "\n",
    "# Uncomment for Specific Configuration\n",
    "# NN_RANDOM_SEARCH_BUDGET = 1\n",
    "# lstm_output_sizes = [200]\n",
    "# w_dropout_options = [0.2]\n",
    "# u_dropout_options = [0.2]\n",
    "\n",
    "np.random.seed(NN_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MetricsCallback(keras.callbacks.Callback):\n",
    "    \n",
    "    EPOCHS_BEFORE_VALIDATION = epochs_before_validation[classifications_type]\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.epoch_index = 0\n",
    "        self.val_loss_reductions = 0\n",
    "        self.metrics_dict = {}\n",
    "        self.best_val_loss = np.iinfo(np.int32).max\n",
    "        self.best_weights = None\n",
    "        self.best_validation_metrics = None\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epoch_index += 1\n",
    "        if logs['val_loss'] < self.best_val_loss:\n",
    "            self.val_loss_reductions += 1\n",
    "            self.best_val_loss = logs['val_loss']\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            print '\\r    \\r' # to remove the previous line of verbose output of model fit\n",
    "            time.sleep(0.2)\n",
    "            info('Found lower val loss for epoch {} => {}'.format(self.epoch_index, round(logs['val_loss'], 5)))\n",
    "            if self.val_loss_reductions % MetricsCallback.EPOCHS_BEFORE_VALIDATION == 0:\n",
    "                \n",
    "                info('Validation Loss Reduced {} times'.format(self.val_loss_reductions))\n",
    "                info('Evaluating on Validation Data')\n",
    "                yvp = self.model.predict(Xv)\n",
    "                yvp_binary = get_binary_0_5(yvp)\n",
    "                info('Generating Validation Metrics')\n",
    "                validation_metrics = get_metrics(yv, yvp, yvp_binary)\n",
    "                print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "                    validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "                    validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "                self.metrics_dict[self.epoch_index] = validation_metrics\n",
    "#                 self.best_validation_metrics = validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-11 05:50:50,640 : INFO : ***************************************************************************************\n",
      "2017-03-11 05:50:50,641 : INFO : lstm_optimizer_adam_size_200_w-drop_0.2_u-drop_0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_3 (LSTM)                    (None, 200)           320800      lstm_input_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "sigmoid_output (Dense)           (None, 8)             1608        lstm_3[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 322408\n",
      "____________________________________________________________________________________________________\n",
      "Train on 120156 samples, validate on 29675 samples\n",
      "Epoch 1/50\n",
      " 20992/120156 [====>.........................] - ETA: 248s - loss: 0.4625"
     ]
    }
   ],
   "source": [
    "param_sampler = ParameterSampler({\n",
    "    'lstm_output_size':lstm_output_sizes,\n",
    "    'w_dropout':w_dropout_options,\n",
    "    'u_dropout':u_dropout_options,\n",
    "}, n_iter=NN_RANDOM_SEARCH_BUDGET, random_state=NN_PARAM_SAMPLE_SEED)\n",
    "\n",
    "param_results_dict = {}\n",
    "if load_existing_results:\n",
    "    param_results_path = os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                       NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE)))\n",
    "    if os.path.exists(param_results_path):\n",
    "        param_results_dict = pickle.load(open(param_results_path))\n",
    "    else:\n",
    "        info('No Previous results exist in {}'.format(param_results_path))\n",
    "        \n",
    "for parameters in param_sampler:\n",
    "    start_time = time.time()\n",
    "    lstm_output_size = parameters['lstm_output_size']\n",
    "    w_dropout_do = parameters['w_dropout']\n",
    "    u_dropout_do = parameters['u_dropout']\n",
    "\n",
    "    GLOBAL_VARS.NN_MODEL_NAME = 'lstm_optimizer_{}_size_{}_w-drop_{}_u-drop_{}'.format(NN_OPTIMIZER,\n",
    "        lstm_output_size,  w_dropout_do, u_dropout_do\n",
    "    )\n",
    "\n",
    "    if GLOBAL_VARS.NN_MODEL_NAME in param_results_dict.keys() or GLOBAL_VARS.NN_MODEL_NAME in to_skip:\n",
    "        print \"skipping: {}\".format(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "        continue\n",
    "#         if first_hidden_layer_size < DOC2VEC_SIZE or second_hidden_layer_size < NN_OUTPUT_NEURONS:\n",
    "#             print \"skipping: {} due to 1st layer size {} < {} or 2nd layer size {} < {}\".format(GLOBAL_VARS.NN_MODEL_NAME,\n",
    "#                                                                                                 first_hidden_layer_size, DOC2VEC_SIZE, \n",
    "#                                                                                                 second_hidden_layer_size, NN_OUTPUT_NEURONS)\n",
    "#             continue\n",
    "\n",
    "\n",
    "    info('***************************************************************************************')\n",
    "    info(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "\n",
    "    model = create_keras_rnn_model(DOC2VEC_SIZE, NN_OUTPUT_NEURONS, \n",
    "                                  lstm_output_size, w_dropout_do, u_dropout_do)\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    early_stopper = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=EARLY_STOPPER_MIN_DELTA, \\\n",
    "                                                  patience=EARLY_STOPPER_PATIENCE, verbose=1, mode='auto')\n",
    "    metrics_callback = MetricsCallback()\n",
    "\n",
    "\n",
    "\n",
    "    # Model Fitting\n",
    "    %time history = model.fit(x=X, y=y, validation_data=(Xv,yv), batch_size=NN_BATCH_SIZE, \\\n",
    "                              nb_epoch=NN_MAX_EPOCHS, verbose=MODEL_VERBOSITY, \\\n",
    "                              callbacks=[early_stopper, metrics_callback])\n",
    "    \n",
    "    info('Evaluating on Training Data')\n",
    "    yp = model.predict(X)\n",
    "    yp_binary = get_binary_0_5(yp)\n",
    "    #print yvp\n",
    "    info('Generating Training Metrics')\n",
    "    training_metrics = get_metrics(y, yp, yp_binary)\n",
    "    print \"****** Training Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    training_metrics['coverage_error'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "    training_metrics['f1_micro'], training_metrics['f1_macro'])\n",
    "    \n",
    "    \n",
    "    info('Evaluating on Validation Data using saved best weights')\n",
    "    model.set_weights(metrics_callback.best_weights)\n",
    "    yvp = model.predict(Xv)\n",
    "    yvp_binary = get_binary_0_5(yvp)\n",
    "    #print yvp\n",
    "    info('Generating Validation Metrics')\n",
    "    validation_metrics = get_metrics(yv, yvp, yvp_binary)\n",
    "    print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "        validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "        validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "    best_validation_metrics = validation_metrics\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME] = dict()\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['best_validation_metrics'] = best_validation_metrics\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['epochs'] = len(history.history['val_loss'])\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['best_weights'] = metrics_callback.best_weights\n",
    "#         param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['last_weights'] = last_model_weights\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['duration'] =  duration\n",
    "\n",
    "    del history, metrics_callback, model\n",
    "\n",
    "if save_results:\n",
    "    pickle.dump(param_results_dict, open(os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                                   NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE))), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # create nn parameter search directory\n",
    "    if not os.path.exists(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME)):\n",
    "        os.makedirs(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(param_results_dict, open(os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                                   NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE))), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 03:51:31,120 : INFO : Generating Training Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 6s, sys: 1min 35s, total: 5min 41s\n",
      "Wall time: 5min 41s\n",
      "CPU times: user 136 ms, sys: 40 ms, total: 176 ms\n",
      "Wall time: 172 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Training Metrics: Cov Err: 2.940 | Top 3: 0.705 | Top 5: 0.910 | F1 Micro: 0.000 | F1 Macro: 0.000\n",
      "CPU times: user 4min 9s, sys: 1min 35s, total: 5min 44s\n",
      "Wall time: 5min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%time yp = model.predict(X)\n",
    "%time yp_binary = get_binary_0_5(yp)\n",
    "#print yvp\n",
    "info('Generating Training Metrics')\n",
    "training_metrics = get_metrics(y, yp, yp_binary)\n",
    "print \"****** Training Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    training_metrics['coverage_error'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "    training_metrics['f1_micro'], training_metrics['f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2937    \n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2882    \n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2843    \n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2808    \n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2779    \n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2743    \n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2729    \n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2684    \n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2687    \n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2713    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f253497d250>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X, y=y, batch_size=NN_BATCH_SIZE, \\\n",
    "                              nb_epoch=10, verbose=MODEL_VERBOSITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-07 15:30:31,828 : INFO : Generating Training Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.9 s, sys: 17.3 s, total: 56.2 s\n",
      "Wall time: 56.2 s\n",
      "CPU times: user 20 ms, sys: 4 ms, total: 24 ms\n",
      "Wall time: 23.7 ms\n",
      "****** Validation Metrics: Cov Err: 2.181 | Top 3: 0.853 | Top 5: 0.954 | F1 Micro: 0.541 | F1 Macro: 0.403\n",
      "CPU times: user 39.3 s, sys: 17.3 s, total: 56.7 s\n",
      "Wall time: 56.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%time yp = model.predict(X)\n",
    "%time yp_binary = get_binary_0_5(yp)\n",
    "#print yvp\n",
    "info('Generating Training Metrics')\n",
    "training_metrics = get_metrics(y, yp, yp_binary)\n",
    "print \"****** Training Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    training_metrics['coverage_error'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "    training_metrics['f1_micro'], training_metrics['f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2643    \n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2611    \n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2612    \n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2650    \n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2597    \n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2572    \n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2551    \n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2519    \n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2484    \n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2497    \n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2501    \n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2461    \n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2460    \n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2423    \n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2408    \n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2403    \n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2477    \n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2427    \n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2396    \n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2371    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2534933d90>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X, y=y, batch_size=NN_BATCH_SIZE, \\\n",
    "                              nb_epoch=20, verbose=MODEL_VERBOSITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-07 16:05:49,006 : INFO : Generating Training Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38 s, sys: 18.4 s, total: 56.4 s\n",
      "Wall time: 56.4 s\n",
      "CPU times: user 24 ms, sys: 0 ns, total: 24 ms\n",
      "Wall time: 23.7 ms\n",
      "****** Validation Metrics: Cov Err: 1.980 | Top 3: 0.887 | Top 5: 0.963 | F1 Micro: 0.628 | F1 Macro: 0.467\n",
      "CPU times: user 38.4 s, sys: 18.4 s, total: 56.9 s\n",
      "Wall time: 56.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%time yp = model.predict(X)\n",
    "%time yp_binary = get_binary_0_5(yp)\n",
    "#print yvp\n",
    "info('Generating Training Metrics')\n",
    "training_metrics = get_metrics(y, yp, yp_binary)\n",
    "print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    training_metrics['coverage_error'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "    training_metrics['f1_micro'], training_metrics['f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-06 01:24:19,997 : INFO : ****************** Epoch 1 --- Working on doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_1 *******************\n",
      "2017-03-06 01:24:20,042 : INFO : loading Doc2Vec object from /big/s/shalaby/parameter_search_doc2vec_models_extended/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_1/model\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# when resuming, resume from an epoch with a previously created doc2vec model to get the learning rate right\n",
    "start_from = 1\n",
    "for epoch in range(start_from, DOC2VEC_MAX_EPOCHS+1):\n",
    "    GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "    info(\"****************** Epoch {} --- Working on {} *******************\".format(epoch, GLOBAL_VARS.MODEL_NAME))\n",
    "    \n",
    "    # if we have the model, just load it, otherwise train the previous model\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)):\n",
    "        doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "        doc2vec_model.workers = NUM_CORES\n",
    "        GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "    else:\n",
    "        # train the doc2vec model\n",
    "        training_docs_iterator = ExtendedPVDocumentBatchGenerator(training_preprocessed_files_prefix, batch_size=10000)\n",
    "        doc2vec_model.train(sentences=training_docs_iterator, report_delay=REPORT_DELAY)\n",
    "        doc2vec_model.alpha -= DOC2VEC_ALPHA_DECREASE  # decrease the learning rate\n",
    "        doc2vec_model.min_alpha = doc2vec_model.alpha  # fix the learning rate, no decay\n",
    "        ensure_disk_location_exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME))\n",
    "        doc2vec_model.save(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "        GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "        \n",
    "        # get the word2vec analogy accuracy score\n",
    "#         word2vec_result = doc2vec_model.accuracy(word2vec_questions_file, restrict_vocab=None)\n",
    "#         epoch_word2vec_metrics.append(word2vec_result)\n",
    "#         pickle.dump(word2vec_result, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME,\n",
    "#                                                        WORD2VEC_METRICS_FILENAME), 'w'))\n",
    "\n",
    "    # Validation Embeddings\n",
    "    info('Getting Validation Embeddings')\n",
    "    Xv, yv = get_extended_docs_with_inference(doc2vec_model, doc_classification_map, classifications, \n",
    "                                     validation_docs_list, VALIDATION_MATRIX, \n",
    "                                     validation_preprocessed_files_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
