{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IS_SAMPLE = False\n",
    "TRAINING_SAMPLE_PERCENTAGE = 0.001\n",
    "MIN_TRAINING_SAMPLES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "STOP_WORDS = nltk.corpus.stopwords.words('english')\n",
    "NUMBER_INDICATOR = \"number_inidicator\"\n",
    "CURRENCY_INDICATOR = \"currency_inidicator\"\n",
    "CHEMICAL_INDICATOR = \"chemical_inidicator\"\n",
    "MIN_SIZE = 3\n",
    "MIN_DOCUMENTS = 5\n",
    "TOP_N_FEATURES = 10000\n",
    "\n",
    "TEST_SET_PERCENTAGE = 0.2\n",
    "VALIDATION_IN_TRAINING_PERCENTAGE = 0.2\n",
    "MIN_DOCUMENTS_FOR_TEST = 1\n",
    "MIN_DOCUMENTS_FOR_VALIDATION = 1\n",
    "\n",
    "MIN_DOCUMENTS_FOR_TRAINING_SAMPLE = 10\n",
    "MIN_DOCUMENTS_FOR_TEST_SAMPLE = 1\n",
    "MIN_DOCUMENTS_FOR_VALIDATION_SAMPLE = 1\n",
    "\n",
    "SVM_ITERATIONS = 1000\n",
    "SVM_CONVERGENCE = 0.001\n",
    "SVM_REG = 0.1\n",
    "\n",
    "BM25_K = 1.5  # controls power of tf component\n",
    "BM25_b = 0.75  # controls the BM25 length normalization\n",
    "\n",
    "RANDOM_SEED = 10000\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer().stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Manipulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stemtokenizer(text, doc_id):\n",
    "    \"\"\" MAIN FUNCTION to get clean stems out of a text. A list of clean stems are returned \"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\s+', gaps=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stems = []  # result\n",
    "    previous_unigram = None\n",
    "    for token in tokens:\n",
    "        stem = token.lower()\n",
    "        stem = stem.strip(string.punctuation)\n",
    "        if stem:\n",
    "            if is_number(stem):\n",
    "                stem = NUMBER_INDICATOR\n",
    "            elif is_currency(stem):\n",
    "                stem = CURRENCY_INDICATOR\n",
    "            elif is_chemical(stem):\n",
    "                stem = CHEMICAL_INDICATOR\n",
    "            elif is_stopword(stem):\n",
    "                stem = None\n",
    "            else:\n",
    "                stem = stemmer(token)\n",
    "                stem = stem.strip(string.punctuation)\n",
    "            if stem and len(stem) >= MIN_SIZE:\n",
    "                # extract uni-grams\n",
    "                stems.append((stem,{doc_id: 1}))\n",
    "                # extract bi-grams\n",
    "                if previous_unigram: stems.append((previous_unigram + \" \" + stem,{doc_id: 1}))\n",
    "                previous_unigram = stem\n",
    "    del tokens\n",
    "    return stems\n",
    "\n",
    "def is_stopword(word):\n",
    "  return word in STOP_WORDS\n",
    "\n",
    "def is_number(str):\n",
    "    \"\"\" Returns true if given string is a number (float or int)\"\"\"\n",
    "    try:\n",
    "        float(str.replace(\",\", \"\"))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_currency(str):\n",
    "    return str[0] == \"$\"\n",
    "\n",
    "def is_chemical(str):\n",
    "    return str.count(\"-\") > 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_postings(postings_list1, postings_list2):\n",
    "    # key could be either a doc id or a term\n",
    "    for key in postings_list2:\n",
    "        if postings_list1.get(key):\n",
    "            postings_list1[key] += postings_list2[key]\n",
    "        else:\n",
    "            postings_list1[key] = postings_list2[key]\n",
    "    return postings_list1\n",
    "\n",
    "def get_term_dictionary(terms):\n",
    "    \"\"\"\n",
    "    Maps string terms to indexes in an array\n",
    "    \"\"\"\n",
    "    term_dictionary = {}\n",
    "    term_array = [None] * len(terms)\n",
    "    def put(key):\n",
    "        hashvalue = hashfunction(key, len(term_array))\n",
    "        if term_array[hashvalue] == None:\n",
    "            term_array[hashvalue] = key\n",
    "            return hashvalue\n",
    "        else:\n",
    "            nextslot = rehash(hashvalue, len(term_array))\n",
    "            while term_array[nextslot] != None:\n",
    "                nextslot = rehash(nextslot, len(term_array))\n",
    "            if term_array[nextslot] == None:\n",
    "                term_array[nextslot] = key\n",
    "                return nextslot\n",
    "    def hashfunction(key, size):\n",
    "        return hash(key) % size\n",
    "    def rehash(oldhash, size):\n",
    "        return (oldhash + 1) % size\n",
    "    i = 0\n",
    "    for term in terms:\n",
    "        corresponding_index = put(term)\n",
    "        term_dictionary[term] = corresponding_index\n",
    "        i+=1\n",
    "        if i%10000 == 0: print \"finished \" + str(i)\n",
    "    return term_dictionary\n",
    "\n",
    "def jsonKV2str(x):\n",
    "    \"\"\"\n",
    "    Change string keys to int\n",
    "    \"\"\"\n",
    "    if isinstance(x, dict):\n",
    "            #return {doc_id:{int(term_id):x[doc_id][term_id] for term_id in x[doc_id]} for doc_id in x }\n",
    "        \n",
    "            return {int(k):(int(v) if isinstance(v, unicode) else v) for k,v in x.items()}\n",
    "    return x\n",
    "\n",
    "def get_json(json_postings):\n",
    "    return json.loads(json_postings)\n",
    "\n",
    "def get_json_convert_num(json_postings):\n",
    "    return json.loads(json_postings, object_hook=jsonKV2str)\n",
    "\n",
    "def get_doc_index(term, postings_list, term_dictionary):\n",
    "    #return [(doc_id, {term: postings_list[doc_id]}) for doc_id in postings_list]\n",
    "    return [(doc_id, {term_dictionary[term]: postings_list[doc_id]}) for doc_id in postings_list]\n",
    "\n",
    "def get_classes(ipc_classification):\n",
    "    sections = []\n",
    "    classes = []\n",
    "    subclasses = []\n",
    "    for classification in ipc_classification:\n",
    "        # we do the check because some documents have repetitions\n",
    "        section_name = classification['section']\n",
    "        class_name = classification['section'] + \"-\" + classification['class']\n",
    "        subclass_name = classification['section'] + \"-\" + classification['class'] + \"-\" + classification['subclass']\n",
    "        if section_name not in sections:\n",
    "            sections.append(section_name)\n",
    "        if class_name not in classes:\n",
    "            classes.append(class_name)\n",
    "        if subclass_name not in subclasses:\n",
    "            subclasses.append(subclass_name)\n",
    "    return {\"sections\": sections, \"classes\": classes, \"subclasses\": subclasses}\n",
    "\n",
    "\n",
    "def get_training_vector_old(classification, term_list, classifications, classification_key_name, number_of_terms):\n",
    "    clss = 1 if classification in classifications[classification_key_name] else 0\n",
    "    return LabeledPoint(clss, SparseVector(number_of_terms, term_list))\n",
    "\n",
    "def get_training_vector(classification, term_list, classifications, number_of_terms):\n",
    "    clss = 1 if classification in classifications else 0\n",
    "    return LabeledPoint(clss, SparseVector(number_of_terms, term_list))\n",
    "\n",
    "\n",
    "def calculate_sublinear_tf(tf):\n",
    "    # laplace smoothing with +1 in case of term with no documents (useful during testing)\n",
    "    return math.log10(1 + tf)\n",
    "\n",
    "\n",
    "def calculate_tf_idf(tf, df, N):\n",
    "    # laplace smoothing with +1 in case of term with no documents (useful during testing)\n",
    "    return tf * math.log10((N+1) / (df + 1))\n",
    "\n",
    "\n",
    "def calculate_sublinear_tf_idf(tf, df, N):\n",
    "    # laplace smoothing with +1 in case of term with no documents (useful during testing)\n",
    "    return calculate_sublinear_tf(tf) * math.log10((N+1) / (df + 1))\n",
    "\n",
    "\n",
    "def calculate_bm25(tf, df, N, d_len, d_avg):\n",
    "    idf = max(0, math.log10((N-df + 0.5)/(df+0.5))) # in rare cases where the df is over 50% of N, this could become -ve, so we guard against that\n",
    "    tf_comp = float(((BM25_K + 1) * tf)) / ( BM25_K * ((1-BM25_b) + BM25_b*(float(d_len)/d_avg)) + tf)\n",
    "    return tf_comp * idf\n",
    "\n",
    "\n",
    "def calculate_rf(df_relevant, df_non_relevant):\n",
    "    return math.log( (2 + (float(df_relevant)/max(1, df_non_relevant))), 2)\n",
    "\n",
    "\n",
    "def calculate_tf_rf(tf, df_relevant, df_non_relevant):\n",
    "    return tf * calculate_rf(df_relevant, df_non_relevant)\n",
    "\n",
    "\n",
    "def compare_classifications(x,y):\n",
    "    len_comp = cmp(len(x), len(y))\n",
    "    if len_comp == 0:\n",
    "        return cmp(x,y)\n",
    "    return len_comp\n",
    "\n",
    "\n",
    "def create_doc_index(term_index, term_dictionary):\n",
    "    return term_index \\\n",
    "        .flatMap(lambda (term, postings_list): get_doc_index(term, postings_list, term_dictionary)) \\\n",
    "        .reduceByKey(lambda x, y: merge_postings(x, y))\n",
    "\n",
    "\n",
    "def get_rf_stats(postings, classification):\n",
    "    a_plus_c = set(postings.keys())\n",
    "    a_plus_b = set(classifications_index[classification])\n",
    "    # first intersection is to get (a), second difference is to get (c) (checkout tf-rf paper for reference)\n",
    "    a = a_plus_c.intersection(a_plus_b)\n",
    "    c = a_plus_c.difference(a_plus_b)\n",
    "    size_a = len(a)\n",
    "    size_c = len(c)\n",
    "    return size_a, size_c\n",
    "\n",
    "\n",
    "def get_rf_postings(classification):\n",
    "    def get_rf_postings_internal(postings):\n",
    "        size_a, size_c = get_rf_stats(postings, classification)\n",
    "        return {docId: calculate_rf(size_a, size_c)\n",
    "                for docId, tf in postings.items()}\n",
    "    return get_rf_postings_internal\n",
    "\n",
    "\n",
    "def get_tf_rf_postings(classification):\n",
    "    def get_tf_rf_postings_internal(postings):\n",
    "        size_a, size_c = get_rf_stats(postings, classification)\n",
    "        return {docId: calculate_tf_rf(tf, size_a, size_c)\n",
    "                for docId, tf in postings.items()}\n",
    "    return get_tf_rf_postings_internal\n",
    "\n",
    "\n",
    "def train_level_old(docs_with_classes, classification, classification_label):\n",
    "    training_vectors = docs_with_classes.map(\n",
    "        lambda (doc_id, (term_list, classifications)): get_training_vector_old(classification, term_list, classifications,\n",
    "                                                                           classification_label, number_of_terms))\n",
    "    svm = SVMWithSGD.train(training_vectors, iterations=SVM_ITERATIONS, convergenceTol=SVM_CONVERGENCE)\n",
    "    return training_vectors, svm\n",
    "\n",
    "\n",
    "def train_level(docs_with_classes, classification, number_of_terms):\n",
    "    training_vectors = docs_with_classes.map(\n",
    "        lambda (doc_id, (term_list, classifications)): get_training_vector(classification, term_list,\n",
    "                                                                           classifications, number_of_terms))\n",
    "    svm = SVMWithSGD.train(training_vectors, iterations=SVM_ITERATIONS, convergenceTol=SVM_CONVERGENCE, regParam=SVM_REG)\n",
    "    return training_vectors, svm\n",
    "\n",
    "\n",
    "def train_level_new(docs_index, classification, doc_classification_map, number_of_terms):\n",
    "    training_vectors = docs_index.map(\n",
    "        lambda (doc_id, postings): get_training_vector(classification, postings,\n",
    "                                                        doc_classification_map[doc_id], number_of_terms))\n",
    "    svm = SVMWithSGD.train(training_vectors, iterations=SVM_ITERATIONS, convergenceTol=SVM_CONVERGENCE, regParam=SVM_REG)\n",
    "    return training_vectors, svm\n",
    "\n",
    "\n",
    "def get_error(svm, test_vectors):\n",
    "    labelsAndPreds = test_vectors.map(lambda p: (p.label, svm.predict(p.features)))\n",
    "    trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(test_vectors.count())\n",
    "    return trainErr\n",
    "\n",
    "\n",
    "def train_all(docs_with_classes):\n",
    "    training_errors = {}\n",
    "    for section in sections:\n",
    "        training_vectors, svm = train_level(docs_with_classes, section, \"sections\")\n",
    "        train_err = get_error(svm, training_vectors)\n",
    "        training_errors[section] = train_err\n",
    "    #\n",
    "    with open(training_errors_output, 'w') as file:\n",
    "        file.write(json.dumps(training_errors))\n",
    "    #\n",
    "    for clss in classes:\n",
    "        training_vectors, svm = train_level(docs_with_classes, clss, \"classes\")\n",
    "        train_err = get_error(svm, training_vectors)\n",
    "        training_errors[clss] = train_err\n",
    "    \n",
    "    with open(training_errors_output, 'w') as file:\n",
    "        file.write(json.dumps(training_errors))\n",
    "    \n",
    "    for subclass in subclasses:\n",
    "        training_vectors, svm = train_level(docs_with_classes, subclass, \"subclasses\")\n",
    "        train_err = get_error(svm, training_vectors)\n",
    "        training_errors[subclass] = train_err\n",
    "    return training_errors\n",
    "\n",
    "\n",
    "def get_labeled_points_from_doc_index(doc_index, doc_classification_map, number_of_terms):\n",
    "    docs_with_classes = doc_index.map(lambda (doc_id, terms): (doc_id, (terms, doc_classification_map[doc_id])))\n",
    "    training_vectors = docs_with_classes.map(\n",
    "        lambda (doc_id, (term_list, classifications)): get_training_vector(classification, term_list,\n",
    "                                                                           classifications, number_of_terms))\n",
    "    return training_vectors\n",
    "\n",
    "get_binary = lambda x: 1 if x > 0 else 0\n",
    "get_binary = np.vectorize(get_binary)\n",
    "\n",
    "def get_row_top_N(y_score_row, y_true_row):\n",
    "    desc_score_indices = np.argsort(y_score_row)[::-1]\n",
    "    # print y_score_row\n",
    "    # print y_true_row\n",
    "    true_indices = np.where(y_true_row ==1)[0]\n",
    "    # print desc_score_indices\n",
    "    found = 0\n",
    "    top_N = 0\n",
    "    for i, score in enumerate(desc_score_indices):\n",
    "        if score in true_indices:\n",
    "            found += 1\n",
    "            if found == len(true_indices):\n",
    "                top_N = i + 1\n",
    "    # print top_N\n",
    "    return top_N\n",
    "\n",
    "\n",
    "def get_metrics(y_true, y_binary_score):\n",
    "    metrics = {}\n",
    "    metrics['coverage_error'] = coverage_error(y_binary_score, y_true)\n",
    "    metrics['average_num_of_labels'] = np.sum(np.sum(y_true, axis=1))/y_true.shape[0]\n",
    "    metrics['average_precision_micro'] = sklearn.metrics.average_precision_score(y_true, y_binary_score, average='micro')\n",
    "    metrics['average_precision_macro'] = sklearn.metrics.average_precision_score(y_true, y_binary_score, average='macro')\n",
    "    metrics['precision_micro'] = sklearn.metrics.precision_score(y_true, y_binary_score, average='micro')\n",
    "    metrics['precision_macro'] = sklearn.metrics.precision_score(y_true, y_binary_score, average='macro')\n",
    "    metrics['recall_micro'] = sklearn.metrics.recall_score(y_true, y_binary_score, average='micro')\n",
    "    metrics['recall_macro'] = sklearn.metrics.recall_score(y_true, y_binary_score, average='macro')\n",
    "    metrics['f1_micro'] = sklearn.metrics.f1_score(y_true, y_binary_score, average='micro')\n",
    "    metrics['f1_macro'] = sklearn.metrics.f1_score(y_true, y_binary_score, average='macro')\n",
    "\n",
    "    precision_scores = np.zeros(y_true.shape[1])\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        precision_scores[i] = sklearn.metrics.precision_score(y_true[:,i], y_binary_score[:,i])\n",
    "    metrics['precision_scores_array'] = precision_scores.tolist()\n",
    "\n",
    "    recall_scores = np.zeros(y_true.shape[1])\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        recall_scores[i] = sklearn.metrics.recall_score(y_true[:,i], y_binary_score[:,i])\n",
    "    metrics['recall_scores_array'] = recall_scores.tolist()\n",
    "\n",
    "    f1_scores = np.zeros(y_true.shape[1])\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        f1_scores[i] = sklearn.metrics.f1_score(y_true[:,i], y_binary_score[:,i])\n",
    "    metrics['f1_scores_array'] = f1_scores.tolist()\n",
    "\n",
    "    tops = []\n",
    "    for i in xrange(y_score.shape[0]):\n",
    "        tops.append(get_row_top_N(y_score[i,:], y_true[i,:]))\n",
    "    metrics['topN_list'] = np.array(tops).tolist()\n",
    "    metrics['topN_avg'] = np.mean(tops)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    \n",
    "    def __init__(self, labels, scores, threshold=0):\n",
    "        self.threshold = 0\n",
    "        self.count = len(labels)\n",
    "        \n",
    "        self.tp = 0\n",
    "        self.fp = 0\n",
    "        self.fn = 0\n",
    "        self.tn = 0\n",
    "        \n",
    "        for (l,s) in zip(labels,scores):\n",
    "            if self.is_true(l) and self.is_true(s):\n",
    "                self.tp += 1\n",
    "            if self.is_true(l) and not self.is_true(s):\n",
    "                self.fn += 1\n",
    "            if not self.is_true(l) and self.is_true(s):\n",
    "                self.fp += 1\n",
    "            if not self.is_true(l) and not self.is_true(s):\n",
    "                self.tn += 1\n",
    "        self.precision = self.get_precision()\n",
    "        self.recall = self.get_precision()\n",
    "        self.f1 = self.get_f1()\n",
    "        self.error_rate = self.get_error_rate()\n",
    "        \n",
    "    def calculate_contingency(self, label, contingency):\n",
    "        \n",
    "        self.tp = 0\n",
    "        self.fp = 0\n",
    "        self.fn = 0\n",
    "        self.tn = 0\n",
    "        \n",
    "        for (l,s) in zip(labels,scores):\n",
    "            if self.is_true(l) and self.is_true(s):\n",
    "                self.tp += 1\n",
    "            if self.is_true(l) and not self.is_true(s):\n",
    "                self.fn += 1\n",
    "            if not self.is_true(l) and self.is_true(s):\n",
    "                self.fp += 1\n",
    "            if not self.is_true(l) and not self.is_true(s):\n",
    "                self.tn += 1\n",
    "    \n",
    "    def is_true(self, label):\n",
    "        return label > self.threshold\n",
    "    \n",
    "    def get_error_rate(self):\n",
    "        return float(self.tp + self.tn) / self.count\n",
    "    \n",
    "    def get_precision(self):\n",
    "        # self.calculate_contingency()\n",
    "        if self.tp == 0: return 0\n",
    "        return float(self.tp) / (self.tp + self.fp)\n",
    "        \n",
    "    def get_recall(self):\n",
    "        # self.calculate_contingency()\n",
    "        if self.tp == 0: return 0\n",
    "        return float(self.tp) / (self.tp + self.fn)\n",
    "    \n",
    "    def get_f1(self):\n",
    "        return 2 * (self.get_precision() * self.get_recall()) / (self.get_precision() + self.get_recall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input/Output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sc = SparkContext(\"\", \"Generate Inverted Index Job\")\n",
    "es_server = \"deka.cip.ifi.lmu.de\"\n",
    "es_port = \"9200\"\n",
    "\n",
    "original_parent_save_location = \"hdfs://deka.cip.ifi.lmu.de/svm/new/\"\n",
    "save_parent_location = original_parent_save_location\n",
    "sample_save_parent_location = save_parent_location + \"sample/\"\n",
    "if IS_SAMPLE: \n",
    "    save_parent_location = save_parent_location + \"sample/\"\n",
    "\n",
    "file_name = \"sample.json\"\n",
    "test_file_name = \"sample.json\"\n",
    "#url = \"/media/Work/workspace/thesis/benchmark/output/\" + file_name\n",
    "sample_location = save_parent_location + file_name\n",
    "sample_test_location = save_parent_location + test_file_name\n",
    "docs_output = save_parent_location + \"docs_output\"\n",
    "postings_list_output = save_parent_location + \"postings_list_full.json\"\n",
    "\n",
    "accepted_terms_list_output = original_parent_save_location + \"accepted_terms_list_{}.pkl\"\n",
    "accepted_terms_with_scores_list_output = original_parent_save_location + \"accepted_terms_with_scores_list_{}.pkl\"\n",
    "postings_list_chi_selected_output = original_parent_save_location + \"postings_list_{}.json\"\n",
    "term_df_map_output = original_parent_save_location + \"term_df_map_output_{}.json\"\n",
    "doc_index_chi_selected_output = original_parent_save_location + \"doc_index_for_postings_{}.json\"\n",
    "term_dictionary_output = original_parent_save_location + \"term_dictionary_{}.pkl\"\n",
    "\n",
    "\n",
    "postings_list_training_chi_selected_output = save_parent_location + \"training_postings_list_{}.json\"\n",
    "postings_list_validation_chi_selected_output = save_parent_location + \"validation_postings_list_{}.json\"\n",
    "postings_list_test_chi_selected_output = save_parent_location + \"test_postings_list_{}.json\"\n",
    "\n",
    "# Classification objects, unrelated to sample size\n",
    "classification_index_output = original_parent_save_location + \"classification_index.pkl\"\n",
    "doc_classification_map_output = original_parent_save_location + \"doc_classification_map.pkl\"\n",
    "sections_output = original_parent_save_location + \"sections.pkl\"\n",
    "classes_output = original_parent_save_location + \"classes.pkl\"\n",
    "subclasses_output = original_parent_save_location + \"subclasses.pkl\"\n",
    "classifications_output = original_parent_save_location + \"classifications.pkl\"\n",
    "doc_lengths_map_output = original_parent_save_location + \"doc_lengths_map.pkl\"\n",
    "# training, validation and test set lists\n",
    "training_docs_list_output = original_parent_save_location + \"training_docs_list.pkl\"\n",
    "validation_docs_list_output = original_parent_save_location + \"validation_docs_list.pkl\"\n",
    "test_docs_list_output = original_parent_save_location + \"test_docs_list.pkl\"\n",
    "sample_training_docs_list_output = sample_save_parent_location + \"training_docs_list.pkl\"\n",
    "\n",
    "\n",
    "training_predictions_sections_output = save_parent_location + \"training_predictions_sections_list.pkl\"\n",
    "training_labels_sections_list_output = save_parent_location + \"training_labels_sections_list.pkl\"\n",
    "valdiation_predictions_sections_output = save_parent_location + \"validation_predictions_sections_list.pkl\"\n",
    "validation_labels_sections_list_output = save_parent_location + \"validation_labels_sections_list.pkl\"\n",
    "\n",
    "\n",
    "test_postings_list_output = save_parent_location + \"test_postings_list_50000.json\"\n",
    "training_errors_output = save_parent_location + \"training_errors.json\"\n",
    "model_output = save_parent_location + \"models/\" + \"iter_\" + str(SVM_ITERATIONS) + \"_reg_\" + str(SVM_REG) + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model_name(method, classification, reg=SVM_REG, iterations=SVM_ITERATIONS):\n",
    "    return save_parent_location + \"models/\" + \"iter_\" + str(iterations) + \"_reg_\" + str(reg) + \"/\" + method + \"_\" + classification + \"_model.svm\"\n",
    "def get_data_output_name(method, no_of_features=TOP_N_FEATURES, data_type=\"training\"):\n",
    "    return save_parent_location + \"models/\" + data_type + \"_data/\" + method  + \"_data.json\"\n",
    "def get_data_classification_output_name(method, classification, data_type=\"training\"):\n",
    "    return save_parent_location + \"models/\" + data_type + \"_data/\" + method + \"_\" + classification + \"_data.json\"\n",
    "def get_prediction_output_name(method, data_type=\"training\", subset=\"sections\", reg=SVM_REG, iterations=SVM_ITERATIONS):\n",
    "    return save_parent_location + \"models/\" + \"iter_\" + str(iterations) + \"_reg_\" + str(reg) + \"/\" + method + \"_\" + data_type + \"_\" + subset + \"_predictions.svm\"\n",
    "def get_labels_output_name(data_type=\"training\", subset=\"sections\", reg=SVM_REG, iterations=SVM_ITERATIONS):\n",
    "    return save_parent_location + \"models/\" + \"iter_\" + str(iterations) + \"_reg_\" + str(reg) + \"/\" + data_type + \"_\" + subset + \"_labels.svm\"\n",
    "def get_metrics_output_name(method, data_type=\"training\", subset=\"sections\", reg=SVM_REG, iterations=SVM_ITERATIONS):\n",
    "    return save_parent_location + \"models/\" + \"iter_\" + str(iterations) + \"_reg_\" + str(reg) + \"/\" + method + \"_\" + data_type + \"_\" + subset + \"_metrics.pkl\"\n",
    "def get_save_location(location, sample=False):\n",
    "    if sample:\n",
    "        return location.replace(save_parent_location, sample_save_parent_location)\n",
    "    return location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD.\n: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'\n\tat org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:190)\n\tat org.elasticsearch.hadoop.rest.RestService.findPartitions(RestService.java:231)\n\tat org.elasticsearch.hadoop.mr.EsInputFormat.getSplits(EsInputFormat.java:457)\n\tat org.elasticsearch.hadoop.mr.EsInputFormat.getSplits(EsInputFormat.java:438)\n\tat org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:120)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1307)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1302)\n\tat org.apache.spark.api.python.SerDeUtil$.pairRDDToPython(SerDeUtil.scala:201)\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopRDD(PythonRDD.scala:530)\n\tat org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[141.84.220.203:9200]] \n\tat org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:142)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:434)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:414)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:418)\n\tat org.elasticsearch.hadoop.rest.RestClient.get(RestClient.java:122)\n\tat org.elasticsearch.hadoop.rest.RestClient.esVersion(RestClient.java:564)\n\tat org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:178)\n\t... 32 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6f25eb1e649a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'\\nread_conf = {\\n    \\'es.nodes\\': es_server,\\n    \\'es.port\\': es_port,\\n    \\'es.resource\\': \\'patents3/patent\\',\\n    \\'es.query\\': \\'{ \"query\" : { \"match_all\" : {} }}\\',\\n    \\'es.scroll.keepalive\\': \\'120m\\',\\n    \\'es.scroll.size\\': \\'1000\\',\\n    \\'es.http.timeout\\': \\'20m\\'\\n}\\ndata = sc.newAPIHadoopRDD(\\n    inputFormatClass = \\'org.elasticsearch.hadoop.mr.EsInputFormat\\',\\n    keyClass = \\'org.apache.hadoop.io.NullWritable\\', \\n    valueClass = \\'org.elasticsearch.hadoop.mr.LinkedMapWritable\\',\\n    conf = read_conf\\n)\\n\\n#data = sc.textFile(sample_location)\\n#doc_count = data.count()\\n#doc_objs = data.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\\ndoc_objs = data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/s/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/s/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/s/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/home/s/shalaby/spark//python/pyspark/context.pyc\u001b[0m in \u001b[0;36mnewAPIHadoopRDD\u001b[1;34m(self, inputFormatClass, keyClass, valueClass, keyConverter, valueConverter, conf, batchSize)\u001b[0m\n\u001b[0;32m    642\u001b[0m         jrdd = self._jvm.PythonRDD.newAPIHadoopRDD(self._jsc, inputFormatClass, keyClass,\n\u001b[0;32m    643\u001b[0m                                                    \u001b[0mvalueClass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeyConverter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalueConverter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m                                                    jconf, batchSize)\n\u001b[0m\u001b[0;32m    645\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/s/shalaby/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/s/shalaby/spark//python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/s/shalaby/spark/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD.\n: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'\n\tat org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:190)\n\tat org.elasticsearch.hadoop.rest.RestService.findPartitions(RestService.java:231)\n\tat org.elasticsearch.hadoop.mr.EsInputFormat.getSplits(EsInputFormat.java:457)\n\tat org.elasticsearch.hadoop.mr.EsInputFormat.getSplits(EsInputFormat.java:438)\n\tat org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:120)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1307)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1302)\n\tat org.apache.spark.api.python.SerDeUtil$.pairRDDToPython(SerDeUtil.scala:201)\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopRDD(PythonRDD.scala:530)\n\tat org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[141.84.220.203:9200]] \n\tat org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:142)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:434)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:414)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:418)\n\tat org.elasticsearch.hadoop.rest.RestClient.get(RestClient.java:122)\n\tat org.elasticsearch.hadoop.rest.RestClient.esVersion(RestClient.java:564)\n\tat org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:178)\n\t... 32 more\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "read_conf = {\n",
    "    'es.nodes': es_server,\n",
    "    'es.port': es_port,\n",
    "    'es.resource': 'patents3/patent',\n",
    "    'es.query': '{ \"query\" : { \"match_all\" : {} }}',\n",
    "    'es.scroll.keepalive': '120m',\n",
    "    'es.scroll.size': '1000',\n",
    "    'es.http.timeout': '20m'\n",
    "}\n",
    "data = sc.newAPIHadoopRDD(\n",
    "    inputFormatClass = 'org.elasticsearch.hadoop.mr.EsInputFormat',\n",
    "    keyClass = 'org.apache.hadoop.io.NullWritable', \n",
    "    valueClass = 'org.elasticsearch.hadoop.mr.LinkedMapWritable',\n",
    "    conf = read_conf\n",
    ")\n",
    "\n",
    "#data = sc.textFile(sample_location)\n",
    "#doc_count = data.count()\n",
    "#doc_objs = data.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "doc_objs = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading document texts from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_text_objs = sc.textFile(docs_output).map(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.3 s, sys: 6.48 s, total: 34.8 s\n",
      "Wall time: 15min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### doc_objs = data.map(lambda x: json.loads(x))\n",
    "\n",
    "doc_class_map = doc_objs.map(lambda (doc_id, doc): (doc_id, get_classes(doc['classification-ipc']))).cache()\n",
    "doc_classification_map = doc_class_map.map(lambda (doc_id, classification_obj): (doc_id, sorted(reduce(lambda x, lst: x + lst, classification_obj.values(), [])))).collectAsMap()\n",
    "doc_count = len(doc_classification_map)\n",
    "# contains [(classification,  list of docs)]\n",
    "# second list comprehension is to get list of lists [[\"A\", \"B\"],[\"A-01\",\"B-03\"]] to one list [\"A\", \"B\", \"A-01\",\"B-03\"], we could have also used a reduce as in doc_classifications_map\n",
    "classifications_index = doc_class_map.flatMap(lambda (doc_id, classifications_obj): [(classification, doc_id) for classification in [classif for cat in classifications_obj.values() for classif in cat]])\\\n",
    "    .groupByKey().map(lambda (classf, classf_docs): (classf, list(set(classf_docs)))).collectAsMap()\n",
    "\n",
    "sections = sorted(doc_class_map.flatMap(lambda (doc_id, classifications): classifications['sections']).distinct().collect())\n",
    "classes = sorted(doc_class_map.flatMap(lambda (doc_id, classifications): classifications['classes']).distinct().collect())\n",
    "subclasses = sorted(doc_class_map.flatMap(lambda (doc_id, classifications): classifications['subclasses']).distinct().collect())\n",
    "classifications = sorted(classifications_index.keys(), cmp=compare_classifications)\n",
    "# classifications = sorted(set(reduce(lambda x, lst: x + lst, map(lambda doc_id: classifications_index[doc_id], classifications_index), [])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save classification objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.2 s, sys: 2.54 s, total: 34.8 s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sc.parallelize(doc_classification_map.items()).repartition(1).saveAsPickleFile(doc_classification_map_output)\n",
    "sc.parallelize(classifications_index.items()).repartition(1).saveAsPickleFile(classification_index_output)\n",
    "sc.parallelize(sections).repartition(1).saveAsPickleFile(sections_output)\n",
    "sc.parallelize(classes).repartition(1).saveAsPickleFile(classes_output)\n",
    "sc.parallelize(subclasses).repartition(1).saveAsPickleFile(subclasses_output)\n",
    "sc.parallelize(classifications).repartition(1).saveAsPickleFile(classifications_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Classification Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_classification_map = dict(sc.pickleFile(doc_classification_map_output).collect())\n",
    "doc_count = len(doc_classification_map)\n",
    "classifications_index = dict(sc.pickleFile(classification_index_output).collect())\n",
    "sections = sc.pickleFile(sections_output).collect()\n",
    "classes = sc.pickleFile(classes_output).collect()\n",
    "subclasses = sc.pickleFile(subclasses_output).collect()\n",
    "classifications = sc.pickleFile(classifications_output).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "pickle.dump(classes, open('/big/s/shalaby/exported_data/classes.pkl', 'w'))\n",
    "pickle.dump(subclasses, open('/big/s/shalaby/exported_data/subclasses.pkl', 'w'))\n",
    "pickle.dump(classifications, open('/big/s/shalaby/exported_data/classifications.pkl', 'w'))\n",
    "pickle.dump(classifications_index, open('/big/s/shalaby/exported_data/classification_index.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accelerates the chi squared calculation a lot\n",
    "classifications_index_set = {k:set(docs) for k,docs in classifications_index.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2009750"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'G-20-B', [u'07433566', u'07896523', u'06985663', u'07116477', u'07218441'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications_index.items()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'07007598', [u'B', u'B-30', u'B-30-B'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_classification_map.items()[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'A', u'B', u'C', u'D', u'E', u'F', u'G', u'H']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Training, Validation and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get min number of documents for any classification\n",
    "min = 1000\n",
    "from collections import defaultdict\n",
    "min_classf = defaultdict(list)\n",
    "for (classf, documents) in classifications_index.items():\n",
    "    if len(documents) == 2: \n",
    "        min = len(documents)\n",
    "        min_classf[classf].append(min)\n",
    "min_classf, min\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "760"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(min_classf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2235"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classifications_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_documents = set()\n",
    "validation_documents = set()\n",
    "test_documents = set()\n",
    "for (classf, documents) in classifications_index.items():\n",
    "    # only worry about subclasses, classes and sections will be already included\n",
    "    if(classf in sections or classf in classes): pass\n",
    "    \n",
    "    # remove any documents that have already been picked before\n",
    "    docs_set = set(documents)\n",
    "    docs_set-=training_documents\n",
    "    docs_set-=validation_documents\n",
    "    docs_set-=test_documents\n",
    "    \n",
    "    base_test_docs_num = int(len(docs_set)* TEST_SET_PERCENTAGE)\n",
    "    num_test_docs = base_test_docs_num if base_test_docs_num > 0 else MIN_DOCUMENTS_FOR_TEST if MIN_DOCUMENTS_FOR_TEST < len(docs_set) else 0\n",
    "    print len(docs_set), num_test_docs\n",
    "    classif_test_docs = random.sample(docs_set, num_test_docs)\n",
    "    \n",
    "    remaining_docs = docs_set.difference(set(classif_test_docs))\n",
    "    base_validation_docs_num = int(len(remaining_docs)* VALIDATION_IN_TRAINING_PERCENTAGE)\n",
    "    num_validation_docs = base_validation_docs_num if base_validation_docs_num > 0 else MIN_DOCUMENTS_FOR_VALIDATION if MIN_DOCUMENTS_FOR_VALIDATION < len(remaining_docs) else 0\n",
    "    classif_validation_docs = random.sample(remaining_docs, num_validation_docs)\n",
    "    \n",
    "    classif_training_docs = set(remaining_docs).difference(set(classif_validation_docs))\n",
    "    \n",
    "    training_documents.update(classif_training_docs)\n",
    "    validation_documents.update(classif_validation_docs)\n",
    "    test_documents.update(classif_test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the training, validation and test document lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.parallelize(training_documents).saveAsPickleFile(training_docs_list_output)\n",
    "sc.parallelize(validation_documents).saveAsPickleFile(validation_docs_list_output)\n",
    "sc.parallelize(test_documents).saveAsPickleFile(test_docs_list_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the training, validation and test document lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_documents = sc.pickleFile(training_docs_list_output).collect()\n",
    "validation_documents = sc.pickleFile(validation_docs_list_output).collect()\n",
    "test_documents = sc.pickleFile(test_docs_list_output).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401877"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(test_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286325"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321473"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "pickle.dump(training_documents, open('/big/s/shalaby/exported_data_merged/training_docs_list.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17229"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_training_documents = set()\n",
    "i = 0\n",
    "for (classf, documents) in classifications_index.items():\n",
    "    if len(documents) > MIN_TRAINING_SAMPLES:\n",
    "        base_sample_docs_len = int(len(documents)* TRAINING_SAMPLE_PERCENTAGE)\n",
    "        num_sample_docs = base_sample_docs_len if base_sample_docs_len > 0 else MIN_TRAINING_SAMPLES\n",
    "        #print \"%s: Total %d, sample: %d\" % (classf, len(documents), num_sample_docs)\n",
    "        classif_training_docs = random.sample(documents, num_sample_docs)\n",
    "        \n",
    "        sample_training_documents.update(set(classif_training_docs))\n",
    "    else:\n",
    "        sample_training_documents.update(documents)\n",
    "    i+=1\n",
    "    \n",
    "    #if i > 100: break\n",
    "len(sample_training_documents)\n",
    "#sc.parallelize(sample_training_documents).saveAsPickleFile(sample_training_docs_list_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.parallelize(sample_training_documents).saveAsPickleFile(sample_save_parent_location + str(TRAINING_SAMPLE_PERCENTAGE) + \"_sample.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_training_documents = sc.pickleFile(sample_training_docs_list_output).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_documents = sample_training_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for classif in sorted(classifications_index.keys()):\n",
    "    if len(classif) == 1:\n",
    "        print \"%s : %d, %.3f\" % (classif, len(set(classifications_index[classif])), float(len(classifications_index[classif]))/doc_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "overlap_df = pd.DataFrame({section: [0]*len(sections) for section in sections} , index=sections, columns=sections)\n",
    "for doc_id in doc_classification_map:\n",
    "    for classif in doc_classification_map[doc_id]:\n",
    "        if len(classif) == 1:\n",
    "            for classif2 in doc_classification_map[doc_id]:\n",
    "                if len(classif2) == 1:\n",
    "                    overlap_df[classif][classif2] += 1\n",
    "overlap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mpl.colors.Normalize(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overlap_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,8), dpi=120)\n",
    "#ax = fig.add_subplot(111, frameon=True, xticks=[], yticks=[])\n",
    "vals = overlap_df.values\n",
    "normal = mpl.colors.Normalize()\n",
    "normal = mpl.colors.Normalize(vals.min()-1, vals.max()+vals.max()/2)\n",
    "formatter = lambda x: \"{:,d}\".format(int(x))\n",
    "\n",
    "the_table=plt.table(cellText=np.vectorize(formatter)(vals), rowLabels=overlap_df.index, colLabels=overlap_df.columns, \n",
    "                    colWidths = [0.1]*(vals.shape[1]+3), loc='center',\n",
    "                    cellColours=plt.cm.YlGn(normal(vals)))\n",
    "the_table.set_fontsize(30)\n",
    "the_table.scale(2, 4)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "### Create Postings List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "# Create Postings List (old one)\n",
    "#postings_lists = doc_text_objs.flatMap(lambda (doc_id, doc): stemtokenizer(doc['description'], doc_id)).reduceByKey(lambda x,y: merge_postings(x,y))\n",
    "### postings_lists = doc_objs.flatMap(lambda x: stemtokenizer(x['description'], x['id'])).reduceByKey(lambda x,y: merge_postings(x,y))\n",
    "#min_doc_postings_lists = postings_lists.filter(lambda (x,y): len(y) > MIN_DOCUMENTS)\n",
    "#number_of_terms = min_doc_postings_lists.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create Postings List\n",
    "postings_lists = doc_text_objs.flatMap(lambda (doc_id, doc): stemtokenizer(doc, doc_id)).reduceByKey(lambda x,y: merge_postings(x,y))\n",
    "### postings_lists = doc_objs.flatMap(lambda x: stemtokenizer(x['description'], x['id'])).reduceByKey(lambda x,y: merge_postings(x,y))\n",
    "min_doc_postings_lists = postings_lists.filter(lambda (x,y): len(y) > MIN_DOCUMENTS)\n",
    "#number_of_terms = min_doc_postings_lists.count()\n",
    "\n",
    "# min_doc_postings_lists.map(lambda (term, postings_list): \",\".join([term, json.dumps(postings_list)])).repartition(1).saveAsTextFile(postings_list_output)\n",
    "min_doc_postings_lists.map(lambda postings: json.dumps(postings)).saveAsTextFile(postings_list_output)\n",
    "\n",
    "def get_chi_index(term_index, classifications_index, subclasses, number_of_docs):\n",
    "    return term_index.map(lambda (term, postings_list): (term, calculate_chi_squared(postings_list.keys(), classifications_index, subclasses, number_of_docs)))\n",
    "\n",
    "def calculate_chi_squared(document_list, classifications_index, subclasses, number_of_docs):\n",
    "    chi_score = 0\n",
    "    for subclass in subclasses:\n",
    "        Nt1 = len(document_list) # actual collection frequency of having the word\n",
    "        Nt0 = number_of_docs - len(document_list) # actual collection frequency of not having the word\n",
    "        Pt1 = float(len(document_list))/ number_of_docs\n",
    "        Pt0 = float(number_of_docs - len(document_list))/ number_of_docs\n",
    "        Pc1 = float(len(classifications_index[subclass]))/ number_of_docs\n",
    "        Et1c1 = Pt1 * Pc1 * number_of_docs # expected frequency of docs in subclass with term (assuming independence)\n",
    "        Et0c1 = Pt0 * Pc1 * number_of_docs # expected frequency of docs in subclass without term (assuming independence)\n",
    "        chi_score += math.pow( Nt1 - Et1c1, 2) / Et1c1 \n",
    "        chi_score += math.pow( Nt0 - Et0c1, 2) / Et0c1\n",
    "    return chi_score\n",
    "\n",
    "term_accepted_chi_list = get_chi_index(min_doc_postings_lists, classifications_index, subclasses, doc_count).takeOrdered(TOP_N_FEATURES, lambda (term,score): -score)\n",
    "term_accepted_chi_list = map(lambda (x,y): x, term_accepted_chi_list)\n",
    "\n",
    "# gets a bit slower at the end but finishes eventually \n",
    "term_dictionary = get_term_dictionary(term_accepted_chi_list)\n",
    "\n",
    "min_doc_postings_lists = min_doc_postings_lists.filter(lambda (term, postings): term in term_accepted_chi_list).cache()\n",
    "\n",
    "number_of_terms = min_doc_postings_lists.count()\n",
    "number_of_terms\n",
    "\n",
    "min_doc_postings_lists.map(lambda postings: json.dumps(postings)).saveAsTextFile(postings_list_chi_selected_output.format(str(TOP_N_FEATURES)))\n",
    "sc.parallelize(term_dictionary.items()).saveAsPickleFile(term_dictionary_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Document Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_lengths_dict = doc_text_objs.map(lambda (doc_id, document_text): (doc_id, len(document_text))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_doc_length = sum(doc_lengths_dict.values())/len(doc_lengths_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'08369259', 85861)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lengths_dict.items()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46477"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_doc_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Postings List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save Postings List\n",
    "# min_doc_postings_lists.map(lambda (term, postings_list): \",\".join([term, json.dumps(postings_list)])).repartition(1).saveAsTextFile(postings_list_output)\n",
    "min_doc_postings_lists.map(lambda postings: json.dumps(postings)).saveAsTextFile(postings_list_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Postings List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Postings Lists\n",
    "min_doc_postings_lists = sc.textFile(postings_list_output).map(lambda json_postings: json.loads(json_postings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_chi_index(term_index, classifications_index_set, subclasses, number_of_docs):\n",
    "    return term_index.map(lambda (term, postings_list): (term, calculate_chi_squared(postings_list.keys(), classifications_index_set, subclasses, number_of_docs)))\n",
    "\n",
    "def calculate_chi_squared(document_list, classifications_index_set, subclasses, number_of_docs):\n",
    "    \"\"\"\n",
    "    Chi squared is the ratio of the difference between actual frequency and expected frequency of a term relative to the expected frequency\n",
    "    summed up across all classes and whether the term appears or not\n",
    "    Here we calculate the average chi squared score which is one of two options in multi-lable classification (the other being max)\n",
    "    \"\"\"\n",
    "#     chi_score = 0\n",
    "#     Nt1 = len(document_list) # actual collection frequency of having the word\n",
    "#     Nt0 = number_of_docs - len(document_list) # actual collection frequency of not having the word\n",
    "#     Pt1 = float(len(document_list))/ number_of_docs # probability of the term happening\n",
    "#     Pt0 = float(number_of_docs - len(document_list))/ number_of_docs # probablility of the term not happening\n",
    "#     print \"Docs Stats: Term present in %d (%.7f), Not Present in %d (%.7f) \" % (Nt1, Pt1, Nt0, Pt0)\n",
    "#     for subclass in subclasses:\n",
    "#         Pc1 = float(len(classifications_index[subclass]))/ number_of_docs # probability of the class happening\n",
    "#         Et1c1 = Pt1 * Pc1 * number_of_docs # expected frequency of docs in subclass with term (assuming independence)\n",
    "#         Et0c1 = Pt0 * Pc1 * number_of_docs # expected frequency of docs in subclass without term (assuming independence)\n",
    "#         chi_score += float(math.pow( Nt1 - Et1c1, 2)) / Et1c1\n",
    "#         chi_score += float(math.pow( Nt0 - Et0c1, 2)) / Et0c1\n",
    "#         print \"subclass %s: %.7f, %d, %d, %.7f\" % (subclass, Pc1, Et1c1, Et0c1, chi_score)\n",
    "#     return chi_score\n",
    "    chi_score = 0\n",
    "    N = len(document_list)\n",
    "    doc_set = set(document_list)\n",
    "    Nt1 = N # actual collection frequency of having the word\n",
    "    Nt0 = number_of_docs - N # actual collection frequency of not having the word\n",
    "    Pt1 = float(N)/ number_of_docs # probability of the term happening\n",
    "    Pt0 = float(number_of_docs - N)/ number_of_docs # probablility of the term not happening\n",
    "    #print \"Docs Stats: Term present in %d (%.7f), Not Present in %d (%.7f) \" % (Nt1, Pt1, Nt0, Pt0)\n",
    "    for subclass in subclasses:\n",
    "        Pc1 = float(len(classifications_index_set[subclass]))/ number_of_docs # probability of the class happening\n",
    "        Pc0 = 1 - Pc1\n",
    "        Pt1c1 = float(len(doc_set & classifications_index_set[subclass])) / number_of_docs\n",
    "        Pt1c0 = Pt1 - Pt1c1\n",
    "        Pt0c1 = Pc1 - Pt1c1\n",
    "        Pt0c0 = 1 - Pt1c0 - Pt0c1 - Pt1c1\n",
    "        \n",
    "        cat_chi_score = (number_of_docs * math.pow(Pt1c1 * Pt0c0 - Pt1c0 * Pt0c1, 2))/(Pt1 * Pt0 * Pc1 * Pc0)\n",
    "        # calculate average chi score\n",
    "        chi_score += Pc1 * cat_chi_score\n",
    "        #print \"subclass %s: %.7f, %.7f, %.7f, %.7f, %.7f, %.7f\" % (subclass, Pc1, Pt1c1, Pt1c0, Pt0c1, Pt0c0, chi_score)\n",
    "    return chi_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44846888"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_doc_postings_lists.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# min_doc_postings_lists = sc.parallelize(min_doc_postings_lists.take(10000))\n",
    "\n",
    "# term_accepted_chi_list_with_scores = get_chi_index(min_doc_postings_lists, classifications_index, subclasses, doc_count).takeOrdered(TOP_N_FEATURES, lambda (term,score): -score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order by Chi Squared and get Top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term_accepted_chi_list_with_scores = get_chi_index(min_doc_postings_lists, classifications_index_set, subclasses, doc_count).takeOrdered(TOP_N_FEATURES, lambda (term,score): -score)\n",
    "term_accepted_chi_list = map(lambda (x,y): x, term_accepted_chi_list_with_scores)\n",
    "# gets a bit slower at the end but finishes eventually \n",
    "term_dictionary = get_term_dictionary(term_accepted_chi_list)\n",
    "min_doc_postings_lists = min_doc_postings_lists.filter(lambda (term, postings): term in term_accepted_chi_list).cache()\n",
    "number_of_terms = min_doc_postings_lists.count()\n",
    "term_df_map = min_doc_postings_lists.map(lambda (term, postings): (term, len(postings))).collectAsMap()\n",
    "\n",
    "# Save Postings List and the supporting objects\n",
    "# min_doc_postings_lists.map(lambda (term, postings_list): \",\".join([term, json.dumps(postings_list)])).repartition(1).saveAsTextFile(postings_list_output)\n",
    "min_doc_postings_lists.map(lambda postings: json.dumps(postings)).saveAsTextFile(postings_list_chi_selected_output.format(str(TOP_N_FEATURES)))\n",
    "sc.parallelize(term_dictionary.items()).saveAsPickleFile(term_dictionary_output.format(str(TOP_N_FEATURES)))\n",
    "sc.parallelize(term_accepted_chi_list).saveAsPickleFile(accepted_terms_list_output.format(str(TOP_N_FEATURES)))\n",
    "sc.parallelize(term_accepted_chi_list_with_scores).saveAsPickleFile(accepted_terms_with_scores_list_output.format(str(TOP_N_FEATURES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#min_doc_postings_lists.map(lambda postings: json.dumps(postings)).repartition(100).saveAsTextFile(postings_list_chi_selected_output.format(str(TOP_N_FEATURES)))\n",
    "#term_df_map = min_doc_postings_lists.map(lambda (term, postings): (term, len(postings))).collectAsMap()\n",
    "sc.parallelize(term_dictionary.items()).repartition(1).saveAsPickleFile(term_dictionary_output.format(str(TOP_N_FEATURES)))\n",
    "sc.parallelize(term_df_map.items()).saveAsPickleFile(term_df_map_output.format(str(TOP_N_FEATURES)))\n",
    "sc.parallelize(term_accepted_chi_list).repartition(1).saveAsPickleFile(accepted_terms_list_output.format(str(TOP_N_FEATURES)))\n",
    "sc.parallelize(term_accepted_chi_list_with_scores).repartition(1).saveAsPickleFile(accepted_terms_with_scores_list_output.format(str(TOP_N_FEATURES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'server', 65022.23210679769),\n",
       " (u'execut', 60767.31792863743),\n",
       " (u'network', 58732.915422222875),\n",
       " (u'request', 58148.748110792854),\n",
       " (u'comput', 55771.137483335304),\n",
       " (u'softwar', 52868.551734217064),\n",
       " (u'Internet', 51907.35286510473),\n",
       " (u'program', 50474.68119787968),\n",
       " (u'comput system', 50072.134620861594),\n",
       " (u'pharmaceut accept', 49650.26318563324)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_accepted_chi_list_with_scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.parallelize(term_accepted_chi_list_with_scores).saveAsPickleFile(accepted_terms_with_scores_list_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'server',\n",
       " u'execut',\n",
       " u'network',\n",
       " u'request',\n",
       " u'comput',\n",
       " u'softwar',\n",
       " u'Internet',\n",
       " u'program',\n",
       " u'comput system',\n",
       " u'pharmaceut accept',\n",
       " u'memori',\n",
       " u'pharmaceut',\n",
       " u'hardwar',\n",
       " u'client',\n",
       " u'instruct',\n",
       " u'inform',\n",
       " u'manag',\n",
       " u'pharmaceut composit',\n",
       " u'oper system',\n",
       " u'oral',\n",
       " u'processor',\n",
       " u'updat',\n",
       " u'surfac',\n",
       " u'information',\n",
       " u'data',\n",
       " u'memory',\n",
       " u'administration',\n",
       " u'store',\n",
       " u'therapeut',\n",
       " u'user',\n",
       " u'substrat number_inidicator',\n",
       " u'administ',\n",
       " u'protein',\n",
       " u'access',\n",
       " u'dosag',\n",
       " u'computer',\n",
       " u'assay',\n",
       " u'disk',\n",
       " u'etch',\n",
       " u'code',\n",
       " u'diseas',\n",
       " u'resourc',\n",
       " u'logic',\n",
       " u'commun',\n",
       " u'memori number_inidicator',\n",
       " u'server number_inidicator',\n",
       " u'substrat',\n",
       " u'network number_inidicator',\n",
       " u'acid',\n",
       " u'software',\n",
       " u'vivo',\n",
       " u'amino',\n",
       " u'implement',\n",
       " u'databas',\n",
       " u'vitro',\n",
       " u'retriev',\n",
       " u'RAM',\n",
       " u'parenter',\n",
       " u'Pharmaceut',\n",
       " u'send',\n",
       " u'accept salt',\n",
       " u'block diagram',\n",
       " u'messag',\n",
       " u'effect amount',\n",
       " u'storag',\n",
       " u'interfac',\n",
       " u'purifi',\n",
       " u'substrate',\n",
       " u'incub',\n",
       " u'number_inidicator network',\n",
       " u'accept carrier',\n",
       " u'amino acid',\n",
       " u'excipi',\n",
       " u'capsules',\n",
       " u'servic',\n",
       " u'gene',\n",
       " u'number_inidicator execut',\n",
       " u'disease',\n",
       " u'dose',\n",
       " u'cach',\n",
       " u'sodium',\n",
       " u'drug',\n",
       " u'nitrid',\n",
       " u'enzym',\n",
       " u'keyboard',\n",
       " u'materi',\n",
       " u'therapeut effect',\n",
       " u'cultur',\n",
       " u'number_inidicator store',\n",
       " u'storag devic',\n",
       " u'patient',\n",
       " u'receptor',\n",
       " u'activ ingredi',\n",
       " u'dosag form',\n",
       " u'intramuscular',\n",
       " u'tissu',\n",
       " u'readabl',\n",
       " u'peptid',\n",
       " u'identifi',\n",
       " u'serum']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_accepted_chi_list[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recreate term dictionary with just the accepted terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 10000\n"
     ]
    }
   ],
   "source": [
    "# gets a bit slower at the end but finishes eventually \n",
    "term_dictionary = get_term_dictionary(term_accepted_chi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_doc_postings_lists = min_doc_postings_lists.filter(lambda (term, postings): term in term_accepted_chi_list).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_terms = min_doc_postings_lists.count()\n",
    "number_of_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Reduced Postings List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save Postings List\n",
    "## min_doc_postings_lists.map(lambda (term, postings_list): \",\".join([term, json.dumps(postings_list)])).repartition(1).saveAsTextFile(postings_list_output)\n",
    "min_doc_postings_lists.map(lambda postings: json.dumps(postings)).saveAsTextFile(postings_list_chi_selected_output.format(str(TOP_N_FEATURES)))\n",
    "#sc.parallelize(term_dictionary.items()).saveAsPickleFile(term_dictionary_output)\n",
    "#sc.parallelize(term_accepted_chi_list).saveAsPickleFile(accepted_terms_list_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Reduced Postings List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-89dbcbd1ab08>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-89dbcbd1ab08>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    term_dictionary = dict(sc.pickleFile(term_dictionary_output).collect())\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "min_doc_postings_lists = sc.textFile(postings_list_chi_selected_output.format(str(TOP_N_FEATURES)).map(lambda json_postings: json.loads(json_postings)).cache()\n",
    "term_dictionary = dict(sc.pickleFile(term_dictionary_output).collect())\n",
    "number_of_terms = min_doc_postings_lists.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect document lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to collect the document lengths since they are used in the BM25 calculation\n",
    "all_doc_index = create_doc_index(min_doc_postings_lists, term_dictionary)\n",
    "\n",
    "doc_lengths_rdd = all_doc_index.mapValues(lambda postings_dictionary: reduce(lambda x, term: x + postings_dictionary[term], postings_dictionary, 0))\n",
    "avg_doc_length = doc_lengths_rdd.map(lambda (term, count): count).reduce(lambda count1, count2: count1 + count2) / doc_count\n",
    "doc_lengths_dict = doc_lengths_rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_doc_index.map(lambda postings: json.dumps(postings)).saveAsTextFile(doc_index_chi_selected_output.format(str(TOP_N_FEATURES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Document Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.parallelize(doc_lengths_dict.items()).saveAsPickleFile(doc_lengths_map_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_doc_index.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_doc_index.saveAsPickleFile(doc_index_chi_selected_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Document Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_lengths_dict = dict(sc.pickleFile(doc_lengths_map_output).collect())\n",
    "avg_doc_length = sum(doc_lengths_dict.values())/len(doc_lengths_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'08226314', 3466)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lengths_dict.items()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2009750"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_lengths_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load everything for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_doc_postings_lists = sc.textFile(postings_list_chi_selected_output.format(str(TOP_N_FEATURES))).map(lambda json_postings: json.loads(json_postings)).cache()\n",
    "term_dictionary = dict(sc.pickleFile(term_dictionary_output.format(str(TOP_N_FEATURES))).collect())\n",
    "term_df_map = dict(sc.pickleFile(term_df_map_output.format(str(TOP_N_FEATURES))).collect())\n",
    "number_of_terms = len(term_df_map) # min_doc_postings_lists.count()\n",
    "doc_lengths_dict = dict(sc.pickleFile(doc_lengths_map_output).collect())\n",
    "avg_doc_length = sum(doc_lengths_dict.values())/len(doc_lengths_dict)\n",
    "#all_doc_index = sc.textFile(doc_index_chi_selected_output.format(str(TOP_N_FEATURES))).map(lambda json_postings: json.loads(json_postings)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_doc_index = all_doc_index.map(lambda (doc_id, postings): (doc_id, {int(key): postings[key] for key in postings})).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get min_doc_postings_lists for the sample only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_docs_set = set(training_documents)\n",
    "validation_docs_set = set(validation_documents)\n",
    "test_docs_set = set(test_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_doc_postings_lists = sc.textFile(postings_list_chi_selected_output.format(str(TOP_N_FEATURES))).map(lambda json_postings: json.loads(json_postings)).cache()\n",
    "min_doc_postings_lists = min_doc_postings_lists.map(lambda (term, postings): (term, {doc_id:postings[doc_id] for doc_id in postings if doc_id in training_docs_set or doc_id in validation_docs_set or doc_id in test_docs_set}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_doc_postings_lists.map(lambda postings: json.dumps(postings)).saveAsTextFile(get_save_location(postings_list_chi_selected_output.format(str(TOP_N_FEATURES)), sample=IS_SAMPLE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_postings_output = get_save_location(postings_list_training_chi_selected_output.format(str(TOP_N_FEATURES)), sample=IS_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_min_doc_postings_lists = min_doc_postings_lists.map(lambda (term, postings): (term, {doc_id:postings[doc_id] for doc_id in postings if doc_id in training_docs_set}))\n",
    "# training_min_doc_postings_lists.map(lambda postings: json.dumps(postings)).saveAsTextFile(training_postings_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_min_doc_postings_lists = sc.textFile(training_postings_output).map(get_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_postings_output = get_save_location(postings_list_validation_chi_selected_output.format(str(TOP_N_FEATURES)), sample=IS_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_min_doc_postings_lists = min_doc_postings_lists.map(lambda (term, postings): (term, {doc_id:postings[doc_id] for doc_id in postings if doc_id in validation_docs_set}))\n",
    "validation_min_doc_postings_lists.map(lambda postings: json.dumps(postings)).saveAsTextFile(validation_postings_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_min_doc_postings_lists = sc.textFile(validation_postings_output).map(get_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_postings_output = get_save_location(postings_list_test_chi_selected_output.format(str(TOP_N_FEATURES)), sample=IS_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_min_doc_postings_lists = min_doc_postings_lists.map(lambda (term, postings): (term, {doc_id:postings[doc_id] for doc_id in postings if doc_id in test_docs_set}))\n",
    "test_min_doc_postings_lists.map(lambda postings: json.dumps(postings)).saveAsTextFile(test_postings_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_min_doc_postings_lists = sc.textFile(test_postings_output).map(get_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start creating term weighting postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_written_doc_index(term_index, name, data_type=\"training\"):\n",
    "    doc_index = create_doc_index(term_index, term_dictionary)\n",
    "    output_name = get_data_output_name(name, data_type=data_type)\n",
    "    doc_index.map(lambda postings: json.dumps(postings)).repartition(100).saveAsTextFile(output_name)\n",
    "    doc_index = sc.textFile(output_name).map(get_json_convert_num)# .cache()\n",
    "    return doc_index\n",
    "\n",
    "def read_written_doc_index(name, data_type=\"training\"):\n",
    "    output_name = get_data_output_name(name, data_type=data_type)\n",
    "    doc_index = sc.textFile(output_name).map(get_json_convert_num)\n",
    "    return doc_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tf_postings = training_min_doc_postings_lists\n",
    "# tf_doc_index_training = create_written_doc_index(tf_postings, \"tf\")\n",
    "\n",
    "# sublinear_tf_postings = tf_postings.mapValues(lambda postings: {docId:  calculate_sublinear_tf(tf) for docId, tf in postings.items()})\n",
    "# sublinear_tf_doc_index_training = create_written_doc_index(sublinear_tf_postings, \"tf-sublinear\")\n",
    "\n",
    "# tf_idf_postings = tf_postings.mapValues(lambda postings: {docId:  calculate_tf_idf(tf, len(postings), len(training_documents)) for docId, tf in postings.items()})\n",
    "# tf_idf_doc_index_training = create_written_doc_index(tf_idf_postings, \"tf-idf\")\n",
    "\n",
    "# sublinear_tf_idf_postings = tf_postings.mapValues(lambda postings: {docId:  calculate_sublinear_tf_idf(tf, len(postings), len(training_documents)) for docId, tf in postings.items()})\n",
    "# sublinear_tf_idf_doc_index_training = create_written_doc_index(sublinear_tf_idf_postings, \"sublinear-tf-idf\")\n",
    "\n",
    "# bm25_postings = tf_postings.mapValues(lambda postings: {docId: calculate_bm25(tf, len(postings), len(training_documents), doc_lengths_dict[docId], avg_doc_length) for docId, tf in postings.items()})\n",
    "# bm25_doc_index_training = create_written_doc_index(bm25_postings, \"bm25\")\n",
    "\n",
    "bm25_postings = tf_postings.mapValues(lambda postings: {docId: calculate_bm25(tf, len(postings), len(training_documents), doc_lengths_dict[docId], avg_doc_length) for docId, tf in postings.items()})\n",
    "bm25_doc_index_training = create_written_doc_index(bm25_postings, \"bm25_0.001_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_doc_index_training = read_written_doc_index(\"tf\")\n",
    "sublinear_tf_doc_index_training = read_written_doc_index(\"tf-sublinear\")\n",
    "tf_idf_doc_index_training = read_written_doc_index(\"tf-idf\")\n",
    "sublinear_tf_idf_doc_index_training = read_written_doc_index(\"sublinear-tf-idf\")\n",
    "bm25_doc_index_training = read_written_doc_index(\"bm25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.07 s, sys: 156 ms, total: 3.23 s\n",
      "Wall time: 12min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tf_postings_validation = validation_min_doc_postings_lists\n",
    "# tf_doc_index_validation = create_written_doc_index(tf_postings_validation, \"tf\", data_type=\"validation\")\n",
    "\n",
    "# sublinear_tf_postings_validation = tf_postings_validation.mapValues(lambda postings: {docId:  calculate_sublinear_tf(tf) for docId, tf in postings.items()})\n",
    "# sublinear_tf_doc_index_validation = create_written_doc_index(sublinear_tf_postings_validation, \"tf-sublinear\", data_type=\"validation\")\n",
    "\n",
    "# tf_idf_postings_validation = tf_postings_validation.mapValues(lambda postings: {docId:  calculate_tf_idf(tf, len(postings), len(validation_documents)) for docId, tf in postings.items()})\n",
    "# tf_idf_doc_index_validation = create_written_doc_index(tf_idf_postings_validation, \"tf-idf\", data_type=\"validation\")\n",
    "\n",
    "sublinear_tf_idf_postings_validation = tf_postings_validation.mapValues(lambda postings: {docId:  calculate_sublinear_tf_idf(tf, len(postings), len(validation_documents)) for docId, tf in postings.items()})\n",
    "sublinear_tf_idf_doc_index_validation = create_written_doc_index(sublinear_tf_idf_postings_validation, \"sublinear-tf-idf\", data_type=\"validation\")\n",
    "\n",
    "# bm25_postings_validation = tf_postings_validation.mapValues(lambda postings: {docId: calculate_bm25(tf, len(postings), len(validation_documents), doc_lengths_dict[docId], avg_doc_length) for docId, tf in postings.items()})\n",
    "# bm25_doc_index_validation = create_written_doc_index(bm25_postings_validation, \"bm25\", data_type=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_doc_index_validation = read_written_doc_index(\"tf\", data_type=\"validation\")\n",
    "sublinear_tf_doc_index_validation = read_written_doc_index(\"tf-sublinear\", data_type=\"validation\")\n",
    "tf_idf_doc_index_validation = read_written_doc_index(\"tf-idf\", data_type=\"validation\")\n",
    "sublinear_tf_idf_doc_index_validation = read_written_doc_index(\"sublinear-tf-idf\", data_type=\"validation\")\n",
    "bm25_doc_index_validation = read_written_doc_index(\"bm25\", data_type=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'oooidii', {3: 4}], [u'232323', {3: 2}]]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jsonKV2str(x):\n",
    "    if isinstance(x, dict):\n",
    "            return {int(k):(int(v) if isinstance(v, unicode) else v) for k,v in x.items()}\n",
    "    return x\n",
    "\n",
    "output_namee = \"hdfs://deka.cip.ifi.lmu.de/svm/new/lskd4.json\"\n",
    "dd = {\"232323\":{3:2},\"oooidii\": {3:4}}\n",
    "#sc.parallelize(dd.items()).take(1)\n",
    "#sc.parallelize(dd.items()).map(lambda postings: json.dumps(postings)).saveAsTextFile(output_namee)\n",
    "sc.parallelize(dd.items()).map(lambda postings: json.dumps(postings)).take(1)\n",
    "sc.parallelize(dd.items()).map(lambda postings: json.dumps(postings)).map(lambda postings: json.loads(postings, object_hook=jsonKV2str)).collect()\n",
    "\n",
    "#map(json.dumps,dd.items() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_idf_doc_index_validation.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (OLD) Start creating term weighting postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tf_postings = min_doc_postings_lists\n",
    "tf_doc_index_training = all_doc_index.filter(lambda (doc_id, postings): doc_id in training_documents).cache()\n",
    "\n",
    "sublinear_tf_postings = tf_postings.mapValues(lambda postings: {docId:  calculate_sublinear_tf(tf) for docId, tf in postings.items()})\n",
    "sublinear_tf_doc_index = create_doc_index(sublinear_tf_postings, term_dictionary)\n",
    "sublinear_tf_doc_index_training = sublinear_tf_doc_index.filter(lambda (doc_id, postings): doc_id in training_documents).cache()\n",
    "\n",
    "tf_idf_postings = tf_postings.mapValues(lambda postings: {docId:  calculate_tf_idf(tf, len(postings), doc_count) for docId, tf in postings.items()})\n",
    "tf_id_doc_index = create_doc_index(tf_postings, term_dictionary)\n",
    "tf_id_doc_index_training = tf_id_doc_index.filter(lambda (doc_id, postings): doc_id in training_documents).cache()\n",
    "tf_id_doc_index.map(lambda postings: json.dumps(postings)).saveAsTextFile(get_data_output_name(\"tf-idf\"))\n",
    "\n",
    "bm25_postings = tf_postings.mapValues(lambda postings: {docId: calculate_bm25(tf, len(postings), doc_count, doc_lengths_dict[docId], avg_doc_length) for docId, tf in postings.items()})\n",
    "bm25_doc_index = create_doc_index(bm25_postings, term_dictionary)\n",
    "bm25_doc_index_training = bm25_doc_index.filter(lambda (doc_id, postings): doc_id in training_documents).cache()\n",
    "bm25_doc_index.map(lambda postings: json.dumps(postings)).saveAsTextFile(get_data_output_name(\"bm25\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 s, sys: 332 ms, total: 11.5 s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_doc_index_val = all_doc_index.filter(lambda (doc_id, postings): doc_id in validation_documents).cache()\n",
    "sublinear_tf_doc_index_val = sublinear_tf_doc_index.filter(lambda (doc_id, postings): doc_id in validation_documents).cache()\n",
    "tf_id_doc_index_val = tf_id_doc_index.filter(lambda (doc_id, postings): doc_id in validation_documents).cache()\n",
    "bm25_doc_index_val = bm25_doc_index.filter(lambda (doc_id, postings): doc_id in validation_documents).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_evaluations = {}\n",
    "validation_evaluations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_exists(path):\n",
    "    try:\n",
    "        model = SVMModel.load(sc, path)\n",
    "        return True;\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A-00\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-01\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-02\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-03\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-04\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-05\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-06\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-07\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-10\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-11\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-12\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-13\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-15\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-16\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-18\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-21\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-22\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-23\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-24\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-25\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-26\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-27\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-28\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-29\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-31\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-32\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-33\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-34\n",
      "Trying: bm25\n",
      "Model Exists\n",
      "A-35\n",
      "Trying: bm25\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i=0\n",
    "# for section in sections:\n",
    "#     classification = section\n",
    "for clss in classes:\n",
    "    classification = clss\n",
    "    print classification\n",
    "    #if classification == \"A\" or classification == \"B\" or classification == \"C\" or classification == \"D\": continue\n",
    "    i+=1\n",
    "    training_evaluations[classification] = {}\n",
    "    validation_evaluations[classification] = {}\n",
    "    representations_to_test = [\n",
    "#                                (\"tf\", tf_doc_index_training, tf_doc_index_validation), \n",
    "#                                (\"tf-sublinear\", sublinear_tf_doc_index_training, sublinear_tf_doc_index_validation),\n",
    "#                                (\"tf-idf\", tf_idf_doc_index_training, tf_idf_doc_index_validation),  \n",
    "#                                (\"sublinear-tf-idf\", sublinear_tf_idf_doc_index_training, sublinear_tf_idf_doc_index_validation), \n",
    "                               (\"bm25\", bm25_doc_index_training, bm25_doc_index_validation)\n",
    "                                ]\n",
    "    #representations_to_test = [(\"tf\", tf_doc_index), (\"tf-sublinear\", sublinear_tf_doc_index), (\"tf-idf\", tf_id_doc_index), (\"bm25\", bm25_doc_index)]\n",
    "    \n",
    "    for name, doc_index, val_doc_index in representations_to_test:\n",
    "        try:\n",
    "            print \"Trying: \" + name\n",
    "            model_path = get_model_name(name, classification)\n",
    "            if not model_exists(model_path):\n",
    "                training_vectors, svm = train_level_new(doc_index, classification, doc_classification_map, number_of_terms)\n",
    "                svm.save(sc, model_path)\n",
    "            else:\n",
    "                print \"Model Exists\"\n",
    "        except:\n",
    "            print \"Problem creating: %s: %s\" % (classification, name)\n",
    "            raise\n",
    "#         print \"Trying: \" + name\n",
    "#         docs_with_classes = doc_index.map(lambda (doc_id, terms): (doc_id, (terms, doc_classification_map[doc_id])))\n",
    "#         training_vectors, svm = train_level(docs_with_classes, classification, number_of_terms)\n",
    "#         svm.save(sc, get_model_name(name, classification))\n",
    "#         labels = training_vectors.map(lambda p: p.label).collect()\n",
    "#         predictions = training_vectors.map(lambda p: svm.predict(p.features)).collect()\n",
    "#         training_evaluations[classification][name] = Evaluator(labels, predictions)\n",
    "#         # validation\n",
    "#         print \"Validating\"\n",
    "#         validation_vectors = get_labeled_points_from_doc_index(val_doc_index, doc_classification_map, number_of_terms)\n",
    "#         labels_val = validation_vectors.map(lambda p: p.label).collect()\n",
    "#         predictions_val = validation_vectors.map(lambda p: svm.predict(p.features)).collect()\n",
    "#         validation_evaluations[classification][name] = Evaluator(labels_val, predictions_val)\n",
    "    \n",
    "#     rf_postings = tf_postings.mapValues(get_rf_postings(classification))\n",
    "#     rf_doc_index = create_doc_index(rf_postings, term_dictionary)\n",
    "#     # save the doc index so we don't have to create it again\n",
    "#     rf_doc_index.map(lambda postings: json.dumps(postings)).saveAsTextFile(get_data_classification_output_name(\"rf\", classification))\n",
    "#     rf_doc_index_training = rf_doc_index.filter(lambda (doc_id, postings): doc_id in training_documents)\n",
    "#     rf_doc_index_val = rf_doc_index.filter(lambda (doc_id, postings): doc_id in validation_documents)\n",
    "#     docs_with_classes = rf_doc_index_training.map(lambda (doc_id, terms): (doc_id, (terms, doc_classification_map[doc_id])))\n",
    "#     training_vectors, svm = train_level(docs_with_classes, classification, number_of_terms)\n",
    "#     svm.save(sc, get_model_name(\"rf\", classification))\n",
    "#     labels = training_vectors.map(lambda p: p.label).collect()\n",
    "#     predictions = training_vectors.map(lambda p: svm.predict(p.features)).collect()\n",
    "#     training_evaluations[classification][\"rf\"] = Evaluator(labels, predictions)\n",
    "#     # validation\n",
    "#     validation_vectors = get_labeled_points_from_doc_index(rf_doc_index_val, doc_classification_map, number_of_terms)\n",
    "#     labels_val = validation_vectors.map(lambda p: p.label).collect()\n",
    "#     predictions_val = validation_vectors.map(lambda p: svm.predict(p.features)).collect()\n",
    "#     validation_evaluations[classification][name] = Evaluator(labels_val, predictions_val)\n",
    "    \n",
    "    \n",
    "#     tf_rf_postings = tf_postings.mapValues(get_tf_rf_postings(classification))\n",
    "#     tf_rf_doc_index = create_doc_index(tf_rf_postings, term_dictionary)\n",
    "#     # save the doc index so we don't have to create it again\n",
    "#     tf_rf_doc_index.map(lambda postings: json.dumps(postings)).saveAsTextFile(get_data_classification_output_name(\"tf-rf\", classification))\n",
    "#     tf_rf_doc_index_training = tf_rf_doc_index.filter(lambda (doc_id, postings): doc_id in training_documents)\n",
    "#     tf_rf_doc_index_val = tf_rf_doc_index.filter(lambda (doc_id, postings): doc_id in validation_documents)\n",
    "#     docs_with_classes = tf_rf_doc_index_training.map(lambda (doc_id, terms): (doc_id, (terms, doc_classification_map[doc_id])))\n",
    "#     training_vectors, svm = train_level(docs_with_classes, classification, number_of_terms)\n",
    "#     svm.save(sc, get_model_name(\"tf-rf\", classification))\n",
    "#     labels = training_vectors.map(lambda p: p.label).collect()\n",
    "#     predictions = training_vectors.map(lambda p: svm.predict(p.features)).collect()\n",
    "#     training_evaluations[classification][\"tf-rf\"] = Evaluator(labels, predictions)\n",
    "#     # validation\n",
    "#     validation_vectors = get_labeled_points_from_doc_index(tf_rf_doc_index_val, doc_classification_map, number_of_terms)\n",
    "#     labels_val = validation_vectors.map(lambda p: p.label).collect()\n",
    "#     predictions_val = validation_vectors.map(lambda p: svm.predict(p.features)).collect()\n",
    "#     validation_evaluations[classification][name] = Evaluator(labels_val, predictions_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_exists(get_model_name(\"tf\", \"A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_evaluations = {}\n",
    "validation_evaluations = {}\n",
    "\n",
    "classification = \"A-01\"\n",
    "\n",
    "training_evaluations[classification] = {}\n",
    "validation_evaluations[classification] = {}\n",
    "representations_to_test = [\n",
    "#     (\"tf\", tf_doc_index_training, tf_doc_index_validation),\n",
    "#     (\"tf-sublinear\", sublinear_tf_doc_index_training, sublinear_tf_doc_index_validation), \n",
    "#     (\"bm25\", bm25_doc_index_training, bm25_doc_index_validation),\n",
    "#     (\"tf-idf\", tf_idf_doc_index_training, tf_idf_doc_index_validation)\n",
    "    (\"sublinear-tf-idf\", sublinear_tf_idf_doc_index_training, sublinear_tf_idf_doc_index_validation), \n",
    "]\n",
    "name, doc_index, val_doc_index = representations_to_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#doc_index.map(lambda postings: json.dumps(postings)).saveAsTextFile(get_data_output_name(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying: tf\n"
     ]
    }
   ],
   "source": [
    "print \"Trying: \" + name\n",
    "docs_with_classes = doc_index.map(lambda (doc_id, terms): (doc_id, (terms, doc_classification_map[doc_id])))\n",
    "training_vectors, svm = train_level(docs_with_classes, classification, number_of_terms)\n",
    "svm.save(sc, get_model_name(name, classification))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_vectors = docs_with_classes.map(\n",
    "        lambda (doc_id, (term_list, classifications)): get_training_vector(classification, term_list,\n",
    "                                                                           classifications, number_of_terms))\n",
    "svm = SVMWithSGD.train(training_vectors, iterations=SVM_ITERATIONS, convergenceTol=SVM_CONVERGENCE, regParam=SVM_REG, validateData=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = training_vectors.map(lambda p: p.label).collect()\n",
    "predictions = training_vectors.map(lambda p: svm.predict(p.features)).collect()\n",
    "training_evaluations[classification][name] = Evaluator(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# validation\n",
    "print \"Validating\"\n",
    "validation_vectors = get_labeled_points_from_doc_index(val_doc_index, doc_classification_map, number_of_terms)\n",
    "labels_val = validation_vectors.map(lambda p: p.label).collect()\n",
    "predictions_val = validation_vectors.map(lambda p: svm.predict(p.features)).collect()\n",
    "validation_evaluations[classification][name] = Evaluator(labels_val, predictions_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321473\n",
      "A-00\n",
      "Loaded the model\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "representations_to_test = [\n",
    "#    (\"tf\", tf_doc_index_validation), \n",
    "#    (\"tf-sublinear\", sublinear_tf_doc_index_validation), \n",
    "#    (\"tf-idf\", tf_idf_doc_index_validation), \n",
    "#    (\"sublinear-tf-idf\", sublinear_tf_idf_doc_index_validation), \n",
    "   (\"bm25\", bm25_doc_index_validation)\n",
    "]\n",
    "results = {}\n",
    "# subset = \"sections\"\n",
    "subset = \"classes\"\n",
    "for method, val_doc_set in representations_to_test:\n",
    "# method = representations_fto_test[0][0]\n",
    "# val_doc_set = representations_to_test[0][1]\n",
    "    results[method] = {}\n",
    "    #val_doc_set.cache()\n",
    "    doc_count = len(validation_documents)\n",
    "    print doc_count\n",
    "#     y_score = np.zeros((doc_count, len(sections)))\n",
    "#     y_true = np.zeros((doc_count, len(sections)))\n",
    "    y_score = np.zeros((doc_count, len(classes)))\n",
    "    y_true = np.zeros((doc_count, len(classes)))\n",
    "    i=0\n",
    "\n",
    "#     for section in sections:\n",
    "#         print section\n",
    "#         classification = section\n",
    "    for clss in classes:\n",
    "        print clss\n",
    "        classification = clss\n",
    "        \n",
    "        val_doc_set_vectors = val_doc_set.map(lambda (doc_id, postings): \n",
    "                                              get_training_vector(classification, postings, doc_classification_map[doc_id], \n",
    "                                                                  number_of_terms) )\n",
    "\n",
    "        binarySvm = SVMModel.load(sc, get_model_name(method, classification))\n",
    "        print \"Loaded the model\"\n",
    "        binarySvm.clearThreshold()\n",
    "        %time labels_predictions = val_doc_set_vectors.map(lambda p: (p.label, binarySvm.predict(p.features))).collect()\n",
    "        #labels = test_labeled_points.map(lambda p: p.labels)\n",
    "        y_true[:,i] = [label_pred[0] for label_pred in labels_predictions]\n",
    "        y_score[:,i] = [label_pred[1] for label_pred in labels_predictions]\n",
    "        i+=1\n",
    "    y_binary_score = get_binary(y_score)\n",
    "    results[method][\"y_true\"] = y_true\n",
    "    results[method][\"y_score\"] = y_score\n",
    "    results[method][\"y_binary_score\"] = y_binary_score\n",
    "    metrics = get_metrics(y_true, y_binary_score)\n",
    "    \n",
    "    sc.parallelize(y_true).repartition(1).saveAsPickleFile(get_labels_output_name(data_type=\"validation\", subset=subset))\n",
    "    sc.parallelize(y_score).repartition(1).saveAsPickleFile(get_prediction_output_name(method=method, data_type=\"validation\", subset=subset))\n",
    "    sc.parallelize((\"metrics\", json.dumps(metrics))).saveAsTextFile(get_metrics_output_name(method=method, data_type=\"validation\", subset=subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "pickle.dump(y_true, open('/home/s/shalaby/y_true.pkl'))\n",
    "pickle.dump(y_score, open('/home/s/shalaby/y_score.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'method' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3a6fd3ef9394>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'method' is not defined"
     ]
    }
   ],
   "source": [
    "method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method = 'bm25'\n",
    "y_true = results[method]['y_true']\n",
    "y_score = results[method]['y_score']\n",
    "y_binary_score = results[method]['y_binary_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SVMModel.load(sc, \"/svm/new/lskdjflsdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.parallelize(y_true).repartition(1).saveAsPickleFile(get_labels_output_name(data_type=\"validation\", subset=\"sections\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.parallelize(y_score).repartition(1).saveAsPickleFile(get_prediction_output_name(method=method, data_type=\"validation\", subset=\"sections\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Labels and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method = \"sublinear-tf-idf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hdfs://deka.cip.ifi.lmu.de/svm/new/models/iter_1000_reg_0.01/sublinear-tf-idf_validation_sections_predictions.svm'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction_output_name(method=method, data_type=\"validation\", subset=\"sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = np.array(sc.pickleFile(get_labels_output_name(data_type=\"validation\", subset=\"sections\")).collect())\n",
    "y_score = np.array(sc.pickleFile(get_prediction_output_name(method=method, data_type=\"validation\", subset=\"sections\")).collect())\n",
    "y_binary_score = get_binary(y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.62894004, -4.5963389 , -0.86146197, ..., -9.05835662,\n",
       "        -4.38929899, -8.42909693],\n",
       "       [-1.81517444, -2.07990448, -2.53543284, ..., -2.37632697,\n",
       "        -0.19371968,  0.43062112],\n",
       "       [-2.57868501, -2.65605152, -3.48412654, ..., -2.90845723,\n",
       "         0.21642679,  0.02513824],\n",
       "       ..., \n",
       "       [-2.3536382 , -1.56966217, -2.2091566 , ..., -2.19340324,\n",
       "        -0.28346742,  0.68576182],\n",
       "       [-0.92857442, -0.63666347, -1.38252274, ..., -0.73433494,\n",
       "        -0.90276557, -0.79875533],\n",
       "       [-0.73324228, -0.55649702, -1.26487436, ..., -0.71490296,\n",
       "        -0.88660098, -0.8348988 ]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(321473,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_score[:,1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(321473, 8)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 1],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       ..., \n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_binary_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1237"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_binary_score[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4f93ec56165b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'sublinear-tf-idf'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_true'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_binary_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_binary_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "method = 'sublinear-tf-idf'\n",
    "y_true = results[method]['y_true']\n",
    "y_score = results[method]['y_score']\n",
    "y_binary_score = results[method]['y_binary_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/shalaby/.virtualenv/thesis-env/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'average_num_of_labels': 1.1485630208446744,\n",
       " 'average_precision_macro': 0.29945386345673969,\n",
       " 'average_precision_micro': 0.28740309049121615,\n",
       " 'coverage_error': 4.1294914347394647,\n",
       " 'f1_macro': 0.1032281117589451,\n",
       " 'f1_micro': 0.21762015163857384,\n",
       " 'f1_scores_array': [0.10546985326752116,\n",
       "  0.025147841804357647,\n",
       "  0.09304370481339153,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0049892774300844675,\n",
       "  0.33211480927449516,\n",
       "  0.26505940748171086],\n",
       " 'precision_macro': 0.13921380824027155,\n",
       " 'precision_micro': 0.27817967202057248,\n",
       " 'precision_scores_array': [0.1440220385674931,\n",
       "  0.1426342767996434,\n",
       "  0.10077245952523721,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.07019704433497537,\n",
       "  0.35879643657960125,\n",
       "  0.297288210115222],\n",
       " 'recall_macro': 0.091781605664028196,\n",
       " 'recall_micro': 0.17871419595268015,\n",
       " 'recall_scores_array': [0.08319894754710576,\n",
       "  0.013789537188658106,\n",
       "  0.08641601887420224,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0025865589690066706,\n",
       "  0.30912682527803403,\n",
       "  0.23913495745521876],\n",
       " 'topN_avg': 3.2088231360020902,\n",
       " 'topN_list': [5,\n",
       "  4,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  6,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  6,\n",
       "  4,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  5,\n",
       "  2,\n",
       "  2,\n",
       "  6,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  6,\n",
       "  2,\n",
       "  7,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  3,\n",
       "  7,\n",
       "  5,\n",
       "  3,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  6,\n",
       "  3,\n",
       "  4,\n",
       "  2,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  6,\n",
       "  1,\n",
       "  6,\n",
       "  6,\n",
       "  5,\n",
       "  2,\n",
       "  5,\n",
       "  6,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  6,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  3,\n",
       "  5,\n",
       "  2,\n",
       "  6,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  2,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  6,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  1,\n",
       "  5,\n",
       "  5,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  3,\n",
       "  6,\n",
       "  3,\n",
       "  3,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  6,\n",
       "  4,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  6,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  2,\n",
       "  5,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  7,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  6,\n",
       "  5,\n",
       "  4,\n",
       "  2,\n",
       "  6,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  5,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  5,\n",
       "  7,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  6,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  6,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  6,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  6,\n",
       "  4,\n",
       "  2,\n",
       "  4,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  6,\n",
       "  3,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  2,\n",
       "  3,\n",
       "  6,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  6,\n",
       "  4,\n",
       "  5,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  8,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  7,\n",
       "  5,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  5,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  7,\n",
       "  5,\n",
       "  2,\n",
       "  4,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  6,\n",
       "  2,\n",
       "  5,\n",
       "  7,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  6,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  6,\n",
       "  2,\n",
       "  6,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  6,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  4,\n",
       "  6,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  6,\n",
       "  2,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  2,\n",
       "  4,\n",
       "  3,\n",
       "  5,\n",
       "  3,\n",
       "  5,\n",
       "  8,\n",
       "  5,\n",
       "  6,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  3,\n",
       "  3,\n",
       "  5,\n",
       "  4,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  5,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  3,\n",
       "  5,\n",
       "  7,\n",
       "  4,\n",
       "  3,\n",
       "  5,\n",
       "  2,\n",
       "  2,\n",
       "  6,\n",
       "  2,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  5,\n",
       "  4,\n",
       "  6,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  8,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  3,\n",
       "  5,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  6,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  6,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  6,\n",
       "  2,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  6,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  4,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  3,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  6,\n",
       "  1,\n",
       "  5,\n",
       "  3,\n",
       "  6,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  3,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  6,\n",
       "  2,\n",
       "  2,\n",
       "  6,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  7,\n",
       "  1,\n",
       "  7,\n",
       "  7,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  6,\n",
       "  4,\n",
       "  2,\n",
       "  3,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  6,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  6,\n",
       "  6,\n",
       "  1,\n",
       "  3,\n",
       "  6,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  7,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  5,\n",
       "  2,\n",
       "  7,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  5,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  3,\n",
       "  4,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  4,\n",
       "  2,\n",
       "  6,\n",
       "  4,\n",
       "  6,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  5,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  4,\n",
       "  1,\n",
       "  5,\n",
       "  4,\n",
       "  5,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  8,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  7,\n",
       "  1,\n",
       "  2,\n",
       "  6,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  7,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  6,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  6,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  6,\n",
       "  3,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  6,\n",
       "  1,\n",
       "  4,\n",
       "  6,\n",
       "  2,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  6,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  6,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  5,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  6,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  5,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  6,\n",
       "  4,\n",
       "  7,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  5,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  7,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  5,\n",
       "  3,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  6,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  6,\n",
       "  6,\n",
       "  1,\n",
       "  5,\n",
       "  3,\n",
       "  3,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  4,\n",
       "  4,\n",
       "  7,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  6,\n",
       "  4,\n",
       "  6,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  5,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  5,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  6,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  5,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  6,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  6,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  6,\n",
       "  4,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  ...]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = get_metrics(y_true, y_binary_score)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.parallelize((\"metrics\", json.dumps(metrics))).saveAsTextFile(get_metrics_output_name(method=method, data_type=\"validation\", subset=\"sections\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hdfs://deka.cip.ifi.lmu.de/svm/new/models/iter_1000_reg_0.01/bm25_validation_sections_metrics.pkl'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics_output_name(method=method, data_type=\"validation\", subset=\"sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_REG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method = \"tf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loaded_metrics = json.loads(sc.textFile(get_metrics_output_name(method=method, data_type=\"validation\", subset=\"sections\")).collect()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'average_num_of_labels': 1.1485630208446744,\n",
       " u'average_precision_macro': 0.6181398347172771,\n",
       " u'average_precision_micro': 0.6740106535881538,\n",
       " u'coverage_error': 3.347438198542335,\n",
       " u'f1_macro': 0.4453958787293348,\n",
       " u'f1_micro': 0.6465376531616135,\n",
       " u'f1_scores_array': [0.6598160048844598,\n",
       "  0.5282436574152903,\n",
       "  0.6680647596678131,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.3551263915369949,\n",
       "  0.760959623844551,\n",
       "  0.5909565924855693],\n",
       " u'precision_macro': 0.5022584534761925,\n",
       " u'precision_micro': 0.6678927795999445,\n",
       " u'precision_scores_array': [0.643975356024644,\n",
       "  0.5218899823366137,\n",
       "  0.750075346594334,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5509016315237095,\n",
       "  0.6441524294269122,\n",
       "  0.9070728819033265],\n",
       " u'recall_macro': 0.43039851669590423,\n",
       " u'recall_micro': 0.6265058283139057,\n",
       " u'recall_scores_array': [0.6764556102529282,\n",
       "  0.5347539429457899,\n",
       "  0.6022201385318049,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.2620138857376231,\n",
       "  0.9295125050037419,\n",
       "  0.4382320510953461],\n",
       " u'topN_avg': 2.0154507532514394,\n",
       " u'topN_list': [5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  8,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  6,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  8,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  7,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  7,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  7,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  5,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  7,\n",
       "  1,\n",
       "  8,\n",
       "  6,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  8,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  4,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  7,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  2,\n",
       "  4,\n",
       "  4,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  6,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  6,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  6,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  3,\n",
       "  1,\n",
       "  7,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  7,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  7,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  8,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  6,\n",
       "  5,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  6,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  ...]}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 23; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-36ae925c2194>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmetrics_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpickleFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_metrics_output_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"validation\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sections\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAsMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/s/shalaby/spark//python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollectAsMap\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1518\u001b[0m         \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m         \"\"\"\n\u001b[1;32m-> 1520\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1522\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: dictionary update sequence element #0 has length 23; 2 is required"
     ]
    }
   ],
   "source": [
    "metrics_loaded = sc.pickleFile(get_metrics_output_name(method=method, data_type=\"validation\", subset=\"sections\")).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['average_precision_micro',\n",
       " 'f1_macro',\n",
       " 'recall_macro',\n",
       " 'precision_micro',\n",
       " 'recall_micro',\n",
       " 'average_precision_macro',\n",
       " 'f1_micro',\n",
       " 'precision_macro',\n",
       " 'f1_scores_array',\n",
       " 'coverage_error',\n",
       " 'average_num_of_labels',\n",
       " 'precision_scores_array',\n",
       " 'recall_scores_array']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.64397536,  0.52188998,  0.75007535,  0.        ,  0.        ,\n",
       "        0.55090163,  0.64415243,  0.90707288])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_scores = np.zeros((len(sections)))\n",
    "for i in range(0,len(sections)):\n",
    "    precision_scores[i] = sklearn.metrics.precision_score(y_true[:,i], y_binary_score[:,i])\n",
    "metrics['precision_scores_array']precision_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1434134418.46\n",
      "22037.0\n"
     ]
    }
   ],
   "source": [
    "section_index = 5\n",
    "print np.sum(y_binary_score[:, section_index])\n",
    "print np.sum(y_true[:, section_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e = Evaluator(y_true[:,1], y_binary_score[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0154507532514394"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -160.73221359   -63.53035615 -1389.43386875 -2086.39085259 -1708.9396494\n",
      "  -257.07255332    -4.29960961   -15.17807309]\n",
      "[ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "[6 7 1 0 5 2 4 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docu_index = 0\n",
    "\n",
    "get_row_top_N(y_score[docu_index,:], y_true[docu_index,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5282436574152903"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.64397536,  0.52188998,  0.75007535,  0.        ,  0.        ,\n",
       "        0.55090163,  0.64415243,  0.90707288])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision_scores = np.zeros((len(sections),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lskdjfls\n",
      "lskdjfls\n",
      "lskdjfls\n",
      "lskdjfls\n",
      "lskdjfls\n",
      "lskdjfls\n",
      "lskdjfls\n",
      "lskdjfls\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(0,len(sections)):\n",
    "    print \"lskdjfls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67401065358815382"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64653765316161349"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.f1_score(y_true, y_binary_score, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coverage_error(test_labeled_points, classifications, method):\n",
    "    #test_labeled_points.cache()\n",
    "    y_score = np.zeros(test_labeled_points.count(), len(classifications))\n",
    "    y_true = np.zeros(test_labeled_points.count(), len(classifications))\n",
    "    \n",
    "    i = 0\n",
    "    for classification in classifications:\n",
    "        binarySvm = SVMModel.load(sc, get_model_name(method, classification))\n",
    "        binarySvm.clearThreshold()\n",
    "        predictions = test_labeled_points.map(lambda p: binarySvm.predict(p.features))\n",
    "        labels = test_labeled_points.map(lambda p: p.labels)\n",
    "        y_score[:][i] = predictions\n",
    "        y_true[:][i] = labels\n",
    "        i += 1\n",
    "    return coverage_error(y_score, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tf_doc_index_test = create_doc_index(tf_postings, term_dictionary).filter(lambda (doc_id, postings): doc_id in validation_documents)\n",
    "sublinear_tf_doc_index_test = create_doc_index(sublinear_tf_postings, term_dictionary).filter(lambda (doc_id, postings): doc_id in validation_documents)\n",
    "tf_id_doc_index_test = create_doc_index(tf_postings, term_dictionary).filter(lambda (doc_id, postings): doc_id in validation_documents)\n",
    "bm25_doc_index_test = create_doc_index(bm25_postings, term_dictionary).filter(lambda (doc_id, postings): doc_id in validation_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method = \"bm25\"\n",
    "test_vectors = get_labeled_points_from_doc_index(bm25_doc_index_test, doc_classification_map, number_of_terms)\n",
    "get_coverage_error(test_vectors, sections, method)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.6.1)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
