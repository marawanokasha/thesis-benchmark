{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "# import pyspark\n",
    "# from pyspark.mllib.regression import LabeledPoint\n",
    "# from pyspark.mllib.linalg import SparseVector\n",
    "# from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import logging\n",
    "from logging import info\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IS_SAMPLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_SEED = 1234\n",
    "DOC2VEC_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUMBER_INDICATOR = \"number_inidicator\"\n",
    "CURRENCY_INDICATOR = \"currency_inidicator\"\n",
    "CHEMICAL_INDICATOR = \"chemical_inidicator\"\n",
    "MIN_WORD_COUNT = 5\n",
    "MIN_SIZE = 0\n",
    "NUM_CORES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GLOBAL_VARS = namedtuple('GLOBAL_VARS', ['MODEL_NAME', 'DOC2VEC_MODEL', 'SVM_MODEL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATIO = 0.0001\n",
    "VALIDATION_SAMPLE_RATIO = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_MODEL = \"vocab_model\"\n",
    "MODEL_PREFIX = \"model\"\n",
    "VALIDATION_MATRIX = \"validation_matrix.pkl\"\n",
    "METRICS = \"metrics.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training_file = \"/home/local/shalaby/docs_output_sample_100.json\"\n",
    "\n",
    "save_parent_location = \"hdfs://deka.cip.ifi.lmu.de/pg-vectors/\"\n",
    "if IS_SAMPLE: \n",
    "    save_parent_location = save_parent_location + \"sample_\" + str(SAMPLE_RATIO) + \"/\"\n",
    "\n",
    "\n",
    "root_location = \"/mnt/data2/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "doc2vec_model_save_location = os.path.join(root_location, \"parameter_search_doc2vec_models\", \"sample_\" + str(SAMPLE_RATIO))\n",
    "if not os.path.exists(doc2vec_model_save_location):\n",
    "    os.makedirs(doc2vec_model_save_location)\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL)):\n",
    "    os.makedirs(os.path.join(doc2vec_model_save_location, VOCAB_MODEL))\n",
    "\n",
    "#training_file = root_location + \"docs_output.json\"\n",
    "training_file = root_location + 'docs_output_training_validation_documents_' + str(SAMPLE_RATIO)\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "training_docs_list_file = exports_location + \"training_documents_\" + str(SAMPLE_RATIO) + \"_sample.pkl\"\n",
    "validation_docs_list_file = exports_location + \"validation_documents_\" + str(VALIDATION_SAMPLE_RATIO) + \"_sample.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.2 s, sys: 0 ns, total: 18.2 s\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemtokenizer(text):\n",
    "    \"\"\" MAIN FUNCTION to get clean stems out of a text. A list of clean stems are returned \"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\s+', gaps=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stems = []  # result\n",
    "    for token in tokens:\n",
    "        stem = token.lower()\n",
    "        stem = stem.strip(string.punctuation)\n",
    "        if stem:\n",
    "            if is_number(stem):\n",
    "                stem = NUMBER_INDICATOR\n",
    "            elif is_currency(stem):\n",
    "                stem = CURRENCY_INDICATOR\n",
    "            elif is_chemical(stem):\n",
    "                stem = CHEMICAL_INDICATOR\n",
    "            else:\n",
    "                stem = stem.strip(string.punctuation)\n",
    "            if stem and len(stem) >= MIN_SIZE:\n",
    "                # extract uni-grams\n",
    "                stems.append(stem)\n",
    "    del tokens\n",
    "    return stems\n",
    "\n",
    "def is_number(str):\n",
    "    \"\"\" Returns true if given string is a number (float or int)\"\"\"\n",
    "    try:\n",
    "        float(str.replace(\",\", \"\"))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_currency(str):\n",
    "    return str[0] == \"$\"\n",
    "\n",
    "def is_chemical(str):\n",
    "    return str.count(\"-\") > 3\n",
    "\n",
    "def get_training_vector(classification, term_list, classifications, number_of_terms):\n",
    "    clss = 1 if classification in classifications else 0\n",
    "    return LabeledPoint(clss, SparseVector(number_of_terms, term_list))\n",
    "\n",
    "def train_level_new(docs_index, classification, doc_classification_map, number_of_terms):\n",
    "    training_vectors = docs_index.map(\n",
    "        lambda (doc_id, postings): get_training_vector(classification, postings,\n",
    "                                                        doc_classification_map[doc_id], number_of_terms))\n",
    "    svm = SVMWithSGD.train(training_vectors, iterations=SVM_ITERATIONS, convergenceTol=SVM_CONVERGENCE, regParam=SVM_REG)\n",
    "    return training_vectors, svm\n",
    "\n",
    "get_binary = lambda x: 1 if x > 0 else 0\n",
    "get_binary = np.vectorize(get_binary)\n",
    "\n",
    "\n",
    "def get_top_N_percentage(y_score, y_true, max_N=3):\n",
    "    \"\"\"\n",
    "    Get percentage of correct labels that are in the top N scores\n",
    "    \"\"\"\n",
    "    num_all_true = 0\n",
    "    num_found_in_max_N = 0\n",
    "    for i in xrange(y_score.shape[0]):\n",
    "        y_score_row = y_score[i,:]\n",
    "        y_true_row = y_true[i,:]\n",
    "        desc_score_indices = np.argsort(y_score_row)[::-1]\n",
    "        true_indices = np.where(y_true_row ==1)[0]\n",
    "        \n",
    "        num_true_in_row = len(true_indices)\n",
    "        num_all_true += num_true_in_row\n",
    "        for i, score_index in enumerate(desc_score_indices):\n",
    "            # only iterate through the score list till depth N, but make sure you also account for the case where \n",
    "            # the number of true labels for the current row is higher than N\n",
    "            if i >= max_N and i >= num_true_in_row:\n",
    "                break\n",
    "            if score_index in true_indices:\n",
    "                num_found_in_max_N += 1\n",
    "    return float(num_found_in_max_N)/ num_all_true\n",
    "\n",
    "\n",
    "def get_metrics(y_true, y_score, y_binary_score):\n",
    "    metrics = {}\n",
    "    metrics['total_positive'] = np.sum(np.sum(y_binary_score))\n",
    "    #TODO remove those two when running on the whole set to avoid excessive storage costs\n",
    "    metrics['y_true'] = y_true\n",
    "    metrics['y_score'] = y_score\n",
    "    metrics['y_binary_score'] = y_binary_score\n",
    "    metrics['coverage_error'] = coverage_error(y_true, y_binary_score)\n",
    "    metrics['average_num_of_labels'] = np.sum(np.sum(y_true, axis=1))/y_true.shape[0]\n",
    "    metrics['average_precision_micro'] = sklearn.metrics.average_precision_score(y_true, y_binary_score, average='micro')\n",
    "    metrics['average_precision_macro'] = sklearn.metrics.average_precision_score(y_true, y_binary_score, average='macro')\n",
    "    metrics['precision_micro'] = sklearn.metrics.precision_score(y_true, y_binary_score, average='micro')\n",
    "    metrics['precision_macro'] = sklearn.metrics.precision_score(y_true, y_binary_score, average='macro')\n",
    "    metrics['recall_micro'] = sklearn.metrics.recall_score(y_true, y_binary_score, average='micro')\n",
    "    metrics['recall_macro'] = sklearn.metrics.recall_score(y_true, y_binary_score, average='macro')\n",
    "    metrics['f1_micro'] = sklearn.metrics.f1_score(y_true, y_binary_score, average='micro')\n",
    "    metrics['f1_macro'] = sklearn.metrics.f1_score(y_true, y_binary_score, average='macro')\n",
    "\n",
    "    precision_scores = np.zeros(y_true.shape[1])\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        precision_scores[i] = sklearn.metrics.precision_score(y_true[:,i], y_binary_score[:,i])\n",
    "    metrics['precision_scores_array'] = precision_scores.tolist()\n",
    "\n",
    "    recall_scores = np.zeros(y_true.shape[1])\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        recall_scores[i] = sklearn.metrics.recall_score(y_true[:,i], y_binary_score[:,i])\n",
    "    metrics['recall_scores_array'] = recall_scores.tolist()\n",
    "\n",
    "    f1_scores = np.zeros(y_true.shape[1])\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        f1_scores[i] = sklearn.metrics.f1_score(y_true[:,i], y_binary_score[:,i])\n",
    "    metrics['f1_scores_array'] = f1_scores.tolist()\n",
    "\n",
    "    metrics['top_1'] = get_top_N_percentage(y_score, y_true, max_N=1)\n",
    "    metrics['top_3'] = get_top_N_percentage(y_score, y_true, max_N=3)\n",
    "    metrics['top_5'] = get_top_N_percentage(y_score, y_true, max_N=5)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def model_exists(path):\n",
    "    try:\n",
    "        model = SVMModel.load(sc, path)\n",
    "        return True;\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def get_training_vector(classification, dense_vector, classifications):\n",
    "    clss = 1 if classification in classifications else 0\n",
    "    return LabeledPoint(clss, dense_vector)\n",
    "\n",
    "def train_level_doc2vec(classification, doc_classification_map):\n",
    "    doc2vec_model = GLOBAL_VARS.DOC2VEC_MODEL\n",
    "    training_vectors = []\n",
    "    for doc_id in training_docs_list:\n",
    "        # converting from memmap to a normal array as spark is unable to convert memmap to a spark Vector\n",
    "        normal_array = []\n",
    "        normal_array[:] = doc2vec_model.docvecs[doc_id][:]\n",
    "        training_vectors.append(get_training_vector(classification, normal_array, \n",
    "                                                    doc_classification_map[doc_id]))\n",
    "    info(\"Finished getting training vectors\")\n",
    "    training_vectors = sc.parallelize(training_vectors)\n",
    "    info(\"Finished parallelization\")\n",
    "    svm = SVMWithSGD.train(training_vectors, iterations=SVM_ITERATIONS, convergenceTol=SVM_CONVERGENCE, regParam=SVM_REG)\n",
    "    return training_vectors, svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensure_hdfs_location_exists(location):\n",
    "    parent = os.path.dirname(location)\n",
    "    os.system(\"hdfs dfs -mkdir -p \" + location)\n",
    "\n",
    "def ensure_disk_location_exists(location):\n",
    "    if not os.path.exists(location):\n",
    "        os.makedirs(location)\n",
    "        \n",
    "def train_classifications(classifications):\n",
    "    info(\"====== Doing Training\")\n",
    "    i=0\n",
    "    for classification in classifications:\n",
    "        print classification\n",
    "        try:\n",
    "            model_path = get_svm_model_path(GLOBAL_VARS.MODEL_NAME, classification)\n",
    "            if not model_exists(model_path):\n",
    "                training_vectors, svm = train_level_doc2vec(classification, doc_classification_map)\n",
    "                svm.save(sc, model_path)\n",
    "            else:\n",
    "                print \"Model Exists\"\n",
    "        except:\n",
    "            print \"Problem creating: %s: %s\" % (classification, GLOBAL_VARS.MODEL_NAME)\n",
    "            raise\n",
    "\n",
    "def do_validation(validation_vectors_matrix, doc_classification_map, classifications, classifications_name):\n",
    "\n",
    "    info(\"====== Doing Validation\")\n",
    "    method = GLOBAL_VARS.MODEL_NAME\n",
    "    subset = classifications_name\n",
    "\n",
    "    doc_count = validation_vectors_matrix.shape[0]\n",
    "    y_score = np.zeros((doc_count, len(classifications)))\n",
    "    y_true = np.zeros((doc_count, len(classifications)))\n",
    "    i=0\n",
    "\n",
    "    for classification in classifications:\n",
    "        print classification\n",
    "\n",
    "        validation_vectors = get_validation_doc2vec_spark_vectors(validation_vectors_matrix, \n",
    "                                                                  classification, doc_classification_map)\n",
    "        #global binarySvm\n",
    "        binarySvm = SVMModel.load(sc, get_svm_model_path(GLOBAL_VARS.MODEL_NAME, classification))\n",
    "        info(\"Loaded the model, Doing the prediction now....\")\n",
    "        binarySvm.clearThreshold()\n",
    "        binarySvmB = sc.broadcast(binarySvm)\n",
    "        # using the broadcasted binarySvm variable, fixes global name 'binarySvm' is not defined as this variable was not\n",
    "        # available in the workers, so we pass it explicitly to the mapper using partial\n",
    "        labels_predictions = validation_vectors.map( \\\n",
    "            partial(lambda svm, p: (p.label, svm.value.predict(p.features)), binarySvmB) \\\n",
    "        ).collect()\n",
    "        #labels = test_labeled_points.map(lambda p: p.labels)\n",
    "        y_true[:,i] = [label_pred[0] for label_pred in labels_predictions]\n",
    "        y_score[:,i] = [label_pred[1] for label_pred in labels_predictions]\n",
    "        i+=1\n",
    "    y_binary_score = get_binary(y_score)\n",
    "    # results[method][\"y_true\"] = y_true\n",
    "    # results[method][\"y_score\"] = y_score\n",
    "    # results[method][\"y_binary_score\"] = y_binary_score\n",
    "    metrics = get_metrics(y_true, y_score, y_binary_score)\n",
    "    return metrics\n",
    "\n",
    "def get_validation_docs_with_inference(doc2vec_model, doc_classification_map):\n",
    "    \"\"\"\n",
    "    Use the trained doc2vec model to get the paragraph vector representations of the validation documents\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX)):\n",
    "        info(\"===== Loading validation vectors\")\n",
    "        validation_vectors_matrix = pickle.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX)))\n",
    "    else:\n",
    "        validation_documents_reps = {}\n",
    "        validation_vectors = []\n",
    "        validation_labels = []\n",
    "        info(\"===== Getting validation vectors with inference\")\n",
    "\n",
    "        # do inference and store results in dict\n",
    "        i = 0\n",
    "        for (doc_id, doc_contents_array) in ValidationDocumentGenerator(training_file, validation_docs_list):\n",
    "            i += 1\n",
    "            if i % 1000 == 0: info(\"Finished: {}\".format(str(i)))\n",
    "            validation_documents_reps[doc_id] = doc2vec_model.infer_vector(doc_contents_array)\n",
    "\n",
    "        # create matrix for the validation vectors\n",
    "        for validation_doc_id in validation_docs_list:\n",
    "            validation_vectors.append(validation_documents_reps[validation_doc_id])\n",
    "            validation_labels.append([classf for classf in doc_classification_map[validation_doc_id] if classf in sections])\n",
    "        validation_vectors_matrix = np.array(validation_vectors)\n",
    "        pickle.dump(validation_vectors_matrix, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX), 'w'))\n",
    "    \n",
    "    return validation_vectors_matrix\n",
    "\n",
    "def get_validation_doc2vec_spark_vectors(validation_vectors_matrix, classification, doc_classification_map):\n",
    "    validation_vectors = []\n",
    "    for (index, doc_id) in enumerate(validation_docs_list):\n",
    "        # converting from memmap to a normal array as spark is unable to convert memmap to a spark Vector\n",
    "        validation_vector = validation_vectors_matrix[index]\n",
    "        validation_vectors.append(get_training_vector(classification, validation_vector, \n",
    "                                                    doc_classification_map[doc_id]))\n",
    "    validation_vectors = sc.parallelize(validation_vectors)\n",
    "    info(\"Finished getting validation vectors\")\n",
    "    return validation_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MetricsGraph:\n",
    "    def __init__(self):\n",
    "        self.coverage_errors = []\n",
    "        self.average_num_labels = []\n",
    "        \n",
    "        self.f1_micros = []\n",
    "        self.precision_micros = []\n",
    "        self.recall_micros = []\n",
    "        self.f1_macros = []\n",
    "        self.precision_macros = []\n",
    "        self.recall_macros = []\n",
    "        \n",
    "        self.top_1s = []\n",
    "        self.top_3s = []\n",
    "        self.top_5s = []\n",
    "        \n",
    "        self.epochs = []\n",
    "        \n",
    "        self.fig = None\n",
    "        self.ax = None\n",
    "        self.ax2 = None\n",
    "    \n",
    "    def init_graph(self):\n",
    "        self.fig = plt.figure(figsize=(12,6), dpi=80)\n",
    "        self.ax = plt.subplot(121)\n",
    "        self.ax2 = plt.subplot(122)\n",
    "        self.fig.subplots_adjust(top=0.72, bottom=0.1, left=0.05, right=0.95)\n",
    "        self.ax.set_xlabel(\"Epochs\")\n",
    "        self.ax2.set_xlabel(\"Epochs\")\n",
    "    \n",
    "    def _add_metrics(self, metrics, epoch):\n",
    "        self.coverage_errors.append(metrics['coverage_error'])\n",
    "        self.average_num_labels.append(metrics['average_num_of_labels'])\n",
    "        \n",
    "        self.f1_micros.append(metrics['f1_micro'])\n",
    "        self.precision_micros.append(metrics['precision_micro'])\n",
    "        self.recall_micros.append(metrics['recall_micro'])\n",
    "        self.f1_macros.append(metrics['f1_macro'])\n",
    "        self.precision_macros.append(metrics['precision_macro'])\n",
    "        self.recall_macros.append(metrics['recall_macro'])\n",
    "        \n",
    "        self.top_1s.append(metrics['top_1']  if 'top_1' in metrics else get_top_N_percentage(metrics['y_score'], metrics['y_true'], max_N=1))\n",
    "        self.top_3s.append(metrics['top_3']  if 'top_3' in metrics else get_top_N_percentage(metrics['y_score'], metrics['y_true'], max_N=3))\n",
    "        self.top_5s.append(metrics['top_5']  if 'top_5' in metrics else get_top_N_percentage(metrics['y_score'], metrics['y_true'], max_N=5))\n",
    "        \n",
    "        self.epochs.append(epoch)\n",
    "        \n",
    "    def add_metrics_to_graph(self, metrics, epoch, draw_now=True):\n",
    "        \n",
    "        self._add_metrics(metrics, epoch)\n",
    "        if draw_now:\n",
    "            self.draw()\n",
    "\n",
    "    def draw(self):\n",
    "        \n",
    "        first_epoch = self.epochs[0]\n",
    "        last_epoch = self.epochs[-1]\n",
    "        \n",
    "        coverage_error_line, = self.ax.plot(self.epochs, self.coverage_errors, 'r-', label='Coverage Error')\n",
    "        average_num_labels_line, = self.ax.plot(self.epochs, self.average_num_labels, 'g-', label='Avg Num. of Labels')\n",
    "\n",
    "        self.ax.legend(handles=[coverage_error_line, average_num_labels_line],\n",
    "                  bbox_to_anchor=(0., 1.02, 1., .102), loc=3, ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "        self.ax.axis([first_epoch, last_epoch, 0, 10])\n",
    "        # show the average number of labels as a separate y-tick\n",
    "        curr_min_cov = [min(self.coverage_errors)]\n",
    "        prev_min_cov = [min(self.coverage_errors[:-1])] if len(self.coverage_errors) > 1 else []\n",
    "        self.ax.set_yticks(list(set(self.ax.get_yticks())- set(prev_min_cov) ) + [self.average_num_labels[0]] + curr_min_cov)\n",
    "\n",
    "        f1_micro_line, = self.ax2.plot(self.epochs, self.f1_micros, 'g-', label='F1 Micro')\n",
    "        precision_micro_line, = self.ax2.plot(self.epochs, self.precision_micros, 'r-', label='Precision Micro')\n",
    "        recall_micro_line, = self.ax2.plot(self.epochs, self.recall_micros, 'b-', label='Recall Micro')\n",
    "        f1_macro_line, = self.ax2.plot(self.epochs, self.f1_macros, 'g--', label='F1 Macro')\n",
    "        precision_macro_line, = self.ax2.plot(self.epochs, self.precision_macros, 'r--', label='Precision Macro')\n",
    "        recall_macro_line, = self.ax2.plot(self.epochs, self.recall_macros, 'b--', label='Recall Macro')\n",
    "        \n",
    "        top_1_line, = self.ax2.plot(self.epochs, self.top_1s, 'g-.', label='Top 1 %')\n",
    "        top_3_line, = self.ax2.plot(self.epochs, self.top_3s, 'r-.', label='Top 3 %')\n",
    "        top_5_line, = self.ax2.plot(self.epochs, self.top_5s, 'b-.', label='Top 5 %')\n",
    "        \n",
    "        self.ax2.legend(handles=[f1_micro_line, precision_micro_line, recall_micro_line, \n",
    "                                 f1_macro_line, precision_macro_line, recall_macro_line,\n",
    "                                 top_1_line, top_3_line, top_5_line],\n",
    "                  bbox_to_anchor=(0., 1.02, 1., .102), loc=3, ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "        self.ax2.axis([first_epoch, last_epoch, 0, 1])\n",
    "        curr_max_f1_micro = [max(self.f1_micros)]\n",
    "        prev_max_f1_micro = [max(self.f1_micros[:-1])] if len(self.f1_micros) > 1 else []\n",
    "        self.ax2.set_yticks(list(set(self.ax2.get_yticks())- set(prev_max_f1_micro) ) + curr_max_f1_micro)\n",
    "        \n",
    "        self.fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOC2VEC_SIZE = 64\n",
    "DOC2VEC_WINDOW = 8\n",
    "DOC2VEC_MAX_VOCAB_SIZE = None\n",
    "DOC2VEC_SAMPLE = 1e-5\n",
    "DOC2VEC_TYPE = 1\n",
    "DOC2VEC_HIERARCHICAL_SAMPLE = 0\n",
    "DOC2VEC_NEGATIVE_SAMPLE_SIZE = 10\n",
    "DOC2VEC_CONCAT = 1\n",
    "DOC2VEC_MEAN = 0\n",
    "DOC2VEC_TRAIN_WORDS = 0\n",
    "DOC2VEC_EPOCHS = 1 # we do our training manually one epoch at a time\n",
    "DOC2VEC_MAX_EPOCHS = 20\n",
    "REPORT_DELAY = 60 # report the progress every x seconds\n",
    "REPORT_VOCAB_PROGRESS = 1000 # report the progress every x terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_ITERATIONS = 1000\n",
    "SVM_CONVERGENCE = 0.001\n",
    "SVM_REG = 0.1\n",
    "GLOBAL_VARS.SVM_MODEL_NAME = 'iter_{}_reg_{}'.format(SVM_ITERATIONS, SVM_REG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_svm_model_path(method, classification, reg=SVM_REG, iterations=SVM_ITERATIONS):\n",
    "    location = os.path.join(save_parent_location, \"models\", method, \n",
    "                            \"iter_\" + str(iterations) + \"_reg_\" + str(reg),\n",
    "                            classification + \"_model.svm\")\n",
    "    ensure_hdfs_location_exists(location)\n",
    "    return location\n",
    "\n",
    "class TrainingDocumentGenerator(object):\n",
    "    def __init__(self, filename, training_docs_list):\n",
    "        self.filename = filename\n",
    "        self.training_docs_list = training_docs_list\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename):\n",
    "            if not line.strip(): continue\n",
    "            (doc_id, text) = eval(line)\n",
    "            if doc_id in self.training_docs_list:\n",
    "                yield LabeledSentence(words=stemtokenizer(text), tags=[doc_id])\n",
    "                \n",
    "class ValidationDocumentGenerator(object):\n",
    "    def __init__(self, filename, validation_docs_list):\n",
    "        self.filename = filename\n",
    "        self.validation_docs_list = validation_docs_list\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename):\n",
    "            if not line.strip(): continue\n",
    "            (doc_id, text) = eval(line)\n",
    "            if doc_id in self.validation_docs_list:\n",
    "                yield doc_id, stemtokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doc2vec_size_500_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_{}'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}'.format(DOC2VEC_SIZE, \n",
    "                                                                DOC2VEC_WINDOW, \n",
    "                                                                'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                                DOC2VEC_TRAIN_WORDS,\n",
    "                                                                DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                                str(DOC2VEC_MAX_VOCAB_SIZE))\n",
    "placeholder_model_name = placeholder_model_name + \"_curriter_{}\"\n",
    "placeholder_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2vec_model = Doc2Vec(size=DOC2VEC_SIZE , window=DOC2VEC_WINDOW, min_count=MIN_WORD_COUNT, \n",
    "                max_vocab_size= DOC2VEC_MAX_VOCAB_SIZE,\n",
    "                sample=DOC2VEC_SAMPLE, seed=DOC2VEC_SEED, workers=NUM_CORES,\n",
    "                # doc2vec algorithm dm=1 => PV-DM, dm=2 => PV-DBOW, PV-DM dictates CBOW for words\n",
    "                dm=DOC2VEC_TYPE,\n",
    "                # hs=0 => negative sampling, hs=1 => hierarchical softmax\n",
    "                hs=DOC2VEC_HIERARCHICAL_SAMPLE, negative=DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                dm_concat=DOC2VEC_CONCAT,\n",
    "                # would train words with skip-gram on top of cbow, we don't need that for now\n",
    "                dbow_words=DOC2VEC_TRAIN_WORDS,\n",
    "                iter=DOC2VEC_EPOCHS)\n",
    "\n",
    "GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX)):\n",
    "    doc2vec_model.build_vocab(sentences=TrainingDocumentGenerator(training_file, training_docs_list), \n",
    "                              progress_per=REPORT_VOCAB_PROGRESS)\n",
    "    doc2vec_model.save(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "else: \n",
    "    doc2vec_model_vocab_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "    doc2vec_model.reset_from(doc2vec_model_vocab_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Training and Validation and Metrics Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#doc2vec_model.min_alpha = 0.025\n",
    "epoch_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib notebook\n",
    "graph = MetricsGraph()\n",
    "graph.init_graph()\n",
    "# when resuming, resume from an epoch with a previously created doc2vec model to get the learning rate right\n",
    "start_from = 1\n",
    "for epoch in range(start_from,DOC2VEC_MAX_EPOCHS+1):\n",
    "    GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "    info(\"****************** Epoch {} --- Working on {} *******************\".format(epoch, GLOBAL_VARS.MODEL_NAME))\n",
    "    \n",
    "    # if we have the model, just load it, otherwise train the previous model\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)):\n",
    "        docvec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "        GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "    else:\n",
    "        # train the doc2vec model\n",
    "        doc2vec_model.train(sentences=StochasticDocumentGenerator(training_file, training_docs_list, line_positions), \n",
    "                            report_delay=REPORT_DELAY)\n",
    "        #doc2vec_model.alpha -= 0.001  # decrease the learning rate\n",
    "        #doc2vec_model.min_alpha = doc2vec_model.alpha  # fix the learning rate, no decay\n",
    "        ensure_disk_location_exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME))\n",
    "        doc2vec_model.save(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "        GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "\n",
    "    # Training and validation of SVMs using those docvecs\n",
    "    train_classifications(sections)\n",
    "    validation_vectors_matrix = get_validation_docs_with_inference(doc2vec_model, doc_classification_map)\n",
    "    metrics = do_validation(validation_vectors_matrix, doc_classification_map, sections, \"sections\")\n",
    "    ensure_disk_location_exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                             GLOBAL_VARS.SVM_MODEL_NAME))\n",
    "    pickle.dump(metrics, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, GLOBAL_VARS.SVM_MODEL_NAME, METRICS), 'w'))\n",
    "    print \"Coverage Error: {}, Average No of Labels: {}, Top 1: {}, Top 3: {}, Top 5: {}, F1 Micro: {}, Total Positive: {}\".format(\n",
    "        metrics['coverage_error'], metrics['average_num_of_labels'], metrics['top_1'], metrics['top_3'], metrics['top_5'], \n",
    "        metrics['f1_micro'], metrics['total_positive'])\n",
    "                                                                                     \n",
    "    epoch_metrics.append(metrics)\n",
    "    graph.add_metrics_to_graph(metrics, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from word2veckeras.doc2veckeras import Doc2VecKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-10-11 13:07:03,410 : INFO : collecting all words and their counts\n",
      "2016-10-11 13:07:03,436 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2016-10-11 13:07:28,450 : INFO : PROGRESS: at example #1000, processed 6730672 words (269091/s), 101429 word types, 1000 tags\n",
      "2016-10-11 13:07:52,650 : INFO : PROGRESS: at example #2000, processed 13377669 words (274686/s), 173358 word types, 2000 tags\n",
      "2016-10-11 13:08:15,293 : INFO : PROGRESS: at example #3000, processed 19721952 words (280202/s), 237449 word types, 3000 tags\n",
      "2016-10-11 13:08:36,708 : INFO : PROGRESS: at example #4000, processed 26105730 words (298122/s), 283534 word types, 4000 tags\n",
      "2016-10-11 13:08:58,168 : INFO : PROGRESS: at example #5000, processed 32474320 words (296786/s), 327931 word types, 5000 tags\n",
      "2016-10-11 13:09:20,385 : INFO : PROGRESS: at example #6000, processed 39183207 words (301998/s), 377546 word types, 6000 tags\n",
      "2016-10-11 13:09:42,136 : INFO : PROGRESS: at example #7000, processed 45696312 words (299448/s), 420401 word types, 7000 tags\n",
      "2016-10-11 13:10:04,218 : INFO : PROGRESS: at example #8000, processed 52312044 words (299618/s), 461851 word types, 8000 tags\n",
      "2016-10-11 13:10:27,210 : INFO : collected 502773 word types and 8979 unique tags from a corpus of 8979 examples and 59217428 words\n",
      "2016-10-11 13:10:27,512 : INFO : min_count=5 retains 104357 unique words (drops 398416)\n",
      "2016-10-11 13:10:27,513 : INFO : min_count leaves 58631278 word corpus (99% of original 59217428)\n",
      "2016-10-11 13:10:27,745 : INFO : deleting the raw counts dictionary of 502773 items\n",
      "2016-10-11 13:10:27,865 : INFO : sample=1e-05 downsamples 3315 most-common words\n",
      "2016-10-11 13:10:27,866 : INFO : downsampling leaves estimated 15522780 word corpus (26.5% of prior 58631278)\n",
      "2016-10-11 13:10:27,867 : INFO : estimated required memory for 104357 words and 300 dimensions: 2318860300 bytes\n",
      "2016-10-11 13:10:28,253 : INFO : using concatenative 5100-dimensional layer1\n",
      "2016-10-11 13:10:28,254 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 48s, sys: 6min 39s, total: 16min 27s\n",
      "Wall time: 15min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc2vec_model = Doc2VecKeras(size=DOC2VEC_SIZE , window=DOC2VEC_WINDOW, min_count=MIN_WORD_COUNT, \n",
    "                max_vocab_size= DOC2VEC_MAX_VOCAB_SIZE,\n",
    "                sample=DOC2VEC_SAMPLE, seed=DOC2VEC_SEED, workers=NUM_CORES,\n",
    "                # doc2vec algorithm dm=1 => PV-DM, dm=2 => PV-DBOW, PV-DM dictates CBOW for words\n",
    "                dm=DOC2VEC_TYPE,\n",
    "                # hs=0 => negative sampling, hs=1 => hierarchical softmax\n",
    "                hs=DOC2VEC_HIERARCHICAL_SAMPLE, negative=DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                dm_concat=DOC2VEC_CONCAT,\n",
    "                # would train words with skip-gram on top of cbow, we don't need that for now\n",
    "                dbow_words=DOC2VEC_TRAIN_WORDS,\n",
    "                iter=DOC2VEC_EPOCHS)\n",
    "doc2vec_model.build_vocab(sentences=TrainingDocumentGenerator(training_file, training_docs_list), \n",
    "                              progress_per=REPORT_VOCAB_PROGRESS)\n",
    "doc2vec_model.train(docs=TrainingDocumentGenerator(training_file, training_docs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-10-11 13:30:55,958 : INFO : collecting all words and their counts\n",
      "2016-10-11 13:30:55,981 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2016-10-11 13:31:18,953 : INFO : PROGRESS: at example #1000, processed 6730672 words (293002/s), 101429 word types, 1000 tags\n",
      "2016-10-11 13:31:41,711 : INFO : PROGRESS: at example #2000, processed 13377669 words (292089/s), 173358 word types, 2000 tags\n",
      "2016-10-11 13:32:03,212 : INFO : PROGRESS: at example #3000, processed 19721952 words (295078/s), 237449 word types, 3000 tags\n",
      "2016-10-11 13:32:24,848 : INFO : PROGRESS: at example #4000, processed 26105730 words (295062/s), 283534 word types, 4000 tags\n",
      "2016-10-11 13:32:46,536 : INFO : PROGRESS: at example #5000, processed 32474320 words (293658/s), 327931 word types, 5000 tags\n",
      "2016-10-11 13:33:09,080 : INFO : PROGRESS: at example #6000, processed 39183207 words (297597/s), 377546 word types, 6000 tags\n",
      "2016-10-11 13:33:31,019 : INFO : PROGRESS: at example #7000, processed 45696312 words (296887/s), 420401 word types, 7000 tags\n",
      "2016-10-11 13:33:53,335 : INFO : PROGRESS: at example #8000, processed 52312044 words (296465/s), 461851 word types, 8000 tags\n",
      "2016-10-11 13:34:16,501 : INFO : collected 502773 word types and 8979 unique tags from a corpus of 8979 examples and 59217428 words\n",
      "2016-10-11 13:34:16,815 : INFO : min_count=5 retains 104357 unique words (drops 398416)\n",
      "2016-10-11 13:34:16,816 : INFO : min_count leaves 58631278 word corpus (99% of original 59217428)\n",
      "2016-10-11 13:34:17,029 : INFO : deleting the raw counts dictionary of 502773 items\n",
      "2016-10-11 13:34:17,155 : INFO : sample=1e-05 downsamples 3315 most-common words\n",
      "2016-10-11 13:34:17,155 : INFO : downsampling leaves estimated 15522780 word corpus (26.5% of prior 58631278)\n",
      "2016-10-11 13:34:17,156 : INFO : estimated required memory for 104357 words and 300 dimensions: 2318860300 bytes\n",
      "2016-10-11 13:34:17,579 : INFO : using concatenative 5100-dimensional layer1\n",
      "2016-10-11 13:34:17,580 : INFO : resetting layer weights\n",
      "2016-10-11 13:34:18,911 : INFO : training model with 20 workers on 104358 vocabulary and 5100 features, using sg=0 hs=0 sample=1e-05 negative=10\n",
      "2016-10-11 13:34:18,912 : INFO : expecting 8979 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-10-11 13:34:19,999 : INFO : PROGRESS: at 0.17% examples, 21753 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:21,045 : INFO : PROGRESS: at 0.41% examples, 33302 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:22,114 : INFO : PROGRESS: at 0.71% examples, 35916 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:34:23,177 : INFO : PROGRESS: at 1.01% examples, 38384 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:24,201 : INFO : PROGRESS: at 1.43% examples, 40335 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:25,288 : INFO : PROGRESS: at 1.75% examples, 40780 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:26,309 : INFO : PROGRESS: at 2.17% examples, 42174 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:27,366 : INFO : PROGRESS: at 2.48% examples, 41308 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:28,421 : INFO : PROGRESS: at 2.82% examples, 42469 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:29,502 : INFO : PROGRESS: at 3.14% examples, 42193 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:30,513 : INFO : PROGRESS: at 3.41% examples, 41857 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:34:31,610 : INFO : PROGRESS: at 3.75% examples, 42114 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:32,663 : INFO : PROGRESS: at 4.03% examples, 42522 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:33,713 : INFO : PROGRESS: at 4.33% examples, 42009 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:34,726 : INFO : PROGRESS: at 4.60% examples, 41787 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:35,779 : INFO : PROGRESS: at 4.86% examples, 41872 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:36,807 : INFO : PROGRESS: at 5.05% examples, 41494 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:34:37,907 : INFO : PROGRESS: at 5.32% examples, 41869 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:34:39,003 : INFO : PROGRESS: at 5.66% examples, 42135 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:40,060 : INFO : PROGRESS: at 5.95% examples, 42102 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:34:41,075 : INFO : PROGRESS: at 6.31% examples, 42058 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:42,096 : INFO : PROGRESS: at 6.63% examples, 42370 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:43,117 : INFO : PROGRESS: at 6.85% examples, 41839 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:44,155 : INFO : PROGRESS: at 7.16% examples, 42136 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:45,202 : INFO : PROGRESS: at 7.47% examples, 41811 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:46,217 : INFO : PROGRESS: at 7.74% examples, 41597 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:47,227 : INFO : PROGRESS: at 8.11% examples, 41982 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:34:48,232 : INFO : PROGRESS: at 8.35% examples, 41844 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:49,370 : INFO : PROGRESS: at 8.66% examples, 41831 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:50,405 : INFO : PROGRESS: at 8.93% examples, 42058 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:51,381 : INFO : PROGRESS: at 9.17% examples, 42078 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:52,466 : INFO : PROGRESS: at 9.39% examples, 41864 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:53,515 : INFO : PROGRESS: at 9.63% examples, 41812 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:54,612 : INFO : PROGRESS: at 9.95% examples, 41896 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:55,960 : INFO : PROGRESS: at 10.27% examples, 41404 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:34:57,118 : INFO : PROGRESS: at 10.50% examples, 41444 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:34:58,114 : INFO : PROGRESS: at 10.78% examples, 41350 words/s, in_qsize 1, out_qsize 0\n",
      "2016-10-11 13:34:59,171 : INFO : PROGRESS: at 10.97% examples, 41325 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:00,191 : INFO : PROGRESS: at 11.24% examples, 41293 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:01,344 : INFO : PROGRESS: at 11.55% examples, 41373 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:02,314 : INFO : PROGRESS: at 11.87% examples, 41506 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:03,339 : INFO : PROGRESS: at 12.20% examples, 41589 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:04,368 : INFO : PROGRESS: at 12.45% examples, 41532 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:05,374 : INFO : PROGRESS: at 12.79% examples, 41696 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:06,484 : INFO : PROGRESS: at 13.19% examples, 41648 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:07,573 : INFO : PROGRESS: at 13.53% examples, 41650 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:08,621 : INFO : PROGRESS: at 13.85% examples, 41793 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:35:09,700 : INFO : PROGRESS: at 14.18% examples, 41686 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:10,739 : INFO : PROGRESS: at 14.46% examples, 41703 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:35:11,815 : INFO : PROGRESS: at 14.80% examples, 41767 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:12,868 : INFO : PROGRESS: at 15.09% examples, 41827 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:13,938 : INFO : PROGRESS: at 15.42% examples, 41902 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:15,031 : INFO : PROGRESS: at 15.73% examples, 41743 words/s, in_qsize 1, out_qsize 1\n",
      "2016-10-11 13:35:16,085 : INFO : PROGRESS: at 16.10% examples, 41954 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:17,137 : INFO : PROGRESS: at 16.45% examples, 41982 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:18,182 : INFO : PROGRESS: at 16.78% examples, 42141 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:19,211 : INFO : PROGRESS: at 17.06% examples, 42175 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:20,248 : INFO : PROGRESS: at 17.41% examples, 42149 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:21,250 : INFO : PROGRESS: at 17.72% examples, 42222 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:22,259 : INFO : PROGRESS: at 18.01% examples, 42217 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:23,342 : INFO : PROGRESS: at 18.26% examples, 42086 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:24,386 : INFO : PROGRESS: at 18.53% examples, 42273 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:25,474 : INFO : PROGRESS: at 18.83% examples, 42212 words/s, in_qsize 1, out_qsize 2\n",
      "2016-10-11 13:35:26,507 : INFO : PROGRESS: at 19.23% examples, 42284 words/s, in_qsize 1, out_qsize 0\n",
      "2016-10-11 13:35:27,515 : INFO : PROGRESS: at 19.56% examples, 42325 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:28,627 : INFO : PROGRESS: at 19.89% examples, 42309 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:29,637 : INFO : PROGRESS: at 20.00% examples, 42185 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:30,676 : INFO : PROGRESS: at 20.22% examples, 42331 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:31,709 : INFO : PROGRESS: at 20.40% examples, 42092 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:32,717 : INFO : PROGRESS: at 20.65% examples, 42057 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:33,723 : INFO : PROGRESS: at 21.00% examples, 42285 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:34,953 : INFO : PROGRESS: at 21.28% examples, 42254 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:35,981 : INFO : PROGRESS: at 21.51% examples, 42087 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:36,983 : INFO : PROGRESS: at 21.85% examples, 42108 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:38,086 : INFO : PROGRESS: at 22.20% examples, 42031 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:39,102 : INFO : PROGRESS: at 22.44% examples, 42074 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:40,151 : INFO : PROGRESS: at 22.75% examples, 42157 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:41,168 : INFO : PROGRESS: at 23.11% examples, 42185 words/s, in_qsize 1, out_qsize 0\n",
      "2016-10-11 13:35:42,192 : INFO : PROGRESS: at 23.41% examples, 42216 words/s, in_qsize 1, out_qsize 0\n",
      "2016-10-11 13:35:43,304 : INFO : PROGRESS: at 23.63% examples, 42177 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:44,376 : INFO : PROGRESS: at 24.02% examples, 42221 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:45,434 : INFO : PROGRESS: at 24.38% examples, 42266 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:46,453 : INFO : PROGRESS: at 24.75% examples, 42298 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:47,579 : INFO : PROGRESS: at 24.98% examples, 42245 words/s, in_qsize 0, out_qsize 2\n",
      "2016-10-11 13:35:48,590 : INFO : PROGRESS: at 25.37% examples, 42270 words/s, in_qsize 1, out_qsize 0\n",
      "2016-10-11 13:35:49,696 : INFO : PROGRESS: at 25.74% examples, 42239 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:50,722 : INFO : PROGRESS: at 26.05% examples, 42300 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:51,819 : INFO : PROGRESS: at 26.45% examples, 42298 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:35:52,866 : INFO : PROGRESS: at 26.73% examples, 42236 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:53,872 : INFO : PROGRESS: at 27.05% examples, 42305 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:54,955 : INFO : PROGRESS: at 27.42% examples, 42276 words/s, in_qsize 1, out_qsize 0\n",
      "2016-10-11 13:35:55,963 : INFO : PROGRESS: at 27.73% examples, 42312 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:57,080 : INFO : PROGRESS: at 28.07% examples, 42317 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:58,129 : INFO : PROGRESS: at 28.39% examples, 42402 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:35:59,273 : INFO : PROGRESS: at 28.69% examples, 42380 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:00,315 : INFO : PROGRESS: at 28.96% examples, 42450 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:01,377 : INFO : PROGRESS: at 29.35% examples, 42446 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:02,432 : INFO : PROGRESS: at 29.66% examples, 42422 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:03,448 : INFO : PROGRESS: at 29.96% examples, 42519 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:04,530 : INFO : PROGRESS: at 30.20% examples, 42457 words/s, in_qsize 0, out_qsize 2\n",
      "2016-10-11 13:36:05,574 : INFO : PROGRESS: at 30.49% examples, 42493 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:06,682 : INFO : PROGRESS: at 30.75% examples, 42495 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:07,677 : INFO : PROGRESS: at 31.07% examples, 42557 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:08,803 : INFO : PROGRESS: at 31.33% examples, 42349 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:09,807 : INFO : PROGRESS: at 31.74% examples, 42447 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:36:10,807 : INFO : PROGRESS: at 32.00% examples, 42465 words/s, in_qsize 1, out_qsize 1\n",
      "2016-10-11 13:36:11,820 : INFO : PROGRESS: at 32.20% examples, 42358 words/s, in_qsize 1, out_qsize 0\n",
      "2016-10-11 13:36:12,888 : INFO : PROGRESS: at 32.46% examples, 42472 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:13,894 : INFO : PROGRESS: at 32.79% examples, 42493 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:14,963 : INFO : PROGRESS: at 33.10% examples, 42500 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:15,996 : INFO : PROGRESS: at 33.37% examples, 42420 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:17,009 : INFO : PROGRESS: at 33.70% examples, 42541 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:18,198 : INFO : PROGRESS: at 33.96% examples, 42490 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:19,217 : INFO : PROGRESS: at 34.24% examples, 42576 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:20,239 : INFO : PROGRESS: at 34.54% examples, 42570 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:21,349 : INFO : PROGRESS: at 34.86% examples, 42521 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:22,356 : INFO : PROGRESS: at 35.17% examples, 42568 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:23,401 : INFO : PROGRESS: at 35.49% examples, 42599 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:24,616 : INFO : PROGRESS: at 35.74% examples, 42514 words/s, in_qsize 1, out_qsize 2\n",
      "2016-10-11 13:36:25,657 : INFO : PROGRESS: at 36.06% examples, 42569 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:26,734 : INFO : PROGRESS: at 36.30% examples, 42497 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:36:27,722 : INFO : PROGRESS: at 36.59% examples, 42496 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:36:28,754 : INFO : PROGRESS: at 36.86% examples, 42611 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:29,828 : INFO : PROGRESS: at 37.22% examples, 42627 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:30,873 : INFO : PROGRESS: at 37.52% examples, 42607 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:31,991 : INFO : PROGRESS: at 37.89% examples, 42608 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:33,068 : INFO : PROGRESS: at 38.22% examples, 42636 words/s, in_qsize 1, out_qsize 0\n",
      "2016-10-11 13:36:34,159 : INFO : PROGRESS: at 38.46% examples, 42635 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:35,232 : INFO : PROGRESS: at 38.75% examples, 42677 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:36,260 : INFO : PROGRESS: at 39.11% examples, 42712 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:37,294 : INFO : PROGRESS: at 39.50% examples, 42683 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:38,353 : INFO : PROGRESS: at 39.80% examples, 42650 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:39,370 : INFO : PROGRESS: at 40.04% examples, 42703 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:40,405 : INFO : PROGRESS: at 40.35% examples, 42669 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:36:41,429 : INFO : PROGRESS: at 40.61% examples, 42664 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:42,471 : INFO : PROGRESS: at 40.92% examples, 42651 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:36:43,491 : INFO : PROGRESS: at 41.24% examples, 42690 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:44,524 : INFO : PROGRESS: at 41.62% examples, 42727 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:45,607 : INFO : PROGRESS: at 41.95% examples, 42658 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:46,656 : INFO : PROGRESS: at 42.32% examples, 42732 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:36:47,672 : INFO : PROGRESS: at 42.64% examples, 42770 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:48,674 : INFO : PROGRESS: at 42.89% examples, 42786 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:49,751 : INFO : PROGRESS: at 43.15% examples, 42688 words/s, in_qsize 0, out_qsize 2\n",
      "2016-10-11 13:36:50,818 : INFO : PROGRESS: at 43.48% examples, 42758 words/s, in_qsize 1, out_qsize 1\n",
      "2016-10-11 13:36:51,856 : INFO : PROGRESS: at 43.89% examples, 42753 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:52,943 : INFO : PROGRESS: at 44.18% examples, 42779 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:53,938 : INFO : PROGRESS: at 44.46% examples, 42798 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:55,266 : INFO : PROGRESS: at 44.80% examples, 42801 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:36:55,957 : INFO : PROGRESS: at 44.96% examples, 42661 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:56,958 : INFO : PROGRESS: at 45.24% examples, 42716 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:57,979 : INFO : PROGRESS: at 45.58% examples, 42710 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:36:59,070 : INFO : PROGRESS: at 45.95% examples, 42687 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:37:00,118 : INFO : PROGRESS: at 46.34% examples, 42686 words/s, in_qsize 1, out_qsize 0\n",
      "2016-10-11 13:37:01,169 : INFO : PROGRESS: at 46.54% examples, 42641 words/s, in_qsize 1, out_qsize 1\n",
      "2016-10-11 13:37:02,191 : INFO : PROGRESS: at 46.93% examples, 42669 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:03,239 : INFO : PROGRESS: at 47.13% examples, 42631 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:04,267 : INFO : PROGRESS: at 47.38% examples, 42631 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:05,352 : INFO : PROGRESS: at 47.61% examples, 42606 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:37:06,450 : INFO : PROGRESS: at 48.01% examples, 42644 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:07,512 : INFO : PROGRESS: at 48.40% examples, 42668 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:37:08,537 : INFO : PROGRESS: at 48.64% examples, 42651 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:37:09,543 : INFO : PROGRESS: at 48.86% examples, 42656 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:10,632 : INFO : PROGRESS: at 49.21% examples, 42653 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:11,721 : INFO : PROGRESS: at 49.45% examples, 42637 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:12,763 : INFO : PROGRESS: at 49.76% examples, 42626 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:37:13,845 : INFO : PROGRESS: at 50.07% examples, 42583 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:14,882 : INFO : PROGRESS: at 50.33% examples, 42607 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:15,986 : INFO : PROGRESS: at 50.64% examples, 42587 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:17,000 : INFO : PROGRESS: at 50.97% examples, 42579 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:18,025 : INFO : PROGRESS: at 51.23% examples, 42606 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:19,074 : INFO : PROGRESS: at 51.62% examples, 42602 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:37:20,083 : INFO : PROGRESS: at 52.01% examples, 42623 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:21,124 : INFO : PROGRESS: at 52.32% examples, 42640 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:22,127 : INFO : PROGRESS: at 52.65% examples, 42641 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:23,136 : INFO : PROGRESS: at 53.08% examples, 42664 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:24,263 : INFO : PROGRESS: at 53.46% examples, 42647 words/s, in_qsize 0, out_qsize 2\n",
      "2016-10-11 13:37:25,280 : INFO : PROGRESS: at 53.75% examples, 42646 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:26,294 : INFO : PROGRESS: at 54.03% examples, 42647 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:27,333 : INFO : PROGRESS: at 54.26% examples, 42649 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:28,488 : INFO : PROGRESS: at 54.56% examples, 42585 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:29,518 : INFO : PROGRESS: at 54.77% examples, 42610 words/s, in_qsize 0, out_qsize 2\n",
      "2016-10-11 13:37:30,553 : INFO : PROGRESS: at 55.20% examples, 42618 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:31,726 : INFO : PROGRESS: at 55.56% examples, 42602 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:32,728 : INFO : PROGRESS: at 55.82% examples, 42618 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:33,742 : INFO : PROGRESS: at 56.12% examples, 42580 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:34,774 : INFO : PROGRESS: at 56.49% examples, 42615 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:35,757 : INFO : PROGRESS: at 56.78% examples, 42642 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:36,827 : INFO : PROGRESS: at 57.11% examples, 42644 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:37,990 : INFO : PROGRESS: at 57.42% examples, 42610 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:39,045 : INFO : PROGRESS: at 57.87% examples, 42673 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:40,060 : INFO : PROGRESS: at 58.12% examples, 42670 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:37:41,054 : INFO : PROGRESS: at 58.39% examples, 42665 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:42,069 : INFO : PROGRESS: at 58.64% examples, 42667 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:43,197 : INFO : PROGRESS: at 58.90% examples, 42635 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:44,275 : INFO : PROGRESS: at 59.27% examples, 42690 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:45,276 : INFO : PROGRESS: at 59.58% examples, 42656 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:46,373 : INFO : PROGRESS: at 59.93% examples, 42676 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:47,386 : INFO : PROGRESS: at 60.23% examples, 42692 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:48,457 : INFO : PROGRESS: at 60.51% examples, 42685 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:49,525 : INFO : PROGRESS: at 60.79% examples, 42708 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:50,537 : INFO : PROGRESS: at 60.86% examples, 42584 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:51,578 : INFO : PROGRESS: at 61.22% examples, 42662 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:52,580 : INFO : PROGRESS: at 61.60% examples, 42649 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:53,661 : INFO : PROGRESS: at 61.82% examples, 42647 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:54,686 : INFO : PROGRESS: at 62.15% examples, 42660 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:55,691 : INFO : PROGRESS: at 62.43% examples, 42652 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:56,708 : INFO : PROGRESS: at 62.69% examples, 42661 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:37:57,746 : INFO : PROGRESS: at 62.95% examples, 42664 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:58,753 : INFO : PROGRESS: at 63.21% examples, 42677 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:37:59,799 : INFO : PROGRESS: at 63.51% examples, 42632 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:00,969 : INFO : PROGRESS: at 63.80% examples, 42640 words/s, in_qsize 0, out_qsize 2\n",
      "2016-10-11 13:38:01,987 : INFO : PROGRESS: at 64.13% examples, 42656 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:03,022 : INFO : PROGRESS: at 64.37% examples, 42673 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:04,190 : INFO : PROGRESS: at 64.62% examples, 42655 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:05,202 : INFO : PROGRESS: at 64.90% examples, 42664 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:06,230 : INFO : PROGRESS: at 65.24% examples, 42693 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:07,244 : INFO : PROGRESS: at 65.55% examples, 42703 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:08,263 : INFO : PROGRESS: at 65.82% examples, 42674 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:09,304 : INFO : PROGRESS: at 66.04% examples, 42679 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:10,350 : INFO : PROGRESS: at 66.38% examples, 42730 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:11,536 : INFO : PROGRESS: at 66.69% examples, 42684 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:12,611 : INFO : PROGRESS: at 66.96% examples, 42688 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:13,717 : INFO : PROGRESS: at 67.21% examples, 42669 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:14,735 : INFO : PROGRESS: at 67.55% examples, 42714 words/s, in_qsize 1, out_qsize 0\n",
      "2016-10-11 13:38:15,793 : INFO : PROGRESS: at 67.88% examples, 42705 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:16,890 : INFO : PROGRESS: at 68.19% examples, 42709 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:17,959 : INFO : PROGRESS: at 68.37% examples, 42643 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:18,969 : INFO : PROGRESS: at 68.74% examples, 42682 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:20,014 : INFO : PROGRESS: at 69.13% examples, 42675 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:21,048 : INFO : PROGRESS: at 69.38% examples, 42680 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:22,061 : INFO : PROGRESS: at 69.70% examples, 42709 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:23,108 : INFO : PROGRESS: at 70.07% examples, 42707 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:24,175 : INFO : PROGRESS: at 70.40% examples, 42698 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:25,177 : INFO : PROGRESS: at 70.70% examples, 42707 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:26,229 : INFO : PROGRESS: at 71.05% examples, 42718 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:27,234 : INFO : PROGRESS: at 71.46% examples, 42705 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:28,309 : INFO : PROGRESS: at 71.73% examples, 42700 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:29,363 : INFO : PROGRESS: at 72.05% examples, 42692 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:30,631 : INFO : PROGRESS: at 72.35% examples, 42638 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:31,651 : INFO : PROGRESS: at 72.68% examples, 42660 words/s, in_qsize 1, out_qsize 0\n",
      "2016-10-11 13:38:32,789 : INFO : PROGRESS: at 73.03% examples, 42618 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:33,841 : INFO : PROGRESS: at 73.24% examples, 42648 words/s, in_qsize 0, out_qsize 3\n",
      "2016-10-11 13:38:34,860 : INFO : PROGRESS: at 73.62% examples, 42655 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:35,880 : INFO : PROGRESS: at 73.91% examples, 42660 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:36,913 : INFO : PROGRESS: at 74.16% examples, 42587 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:37,936 : INFO : PROGRESS: at 74.48% examples, 42649 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:38,986 : INFO : PROGRESS: at 74.72% examples, 42636 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:39,997 : INFO : PROGRESS: at 74.96% examples, 42647 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:41,110 : INFO : PROGRESS: at 75.30% examples, 42615 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:42,144 : INFO : PROGRESS: at 75.50% examples, 42603 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:38:43,188 : INFO : PROGRESS: at 75.88% examples, 42613 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:38:44,260 : INFO : PROGRESS: at 76.14% examples, 42620 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:45,305 : INFO : PROGRESS: at 76.46% examples, 42615 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:46,327 : INFO : PROGRESS: at 76.72% examples, 42605 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:47,446 : INFO : PROGRESS: at 76.98% examples, 42567 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:48,496 : INFO : PROGRESS: at 77.29% examples, 42605 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:49,571 : INFO : PROGRESS: at 77.66% examples, 42612 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:50,611 : INFO : PROGRESS: at 77.86% examples, 42602 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:38:51,702 : INFO : PROGRESS: at 78.18% examples, 42608 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:52,735 : INFO : PROGRESS: at 78.53% examples, 42581 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:53,744 : INFO : PROGRESS: at 78.79% examples, 42613 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:54,818 : INFO : PROGRESS: at 79.13% examples, 42602 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:55,823 : INFO : PROGRESS: at 79.47% examples, 42631 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:56,870 : INFO : PROGRESS: at 79.81% examples, 42653 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:38:57,994 : INFO : PROGRESS: at 80.08% examples, 42637 words/s, in_qsize 1, out_qsize 2\n",
      "2016-10-11 13:38:58,997 : INFO : PROGRESS: at 80.32% examples, 42626 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:00,023 : INFO : PROGRESS: at 80.64% examples, 42669 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:01,069 : INFO : PROGRESS: at 80.90% examples, 42640 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:02,170 : INFO : PROGRESS: at 81.22% examples, 42650 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:03,199 : INFO : PROGRESS: at 81.49% examples, 42678 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:04,251 : INFO : PROGRESS: at 81.85% examples, 42654 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:05,277 : INFO : PROGRESS: at 82.09% examples, 42661 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:06,283 : INFO : PROGRESS: at 82.30% examples, 42626 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:07,318 : INFO : PROGRESS: at 82.58% examples, 42650 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:08,387 : INFO : PROGRESS: at 83.00% examples, 42661 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:39:09,405 : INFO : PROGRESS: at 83.32% examples, 42667 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:10,530 : INFO : PROGRESS: at 83.54% examples, 42633 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:11,596 : INFO : PROGRESS: at 83.90% examples, 42636 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:39:12,600 : INFO : PROGRESS: at 84.23% examples, 42648 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:13,607 : INFO : PROGRESS: at 84.46% examples, 42634 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:14,706 : INFO : PROGRESS: at 84.66% examples, 42606 words/s, in_qsize 0, out_qsize 2\n",
      "2016-10-11 13:39:15,787 : INFO : PROGRESS: at 84.92% examples, 42553 words/s, in_qsize 0, out_qsize 4\n",
      "2016-10-11 13:39:16,891 : INFO : PROGRESS: at 85.14% examples, 42558 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:39:17,894 : INFO : PROGRESS: at 85.42% examples, 42541 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:18,911 : INFO : PROGRESS: at 85.78% examples, 42572 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:19,962 : INFO : PROGRESS: at 86.06% examples, 42552 words/s, in_qsize 1, out_qsize 1\n",
      "2016-10-11 13:39:21,109 : INFO : PROGRESS: at 86.47% examples, 42534 words/s, in_qsize 0, out_qsize 2\n",
      "2016-10-11 13:39:22,115 : INFO : PROGRESS: at 86.74% examples, 42554 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:23,189 : INFO : PROGRESS: at 87.06% examples, 42554 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:24,237 : INFO : PROGRESS: at 87.40% examples, 42564 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:39:25,219 : INFO : PROGRESS: at 87.66% examples, 42541 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:26,289 : INFO : PROGRESS: at 87.84% examples, 42557 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:39:27,418 : INFO : PROGRESS: at 88.22% examples, 42548 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:28,471 : INFO : PROGRESS: at 88.51% examples, 42544 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:29,641 : INFO : PROGRESS: at 88.77% examples, 42518 words/s, in_qsize 0, out_qsize 2\n",
      "2016-10-11 13:39:30,661 : INFO : PROGRESS: at 89.15% examples, 42553 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:39:31,696 : INFO : PROGRESS: at 89.54% examples, 42581 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:32,714 : INFO : PROGRESS: at 89.82% examples, 42561 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:33,763 : INFO : PROGRESS: at 90.11% examples, 42571 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:34,895 : INFO : PROGRESS: at 90.43% examples, 42584 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:39:35,915 : INFO : PROGRESS: at 90.70% examples, 42594 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:36,935 : INFO : PROGRESS: at 90.97% examples, 42590 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:37,955 : INFO : PROGRESS: at 91.25% examples, 42598 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:39,698 : INFO : PROGRESS: at 91.47% examples, 42476 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:40,728 : INFO : PROGRESS: at 91.79% examples, 42498 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:41,814 : INFO : PROGRESS: at 92.08% examples, 42478 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:42,903 : INFO : PROGRESS: at 92.43% examples, 42484 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:39:44,011 : INFO : PROGRESS: at 92.73% examples, 42480 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:45,220 : INFO : PROGRESS: at 92.95% examples, 42426 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:46,234 : INFO : PROGRESS: at 93.18% examples, 42428 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:47,276 : INFO : PROGRESS: at 93.41% examples, 42435 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:39:48,267 : INFO : PROGRESS: at 93.55% examples, 42379 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:49,333 : INFO : PROGRESS: at 93.83% examples, 42366 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:39:50,324 : INFO : PROGRESS: at 94.15% examples, 42407 words/s, in_qsize 0, out_qsize 1\n",
      "2016-10-11 13:39:51,458 : INFO : PROGRESS: at 94.49% examples, 42389 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:52,674 : INFO : PROGRESS: at 94.80% examples, 42397 words/s, in_qsize 0, out_qsize 2\n",
      "2016-10-11 13:39:53,750 : INFO : PROGRESS: at 95.16% examples, 42402 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:54,922 : INFO : PROGRESS: at 95.40% examples, 42372 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:55,942 : INFO : PROGRESS: at 95.75% examples, 42406 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:57,004 : INFO : PROGRESS: at 96.10% examples, 42397 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:58,034 : INFO : PROGRESS: at 96.41% examples, 42382 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:39:59,067 : INFO : PROGRESS: at 96.65% examples, 42395 words/s, in_qsize 1, out_qsize 1\n",
      "2016-10-11 13:40:00,125 : INFO : PROGRESS: at 96.90% examples, 42399 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:40:01,146 : INFO : PROGRESS: at 97.27% examples, 42403 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:40:02,158 : INFO : PROGRESS: at 97.56% examples, 42394 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:40:03,200 : INFO : PROGRESS: at 97.82% examples, 42427 words/s, in_qsize 1, out_qsize 0\n",
      "2016-10-11 13:40:04,233 : INFO : PROGRESS: at 98.11% examples, 42419 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:40:05,323 : INFO : PROGRESS: at 98.49% examples, 42415 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:40:06,561 : INFO : PROGRESS: at 98.75% examples, 42407 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:40:07,622 : INFO : PROGRESS: at 99.10% examples, 42437 words/s, in_qsize 1, out_qsize 0\n",
      "2016-10-11 13:40:08,694 : INFO : PROGRESS: at 99.48% examples, 42424 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:40:09,736 : INFO : PROGRESS: at 99.84% examples, 42443 words/s, in_qsize 0, out_qsize 0\n",
      "2016-10-11 13:40:10,129 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2016-10-11 13:40:10,156 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2016-10-11 13:40:10,163 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2016-10-11 13:40:10,173 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2016-10-11 13:40:10,174 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2016-10-11 13:40:10,174 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2016-10-11 13:40:10,175 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2016-10-11 13:40:10,175 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2016-10-11 13:40:10,176 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2016-10-11 13:40:10,176 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2016-10-11 13:40:10,177 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2016-10-11 13:40:10,177 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2016-10-11 13:40:10,178 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2016-10-11 13:40:10,179 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2016-10-11 13:40:10,179 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2016-10-11 13:40:10,180 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2016-10-11 13:40:10,180 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-10-11 13:40:10,192 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-10-11 13:40:10,243 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-10-11 13:40:10,311 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-10-11 13:40:10,311 : INFO : training on 59217428 raw words (14918933 effective words) took 351.4s, 42457 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23min 9s, sys: 26.3 s, total: 23min 36s\n",
      "Wall time: 9min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc2vec_model_std = Doc2Vec(size=DOC2VEC_SIZE , window=DOC2VEC_WINDOW, min_count=MIN_WORD_COUNT, \n",
    "                max_vocab_size= DOC2VEC_MAX_VOCAB_SIZE,\n",
    "                sample=DOC2VEC_SAMPLE, seed=DOC2VEC_SEED, workers=NUM_CORES,\n",
    "                # doc2vec algorithm dm=1 => PV-DM, dm=2 => PV-DBOW, PV-DM dictates CBOW for words\n",
    "                dm=DOC2VEC_TYPE,\n",
    "                # hs=0 => negative sampling, hs=1 => hierarchical softmax\n",
    "                hs=DOC2VEC_HIERARCHICAL_SAMPLE, negative=DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                dm_concat=DOC2VEC_CONCAT,\n",
    "                # would train words with skip-gram on top of cbow, we don't need that for now\n",
    "                dbow_words=DOC2VEC_TRAIN_WORDS,\n",
    "                iter=DOC2VEC_EPOCHS)\n",
    "doc2vec_model_std.build_vocab(sentences=TrainingDocumentGenerator(training_file, training_docs_list), \n",
    "                              progress_per=REPORT_VOCAB_PROGRESS)\n",
    "doc2vec_model_std.train(sentences=TrainingDocumentGenerator(training_file, training_docs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
