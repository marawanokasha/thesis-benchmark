{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "import pyspark\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import logging\n",
    "from logging import info\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IS_SAMPLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_SEED = 1234\n",
    "DOC2VEC_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUMBER_INDICATOR = \"number_inidicator\"\n",
    "CURRENCY_INDICATOR = \"currency_inidicator\"\n",
    "CHEMICAL_INDICATOR = \"chemical_inidicator\"\n",
    "MIN_WORD_COUNT = 5\n",
    "MIN_SIZE = 0\n",
    "NUM_CORES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_ITERATIONS = 1000\n",
    "SVM_CONVERGENCE = 0.0001\n",
    "SVM_REG = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GLOBAL_VARS = namedtuple('MODEL_NAME', 'DOC2VEC_MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATIO = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_MODEL = \"vocab_model\"\n",
    "MODEL_PREFIX = \"model\"\n",
    "VALIDATION_MATRIX = \"validation_matrix.pkl\"\n",
    "METRICS = \"metrics.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training_file = \"/home/local/shalaby/docs_output_sample_100.json\"\n",
    "\n",
    "save_parent_location = \"hdfs://deka.cip.ifi.lmu.de/pg-vectors/\"\n",
    "if IS_SAMPLE: \n",
    "    save_parent_location = save_parent_location + \"sample_\" + str(SAMPLE_RATIO) + \"/\"\n",
    "\n",
    "\n",
    "root_location = \"/big/s/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "doc2vec_model_save_location = os.path.join(root_location, \"parameter_search_doc2vec_models\", \"sample_\" + str(SAMPLE_RATIO))\n",
    "if not os.path.exists(doc2vec_model_save_location):\n",
    "    os.makedirs(doc2vec_model_save_location)\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL)):\n",
    "    os.makedirs(os.path.join(doc2vec_model_save_location, VOCAB_MODEL))\n",
    "\n",
    "#training_file = root_location + \"docs_output.json\"\n",
    "training_file = root_location + 'docs_output_training_validation_documents_' + str(SAMPLE_RATIO)\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "training_docs_list_file = exports_location + \"training_documents_\" + str(SAMPLE_RATIO) + \"_sample.pkl\"\n",
    "validation_docs_list_file = exports_location + \"validation_documents_\" + str(SAMPLE_RATIO) + \"_sample.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.6 s, sys: 1.42 s, total: 28 s\n",
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8979"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1969"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemtokenizer(text):\n",
    "    \"\"\" MAIN FUNCTION to get clean stems out of a text. A list of clean stems are returned \"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\s+', gaps=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stems = []  # result\n",
    "    for token in tokens:\n",
    "        stem = token.lower()\n",
    "        stem = stem.strip(string.punctuation)\n",
    "        if stem:\n",
    "            if is_number(stem):\n",
    "                stem = NUMBER_INDICATOR\n",
    "            elif is_currency(stem):\n",
    "                stem = CURRENCY_INDICATOR\n",
    "            elif is_chemical(stem):\n",
    "                stem = CHEMICAL_INDICATOR\n",
    "            else:\n",
    "                stem = stem.strip(string.punctuation)\n",
    "            if stem and len(stem) >= MIN_SIZE:\n",
    "                # extract uni-grams\n",
    "                stems.append(stem)\n",
    "    del tokens\n",
    "    return stems\n",
    "\n",
    "def is_number(str):\n",
    "    \"\"\" Returns true if given string is a number (float or int)\"\"\"\n",
    "    try:\n",
    "        float(str.replace(\",\", \"\"))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_currency(str):\n",
    "    return str[0] == \"$\"\n",
    "\n",
    "def is_chemical(str):\n",
    "    return str.count(\"-\") > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_training_vector(classification, term_list, classifications, number_of_terms):\n",
    "    clss = 1 if classification in classifications else 0\n",
    "    return LabeledPoint(clss, SparseVector(number_of_terms, term_list))\n",
    "\n",
    "def train_level_new(docs_index, classification, doc_classification_map, number_of_terms):\n",
    "    training_vectors = docs_index.map(\n",
    "        lambda (doc_id, postings): get_training_vector(classification, postings,\n",
    "                                                        doc_classification_map[doc_id], number_of_terms))\n",
    "    svm = SVMWithSGD.train(training_vectors, iterations=SVM_ITERATIONS, convergenceTol=SVM_CONVERGENCE, regParam=SVM_REG)\n",
    "    return training_vectors, svm\n",
    "\n",
    "get_binary = lambda x: 1 if x > 0 else 0\n",
    "get_binary = np.vectorize(get_binary)\n",
    "\n",
    "\n",
    "def get_row_top_N(y_score_row, y_true_row):\n",
    "    desc_score_indices = np.argsort(y_score_row)[::-1]\n",
    "    # print y_score_row\n",
    "    # print y_true_row\n",
    "    true_indices = np.where(y_true_row ==1)[0]\n",
    "    # print desc_score_indices\n",
    "    found = 0\n",
    "    top_N = 0\n",
    "    for i, score in enumerate(desc_score_indices):\n",
    "        if score in true_indices:\n",
    "            found += 1\n",
    "            if found == len(true_indices):\n",
    "                top_N = i + 1\n",
    "    # print top_N\n",
    "    return top_N\n",
    "\n",
    "\n",
    "def get_metrics(y_true, y_score, y_binary_score):\n",
    "    metrics = {}\n",
    "    metrics['total_positive'] = np.sum(np.sum(y_binary_score))\n",
    "    #TODO remove those two when running on the whole set to avoid excessive storage costs\n",
    "    metrics['y_true'] = y_true\n",
    "    metrics['y_score'] = y_score\n",
    "    metrics['y_binary_score'] = y_binary_score\n",
    "    metrics['coverage_error'] = coverage_error(y_true, y_binary_score)\n",
    "    metrics['average_num_of_labels'] = np.sum(np.sum(y_true, axis=1))/y_true.shape[0]\n",
    "    metrics['average_precision_micro'] = sklearn.metrics.average_precision_score(y_true, y_binary_score, average='micro')\n",
    "    metrics['average_precision_macro'] = sklearn.metrics.average_precision_score(y_true, y_binary_score, average='macro')\n",
    "    metrics['precision_micro'] = sklearn.metrics.precision_score(y_true, y_binary_score, average='micro')\n",
    "    metrics['precision_macro'] = sklearn.metrics.precision_score(y_true, y_binary_score, average='macro')\n",
    "    metrics['recall_micro'] = sklearn.metrics.recall_score(y_true, y_binary_score, average='micro')\n",
    "    metrics['recall_macro'] = sklearn.metrics.recall_score(y_true, y_binary_score, average='macro')\n",
    "    metrics['f1_micro'] = sklearn.metrics.f1_score(y_true, y_binary_score, average='micro')\n",
    "    metrics['f1_macro'] = sklearn.metrics.f1_score(y_true, y_binary_score, average='macro')\n",
    "\n",
    "    precision_scores = np.zeros(y_true.shape[1])\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        precision_scores[i] = sklearn.metrics.precision_score(y_true[:,i], y_binary_score[:,i])\n",
    "    metrics['precision_scores_array'] = precision_scores.tolist()\n",
    "\n",
    "    recall_scores = np.zeros(y_true.shape[1])\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        recall_scores[i] = sklearn.metrics.recall_score(y_true[:,i], y_binary_score[:,i])\n",
    "    metrics['recall_scores_array'] = recall_scores.tolist()\n",
    "\n",
    "    f1_scores = np.zeros(y_true.shape[1])\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        f1_scores[i] = sklearn.metrics.f1_score(y_true[:,i], y_binary_score[:,i])\n",
    "    metrics['f1_scores_array'] = f1_scores.tolist()\n",
    "\n",
    "    tops = []\n",
    "    for i in xrange(y_score.shape[0]):\n",
    "        tops.append(get_row_top_N(y_score[i,:], y_true[i,:]))\n",
    "    metrics['topN_list'] = np.array(tops).tolist()\n",
    "    metrics['topN_avg'] = np.mean(tops)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def model_exists(path):\n",
    "    try:\n",
    "        model = SVMModel.load(sc, path)\n",
    "        return True;\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def get_training_vector(classification, dense_vector, classifications):\n",
    "    clss = 1 if classification in classifications else 0\n",
    "    return LabeledPoint(clss, dense_vector)\n",
    "\n",
    "def train_level_doc2vec(classification, doc_classification_map):\n",
    "    doc2vec_model = GLOBAL_VARS.DOC2VEC_MODEL\n",
    "    training_vectors = []\n",
    "    for doc_id in training_docs_list:\n",
    "        # converting from memmap to a normal array as spark is unable to convert memmap to a spark Vector\n",
    "        normal_array = []\n",
    "        normal_array[:] = doc2vec_model.docvecs[doc_id][:]\n",
    "        training_vectors.append(get_training_vector(classification, normal_array, \n",
    "                                                    doc_classification_map[doc_id]))\n",
    "    info(\"Finished getting training vectors\")\n",
    "    training_vectors = sc.parallelize(training_vectors)\n",
    "    info(\"Finished parallelization\")\n",
    "    svm = SVMWithSGD.train(training_vectors, iterations=SVM_ITERATIONS, convergenceTol=SVM_CONVERGENCE, regParam=SVM_REG)\n",
    "    return training_vectors, svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensure_hdfs_location_exists(location):\n",
    "    parent = os.path.dirname(location)\n",
    "    os.system(\"hdfs dfs -mkdir -p \" + location)\n",
    "    \n",
    "def get_svm_model_path(method, classification, reg=SVM_REG, iterations=SVM_ITERATIONS):\n",
    "    location = os.path.join(save_parent_location, \"models\", method, \n",
    "                            \"iter_\" + str(iterations) + \"_reg_\" + str(reg),\n",
    "                            classification + \"_model.svm\")\n",
    "    ensure_hdfs_location_exists(location)\n",
    "    return location\n",
    "\n",
    "def get_data_output_name(method, data_type=\"training\"):\n",
    "    location = os.path.join(save_parent_location, \"models\", method, \n",
    "                            data_type + \"_data\", \n",
    "                            \"data.json\")\n",
    "    ensure_hdfs_location_exists(location)\n",
    "    return location\n",
    "\n",
    "def get_data_classification_output_name(method, classification, data_type=\"training\"):\n",
    "    location = os.path.join(save_parent_location, \"models\", method, \n",
    "                            data_type + \"_data\", \n",
    "                            classification + \"_data.json\")\n",
    "    ensure_hdfs_location_exists(location)\n",
    "    return location\n",
    "\n",
    "def get_prediction_output_name(method, data_type=\"training\", subset=\"sections\", reg=SVM_REG, iterations=SVM_ITERATIONS):\n",
    "    location = os.path.join(save_parent_location, \"models\", method,\n",
    "                            \"iter_\" + str(iterations) + \"_reg_\" + str(reg),\n",
    "                            data_type + \"_\" + subset + \"_predictions.svm\")\n",
    "    ensure_hdfs_location_exists(location)\n",
    "    return location\n",
    "    \n",
    "def get_labels_output_name(data_type=\"training\", subset=\"sections\", reg=SVM_REG, iterations=SVM_ITERATIONS):\n",
    "    location = os.path.join(save_parent_location, \"models\", \n",
    "                            data_type + \"_\" + subset + \"_labels.svm\")\n",
    "    ensure_hdfs_location_exists(location)\n",
    "    return location\n",
    "\n",
    "def get_metrics_output_name(method, data_type=\"training\", subset=\"sections\", reg=SVM_REG, iterations=SVM_ITERATIONS):\n",
    "    location = os.path.join(save_parent_location, \"models\", method, \n",
    "                            \"iter_\" + str(iterations) + \"_reg_\" + str(reg),\n",
    "                            data_type + \"_\" + subset + \"_metrics.pkl\")\n",
    "    ensure_hdfs_location_exists(location)\n",
    "    return location    \n",
    "                            \n",
    "def get_save_location(location, sample=False):\n",
    "    if sample:\n",
    "        return location.replace(save_parent_location, sample_save_parent_location)\n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_classifications(classifications):\n",
    "    info(\"====== Doing Training\")\n",
    "    i=0\n",
    "    for classification in classifications:\n",
    "        print classification\n",
    "        try:\n",
    "            model_path = get_svm_model_path(GLOBAL_VARS.MODEL_NAME, classification)\n",
    "            if not model_exists(model_path):\n",
    "                training_vectors, svm = train_level_doc2vec(classification, doc_classification_map)\n",
    "                svm.save(sc, model_path)\n",
    "            else:\n",
    "                print \"Model Exists\"\n",
    "        except:\n",
    "            print \"Problem creating: %s: %s\" % (classification, GLOBAL_VARS.MODEL_NAME)\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_validation(validation_vectors_matrix, doc_classification_map, classifications, classifications_name):\n",
    "\n",
    "    info(\"====== Doing Validation\")\n",
    "    method = GLOBAL_VARS.MODEL_NAME\n",
    "    subset = classifications_name\n",
    "\n",
    "    doc_count = validation_vectors_matrix.shape[0]\n",
    "    y_score = np.zeros((doc_count, len(classifications)))\n",
    "    y_true = np.zeros((doc_count, len(classifications)))\n",
    "    i=0\n",
    "\n",
    "    for classification in classifications:\n",
    "        print classification\n",
    "\n",
    "        validation_vectors = get_validation_doc2vec_spark_vectors(validation_vectors_matrix, \n",
    "                                                                  classification, doc_classification_map)\n",
    "        #global binarySvm\n",
    "        binarySvm = SVMModel.load(sc, get_svm_model_path(GLOBAL_VARS.MODEL_NAME, classification))\n",
    "        info(\"Loaded the model, Doing the prediction now....\")\n",
    "        binarySvm.clearThreshold()\n",
    "        binarySvmB = sc.broadcast(binarySvm)\n",
    "        # using the broadcasted binarySvm variable, fixes global name 'binarySvm' is not defined as this variable was not\n",
    "        # available in the workers, so we pass it explicitly to the mapper using partial\n",
    "        labels_predictions = validation_vectors.map( \\\n",
    "            partial(lambda svm, p: (p.label, svm.value.predict(p.features)), binarySvmB) \\\n",
    "        ).collect()\n",
    "        #labels = test_labeled_points.map(lambda p: p.labels)\n",
    "        y_true[:,i] = [label_pred[0] for label_pred in labels_predictions]\n",
    "        y_score[:,i] = [label_pred[1] for label_pred in labels_predictions]\n",
    "        i+=1\n",
    "    y_binary_score = get_binary(y_score)\n",
    "    # results[method][\"y_true\"] = y_true\n",
    "    # results[method][\"y_score\"] = y_score\n",
    "    # results[method][\"y_binary_score\"] = y_binary_score\n",
    "    metrics = get_metrics(y_true, y_score, y_binary_score)\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binarySvm = SVMModel.load(sc, get_svm_model_path(GLOBAL_VARS.MODEL_NAME, \"A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-09-06 20:20:19,825 : INFO : Finished getting training vectors\n"
     ]
    }
   ],
   "source": [
    "validation_vectors = get_validation_doc2vec_spark_vectors(validation_vectors_matrix, \n",
    "                                                                  \"A\", doc_classification_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "non_conf = validation_vectors.filter(lambda x : len(x.features) < 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_vectors.map(lambda p: (p.label, binarySvm.predict(p.features))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doc2vec_size_1000_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_iter_1_curriter_0'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GLOBAL_VARS.MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1937"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_validation_docs_with_inference(doc2vec_model, doc_classification_map):\n",
    "    \"\"\"\n",
    "    Use the trained doc2vec model to get the paragraph vector representations of the validation documents\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX)):\n",
    "        validation_vectors_matrix = pickle.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX)))\n",
    "    else:\n",
    "        validation_documents_reps = {}\n",
    "        validation_vectors = []\n",
    "        validation_labels = []\n",
    "        info(\"===== Getting validation vectors with inference\")\n",
    "\n",
    "        # do inference and store results in dict\n",
    "        i = 0\n",
    "        for (doc_id, doc_contents_array) in ValidationDocumentGenerator(training_file, validation_docs_list):\n",
    "            i += 1\n",
    "            if i % 1000 == 0: info(str(i))\n",
    "            validation_documents_reps[doc_id] = doc2vec_model.infer_vector(doc_contents_array)\n",
    "\n",
    "        # create matrix for the validation vectors\n",
    "        for validation_doc_id in validation_docs_list:\n",
    "            validation_vectors.append(validation_documents_reps[validation_doc_id])\n",
    "            validation_labels.append([classf for classf in doc_classification_map[validation_doc_id] if classf in sections])\n",
    "        validation_vectors_matrix = np.array(validation_vectors)\n",
    "        pickle.dump(validation_vectors_matrix, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX), 'w'))\n",
    "    \n",
    "    return validation_vectors_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_documents_reps = {}\n",
    "validation_vectors = []\n",
    "validation_labels = []\n",
    "info(\"===== Getting validation vectors with inference\")\n",
    "\n",
    "# do inference and store results in dict\n",
    "i = 0\n",
    "for (doc_id, doc_contents_array) in ValidationDocumentGenerator(training_file, validation_docs_list):\n",
    "    i += 1\n",
    "    if i % 1000 == 0: info(str(i))\n",
    "    validation_documents_reps[doc_id] = doc2vec_model.infer_vector(doc_contents_array)\n",
    "\n",
    "# create matrix for the validation vectors\n",
    "for validation_doc_id in validation_docs_list:\n",
    "    validation_vectors.append(validation_documents_reps[validation_doc_id])\n",
    "    validation_labels.append([classf for classf in doc_classification_map[validation_doc_id] if classf in sections])\n",
    "validation_vectors_matrix = np.array(validation_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "found_ids = []\n",
    "for (doc_id, doc_contents_array) in ValidationDocumentGenerator(training_file, validation_docs_list):\n",
    "    found_ids.append(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1969"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1969"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(validation_docs_list) & set(found_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_doc2vec_spark_vectors(validation_vectors_matrix, classification, doc_classification_map):\n",
    "    validation_vectors = []\n",
    "    for (index, doc_id) in enumerate(validation_docs_list):\n",
    "        # converting from memmap to a normal array as spark is unable to convert memmap to a spark Vector\n",
    "        validation_vector = validation_vectors_matrix[index]\n",
    "        validation_vectors.append(get_training_vector(classification, validation_vector, \n",
    "                                                    doc_classification_map[doc_id]))\n",
    "    validation_vectors = sc.parallelize(validation_vectors)\n",
    "    info(\"Finished getting validation vectors\")\n",
    "    return validation_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2vec and SVM Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOC2VEC_SIZE = 1000\n",
    "DOC2VEC_WINDOW = 8\n",
    "DOC2VEC_MAX_VOCAB_SIZE = None\n",
    "DOC2VEC_SAMPLE = 1e-5\n",
    "DOC2VEC_TYPE = 1\n",
    "DOC2VEC_HIERARCHICAL_SAMPLE = 0\n",
    "DOC2VEC_NEGATIVE_SAMPLE_SIZE = 10\n",
    "DOC2VEC_CONCAT = 0\n",
    "DOC2VEC_MEAN = 0\n",
    "DOC2VEC_TRAIN_WORDS = 0\n",
    "DOC2VEC_EPOCHS = 1 # we do our training manually one epoch at a time\n",
    "DOC2VEC_MAX_EPOCHS = 50\n",
    "REPORT_DELAY = 60 # report the progress every x seconds\n",
    "REPORT_VOCAB_PROGRESS = 1000 # report the progress every x terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_ITERATIONS = 1000\n",
    "SVM_CONVERGENCE = 0.001\n",
    "SVM_REG = 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainingDocumentGenerator(object):\n",
    "    def __init__(self, filename, training_docs_list):\n",
    "        self.filename = filename\n",
    "        self.training_docs_list = training_docs_list\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename):\n",
    "            if not line.strip(): continue\n",
    "            (doc_id, text) = eval(line)\n",
    "            if doc_id in self.training_docs_list:\n",
    "                yield LabeledSentence(words=stemtokenizer(text), tags=[doc_id])\n",
    "                \n",
    "class ValidationDocumentGenerator(object):\n",
    "    def __init__(self, filename, validation_docs_list):\n",
    "        self.filename = filename\n",
    "        self.validation_docs_list = validation_docs_list\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename):\n",
    "            if not line.strip(): continue\n",
    "            (doc_id, text) = eval(line)\n",
    "            if doc_id in self.validation_docs_list:\n",
    "                yield doc_id, stemtokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doc2vec_size_1000_w_8_type_dm_concat_0_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_{}'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}'.format(DOC2VEC_SIZE, \n",
    "                                                                DOC2VEC_WINDOW, \n",
    "                                                                'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                                DOC2VEC_TRAIN_WORDS,\n",
    "                                                                DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                                str(DOC2VEC_MAX_VOCAB_SIZE))\n",
    "placeholder_model_name = placeholder_model_name + \"_curriter_{}\"\n",
    "placeholder_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc2vec_model = Doc2Vec(size=DOC2VEC_SIZE , window=DOC2VEC_WINDOW, min_count=MIN_WORD_COUNT, \n",
    "                max_vocab_size= DOC2VEC_MAX_VOCAB_SIZE,\n",
    "                sample=DOC2VEC_SAMPLE, seed=DOC2VEC_SEED, workers=NUM_CORES,\n",
    "                # doc2vec algorithm dm=1 => PV-DM, dm=2 => PV-DBOW, PV-DM dictates CBOW for words\n",
    "                dm=DOC2VEC_TYPE,\n",
    "                # hs=0 => negative sampling, hs=1 => hierarchical softmax\n",
    "                hs=DOC2VEC_HIERARCHICAL_SAMPLE, negative=DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                dm_concat=DOC2VEC_CONCAT,\n",
    "                # would train words with skip-gram on top of cbow, we don't need that for now\n",
    "                dbow_words=DOC2VEC_TRAIN_WORDS,\n",
    "                iter=DOC2VEC_EPOCHS)\n",
    "\n",
    "GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-09-09 17:50:56,170 : INFO : loading Doc2Vec object from /big/s/shalaby/parameter_search_doc2vec_models/sample_0.0001/vocab_model/model\n",
      "2016-09-09 17:50:56,856 : INFO : loading docvecs recursively from /big/s/shalaby/parameter_search_doc2vec_models/sample_0.0001/vocab_model/model.docvecs.* with mmap=None\n",
      "2016-09-09 17:50:56,859 : INFO : loading syn1neg from /big/s/shalaby/parameter_search_doc2vec_models/sample_0.0001/vocab_model/model.syn1neg.npy with mmap=None\n",
      "2016-09-09 17:50:59,949 : INFO : loading syn0 from /big/s/shalaby/parameter_search_doc2vec_models/sample_0.0001/vocab_model/model.syn0.npy with mmap=None\n",
      "2016-09-09 17:51:00,122 : INFO : setting ignored attribute syn0norm to None\n",
      "2016-09-09 17:51:00,123 : INFO : setting ignored attribute cum_table to None\n",
      "2016-09-09 17:51:00,595 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.08 s, sys: 3.58 s, total: 10.7 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX)):\n",
    "    doc2vec_model.build_vocab(sentences=TrainingDocumentGenerator(training_file, training_docs_list), progress_per=REPORT_VOCAB_PROGRESS)\n",
    "    doc2vec_model.save(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "else: \n",
    "    doc2vec_model_vocab_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "    doc2vec_model.reset_from(doc2vec_model_vocab_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2vec_model.min_alpha = 0.025\n",
    "epoch_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-09-10 14:23:27,238 : INFO : ****************** Epoch 21 --- Working on doc2vec_size_1000_w_8_type_dm_concat_0_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_21 *******************\n",
      "2016-09-10 14:23:27,241 : INFO : loading Doc2Vec object from /big/s/shalaby/parameter_search_doc2vec_models/sample_0.0001/doc2vec_size_1000_w_8_type_dm_concat_0_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_21/model\n",
      "2016-09-10 14:23:27,884 : INFO : loading docvecs recursively from /big/s/shalaby/parameter_search_doc2vec_models/sample_0.0001/doc2vec_size_1000_w_8_type_dm_concat_0_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_21/model.docvecs.* with mmap=None\n",
      "2016-09-10 14:23:27,885 : INFO : loading syn1neg from /big/s/shalaby/parameter_search_doc2vec_models/sample_0.0001/doc2vec_size_1000_w_8_type_dm_concat_0_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_21/model.syn1neg.npy with mmap=None\n",
      "2016-09-10 14:23:28,082 : INFO : loading syn0 from /big/s/shalaby/parameter_search_doc2vec_models/sample_0.0001/doc2vec_size_1000_w_8_type_dm_concat_0_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_21/model.syn0.npy with mmap=None\n",
      "2016-09-10 14:23:29,126 : INFO : setting ignored attribute syn0norm to None\n",
      "2016-09-10 14:23:29,127 : INFO : setting ignored attribute cum_table to None\n",
      "2016-09-10 14:23:29,514 : INFO : ====== Doing Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-09-10 14:23:35,165 : INFO : Finished getting training vectors\n",
      "2016-09-10 14:23:36,596 : INFO : Finished parallelization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-09-10 14:24:25,781 : INFO : Finished getting training vectors\n",
      "2016-09-10 14:24:27,193 : INFO : Finished parallelization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-09-10 14:25:11,085 : INFO : Finished getting training vectors\n",
      "2016-09-10 14:25:12,577 : INFO : Finished parallelization\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start_from = 21\n",
    "for epoch in range(start_from, DOC2VEC_MAX_EPOCHS+1):\n",
    "    GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "    info(\"****************** Epoch {} --- Working on {} *******************\".format(epoch, GLOBAL_VARS.MODEL_NAME))\n",
    "    \n",
    "    # if we have the model, just load it, otherwise train the previous model\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)):\n",
    "        docvec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "    else:\n",
    "        # train the doc2vec model\n",
    "        doc2vec_model.train(sentences=TrainingDocumentGenerator(training_file, training_docs_list), report_delay=REPORT_DELAY)\n",
    "        doc2vec_model.alpha -= 0.001  # decrease the learning rate\n",
    "        doc2vec_model.min_alpha = doc2vec_model.alpha  # fix the learning rate, no decay\n",
    "        if not os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME)):\n",
    "            os.makedirs(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME))\n",
    "        doc2vec_model.save(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "\n",
    "    # Training and validation of SVMs using those docvecs\n",
    "    train_classifications(sections)\n",
    "    validation_vectors_matrix = get_validation_docs_with_inference(doc2vec_model, doc_classification_map)\n",
    "    metrics = do_validation(validation_vectors_matrix, doc_classification_map, sections, \"sections\")\n",
    "    pickle.dump(metrics, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, METRICS), 'w'))\n",
    "    print \"Coverage Error: {}, Average No of Labels: {}, Top N: {}, F1 Micro: {}, Total Positive: {}\".format(\n",
    "        metrics['coverage_error'], metrics['average_num_of_labels'], metrics['topN_avg'], metrics['f1_micro'], \n",
    "        metrics['total_positive'])\n",
    "                                                                                         \n",
    "    epoch_metrics.append(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5457, 1000)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_vectors_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lskdjf\n"
     ]
    }
   ],
   "source": [
    "print \"lskdjf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(epoch_metrics[4]['y_binary_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_metrics[4]['precision_micro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "met = epoch_metrics[4]\n",
    "coverage_error(met['y_true'], met['y_binary_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 52,\n",
       " 138,\n",
       " 215,\n",
       " 261,\n",
       " 301,\n",
       " 324,\n",
       " 363,\n",
       " 382,\n",
       " 408,\n",
       " 428,\n",
       " 424,\n",
       " 455,\n",
       " 466,\n",
       " 476,\n",
       " 488]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[metric['total_positive'] for metric in epoch_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.001015744032503809,\n",
       " 0.061452513966480445,\n",
       " 0.19502285424073135,\n",
       " 0.33824276282376842,\n",
       " 0.43067546978161503,\n",
       " 0.49263585576434737,\n",
       " 0.5444388014220416,\n",
       " 0.60690705942102585,\n",
       " 0.63839512442864399,\n",
       " 0.68765871000507872,\n",
       " 0.72574911122397157,\n",
       " 0.71356018283392586,\n",
       " 0.75723717623158959,\n",
       " 0.79075672930421537,\n",
       " 0.81259522600304723,\n",
       " 0.83951244286439819]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[metric['coverage_error'] for metric in epoch_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.88461538461538458,\n",
       " 0.81159420289855078,\n",
       " 0.76279069767441865,\n",
       " 0.73563218390804597,\n",
       " 0.74086378737541525,\n",
       " 0.72839506172839508,\n",
       " 0.73002754820936644,\n",
       " 0.72774869109947649,\n",
       " 0.72303921568627449,\n",
       " 0.71962616822429903,\n",
       " 0.72405660377358494,\n",
       " 0.72747252747252744,\n",
       " 0.71673819742489275,\n",
       " 0.71638655462184875,\n",
       " 0.71106557377049184]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[metric['precision_micro'] for metric in epoch_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0003756574004507889,\n",
       " 0.017280240420736288,\n",
       " 0.042073628850488355,\n",
       " 0.061607813673929375,\n",
       " 0.072126220886551462,\n",
       " 0.083771600300525925,\n",
       " 0.088655146506386173,\n",
       " 0.099549211119459052,\n",
       " 0.10443275732531931,\n",
       " 0.11081893313298272,\n",
       " 0.11570247933884298,\n",
       " 0.11532682193839218,\n",
       " 0.12434259954921112,\n",
       " 0.12546957175056347,\n",
       " 0.128099173553719,\n",
       " 0.13035311795642374]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[metric['recall_micro'] for metric in epoch_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.7343829355002538,\n",
       " 2.7206703910614527,\n",
       " 2.517013712544439,\n",
       " 2.3367191467750126,\n",
       " 2.2691721686135096,\n",
       " 2.249873031995937,\n",
       " 2.2534281361097004,\n",
       " 2.2529202640934485,\n",
       " 2.2585068562722195,\n",
       " 2.2630777044184867,\n",
       " 2.2559674961909599,\n",
       " 2.2717115286947691,\n",
       " 2.26663280853225,\n",
       " 2.2833925850685626,\n",
       " 2.2747587607922806,\n",
       " 2.2889791772473336,\n",
       " 2.2833925850685626,\n",
       " 2.2950736414423565,\n",
       " 2.2757745048247839,\n",
       " 2.2960893854748603]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[metric['topN_avg'] for metric in epoch_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'average_num_of_labels': 1.3519553072625698,\n",
       " 'average_precision_macro': 0.62302616614963602,\n",
       " 'average_precision_micro': 0.61208739711365046,\n",
       " 'coverage_error': 3.6353478923311324,\n",
       " 'f1_macro': 0.48459584410527123,\n",
       " 'f1_micro': 0.56405990016638929,\n",
       " 'f1_scores_array': [0.6084724005134788,\n",
       "  0.4574162679425837,\n",
       "  0.7073170731707318,\n",
       "  0.3741007194244605,\n",
       "  0.0,\n",
       "  0.5020576131687242,\n",
       "  0.629500580720093,\n",
       "  0.5979020979020979],\n",
       " 'precision_macro': 0.5861309895614768,\n",
       " 'precision_micro': 0.63187325256290772,\n",
       " 'precision_scores_array': [0.5895522388059702,\n",
       "  0.6035353535353535,\n",
       "  0.6157112526539278,\n",
       "  0.8666666666666667,\n",
       "  0.0,\n",
       "  0.7134502923976608,\n",
       "  0.6691358024691358,\n",
       "  0.6309963099630996],\n",
       " 'recall_macro': 0.45201123608467181,\n",
       " 'recall_micro': 0.50939143501126971,\n",
       " 'recall_scores_array': [0.6286472148541115,\n",
       "  0.3682588597842835,\n",
       "  0.830945558739255,\n",
       "  0.23853211009174313,\n",
       "  0.0,\n",
       "  0.3873015873015873,\n",
       "  0.5942982456140351,\n",
       "  0.5681063122923588],\n",
       " 'topN_avg': 2.3869984763839511,\n",
       " 'topN_list': [6,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  7,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  8,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  7,\n",
       "  1,\n",
       "  8,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  6,\n",
       "  4,\n",
       "  2,\n",
       "  8,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  8,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  8,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  8,\n",
       "  1,\n",
       "  6,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  8,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  7,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  6,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  7,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  6,\n",
       "  2,\n",
       "  8,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  8,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  6,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  6,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  5,\n",
       "  3,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  7,\n",
       "  3,\n",
       "  6,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  7,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  8,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  6,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  4,\n",
       "  8,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  6,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  7,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  7,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  4,\n",
       "  7,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  8,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  7,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  6,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  8,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  6,\n",
       "  2,\n",
       "  2,\n",
       "  7,\n",
       "  1,\n",
       "  5,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  6,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  3,\n",
       "  5,\n",
       "  7,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  7,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  8,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  2,\n",
       "  7,\n",
       "  5,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  6,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  7,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  6,\n",
       "  6,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  6,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  5,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  2,\n",
       "  8,\n",
       "  8,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  7,\n",
       "  3,\n",
       "  1,\n",
       "  6,\n",
       "  1,\n",
       "  7,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  2,\n",
       "  6,\n",
       "  1,\n",
       "  3,\n",
       "  8,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  8,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  6,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  6,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  ...],\n",
       " 'total_positive': 2146,\n",
       " 'y_binary_score': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 1, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 1, 0, 0]]),\n",
       " 'y_score': array([[-0.72073958, -0.22202389, -1.91543384, ..., -0.63386378,\n",
       "         -1.00756206, -0.51788158],\n",
       "        [-0.79124385, -0.5948643 , -1.7756827 , ..., -0.7074959 ,\n",
       "         -1.89688405, -1.00464824],\n",
       "        [ 0.03251232,  0.06252948,  0.96459193, ..., -2.74178485,\n",
       "         -2.23706071, -1.63016594],\n",
       "        ..., \n",
       "        [ 0.66919114, -0.68596632, -2.8073171 , ..., -0.96900397,\n",
       "         -1.76110755, -2.18885337],\n",
       "        [ 1.34648373, -0.31557401,  0.50906903, ..., -1.17965696,\n",
       "         -1.77594797, -2.16045221],\n",
       "        [-1.50653896, -0.61099368, -1.55593804, ...,  0.80059623,\n",
       "         -0.77699803, -0.73397476]]),\n",
       " 'y_true': array([[ 0.,  0.,  0., ...,  0.,  1.,  1.],\n",
       "        [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  1.,  0.,  0.]])}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_metrics[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'average_num_of_labels': 1.1684075499358622,\n",
       " 'average_precision_macro': 0.5730254718709914,\n",
       " 'average_precision_micro': 0.5730254718709914,\n",
       " 'coverage_error': 0.0,\n",
       " 'f1_macro': 0.0,\n",
       " 'f1_micro': 0.0,\n",
       " 'f1_scores_array': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'precision_macro': 0.0,\n",
       " 'precision_micro': 0.0,\n",
       " 'precision_scores_array': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'recall_macro': 0.0,\n",
       " 'recall_micro': 0.0,\n",
       " 'recall_scores_array': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'topN_avg': 1.1684075499358622,\n",
       " 'topN_list': [0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  ...]}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-16 15:49:28,921 : INFO : using concatenative 6800-dimensional layer1\n",
      "using concatenative 6800-dimensional layer1\n",
      "2016-08-16 15:49:28,925 : INFO : resetting layer weights\n",
      "resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model.reset_from(loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now for the actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-21 05:36:43,143 : INFO : collecting all words and their counts\n",
      "2016-08-21 05:36:43,576 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.build_vocab(sentences=LabeledLineSentence(training_file), progress_per=REPORT_VOCAB_PROGRESS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.6.1)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
