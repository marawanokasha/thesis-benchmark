{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K40m (CNMeM is disabled, cuDNN 5105)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "from functools import partial\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "from thesis.utils.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IS_SAMPLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_SEED = 1234\n",
    "DOC2VEC_SEED = 1234\n",
    "WORD2VEC_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUMBER_INDICATOR = \"number_inidicator\"\n",
    "CURRENCY_INDICATOR = \"currency_inidicator\"\n",
    "CHEMICAL_INDICATOR = \"chemical_inidicator\"\n",
    "MIN_WORD_COUNT = 100\n",
    "MIN_SIZE = 0\n",
    "NUM_CORES = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GLOBAL_VARS = namedtuple('GLOBAL_VARS', ['MODEL_NAME', 'DOC2VEC_MODEL_NAME', 'DOC2VEC_MODEL', \n",
    "                                         'SVM_MODEL_NAME', 'NN_MODEL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_MODEL = \"vocab_model\"\n",
    "MODEL_PREFIX = \"model\"\n",
    "VALIDATION_MATRIX = \"validation_matrix.pkl\"\n",
    "METRICS = \"metrics.pkl\"\n",
    "CLASSIFIER = \"classifier.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training_file = \"/home/local/shalaby/docs_output_sample_100.json\"\n",
    "\n",
    "root_location = \"/mnt/data2/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "doc2vec_model_save_location = os.path.join(root_location, \"parameter_search_doc2vec_models_new\", \"full\")\n",
    "nn_parameter_search_location = os.path.join(root_location, \"nn_parameter_search\")\n",
    "if not os.path.exists(doc2vec_model_save_location):\n",
    "    os.makedirs(doc2vec_model_save_location)\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL)):\n",
    "    os.makedirs(os.path.join(doc2vec_model_save_location, VOCAB_MODEL))\n",
    "\n",
    "training_file = root_location + \"docs_output.json\"\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "classification_index_file = exports_location + \"classification_index.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "valid_classes_file = exports_location + \"valid_classes.pkl\"\n",
    "valid_subclasses_file = exports_location + \"valid_subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "training_docs_list_file = exports_location + \"training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"validation_docs_list.pkl\"\n",
    "test_docs_list_file = exports_location + \"test_docs_list.pkl\"\n",
    "\n",
    "preprocessed_location = root_location + \"preprocessed_data/\"\n",
    "\n",
    "training_preprocessed_files_prefix = preprocessed_location + \"training_docs_merged_data_preprocessed-\"\n",
    "training_preprocessed_docids_files_prefix = preprocessed_location + \"training_docs_merged_docids_preprocessed-\"\n",
    "validation_preprocessed_files_prefix = preprocessed_location + \"validation_docs_merged_data_preprocessed-\"\n",
    "validation_preprocessed_docids_files_prefix = preprocessed_location + \"validation_docs_merged_docids_preprocessed-\"\n",
    "\n",
    "training_preprocessed_additional_balanced_file = preprocessed_location + \"training_docs_additional_data_preprocessed\"\n",
    "validation_preprocessed_additional_balanced_file = preprocessed_location + \"validation_docs_additional_data_preprocessed\"\n",
    "\n",
    "word2vec_questions_file = result = root_location + 'tensorflow/word2vec/questions-words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_docs_additional_list_file = exports_location + \"balanced_additional_training_docs_list.pkl\"\n",
    "validation_docs_additional_list_file = exports_location + \"balanced_additional_validation_docs_list.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.4 s, sys: 1.13 s, total: 25.5 s\n",
      "Wall time: 25.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "valid_classes = pickle.load(open(valid_classes_file))\n",
    "valid_subclasses = pickle.load(open(valid_subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "classifications_index = pickle.load(open(classification_index_file))\n",
    "#test_docs_list = pickle.load(open(test_docs_list_file))\n",
    "\n",
    "additional_training_docs_list = pickle.load(open(training_docs_additional_list_file))\n",
    "additional_validation_docs_list = pickle.load(open(validation_docs_additional_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensure_hdfs_location_exists(location):\n",
    "    parent = os.path.dirname(location)\n",
    "    os.system(\"hdfs dfs -mkdir -p \" + location)\n",
    "\n",
    "def ensure_disk_location_exists(location):\n",
    "    if not os.path.exists(location):\n",
    "        os.makedirs(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_docs_with_inference(doc2vec_model, doc_classification_map):\n",
    "    \"\"\"\n",
    "    Use the trained doc2vec model to get the paragraph vector representations of the validation documents\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX)):\n",
    "        info(\"===== Loading validation vectors\")\n",
    "        validation_vectors_matrix = pickle.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX)))\n",
    "    else:\n",
    "        validation_documents_reps = {}\n",
    "        validation_vectors = []\n",
    "        validation_labels = []\n",
    "        info(\"===== Getting validation vectors with inference\")\n",
    "\n",
    "        # do inference and store results in dict\n",
    "        i = 0\n",
    "        for (doc_id, doc_contents_array) in ValidationDocumentGenerator(training_file, validation_docs_list):\n",
    "            i += 1\n",
    "            if i % 1000 == 0: info(\"Finished: {}\".format(str(i)))\n",
    "            validation_documents_reps[doc_id] = doc2vec_model.infer_vector(doc_contents_array)\n",
    "\n",
    "        # create matrix for the validation vectors\n",
    "        for validation_doc_id in validation_docs_list:\n",
    "            validation_vectors.append(validation_documents_reps[validation_doc_id])\n",
    "            validation_labels.append([classf for classf in doc_classification_map[validation_doc_id] if classf in sections])\n",
    "        validation_vectors_matrix = np.array(validation_vectors)\n",
    "        pickle.dump(validation_vectors_matrix, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX), 'w'))\n",
    "    \n",
    "    return validation_vectors_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_docs_with_inference_new(doc2vec_model, doc_classification_map, classifications, \n",
    "                                           val_docs_list, val_preprocessed_files_prefix, val_preprocessed_docids_files_prefix):\n",
    "    \"\"\"\n",
    "    Use the trained doc2vec model to get the paragraph vector representations of the validation documents\n",
    "    \"\"\"\n",
    "\n",
    "    def infer_one_doc(doc_tuple):\n",
    "        #doc2vec_model.random = np.random.RandomState(DOC2VEC_SEED)\n",
    "        doc_id, doc_tokens = doc_tuple\n",
    "        rep = doc2vec_model.infer_vector(doc_tokens)\n",
    "        return (doc_id, rep)\n",
    "\n",
    "    one_hot_encoder = OneHotEncoder(classifications)\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX)):\n",
    "        info(\"===== Loading validation vectors\")\n",
    "        validation_labels = []\n",
    "        validation_vectors_matrix = pickle.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX)))\n",
    "        for validation_doc_id in val_docs_list:\n",
    "            val_labels = [classf for classf in doc_classification_map[validation_doc_id] if classf in classifications]\n",
    "            validation_labels.append(one_hot_encoder.get_label_vector(val_labels))\n",
    "        validation_labels = np.array(validation_labels)\n",
    "    else:\n",
    "        validation_documents_reps = {}\n",
    "        validation_vectors = []\n",
    "        validation_labels = []\n",
    "        info(\"===== Getting validation vectors with inference\")\n",
    "\n",
    "        # Single-threaded inference\n",
    "        # do inference and store results in dict\n",
    "#         i = 0\n",
    "        \n",
    "#         validation_docs_iterator = DocumentBatchGenerator(val_preprocessed_files_prefix, \n",
    "#                                                         val_preprocessed_docids_files_prefix, batch_size=None)\n",
    "#         for (doc_id, doc_contents_array) in validation_docs_iterator:\n",
    "#             i += 1\n",
    "#             if i % 1000 == 0: info(\"Finished: {}\".format(str(i)))\n",
    "#             validation_documents_reps[doc_id] = doc2vec_model.infer_vector(doc_contents_array)\n",
    "        \n",
    "        # Multi-threaded inference\n",
    "        validation_docs_iterator = DocumentBatchGenerator(validation_preprocessed_files_prefix, \n",
    "                                                          validation_preprocessed_docids_files_prefix, batch_size=None)\n",
    "        generator_func = validation_docs_iterator.__iter__()\n",
    "        pool = ThreadPool(NUM_CORES)\n",
    "        # map consumes the whole iterator on the spot, so we have to use itertools.islice to fake mini-batching\n",
    "        validation_documents_reps = {}\n",
    "        mini_batch_size = 1000\n",
    "        while True:\n",
    "            threaded_reps_partial = pool.map(infer_one_doc, itertools.islice(generator_func, mini_batch_size))\n",
    "            info(\"Finished: {}\".format(str(validation_docs_iterator.curr_index)))\n",
    "            if threaded_reps_partial:\n",
    "                #threaded_reps.extend(threaded_reps_partial)\n",
    "                validation_documents_reps.update(threaded_reps_partial)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "                \n",
    "        # create matrix for the validation vectors\n",
    "        for validation_doc_id in val_docs_list:\n",
    "            validation_vectors.append(validation_documents_reps[validation_doc_id])\n",
    "            val_labels = [classf for classf in doc_classification_map[validation_doc_id] if classf in classifications]\n",
    "            validation_labels.append(one_hot_encoder.get_label_vector(val_labels))\n",
    "        validation_vectors_matrix = np.array(validation_vectors)\n",
    "        validation_labels = np.array(validation_labels)\n",
    "        pickle.dump(validation_vectors_matrix, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX), 'w'))\n",
    "    \n",
    "    return validation_vectors_matrix, validation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_docs_with_inference_new_additional(doc2vec_model, doc_classification_map, classifications, \n",
    "                                           val_docs_list, val_preprocessed_additional_file, val_preprocessed_additional_docids):\n",
    "    \"\"\"\n",
    "    Use the trained doc2vec model to get the paragraph vector representations of the validation documents\n",
    "    \"\"\"\n",
    "\n",
    "    def infer_one_doc(doc_tuple):\n",
    "        #doc2vec_model.random = np.random.RandomState(DOC2VEC_SEED)\n",
    "        doc_id, doc_tokens = doc_tuple\n",
    "        rep = doc2vec_model.infer_vector(doc_tokens)\n",
    "        return (doc_id, rep)\n",
    "\n",
    "    # First load the already computed vectors\n",
    "    one_hot_encoder = OneHotEncoder(classifications)\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX)):\n",
    "        info(\"===== Loading validation vectors\")\n",
    "        validation_labels = []\n",
    "        validation_vectors_matrix = pickle.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX)))\n",
    "        for validation_doc_id in val_docs_list:\n",
    "            val_labels = [classf for classf in doc_classification_map[validation_doc_id] if classf in classifications]\n",
    "            validation_labels.append(one_hot_encoder.get_label_vector(val_labels))\n",
    "        validation_labels = np.array(validation_labels)\n",
    "        \n",
    "    \n",
    "    # Now infer the additionals\n",
    "    validation_documents_reps_additional = {}\n",
    "    validation_vectors_additional = []\n",
    "    validation_labels_additional = []\n",
    "    info(\"===== Getting validation vectors with inference\")\n",
    "\n",
    "    # Multi-threaded inference\n",
    "    validation_docs_iterator = AdditionalBalancedDocumentGenerator(val_preprocessed_additional_file, \n",
    "                                                      val_preprocessed_additional_docids, batch_size=None)\n",
    "    generator_func = validation_docs_iterator.__iter__()\n",
    "    pool = ThreadPool(NUM_CORES)\n",
    "    # map consumes the whole iterator on the spot, so we have to use itertools.islice to fake mini-batching\n",
    "    validation_documents_reps = {}\n",
    "    mini_batch_size = 100\n",
    "    while True:\n",
    "        threaded_reps_partial = pool.map(infer_one_doc, itertools.islice(generator_func, mini_batch_size))\n",
    "        info(\"Finished: {}\".format(str(mini_batch_size)))\n",
    "        if threaded_reps_partial:\n",
    "            #threaded_reps.extend(threaded_reps_partial)\n",
    "            validation_documents_reps_additional.update(threaded_reps_partial)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # create matrix for the additional validation vectors\n",
    "    for validation_doc_id in val_preprocessed_additional_docids:\n",
    "        validation_vectors_additional.append(validation_documents_reps_additional[validation_doc_id])\n",
    "        val_labels = [classf for classf in doc_classification_map[validation_doc_id] if classf in classifications]\n",
    "        validation_labels_additional.append(one_hot_encoder.get_label_vector(val_labels))\n",
    "    validation_vectors_matrix_additional = np.array(validation_vectors_additional)\n",
    "    validation_labels_additional = np.array(validation_labels_additional)\n",
    "\n",
    "    # stack the old validation vectors matrix and labels with the additional ones\n",
    "    validation_vectors_matrix = np.vstack((validation_vectors_matrix, validation_vectors_matrix_additional))\n",
    "    validation_labels = np.vstack((validation_labels, validation_labels_additional))\n",
    "    \n",
    "    \n",
    "    return validation_vectors_matrix, validation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OneHotEncoder():\n",
    "    \n",
    "    def __init__(self, classifications):\n",
    "        self.classifications = classifications\n",
    "        self.one_hot_indices = {}\n",
    "\n",
    "        # convert character classifications to bit vectors\n",
    "        for i, clssf in enumerate(classifications):\n",
    "            bits = [0] * len(classifications)\n",
    "            bits[i] = 1\n",
    "            self.one_hot_indices[clssf] = i\n",
    "    \n",
    "    def get_label_vector(self, labels):\n",
    "        \"\"\"\n",
    "        classes: array of string with the classes assigned to the instance\n",
    "        \"\"\"\n",
    "        output_vector = [0] * len(self.classifications)\n",
    "        for label in labels:\n",
    "            index = self.one_hot_indices[label]\n",
    "            output_vector[index] = 1\n",
    "            \n",
    "        return output_vector\n",
    "\n",
    "def get_training_data(doc2vec_model, classifications):\n",
    "    one_hot_encoder = OneHotEncoder(classifications)\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    for doc_id in training_docs_list:\n",
    "        # converting from memmap to a normal array\n",
    "        normal_array = []\n",
    "        normal_array[:] = doc2vec_model.docvecs[doc_id][:]\n",
    "        training_data.append(normal_array)\n",
    "        eligible_classifications = [clssf for clssf in doc_classification_map[doc_id] if clssf in classifications]\n",
    "        training_labels.append(one_hot_encoder.get_label_vector(eligible_classifications))\n",
    "    training_labels = np.array(training_labels)\n",
    "    training_data = np.array(training_data)\n",
    "    return training_data, training_labels\n",
    "\n",
    "def get_training_data_with_additional(doc2vec_model, classifications):\n",
    "    one_hot_encoder = OneHotEncoder(classifications)\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    for doc_id in training_docs_list + additional_training_docs_list:\n",
    "        # converting from memmap to a normal array\n",
    "        normal_array = []\n",
    "        normal_array[:] = doc2vec_model.docvecs[doc_id][:]\n",
    "        training_data.append(normal_array)\n",
    "        eligible_classifications = [clssf for clssf in doc_classification_map[doc_id] if clssf in classifications]\n",
    "        training_labels.append(one_hot_encoder.get_label_vector(eligible_classifications))\n",
    "    training_labels = np.array(training_labels)\n",
    "    training_data = np.array(training_data)\n",
    "    return training_data, training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainingDocumentGenerator(object):\n",
    "    def __init__(self, filename, training_docs_list):\n",
    "        self.filename = filename\n",
    "        self.training_docs_list = training_docs_list\n",
    "    def __iter__(self):\n",
    "        with open(self.filename) as file_obj:\n",
    "            for line in file_obj:\n",
    "                if not line.strip(): continue\n",
    "                (doc_id, text) = eval(line)\n",
    "                if doc_id in self.training_docs_list:\n",
    "                    yield LabeledSentence(words=stemtokenizer(text), tags=[doc_id])\n",
    "                    \n",
    "class DocumentBatchGenerator(object):\n",
    "    def __init__(self, filename_prefix, filename_docids_prefix, batch_size=10000 ):\n",
    "        \"\"\"\n",
    "        batch_size cant be > 10,000 due to a limitation in doc2vec training, \n",
    "        None means no batching (only use for inference)\n",
    "        \"\"\"\n",
    "        assert batch_size <= 10000 or batch_size is None\n",
    "        self.filename_prefix = filename_prefix\n",
    "        self.filename_docids_prefix = filename_docids_prefix\n",
    "        self.curr_lines = []\n",
    "        self.curr_docids = []\n",
    "        self.batch_size = batch_size\n",
    "        self.curr_index = 0\n",
    "        self.batch_end = -1\n",
    "    def load_new_batch_in_memory(self):\n",
    "        del self.curr_lines, self.curr_docids\n",
    "        self.curr_lines, self.docids = [], []\n",
    "        info(\"Loading new batch for index: {}\".format(self.curr_index) )\n",
    "        try:\n",
    "            with open(self.filename_prefix + str(self.curr_index)) as preproc_file:\n",
    "                for line in preproc_file:\n",
    "                    self.curr_lines.append(line.split(\" \"))\n",
    "#                     if i % 1000 == 0:\n",
    "#                         print i\n",
    "            self.curr_docids = pickle.load(open(self.filename_docids_prefix + str(self.curr_index), \"r\"))\n",
    "            self.batch_end = self.curr_index + len(self.curr_lines) -1 \n",
    "            info(\"Finished loading new batch\")\n",
    "        except IOError:\n",
    "            info(\"No more batches to load, exiting at index: {}\".format(self.curr_index))\n",
    "            raise StopIteration()\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            if self.curr_index > self.batch_end:\n",
    "                self.load_new_batch_in_memory()\n",
    "            for (doc_id, tokens) in zip(self.curr_docids, self.curr_lines):\n",
    "                if self.batch_size is not None:\n",
    "                    curr_batch_iter = 0\n",
    "                    # divide the document to batches according to the batch size\n",
    "                    while curr_batch_iter < len(tokens):\n",
    "                        yield LabeledSentence(words=tokens[curr_batch_iter: curr_batch_iter + self.batch_size], tags=[doc_id])\n",
    "                        curr_batch_iter += self.batch_size\n",
    "                else:\n",
    "                    yield doc_id, tokens\n",
    "                self.curr_index += 1\n",
    "\n",
    "class AdditionalBalancedDocumentGenerator(object):\n",
    "    def __init__(self, filename, docids, batch_size=10000):\n",
    "        self.filename = filename\n",
    "        self.curr_docids = docids\n",
    "        self.batch_size = batch_size\n",
    "        self.curr_lines = []\n",
    "        self.num_total_batches = 0\n",
    "        self.load_file_in_memory()\n",
    "        \n",
    "    def load_file_in_memory(self):\n",
    "        with open(self.filename) as preproc_file:\n",
    "            for line in preproc_file:\n",
    "                tokens = line.split(\" \")\n",
    "                self.curr_lines.append(tokens)\n",
    "                # get the number of total batches as this will be used when training for Doc2vec to calculate the learning rate decay (which we dont do, but is still needed)\n",
    "                if self.batch_size is not None:\n",
    "                    self.num_total_batches += math.ceil(float(len(tokens))/ self.batch_size)\n",
    "                else:\n",
    "                    self.num_total_batches += 1\n",
    "                \n",
    "    def __iter__(self):\n",
    "        for (doc_id, tokens) in zip(self.curr_docids, self.curr_lines):\n",
    "            if self.batch_size is not None:\n",
    "                curr_batch_iter = 0\n",
    "                # divide the document to batches according to the batch size\n",
    "                while curr_batch_iter < len(tokens):\n",
    "                    yield LabeledSentence(words=tokens[curr_batch_iter: curr_batch_iter + self.batch_size], tags=[doc_id])\n",
    "                    curr_batch_iter += self.batch_size\n",
    "            else:\n",
    "                yield doc_id, tokens\n",
    "                \n",
    "class Word2VecTrainingDocumentGenerator(object):\n",
    "    def __init__(self, filename, training_docs_list):\n",
    "        self.filename = filename\n",
    "        self.training_docs_list = training_docs_list\n",
    "    def __iter__(self):\n",
    "        with open(self.filename) as file_obj:\n",
    "            for line in file_obj:\n",
    "                if not line.strip(): continue\n",
    "                (doc_id, text) = eval(line)\n",
    "                if doc_id in self.training_docs_list:\n",
    "                    yield stemtokenizer(text)\n",
    "                \n",
    "class ValidationDocumentGenerator(object):\n",
    "    def __init__(self, filename, validation_docs_list):\n",
    "        self.filename = filename\n",
    "        self.validation_docs_list = validation_docs_list\n",
    "    def __iter__(self):\n",
    "        with open(self.filename) as file_obj:\n",
    "            for line in file_obj:\n",
    "                if not line.strip(): continue\n",
    "                (doc_id, text) = eval(line)\n",
    "                if doc_id in self.validation_docs_list:\n",
    "                    yield doc_id, stemtokenizer(text)\n",
    "                    \n",
    "class StochasticDocumentGenerator(object):\n",
    "    \"\"\"\n",
    "    Randomly shuffle rows while reading them\n",
    "    \"\"\"\n",
    "    def __init__(self, filename, training_docs_list, line_positions):\n",
    "        self.filename = filename\n",
    "        self.training_docs_list = training_docs_list\n",
    "        self.line_positions = line_positions\n",
    "        self.lines = set(line_positions.keys())\n",
    "    def __iter__(self):\n",
    "        with open(self.filename) as file_obj:\n",
    "            while len(self.lines) > 0:\n",
    "                random_line = random.sample(self.lines,1)[0]\n",
    "                self.lines.remove(random_line)\n",
    "                file_obj.seek(self.line_positions[random_line])\n",
    "                line = file_obj.readline()\n",
    "                if not line.strip(): continue\n",
    "#                 print random_line, self.line_positions[random_line], line[:30]\n",
    "                (doc_id, text) = eval(line)\n",
    "                # print random_line , doc_id\n",
    "                if doc_id in self.training_docs_list:\n",
    "                    yield LabeledSentence(words=stemtokenizer(text), tags=[doc_id])\n",
    "#                     yield doc_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ClassificationMetricsCallback(keras.callbacks.Callback):\n",
    "    \n",
    "    EPOCHS_BEFORE_VALIDATION = 10\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.epoch_index = 0\n",
    "        self.val_loss_reductions = 0\n",
    "        self.best_val_loss = np.iinfo(np.int32).max\n",
    "        self.best_weights = None\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epoch_index += 1\n",
    "        if logs['val_loss'] < self.best_val_loss:\n",
    "            self.val_loss_reductions += 1\n",
    "            self.best_val_loss = logs['val_loss']\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            print '\\r    \\r' # to remove the previous line of verbose output of model fit\n",
    "            time.sleep(0.2)\n",
    "            info('Found lower val loss for epoch {} => {}'.format(self.epoch_index, round(logs['val_loss'], 5)))\n",
    "\n",
    "            \n",
    "def create_keras_nn_model(input_size, output_size, \n",
    "                          first_hidden_layer_size, first_hidden_layer_activation, \n",
    "                          second_hidden_layer_size, second_hidden_layer_activation, \n",
    "                          input_dropout_do, hidden_dropout_do):\n",
    "    \n",
    "    doc_input = Input(shape=(DOC2VEC_SIZE,), name='doc_input')\n",
    "    if input_dropout_do:\n",
    "        hidden = Dropout(0.7)(doc_input)\n",
    "    hidden = Dense(first_hidden_layer_size, activation=first_hidden_layer_activation, \n",
    "                   name='hidden_layer_{}'.format(first_hidden_layer_activation))(doc_input if not input_dropout_do else hidden)\n",
    "    if hidden_dropout_do:\n",
    "        hidden = Dropout(0.5)(hidden)\n",
    "    if second_hidden_layer_size is not None:\n",
    "        hidden = Dense(second_hidden_layer_size, activation=second_hidden_layer_activation, \n",
    "                       name='hidden_layer2_{}'.format(second_hidden_layer_activation))(hidden)\n",
    "    softmax_output = Dense(output_size, activation='sigmoid', name='softmax_output')(hidden)\n",
    "\n",
    "    model = Model(input=doc_input, output=softmax_output)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "get_binary_0_5 = lambda x: 1 if x > 0.5 else 0\n",
    "get_binary_0_5 = np.vectorize(get_binary_0_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOC2VEC_SIZE = 100\n",
    "DOC2VEC_WINDOW = 8\n",
    "DOC2VEC_MAX_VOCAB_SIZE = None\n",
    "DOC2VEC_SAMPLE = 1e-3\n",
    "DOC2VEC_TYPE = 0\n",
    "DOC2VEC_HIERARCHICAL_SAMPLE = 0\n",
    "DOC2VEC_NEGATIVE_SAMPLE_SIZE = 10\n",
    "DOC2VEC_CONCAT = 1\n",
    "DOC2VEC_MEAN = 0\n",
    "DOC2VEC_TRAIN_WORDS = 0\n",
    "DOC2VEC_EPOCHS = 1 # we do our training manually one epoch at a time\n",
    "DOC2VEC_MAX_EPOCHS = 20\n",
    "REPORT_DELAY = 20 # report the progress every x seconds\n",
    "REPORT_VOCAB_PROGRESS = 10000 # report vocab progress every x documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doc2vec_size_100_w_8_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_{}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}'.format(DOC2VEC_SIZE, \n",
    "                                                                DOC2VEC_WINDOW, \n",
    "                                                                'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                                DOC2VEC_TRAIN_WORDS,\n",
    "                                                                DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                                str(DOC2VEC_MAX_VOCAB_SIZE))\n",
    "GLOBAL_VARS.DOC2VEC_MODEL_NAME = placeholder_model_name\n",
    "placeholder_model_name = os.path.join(placeholder_model_name, \"epoch_{}\")\n",
    "placeholder_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifications = valid_subclasses\n",
    "classifications_type = 'subclasses'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VALIDATION_METRICS_FILENAME= '{}_validation_metrics.pkl'.format(classifications_type)\n",
    "TRAINING_METRICS_FILENAME = '{}_training_metrics.pkl'.format(classifications_type)\n",
    "METRICS_FIG_PNG_FILENAME = '{}_validation_metrics.png'.format(classifications_type)\n",
    "METRICS_FIG_PDF_FILENAME = '{}_validation_metrics.pdf'.format(classifications_type)\n",
    "WORD2VEC_METRICS_FILENAME = 'word2vec_metrics.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Doc2vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:02:35,375 : INFO : ****************** Epoch 7 --- Loading doc2vec_size_100_w_8_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_7 *******************\n",
      "2017-01-28 01:02:35,380 : INFO : loading Doc2Vec object from /mnt/data2/shalaby/parameter_search_doc2vec_models_new/full/doc2vec_size_100_w_8_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_7/model\n",
      "2017-01-28 01:02:43,217 : INFO : loading docvecs recursively from /mnt/data2/shalaby/parameter_search_doc2vec_models_new/full/doc2vec_size_100_w_8_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_7/model.docvecs.* with mmap=None\n",
      "2017-01-28 01:02:43,219 : INFO : loading doctag_syn0 from /mnt/data2/shalaby/parameter_search_doc2vec_models_new/full/doc2vec_size_100_w_8_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_7/model.docvecs.doctag_syn0.npy with mmap=None\n",
      "2017-01-28 01:02:44,333 : INFO : loading syn1neg from /mnt/data2/shalaby/parameter_search_doc2vec_models_new/full/doc2vec_size_100_w_8_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_7/model.syn1neg.npy with mmap=None\n",
      "2017-01-28 01:02:44,684 : INFO : loading syn0 from /mnt/data2/shalaby/parameter_search_doc2vec_models_new/full/doc2vec_size_100_w_8_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_7/model.syn0.npy with mmap=None\n",
      "2017-01-28 01:02:45,029 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-01-28 01:02:45,031 : INFO : setting ignored attribute cum_table to None\n",
      "2017-01-28 01:02:46,011 : INFO : Getting training Data\n",
      "2017-01-28 01:05:25,447 : INFO : Getting Validation Embeddings\n",
      "2017-01-28 01:05:25,453 : INFO : ===== Loading validation vectors\n"
     ]
    }
   ],
   "source": [
    "epoch = 7\n",
    "GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "info(\"****************** Epoch {} --- Loading {} *******************\".format(epoch, GLOBAL_VARS.MODEL_NAME))\n",
    "\n",
    "# if we have the model, just load it, otherwise train the previous model\n",
    "if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)):\n",
    "    doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "    GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "\n",
    "info('Getting training Data')\n",
    "X, y = get_training_data(doc2vec_model, classifications)\n",
    "\n",
    "info('Getting Validation Embeddings')\n",
    "Xv, yv = get_validation_docs_with_inference_new(doc2vec_model, doc_classification_map, classifications, \n",
    "                                                validation_docs_list, validation_preprocessed_files_prefix,\n",
    "                                                validation_preprocessed_docids_files_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_OUTPUT_NEURONS = len(classifications)\n",
    "EARLY_STOPPER_MIN_DELTA = 0.00001\n",
    "EARLY_STOPPER_PATIENCE = 5\n",
    "\n",
    "NN_OUTPUT_NEURONS = len(classifications)\n",
    "NN_MAX_EPOCHS = 100\n",
    "NN_BATCH_SIZE = 1024\n",
    "NN_RANDOM_SEARCH_BUDGET = 20\n",
    "NN_PARAM_SAMPLE_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:06:08,323 : INFO : ***************************************************************************************\n",
      "2017-01-28 01:06:08,324 : INFO : nn_1st-size_200_1st-act_tanh_2nd-size_1000_2nd-act_relu_in-drop_False_hid-drop_True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 200)           20200       doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 200)           0           hidden_layer_tanh[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer2_relu (Dense)       (None, 1000)          201000      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 940)           940940      hidden_layer2_relu[0][0]         \n",
      "====================================================================================================\n",
      "Total params: 1162140\n",
      "____________________________________________________________________________________________________\n",
      "Train on 1286325 samples, validate on 321473 samples\n",
      "Epoch 1/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:06:46,172 : INFO : Found lower val loss for epoch 1 => 0.00449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 34s - loss: 0.0067 - val_loss: 0.0045\n",
      "Epoch 2/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:07:20,908 : INFO : Found lower val loss for epoch 2 => 0.00428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 34s - loss: 0.0045 - val_loss: 0.0043\n",
      "Epoch 3/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:07:54,956 : INFO : Found lower val loss for epoch 3 => 0.00414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 34s - loss: 0.0044 - val_loss: 0.0041\n",
      "Epoch 4/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:08:30,419 : INFO : Found lower val loss for epoch 4 => 0.004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 35s - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 5/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:09:03,391 : INFO : Found lower val loss for epoch 5 => 0.00396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 32s - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 6/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:09:37,720 : INFO : Found lower val loss for epoch 6 => 0.00396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 34s - loss: 0.0043 - val_loss: 0.0040\n",
      "Epoch 7/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:10:12,590 : INFO : Found lower val loss for epoch 7 => 0.00394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 34s - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 8/100\n",
      "1286325/1286325 [==============================] - 32s - loss: 0.0043 - val_loss: 0.0040\n",
      "Epoch 9/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:11:21,594 : INFO : Found lower val loss for epoch 9 => 0.00383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 36s - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 10/100\n",
      "1286325/1286325 [==============================] - 35s - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 11/100\n",
      "1286325/1286325 [==============================] - 34s - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 12/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:13:06,778 : INFO : Found lower val loss for epoch 12 => 0.00382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 35s - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 13/100\n",
      "1286325/1286325 [==============================] - 33s - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 14/100\n",
      "1286325/1286325 [==============================] - 32s - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 15/100\n",
      "1286325/1286325 [==============================] - 33s - loss: 0.0043 - val_loss: 0.0039"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:14:47,014 : INFO : Evaluating on Validation Data using last weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00014: early stopping\n",
      "CPU times: user 3min 55s, sys: 5min 54s, total: 9min 50s\n",
      "Wall time: 8min 37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:15:53,630 : INFO : Generating Validation Metrics\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "2017-01-28 01:23:12,922 : INFO : Evaluating on Validation Data using saved best weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Validation Metrics: Cov Err: 8.777 | Top 3: 0.700 | Top 5: 0.785 | F1 Micro: 0.456 | F1 Macro: 0.069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:24:22,307 : INFO : Generating Validation Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Validation Metrics: Cov Err: 8.728 | Top 3: 0.702 | Top 5: 0.787 | F1 Micro: 0.463 | F1 Macro: 0.071\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "first_hidden_layer_size = 200\n",
    "first_hidden_layer_activation = 'tanh'\n",
    "second_hidden_layer_size = 1000\n",
    "second_hidden_layer_activation = 'relu'\n",
    "input_dropout_do = False\n",
    "hidden_dropout_do = True\n",
    "\n",
    "#     print \"===================================================================================\\n\" + \\\n",
    "#           \"========== 1st Layer Size: {}, 1st Layer Activation: {}, \\n 2nd Layer Size: {}, 2nd Layer Activation: {}, \\n\" + \\\n",
    "#           \"Input Dropout: {}, Hidden Dropout: {} \\n\" + \\\n",
    "#           \"==========================\".format(first_hidden_layer_size, first_hidden_layer_activation, \n",
    "#                                                 second_hidden_layer_size, second_hidden_layer_activation, \n",
    "#                                                 input_dropout_do, hidden_dropout_do)\n",
    "\n",
    "GLOBAL_VARS.NN_MODEL_NAME = 'nn_1st-size_{}_1st-act_{}_2nd-size_{}_2nd-act_{}_in-drop_{}_hid-drop_{}'.format(\n",
    "    first_hidden_layer_size, first_hidden_layer_activation, second_hidden_layer_size, \n",
    "    second_hidden_layer_activation, input_dropout_do, hidden_dropout_do\n",
    ")\n",
    "\n",
    "info('***************************************************************************************')\n",
    "info(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "\n",
    "model = create_keras_nn_model(DOC2VEC_SIZE, NN_OUTPUT_NEURONS, \n",
    "                              first_hidden_layer_size, first_hidden_layer_activation, \n",
    "                              second_hidden_layer_size, second_hidden_layer_activation, \n",
    "                              input_dropout_do, hidden_dropout_do)\n",
    "model.summary()\n",
    "\n",
    "early_stopper = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=EARLY_STOPPER_MIN_DELTA, \\\n",
    "                                              patience=EARLY_STOPPER_PATIENCE, verbose=1, mode='auto')\n",
    "metrics_callback = ClassificationMetricsCallback()\n",
    "\n",
    "# Model Fitting\n",
    "%time history = model.fit(x=X, y=y, validation_data=(Xv,yv), batch_size=NN_BATCH_SIZE, \\\n",
    "                          nb_epoch=NN_MAX_EPOCHS, verbose=1, callbacks=[early_stopper, metrics_callback])\n",
    "\n",
    "# info('Evaluating on Training Data')\n",
    "# yp = model.predict(X, batch_size=NN_BATCH_SIZE)\n",
    "# yp_binary = get_binary_0_5(yp)\n",
    "# #print yp\n",
    "# info('Generating Training Metrics')\n",
    "# training_metrics = get_metrics(y, yp, yp_binary)\n",
    "# print \"** Training Metrics: Cov Err: {:.3f}, Avg Labels: {:.3f}, \\n\\t\\t Top 1: {:.3f}, Top 3: {:.3f}, Top 5: {:.3f}, \\n\\t\\t F1 Micro: {:.3f}, F1 Macro: {:.3f}, Total Pos: {:,d}\".format(\n",
    "#     training_metrics['coverage_error'], training_metrics['average_num_of_labels'], \n",
    "#     training_metrics['top_1'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "#     training_metrics['f1_micro'],training_metrics['f1_macro'],  training_metrics['total_positive'])\n",
    "\n",
    "info('Evaluating on Validation Data using last weights')\n",
    "yvp = model.predict(Xv)\n",
    "yvp_binary = get_binary_0_5(yvp)\n",
    "#print yvp\n",
    "info('Generating Validation Metrics')\n",
    "validation_metrics = get_metrics(yv, yvp, yvp_binary)\n",
    "print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "    validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "last_validation_metrics = validation_metrics\n",
    "\n",
    "# using the recorded weights of the best recorded validation loss\n",
    "last_model_weights = model.get_weights()\n",
    "info('Evaluating on Validation Data using saved best weights')\n",
    "model.set_weights(metrics_callback.best_weights)\n",
    "yvp = model.predict(Xv)\n",
    "yvp_binary = get_binary_0_5(yvp)\n",
    "#print yvp\n",
    "info('Generating Validation Metrics')\n",
    "validation_metrics = get_metrics(yv, yvp, yvp_binary)\n",
    "print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "    validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "best_validation_metrics = validation_metrics\n",
    "\n",
    "duration = time.time() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "additional_iterator = AdditionalBalancedDocumentGenerator(training_preprocessed_additional_balanced_file, \n",
    "                                                          additional_training_docs_list, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "783"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(additional_iterator.curr_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "969.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_iterator.num_total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017999999999999995"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:32:45,762 : INFO : training model with 20 workers on 391521 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=10\n",
      "2017-01-28 01:32:45,798 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2017-01-28 01:32:47,354 : INFO : PROGRESS: at 0.10% examples, 5583 words/s, in_qsize 2, out_qsize 3\n",
      "2017-01-28 01:32:56,283 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2017-01-28 01:32:56,476 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2017-01-28 01:32:56,489 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2017-01-28 01:32:56,490 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2017-01-28 01:32:56,491 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2017-01-28 01:32:56,492 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2017-01-28 01:32:56,492 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2017-01-28 01:32:56,493 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2017-01-28 01:32:56,494 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2017-01-28 01:32:56,494 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2017-01-28 01:32:56,495 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2017-01-28 01:32:56,503 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2017-01-28 01:32:56,505 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-01-28 01:32:56,506 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-01-28 01:32:56,508 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-01-28 01:32:56,509 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-01-28 01:32:56,510 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-01-28 01:32:56,511 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-01-28 01:32:56,513 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-28 01:32:56,514 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-28 01:32:56,515 : INFO : training on 5339705 raw words (3756116 effective words) took 10.7s, 350467 effective words/s\n",
      "2017-01-28 01:32:56,517 : INFO : training model with 20 workers on 391521 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=10\n",
      "2017-01-28 01:32:57,538 : INFO : PROGRESS: at 33.13% examples, 1275491 words/s, in_qsize 0, out_qsize 3\n",
      "2017-01-28 01:32:59,251 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2017-01-28 01:32:59,254 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2017-01-28 01:32:59,259 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2017-01-28 01:32:59,261 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2017-01-28 01:32:59,262 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2017-01-28 01:32:59,263 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2017-01-28 01:32:59,263 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2017-01-28 01:32:59,264 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2017-01-28 01:32:59,264 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2017-01-28 01:32:59,265 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2017-01-28 01:32:59,266 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2017-01-28 01:32:59,267 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2017-01-28 01:32:59,267 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-01-28 01:32:59,268 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-01-28 01:32:59,269 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-01-28 01:32:59,269 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-01-28 01:32:59,270 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-01-28 01:32:59,271 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-01-28 01:32:59,271 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-28 01:32:59,272 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-28 01:32:59,273 : INFO : training on 5339705 raw words (3756514 effective words) took 2.7s, 1366165 effective words/s\n",
      "2017-01-28 01:32:59,273 : INFO : training model with 20 workers on 391521 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=10\n",
      "2017-01-28 01:33:00,320 : INFO : PROGRESS: at 34.57% examples, 1305348 words/s, in_qsize 0, out_qsize 9\n",
      "2017-01-28 01:33:02,136 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2017-01-28 01:33:02,140 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2017-01-28 01:33:02,144 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2017-01-28 01:33:02,146 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2017-01-28 01:33:02,147 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2017-01-28 01:33:02,148 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2017-01-28 01:33:02,148 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2017-01-28 01:33:02,149 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2017-01-28 01:33:02,149 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2017-01-28 01:33:02,150 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2017-01-28 01:33:02,151 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2017-01-28 01:33:02,151 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2017-01-28 01:33:02,152 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-01-28 01:33:02,152 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-01-28 01:33:02,153 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-01-28 01:33:02,153 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-01-28 01:33:02,154 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-01-28 01:33:02,154 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-01-28 01:33:02,155 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-28 01:33:02,155 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-28 01:33:02,156 : INFO : training on 5339705 raw words (3756512 effective words) took 2.9s, 1305441 effective words/s\n",
      "2017-01-28 01:33:02,157 : INFO : training model with 20 workers on 391521 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=10\n",
      "2017-01-28 01:33:03,168 : INFO : PROGRESS: at 31.06% examples, 1205930 words/s, in_qsize 0, out_qsize 2\n",
      "2017-01-28 01:33:04,948 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2017-01-28 01:33:04,951 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2017-01-28 01:33:04,956 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2017-01-28 01:33:04,958 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2017-01-28 01:33:04,959 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2017-01-28 01:33:04,960 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2017-01-28 01:33:04,960 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2017-01-28 01:33:04,961 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2017-01-28 01:33:04,962 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2017-01-28 01:33:04,963 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2017-01-28 01:33:04,963 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2017-01-28 01:33:04,964 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2017-01-28 01:33:04,964 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-01-28 01:33:04,965 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-01-28 01:33:04,965 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-01-28 01:33:04,966 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-01-28 01:33:04,966 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-01-28 01:33:04,967 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-01-28 01:33:04,967 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-28 01:33:04,968 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-28 01:33:04,968 : INFO : training on 5339705 raw words (3757582 effective words) took 2.8s, 1339022 effective words/s\n",
      "2017-01-28 01:33:04,969 : INFO : training model with 20 workers on 391521 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=10\n",
      "2017-01-28 01:33:05,987 : INFO : PROGRESS: at 35.71% examples, 1384731 words/s, in_qsize 0, out_qsize 2\n",
      "2017-01-28 01:33:07,687 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2017-01-28 01:33:07,696 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2017-01-28 01:33:07,700 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2017-01-28 01:33:07,700 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2017-01-28 01:33:07,702 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2017-01-28 01:33:07,703 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2017-01-28 01:33:07,703 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2017-01-28 01:33:07,704 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2017-01-28 01:33:07,704 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2017-01-28 01:33:07,705 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2017-01-28 01:33:07,706 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2017-01-28 01:33:07,707 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2017-01-28 01:33:07,707 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-01-28 01:33:07,708 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-01-28 01:33:07,708 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-01-28 01:33:07,709 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-01-28 01:33:07,710 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-01-28 01:33:07,710 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-01-28 01:33:07,711 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-28 01:33:07,712 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-28 01:33:07,712 : INFO : training on 5339705 raw words (3757093 effective words) took 2.7s, 1374389 effective words/s\n",
      "2017-01-28 01:33:07,713 : INFO : training model with 20 workers on 391521 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=10\n",
      "2017-01-28 01:33:08,726 : INFO : PROGRESS: at 35.81% examples, 1387966 words/s, in_qsize 0, out_qsize 2\n",
      "2017-01-28 01:33:10,424 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2017-01-28 01:33:10,428 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2017-01-28 01:33:10,434 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2017-01-28 01:33:10,437 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2017-01-28 01:33:10,438 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2017-01-28 01:33:10,440 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2017-01-28 01:33:10,441 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2017-01-28 01:33:10,442 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2017-01-28 01:33:10,442 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2017-01-28 01:33:10,443 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2017-01-28 01:33:10,444 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2017-01-28 01:33:10,445 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2017-01-28 01:33:10,446 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-01-28 01:33:10,446 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-01-28 01:33:10,447 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-01-28 01:33:10,448 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-01-28 01:33:10,448 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-01-28 01:33:10,449 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-01-28 01:33:10,450 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-28 01:33:10,451 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-28 01:33:10,451 : INFO : training on 5339705 raw words (3757061 effective words) took 2.7s, 1374323 effective words/s\n",
      "2017-01-28 01:33:10,452 : INFO : training model with 20 workers on 391521 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=10\n",
      "2017-01-28 01:33:11,468 : INFO : PROGRESS: at 34.37% examples, 1324601 words/s, in_qsize 0, out_qsize 2\n",
      "2017-01-28 01:33:13,147 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2017-01-28 01:33:13,149 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2017-01-28 01:33:13,151 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2017-01-28 01:33:13,158 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2017-01-28 01:33:13,160 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2017-01-28 01:33:13,161 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2017-01-28 01:33:13,162 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2017-01-28 01:33:13,165 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2017-01-28 01:33:13,166 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2017-01-28 01:33:13,168 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2017-01-28 01:33:13,169 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2017-01-28 01:33:13,169 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2017-01-28 01:33:13,171 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-01-28 01:33:13,171 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-01-28 01:33:13,172 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-01-28 01:33:13,173 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-01-28 01:33:13,173 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-01-28 01:33:13,174 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-01-28 01:33:13,175 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-28 01:33:13,175 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-28 01:33:13,176 : INFO : training on 5339705 raw words (3757100 effective words) took 2.7s, 1382138 effective words/s\n",
      "2017-01-28 01:33:13,176 : INFO : training model with 20 workers on 391521 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=10\n",
      "2017-01-28 01:33:14,185 : INFO : PROGRESS: at 33.23% examples, 1288292 words/s, in_qsize 0, out_qsize 2\n",
      "2017-01-28 01:33:16,017 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2017-01-28 01:33:16,024 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2017-01-28 01:33:16,025 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2017-01-28 01:33:16,026 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2017-01-28 01:33:16,027 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2017-01-28 01:33:16,028 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2017-01-28 01:33:16,028 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2017-01-28 01:33:16,029 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2017-01-28 01:33:16,030 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2017-01-28 01:33:16,031 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2017-01-28 01:33:16,031 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2017-01-28 01:33:16,032 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2017-01-28 01:33:16,032 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-01-28 01:33:16,033 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-01-28 01:33:16,034 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-01-28 01:33:16,034 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-01-28 01:33:16,035 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-01-28 01:33:16,035 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-01-28 01:33:16,036 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-28 01:33:16,036 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-28 01:33:16,037 : INFO : training on 5339705 raw words (3756733 effective words) took 2.9s, 1315556 effective words/s\n"
     ]
    }
   ],
   "source": [
    "doc2vec_model.alpha = 0.025\n",
    "doc2vec_model.min_alpha = 0.025\n",
    "for i in range(8):\n",
    "    doc2vec_model.train(sentences=additional_iterator, total_examples=additional_iterator.num_total_batches, report_delay=REPORT_DELAY)\n",
    "    doc2vec_model.alpha -= 0.001  # decrease the learning rate\n",
    "    doc2vec_model.min_alpha = doc2vec_model.alpha  # fix the learning rate, no decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now get the augmented training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:33:16,112 : INFO : Getting training Data with additionals\n",
      "2017-01-28 01:37:04,459 : INFO : Getting Validation Embeddings with additionals\n",
      "2017-01-28 01:37:04,465 : INFO : ===== Loading validation vectors\n",
      "2017-01-28 01:37:51,215 : INFO : ===== Getting validation vectors with inference\n",
      "2017-01-28 01:37:51,252 : INFO : Finished: 100\n",
      "2017-01-28 01:37:51,254 : INFO : Finished: 100\n"
     ]
    }
   ],
   "source": [
    "info('Getting training Data with additionals')\n",
    "X_add, y_add = get_training_data_with_additional(doc2vec_model, classifications)\n",
    "\n",
    "info('Getting Validation Embeddings with additionals')\n",
    "Xv_add, yv_add = get_validation_docs_with_inference_new_additional(doc2vec_model, doc_classification_map, classifications, \n",
    "                                                validation_docs_list, validation_docs_additional_list_file,\n",
    "                                                additional_validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 200)           20200       doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 200)           0           hidden_layer_tanh[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer2_relu (Dense)       (None, 1000)          201000      dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 940)           940940      hidden_layer2_relu[0][0]         \n",
      "====================================================================================================\n",
      "Total params: 1162140\n",
      "____________________________________________________________________________________________________\n",
      "Train on 1287108 samples, validate on 321496 samples\n",
      "Epoch 1/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:38:53,657 : INFO : Found lower val loss for epoch 1 => 0.00438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287108/1287108 [==============================] - 59s - loss: 0.0066 - val_loss: 0.0044\n",
      "Epoch 2/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:39:49,046 : INFO : Found lower val loss for epoch 2 => 0.00424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287108/1287108 [==============================] - 55s - loss: 0.0045 - val_loss: 0.0042\n",
      "Epoch 3/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:40:36,238 : INFO : Found lower val loss for epoch 3 => 0.00411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287108/1287108 [==============================] - 47s - loss: 0.0044 - val_loss: 0.0041\n",
      "Epoch 4/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:41:30,147 : INFO : Found lower val loss for epoch 4 => 0.00402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287108/1287108 [==============================] - 53s - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 5/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:42:25,027 : INFO : Found lower val loss for epoch 5 => 0.00394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287108/1287108 [==============================] - 54s - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 6/100\n",
      "1287108/1287108 [==============================] - 56s - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 7/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:44:14,166 : INFO : Found lower val loss for epoch 7 => 0.00391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287108/1287108 [==============================] - 52s - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 8/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:45:09,883 : INFO : Found lower val loss for epoch 8 => 0.00389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287108/1287108 [==============================] - 55s - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 9/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:46:04,332 : INFO : Found lower val loss for epoch 9 => 0.00387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287108/1287108 [==============================] - 54s - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 10/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:47:02,648 : INFO : Found lower val loss for epoch 10 => 0.00387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287108/1287108 [==============================] - 58s - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 11/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:47:56,794 : INFO : Found lower val loss for epoch 11 => 0.00386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287108/1287108 [==============================] - 54s - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 12/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:48:52,095 : INFO : Found lower val loss for epoch 12 => 0.00385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287108/1287108 [==============================] - 55s - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 13/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:49:48,526 : INFO : Found lower val loss for epoch 13 => 0.00382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287108/1287108 [==============================] - 56s - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 14/100\n",
      "1287108/1287108 [==============================] - 42s - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 15/100\n",
      "1287108/1287108 [==============================] - 44s - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 16/100\n",
      "1287108/1287108 [==============================] - 52s - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 17/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:53:02,283 : INFO : Found lower val loss for epoch 17 => 0.00382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287108/1287108 [==============================] - 53s - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 18/100\n",
      "1287108/1287108 [==============================] - 50s - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 19/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:54:44,082 : INFO : Found lower val loss for epoch 19 => 0.00381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287108/1287108 [==============================] - 51s - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 00018: early stopping\n",
      "CPU times: user 6min 26s, sys: 10min 32s, total: 16min 59s\n",
      "Wall time: 16min 50s\n"
     ]
    }
   ],
   "source": [
    "model = create_keras_nn_model(DOC2VEC_SIZE, NN_OUTPUT_NEURONS, \n",
    "                              first_hidden_layer_size, first_hidden_layer_activation, \n",
    "                              second_hidden_layer_size, second_hidden_layer_activation, \n",
    "                              input_dropout_do, hidden_dropout_do)\n",
    "model.summary()\n",
    "\n",
    "early_stopper = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=EARLY_STOPPER_MIN_DELTA, \\\n",
    "                                              patience=EARLY_STOPPER_PATIENCE, verbose=1, mode='auto')\n",
    "metrics_callback = ClassificationMetricsCallback()\n",
    "\n",
    "# Model Fitting\n",
    "%time history = model.fit(x=X_add, y=y_add, validation_data=(Xv_add,yv_add), batch_size=NN_BATCH_SIZE, \\\n",
    "                          nb_epoch=NN_MAX_EPOCHS, verbose=1, callbacks=[early_stopper, metrics_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 01:54:44,133 : INFO : Evaluating on Validation Data using last weights\n",
      "2017-01-28 01:56:03,888 : INFO : Generating Validation Metrics\n",
      "2017-01-28 02:04:58,356 : INFO : Evaluating on Validation Data using saved best weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Validation Metrics: Cov Err: 8.808 | Top 3: 0.701 | Top 5: 0.786 | F1 Micro: 0.457 | F1 Macro: 0.068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-28 02:06:14,680 : INFO : Generating Validation Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Validation Metrics: Cov Err: 8.808 | Top 3: 0.701 | Top 5: 0.786 | F1 Micro: 0.457 | F1 Macro: 0.068\n"
     ]
    }
   ],
   "source": [
    "\n",
    "info('Evaluating on Validation Data using last weights')\n",
    "yvp = model.predict(Xv_add)\n",
    "yvp_binary = get_binary_0_5(yvp)\n",
    "#print yvp\n",
    "info('Generating Validation Metrics')\n",
    "validation_metrics = get_metrics(yv_add, yvp, yvp_binary)\n",
    "print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "    validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "last_validation_metrics = validation_metrics\n",
    "\n",
    "# using the recorded weights of the best recorded validation loss\n",
    "last_model_weights = model.get_weights()\n",
    "info('Evaluating on Validation Data using saved best weights')\n",
    "model.set_weights(metrics_callback.best_weights)\n",
    "yvp = model.predict(Xv_add)\n",
    "yvp_binary = get_binary_0_5(yvp)\n",
    "#print yvp\n",
    "info('Generating Validation Metrics')\n",
    "validation_metrics = get_metrics(yv_add, yvp, yvp_binary)\n",
    "print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "    validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "best_validation_metrics = validation_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Validation Metrics: Cov Err: 8.691 | Top 3: 0.702 | Top 5: 0.788 | F1 Micro: 0.468 | F1 Macro: 0.072\n",
      "****** Validation Metrics: Cov Err: 8.691 | Top 3: 0.702 | Top 5: 0.788 | F1 Micro: 0.468 | F1 Macro: 0.072\n"
     ]
    }
   ],
   "source": [
    "validation_metrics = last_validation_metrics\n",
    "print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "    validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "\n",
    "validation_metrics = best_validation_metrics\n",
    "print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "    validation_metrics['f1_micro'], validation_metrics['f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
