{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import cPickle as pickle\n",
    "import string\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATIO = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_location = \"/mnt/data2/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "training_file = root_location + 'docs_output_training_validation_documents_' + str(SAMPLE_RATIO)\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "training_docs_list_file = exports_location + \"training_documents_\" + str(SAMPLE_RATIO) + \"_sample.pkl\"\n",
    "validation_docs_list_file = exports_location + \"validation_documents_\" + str(SAMPLE_RATIO) + \"_sample.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.8 s, sys: 748 ms, total: 15.6 s\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemtokenizer(text):\n",
    "    \"\"\" MAIN FUNCTION to get clean stems out of a text. A list of clean stems are returned \"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\s+', gaps=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stems = []  # result\n",
    "    for token in tokens:\n",
    "        stem = token.lower()\n",
    "        stem = stem.strip(string.punctuation)\n",
    "        if stem:\n",
    "            if is_number(stem):\n",
    "                stem = NUMBER_INDICATOR\n",
    "            elif is_currency(stem):\n",
    "                stem = CURRENCY_INDICATOR\n",
    "            elif is_chemical(stem):\n",
    "                stem = CHEMICAL_INDICATOR\n",
    "            else:\n",
    "                stem = stem.strip(string.punctuation)\n",
    "            if stem and len(stem) >= MIN_SIZE:\n",
    "                # extract uni-grams\n",
    "                stems.append(stem)\n",
    "    del tokens\n",
    "    return stems\n",
    "\n",
    "def is_number(str):\n",
    "    \"\"\" Returns true if given string is a number (float or int)\"\"\"\n",
    "    try:\n",
    "        float(str.replace(\",\", \"\"))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_currency(str):\n",
    "    return str[0] == \"$\"\n",
    "\n",
    "def is_chemical(str):\n",
    "    return str.count(\"-\") > 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Separate Training and Validation Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MIN_SIZE = 0\n",
    "NUMBER_INDICATOR = \"number_inidicator\"\n",
    "CURRENCY_INDICATOR = \"currency_inidicator\"\n",
    "CHEMICAL_INDICATOR = \"chemical_inidicator\"\n",
    "\n",
    "\n",
    "BATCH_SIZE = 10000\n",
    "DOC_IDS_PREFIX = \"sdfjsdfsdf\"\n",
    "TRAINING_PREPROCESSED_FILES_PREFIX = \"/mnt/data2/shalaby/training_docs_sample_%s_data_preprocessed-\" % str(SAMPLE_RATIO)\n",
    "TRAINING_PREPROCESSED_DOCIDS_FILES_PREFIX = \"/mnt/data2/shalaby/training_docs_sample_%s_docids_preprocessed-\" % str(SAMPLE_RATIO)\n",
    "VALIDATION_PREPROCESSED_FILES_PREFIX = \"/mnt/data2/shalaby/validation_docs_sample_%s_data_preprocessed-\" % str(SAMPLE_RATIO)\n",
    "VALIDATION_PREPROCESSED_DOCIDS_FILES_PREFIX = \"/mnt/data2/shalaby/validation_docs_sample_%s_docids_preprocessed-\" % str(SAMPLE_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 11.9 Âµs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def write_batch(file_prefix, doc_files_prefix, batch_lines, doc_ids, batch_start):\n",
    "    if len(batch_lines):\n",
    "        print \"writing batch %d\" % batch_start\n",
    "        with open(file_prefix + str(batch_start), 'w') as batch_file:\n",
    "            for line in batch_lines:\n",
    "                batch_file.write((u\" \".join(line) + \"\\n\").encode('utf-8'))\n",
    "        pickle.dump(doc_ids, open(doc_files_prefix + str(batch_start), 'w'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 0\n",
      "CPU times: user 3.5 s, sys: 1.06 s, total: 4.56 s\n",
      "Wall time: 5.2 s\n",
      "writing batch 10000\n",
      "CPU times: user 3.53 s, sys: 1.05 s, total: 4.58 s\n",
      "Wall time: 5.2 s\n",
      "writing batch 20000\n",
      "CPU times: user 3.92 s, sys: 996 ms, total: 4.92 s\n",
      "Wall time: 5.79 s\n",
      "writing batch 30000\n",
      "CPU times: user 3.84 s, sys: 1.1 s, total: 4.94 s\n",
      "Wall time: 5.58 s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'doc_files_prefix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-fe26d36f7106>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'def write_batch(file_prefix, doc_files_prefix, batch_lines, doc_ids, batch_start):\\n    if len(batch_lines):\\n        print \"writing batch %d\" % batch_start\\n        with open(file_prefix + str(batch_start), \\'w\\') as batch_file:\\n            for line in batch_lines:\\n                batch_file.write((u\" \".join(line) + \"\\\\n\").encode(\\'utf-8\\'))\\n        pickle.dump(doc_ids, open(doc_files_prefix + str(batch_start), \\'w\\'))\\n\\nbatch_index = 0\\nfile_prefix = TRAINING_PREPROCESSED_FILES_PREFIX\\ndoc_file_prefix = TRAINING_PREPROCESSED_DOCIDS_FILES_PREFIX\\nwith open(training_file) as file_obj:\\n    token_lines, doc_ids = [], []\\n    for line in file_obj:\\n        (doc_id, text) = eval(line)\\n        if doc_id in training_docs_list:\\n            token_lines.append(stemtokenizer(text))\\n            doc_ids.append(doc_id)\\n            if len(token_lines) % BATCH_SIZE == 0:\\n                %time write_batch(file_prefix, doc_file_prefix, token_lines, doc_ids, batch_index * BATCH_SIZE)\\n                batch_index += 1\\n                token_lines, doc_ids = [], []\\n    write_batch(file_prefix, doc_files_prefix, token_lines, doc_ids, batch_index * BATCH_SIZE)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'doc_files_prefix' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_index = 0\n",
    "file_prefix = TRAINING_PREPROCESSED_FILES_PREFIX\n",
    "doc_file_prefix = TRAINING_PREPROCESSED_DOCIDS_FILES_PREFIX\n",
    "with open(training_file) as file_obj:\n",
    "    token_lines, doc_ids = [], []\n",
    "    for line in file_obj:\n",
    "        (doc_id, text) = eval(line)\n",
    "        if doc_id in training_docs_list:\n",
    "            token_lines.append(stemtokenizer(text))\n",
    "            doc_ids.append(doc_id)\n",
    "            if len(token_lines) % BATCH_SIZE == 0:\n",
    "                %time write_batch(file_prefix, doc_file_prefix, token_lines, doc_ids, batch_index * BATCH_SIZE)\n",
    "                batch_index += 1\n",
    "                token_lines, doc_ids = [], []\n",
    "    write_batch(file_prefix, doc_file_prefix, token_lines, doc_ids, batch_index * BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 0\n",
      "CPU times: user 42.5 s, sys: 832 ms, total: 43.4 s\n",
      "Wall time: 43.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_index = 0\n",
    "file_prefix = VALIDATION_PREPROCESSED_FILES_PREFIX\n",
    "doc_file_prefix = VALIDATION_PREPROCESSED_DOCIDS_FILES_PREFIX\n",
    "with open(training_file) as file_obj:\n",
    "    token_lines, doc_ids = [], []\n",
    "    for line in file_obj:\n",
    "        (doc_id, text) = eval(line)\n",
    "        if doc_id in validation_docs_list:\n",
    "            token_lines.append(stemtokenizer(text))\n",
    "            doc_ids.append(doc_id)\n",
    "            if len(token_lines) % BATCH_SIZE == 0:\n",
    "                %time write_batch(file_prefix, doc_file_prefix, token_lines, doc_ids, batch_index * BATCH_SIZE)\n",
    "                batch_index += 1\n",
    "                token_lines, doc_ids = [], []\n",
    "    write_batch(file_prefix, doc_file_prefix, token_lines, doc_ids, batch_index * BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.53 s, sys: 856 ms, total: 2.38 s\n",
      "Wall time: 2.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "line_tokens = []\n",
    "with open(TRAINING_PREPROCESSED_FILES_PREFIX + str(0)) as preproc_file:\n",
    "    line_lengths = []\n",
    "    for line in preproc_file:\n",
    "        line_lengths.append(len(line))\n",
    "        line_tokens.append(line.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we used to write the doc id and the tokens as tuples, then do an eval on them in reading time, but this turned out to be very slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def write_batch(file_prefix, batch_lines, batch_start):\n",
    "    if len(batch_lines):\n",
    "        print \"writing batch %d\" % batch_start\n",
    "        %time pickle.dump(batch_lines, open(file_prefix + str(batch_start), 'w'))\n",
    "#         with open(file_prefix + str(batch_start), 'w') as batch_file:\n",
    "#             for line in batch_lines:\n",
    "#                 batch_file.write(str(line) + \"\\n\")\n",
    "\n",
    "batch_index = 0\n",
    "file_prefix = TRAINING_PREPROCESSED_FILES_PREFIX\n",
    "with open(training_file) as file_obj:\n",
    "    token_lines = []\n",
    "    for line in file_obj:\n",
    "        (doc_id, text) = eval(line)\n",
    "        if doc_id in training_docs_list:\n",
    "            token_lines.append((doc_id, stemtokenizer(text)))\n",
    "            if len(token_lines) % BATCH_SIZE == 0:\n",
    "                write_batch(file_prefix, token_lines, batch_index * BATCH_SIZE)\n",
    "                batch_index += 1\n",
    "                token_lines = []\n",
    "    write_batch(file_prefix, token_lines, batch_index * BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 0\n",
      "writing batch 10000\n"
     ]
    }
   ],
   "source": [
    "batch_index = 0\n",
    "file_prefix = VALIDATION_PREPROCESSED_FILES_PREFIX\n",
    "with open(training_file) as file_obj:\n",
    "    token_lines = []\n",
    "    for line in file_obj:\n",
    "        (doc_id, text) = eval(line)\n",
    "        if doc_id in validation_docs_list:\n",
    "            token_lines.append((doc_id, stemtokenizer(text)))\n",
    "            if len(token_lines) % BATCH_SIZE == 0:\n",
    "                write_batch(file_prefix, token_lines, batch_index * BATCH_SIZE)\n",
    "                batch_index += 1\n",
    "                token_lines = []\n",
    "    write_batch(file_prefix, token_lines, batch_index * BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 s, sys: 1.2 s, total: 6.19 s\n",
      "Wall time: 6.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "line_tokens = []\n",
    "with open(TRAINING_PREPROCESSED_FILES_PREFIX + str(0)) as preproc_file:\n",
    "    for line in preproc_file:\n",
    "        line_tokens.append(line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['technical',\n",
       " 'field',\n",
       " 'the',\n",
       " 'present',\n",
       " 'invention',\n",
       " 'generally',\n",
       " 'relates',\n",
       " 'to',\n",
       " 'wireless',\n",
       " 'communications']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_tokens[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Thesis Venv",
   "language": "python",
   "name": "python-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
