{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of fixed size paragraph vectors using standard nn\n",
    "should be able to deal with all levels using the PARTS_LEVEL param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: TITAN X (Pascal) (CNMeM is disabled, cuDNN 5105)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple, defaultdict\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "from functools import partial\n",
    "\n",
    "import seaborn\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Dropout, Masking\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Masking\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "from thesis.utils.metrics import *\n",
    "from thesis.utils.file import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables used throughout the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_SEED = 1234\n",
    "DOC2VEC_SEED = 1234\n",
    "WORD2VEC_SEED = 1234\n",
    "NN_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CORES = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GLOBAL_VARS = namedtuple('GLOBAL_VARS', ['MODEL_NAME', 'DOC2VEC_MODEL_NAME', 'DOC2VEC_MODEL', \n",
    "                                         'SVM_MODEL_NAME', 'NN_MODEL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_MODEL = \"vocab_model\"\n",
    "MODEL_PREFIX = \"model\"\n",
    "VALIDATION_MATRIX = \"validation_matrix.pkl\"\n",
    "VALIDATION_DICT = \"validation_dict.pkl\"\n",
    "TEST_MATRIX = \"test_matrix.pkl\"\n",
    "TEST_DICT = \"test_dict.pkl\"\n",
    "METRICS = \"metrics.pkl\"\n",
    "CLASSIFIER = \"classifier.pkl\"\n",
    "TYPE_CLASSIFIER= \"{}_classifier.pkl\"\n",
    "\n",
    "TRAINING_DATA_MATRIX = \"X_level_{}.npy\"\n",
    "TRAINING_LABELS_MATRIX = \"y_{}.npy\"\n",
    "VALIDATION_DATA_MATRIX = \"Xv_level_{}.npy\"\n",
    "VALIDATION_LABELS_MATRIX = \"yv_{}.npy\"\n",
    "TEST_DATA_MATRIX = \"Xt_level_{}.npy\"\n",
    "TEST_LABELS_MATRIX = \"yt_{}.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_PARAMETER_SEARCH_PREFIX = \"standard_nn_{}_level_{}_batch_{}_nn_parameter_searches.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_location = \"/mnt/data2/shalaby/\"\n",
    "big_data_location = \"/mnt/data/shalaby/\"\n",
    "\n",
    "matrices_save_location = big_data_location + \"extended_pv_matrices\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "nn_parameter_search_location = os.path.join(root_location, \"nn_parameter_search_extended_abs_desc_claims_full_chunks\")\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "valid_classes_file = exports_location + \"valid_classes.pkl\"\n",
    "valid_subclasses_file = exports_location + \"valid_subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "training_docs_list_file = exports_location + \"training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"validation_docs_list.pkl\"\n",
    "test_docs_list_file = exports_location + \"test_docs_list.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load general data required for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.7 s, sys: 984 ms, total: 19.7 s\n",
      "Wall time: 19.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "valid_classes = pickle.load(open(valid_classes_file))\n",
    "valid_subclasses = pickle.load(open(valid_subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286325"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321473"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401877"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_training_data(classifications_type, level):\n",
    "    info(\"Loading Training Data from file\")\n",
    "    training_data = np.load(open(os.path.join(matrices_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                              TRAINING_DATA_MATRIX.format(level))))\n",
    "    training_labels = np.load(open(os.path.join(matrices_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                TRAINING_LABELS_MATRIX.format(classifications_type))))\n",
    "    return training_data, training_labels\n",
    "\n",
    "def get_validation_data(classifications_type, level):\n",
    "    info(\"Loading Validation Data from file\")\n",
    "    validation_data = np.load(open(os.path.join(matrices_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                VALIDATION_DATA_MATRIX.format(level))))\n",
    "    validation_labels = np.load(open(os.path.join(matrices_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                  VALIDATION_LABELS_MATRIX.format(classifications_type))))\n",
    "    return validation_data, validation_labels\n",
    "\n",
    "def get_test_data(classifications_type, level):\n",
    "    info(\"Loading Test Data from file\")\n",
    "    test_data = np.load(open(os.path.join(matrices_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                TEST_DATA_MATRIX.format(level))))\n",
    "    test_labels = np.load(open(os.path.join(matrices_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                  TEST_LABELS_MATRIX.format(classifications_type))))\n",
    "    return test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MetricsCallback(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback called by keras after each epoch. Records the best validation loss and periodically checks the \n",
    "    validation metrics\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        MetricsCallback.EPOCHS_BEFORE_VALIDATION = epochs_before_validation[classifications_type]\n",
    "        MetricsCallback.GRAPH_MIN = metrics_graph_ranges[classifications_type]['min']\n",
    "        MetricsCallback.GRAPH_MAX = metrics_graph_ranges[classifications_type]['max']\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.epoch_index = 0\n",
    "        self.val_loss_reductions = 0\n",
    "        self.metrics_dict = {}\n",
    "        self.best_val_loss = np.iinfo(np.int32).max\n",
    "        self.best_weights = None\n",
    "        self.best_validation_metrics = None\n",
    "        \n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.fig = plt.figure(figsize=(12,6), dpi=80)\n",
    "        self.ax = plt.subplot(111)\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epoch_index += 1\n",
    "        self.losses.append(logs['loss'])\n",
    "        self.val_losses.append(logs['val_loss'])\n",
    "        loss_line, = self.ax.plot(range(1,self.epoch_index+1), self.losses, 'g-', label='Training Loss')\n",
    "        val_loss_line, = self.ax.plot(range(1,self.epoch_index+1), self.val_losses, 'r-', label='Validation Loss')\n",
    "        self.ax.legend(handles=[loss_line, val_loss_line])\n",
    "        self.ax.set_ylim((MetricsCallback.GRAPH_MIN, MetricsCallback.GRAPH_MAX))\n",
    "        self.fig.canvas.draw()\n",
    "        if logs['val_loss'] < self.best_val_loss:\n",
    "            self.val_loss_reductions += 1\n",
    "            self.best_val_loss = logs['val_loss']\n",
    "            self.best_weights = self.model.get_weights()\n",
    "#             print '\\r    \\r' # to remove the previous line of verbose output of model fit\n",
    "            #time.sleep(0.1)\n",
    "            info('Found lower val loss for epoch {} => {}'.format(self.epoch_index, round(logs['val_loss'], 5)))\n",
    "            if self.val_loss_reductions % MetricsCallback.EPOCHS_BEFORE_VALIDATION == 0:\n",
    "                \n",
    "                info('Validation Loss Reduced {} times'.format(self.val_loss_reductions))\n",
    "                info('Evaluating on Validation Data')\n",
    "                yvp = self.model.predict(Xv)\n",
    "                yvp_binary = get_binary_0_5(yvp)\n",
    "                info('Generating Validation Metrics')\n",
    "                validation_metrics = get_metrics(yv, yvp, yvp_binary)\n",
    "                print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "                    validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "                    validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "                self.metrics_dict[self.epoch_index] = validation_metrics\n",
    "#                 self.best_validation_metrics = validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_keras_nn_model(input_size, output_size, \n",
    "                          first_hidden_layer_size, first_hidden_layer_activation, \n",
    "                          second_hidden_layer_size, second_hidden_layer_activation, \n",
    "                          input_dropout_do, hidden_dropout_do, second_hidden_dropout_do=False):\n",
    "    \n",
    "    doc_input = Input(shape=(input_size,), name='doc_input')\n",
    "    if input_dropout_do:\n",
    "        hidden = Dropout(0.7)(doc_input)\n",
    "    hidden = Dense(first_hidden_layer_size, activation=first_hidden_layer_activation, \n",
    "                   name='hidden_layer_{}'.format(first_hidden_layer_activation))(doc_input if not input_dropout_do else hidden)\n",
    "    if hidden_dropout_do:\n",
    "        hidden = Dropout(0.5)(hidden)\n",
    "    if second_hidden_layer_size is not None:\n",
    "        hidden = Dense(second_hidden_layer_size, activation=second_hidden_layer_activation, \n",
    "                       name='hidden_layer2_{}'.format(second_hidden_layer_activation))(hidden)\n",
    "    if second_hidden_dropout_do:\n",
    "        hidden = Dropout(0.5)(hidden)\n",
    "    softmax_output = Dense(output_size, activation='sigmoid', name='softmax_output')(hidden)\n",
    "\n",
    "    model = Model(input=doc_input, output=softmax_output)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Param Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# minimum change in val_loss from previous epoch to register as a decrease\n",
    "early_stopper_deltas = {\n",
    "    'sections': 0.00001,\n",
    "    'classes': 0.00001,\n",
    "    'subclasses': 0.00001\n",
    "}\n",
    "# how many epochs to wait when there is no decrease in val_loss before early stopping\n",
    "early_stopper_patience = {\n",
    "    'sections': 15,\n",
    "    'classes': 15,\n",
    "    'subclasses': 15\n",
    "}\n",
    "# number of epochs after which we do periodic evaluation of validation metrics\n",
    "epochs_before_validation = {\n",
    "    'sections': 10,\n",
    "    'classes': 20,\n",
    "    'subclasses': 20\n",
    "}\n",
    "\n",
    "# ranges for learning graph shown\n",
    "metrics_graph_ranges = {\n",
    "    'sections': {'min':0, 'max': 0.3},\n",
    "    'classes': {'min':0, 'max': 0.05},\n",
    "    'subclasses': {'min':0, 'max': 0.05}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEVEL_DOC = 1\n",
    "LEVEL_DIVISIONS = 2\n",
    "LEVEL_CHUNKS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOC2VEC_SIZE = 200\n",
    "DOC2VEC_WINDOW = 2\n",
    "DOC2VEC_MAX_VOCAB_SIZE = None\n",
    "DOC2VEC_SAMPLE = 1e-3\n",
    "DOC2VEC_TYPE = 1\n",
    "DOC2VEC_HIERARCHICAL_SAMPLE = 0\n",
    "DOC2VEC_NEGATIVE_SAMPLE_SIZE = 10\n",
    "DOC2VEC_CONCAT = 0\n",
    "DOC2VEC_MEAN = 1\n",
    "DOC2VEC_TRAIN_WORDS = 0\n",
    "DOC2VEC_EPOCHS = 1 # we do our training manually one epoch at a time\n",
    "DOC2VEC_MAX_EPOCHS = 8\n",
    "REPORT_DELAY = 20 # report the progress every x seconds\n",
    "REPORT_VOCAB_PROGRESS = 100000 # report vocab progress every x documents\n",
    "\n",
    "DOC2VEC_EPOCH = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_RANDOM_SEARCH_BUDGET = 20\n",
    "NN_PARAM_SAMPLE_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GLOBAL_PARMS_TO_RUN = [\n",
    "    {\n",
    "        'doc2vec_epoch': DOC2VEC_EPOCH,\n",
    "        'classifications': sections,\n",
    "        'classifications_type': 'sections',\n",
    "        'parts_level': LEVEL_DOC,\n",
    "        'nn_batch_size': 4096,\n",
    "    },\n",
    "#     {\n",
    "#         'doc2vec_epoch': DOC2VEC_EPOCH,\n",
    "#         'classifications': valid_classes,\n",
    "#         'classifications_type': 'classes',\n",
    "#         'parts_level': LEVEL_DIVISIONS,\n",
    "#         'nn_batch_size': 4096,\n",
    "#     },\n",
    "#     {\n",
    "#         'doc2vec_epoch': DOC2VEC_EPOCH,\n",
    "#         'classifications': sections,\n",
    "#         'classifications_type': 'sections',\n",
    "#         'parts_level': LEVEL_CHUNKS,\n",
    "#         'nn_batch_size': 4096,\n",
    "#     },\n",
    "#     {\n",
    "#         'doc2vec_epoch': DOC2VEC_EPOCH,\n",
    "#         'classifications': valid_classes,\n",
    "#         'classifications_type': 'classes',\n",
    "#         'parts_level': LEVEL_CHUNKS,\n",
    "#         'nn_batch_size': 4096,\n",
    "#     },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================== NEW PARAM SET ============================================\n",
      "{'classifications_type': 'sections', 'parts_level': 1, 'doc2vec_epoch': 8, 'nn_batch_size': 4096}\n",
      "doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 16:36:45,960 : INFO : Loading Training Documents\n",
      "2017-04-18 16:36:45,962 : INFO : Loading Training Data from file\n",
      "2017-04-18 16:36:46,372 : INFO : Loading Validation Documents\n",
      "2017-04-18 16:36:46,373 : INFO : Loading Validation Data from file\n",
      "2017-04-18 16:36:46,466 : INFO : Reshaping\n",
      "2017-04-18 16:36:46,467 : INFO : Loading Previous results in /mnt/data2/shalaby/nn_parameter_search_extended_abs_desc_claims_full_chunks/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/standard_nn_sections_level_1_batch_4096_nn_parameter_searches.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1286325, 1, 200)\n",
      "(1286325, 8)\n",
      "(321473, 1, 200)\n",
      "(321473, 8)\n",
      "(1286325, 200)\n",
      "(321473, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 16:36:51,163 : INFO : ***************************************************************************************\n",
      "2017-04-18 16:36:51,165 : INFO : nn_1st-size_500_1st-act_tanh_2nd-size_2000_2nd-act_sigmoid_in-drop_False_hid-drop_True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 200)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 500)           100500      doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 500)           0           hidden_layer_tanh[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer2_sigmoid (Dense)    (None, 2000)          1002000     dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             16008       hidden_layer2_sigmoid[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 1,118,508\n",
      "Trainable params: 1,118,508\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 1286325 samples, validate on 321473 samples\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        this.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width);\n",
       "        canvas.attr('height', height);\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'];\n",
       "    var y0 = fig.canvas.height - msg['y0'];\n",
       "    var x1 = msg['x1'];\n",
       "    var y1 = fig.canvas.height - msg['y1'];\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x;\n",
       "    var y = canvas_pos.y;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overriden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1282048/1286325 [============================>.] - ETA: 0s - loss: 0.2243"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 16:37:18,431 : INFO : Found lower val loss for epoch 1 => 0.21701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 14s - loss: 0.2242 - val_loss: 0.2170\n",
      "Epoch 2/200\n",
      "1282048/1286325 [============================>.] - ETA: 0s - loss: 0.1917"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 16:37:33,655 : INFO : Found lower val loss for epoch 2 => 0.18025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 15s - loss: 0.1917 - val_loss: 0.1803\n",
      "Epoch 3/200\n",
      "1282048/1286325 [============================>.] - ETA: 0s - loss: 0.1717"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 16:37:48,836 : INFO : Found lower val loss for epoch 3 => 0.17452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 15s - loss: 0.1716 - val_loss: 0.1745\n",
      "Epoch 4/200\n",
      "1286144/1286325 [============================>.] - ETA: 0s - loss: 0.1623"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 16:38:04,835 : INFO : Found lower val loss for epoch 4 => 0.16139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 15s - loss: 0.1623 - val_loss: 0.1614\n",
      "Epoch 5/200\n",
      "1286144/1286325 [============================>.] - ETA: 0s - loss: 0.1568"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 16:38:20,057 : INFO : Found lower val loss for epoch 5 => 0.15879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 15s - loss: 0.1568 - val_loss: 0.1588\n",
      "Epoch 6/200\n",
      "1286144/1286325 [============================>.] - ETA: 0s - loss: 0.1531"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 16:38:34,826 : INFO : Found lower val loss for epoch 6 => 0.15427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 14s - loss: 0.1531 - val_loss: 0.1543\n",
      "Epoch 7/200\n",
      "1286144/1286325 [============================>.] - ETA: 0s - loss: 0.1505"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 16:38:49,642 : INFO : Found lower val loss for epoch 7 => 0.15161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 14s - loss: 0.1505 - val_loss: 0.1516\n",
      "Epoch 8/200\n",
      "1286325/1286325 [==============================] - 14s - loss: 0.1485 - val_loss: 0.1548\n",
      "Epoch 9/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1470 - val_loss: 0.1534\n",
      "Epoch 10/200\n",
      "1282048/1286325 [============================>.] - ETA: 0s - loss: 0.1454"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 16:39:35,964 : INFO : Found lower val loss for epoch 10 => 0.149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 16s - loss: 0.1454 - val_loss: 0.1490\n",
      "Epoch 11/200\n",
      "1286144/1286325 [============================>.] - ETA: 0s - loss: 0.1443"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 16:39:52,005 : INFO : Found lower val loss for epoch 11 => 0.14735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 16s - loss: 0.1443 - val_loss: 0.1473\n",
      "Epoch 12/200\n",
      "1286144/1286325 [============================>.] - ETA: 0s - loss: 0.1433"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 16:40:07,120 : INFO : Found lower val loss for epoch 12 => 0.14716\n",
      "2017-04-18 16:40:07,122 : INFO : Validation Loss Reduced 10 times\n",
      "2017-04-18 16:40:07,123 : INFO : Evaluating on Validation Data\n",
      "2017-04-18 16:40:49,973 : INFO : Generating Validation Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Validation Metrics: Cov Err: 1.449 | Top 3: 0.970 | Top 5: 0.995 | F1 Micro: 0.774 | F1 Macro: 0.704\n",
      "1286325/1286325 [==============================] - 67s - loss: 0.1433 - val_loss: 0.1472\n",
      "Epoch 13/200\n",
      "1286144/1286325 [============================>.] - ETA: 0s - loss: 0.1424"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 16:41:14,006 : INFO : Found lower val loss for epoch 13 => 0.14512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 14s - loss: 0.1424 - val_loss: 0.1451\n",
      "Epoch 14/200\n",
      "1282048/1286325 [============================>.] - ETA: 0s - loss: 0.1416"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 16:41:29,243 : INFO : Found lower val loss for epoch 14 => 0.14322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 15s - loss: 0.1416 - val_loss: 0.1432\n",
      "Epoch 15/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1408 - val_loss: 0.1449\n",
      "Epoch 16/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1402 - val_loss: 0.1437\n",
      "Epoch 17/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1396 - val_loss: 0.1478\n",
      "Epoch 18/200\n",
      "1282048/1286325 [============================>.] - ETA: 0s - loss: 0.1391"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 16:42:29,951 : INFO : Found lower val loss for epoch 18 => 0.14266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 15s - loss: 0.1391 - val_loss: 0.1427\n",
      "Epoch 19/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1386 - val_loss: 0.1441\n",
      "Epoch 20/200\n",
      "1286325/1286325 [==============================] - 14s - loss: 0.1382 - val_loss: 0.1434\n",
      "Epoch 21/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1377 - val_loss: 0.1468\n",
      "Epoch 22/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1373 - val_loss: 0.1468\n",
      "Epoch 23/200\n",
      "1286144/1286325 [============================>.] - ETA: 0s - loss: 0.1370"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 16:43:45,944 : INFO : Found lower val loss for epoch 23 => 0.14223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 15s - loss: 0.1370 - val_loss: 0.1422\n",
      "Epoch 24/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1365 - val_loss: 0.1433\n",
      "Epoch 25/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1363 - val_loss: 0.1422\n",
      "Epoch 26/200\n",
      "1286325/1286325 [==============================] - 14s - loss: 0.1359 - val_loss: 0.1442\n",
      "Epoch 27/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1357 - val_loss: 0.1429\n",
      "Epoch 28/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1354 - val_loss: 0.1431\n",
      "Epoch 29/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1351 - val_loss: 0.1437\n",
      "Epoch 30/200\n",
      "1286144/1286325 [============================>.] - ETA: 0s - loss: 0.1350"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 16:45:32,493 : INFO : Found lower val loss for epoch 30 => 0.14009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 14s - loss: 0.1350 - val_loss: 0.1401\n",
      "Epoch 31/200\n",
      "1286325/1286325 [==============================] - 14s - loss: 0.1346 - val_loss: 0.1423\n",
      "Epoch 32/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1345 - val_loss: 0.1425\n",
      "Epoch 33/200\n",
      "1286325/1286325 [==============================] - 14s - loss: 0.1343 - val_loss: 0.1461\n",
      "Epoch 34/200\n",
      "1286325/1286325 [==============================] - 14s - loss: 0.1341 - val_loss: 0.1445\n",
      "Epoch 35/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1340 - val_loss: 0.1446\n",
      "Epoch 36/200\n",
      "1286325/1286325 [==============================] - 14s - loss: 0.1337 - val_loss: 0.1433\n",
      "Epoch 37/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1335 - val_loss: 0.1403\n",
      "Epoch 38/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1333 - val_loss: 0.1436\n",
      "Epoch 39/200\n",
      "1286325/1286325 [==============================] - 14s - loss: 0.1332 - val_loss: 0.1422\n",
      "Epoch 40/200\n",
      "1286325/1286325 [==============================] - 14s - loss: 0.1330 - val_loss: 0.1408\n",
      "Epoch 41/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1329 - val_loss: 0.1435\n",
      "Epoch 42/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1328 - val_loss: 0.1420\n",
      "Epoch 43/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1326 - val_loss: 0.1401\n",
      "Epoch 44/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1325 - val_loss: 0.1421\n",
      "Epoch 45/200\n",
      "1286325/1286325 [==============================] - 14s - loss: 0.1323 - val_loss: 0.1429\n",
      "Epoch 46/200\n",
      "1286325/1286325 [==============================] - 15s - loss: 0.1322 - val_loss: 0.1406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 16:49:33,716 : INFO : Evaluating on Validation Data using saved best weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00045: early stopping\n",
      "CPU times: user 4min 24s, sys: 6min 32s, total: 10min 56s\n",
      "Wall time: 12min 41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 16:50:18,300 : INFO : Generating Validation Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Validation Metrics: Cov Err: 1.424 | Top 3: 0.973 | Top 5: 0.995 | F1 Micro: 0.794 | F1 Macro: 0.742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 16:50:27,590 : INFO : Loading Previous results from /mnt/data2/shalaby/nn_parameter_search_extended_abs_desc_claims_full_chunks/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/standard_nn_sections_level_1_batch_4096_nn_parameter_searches.pkl\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "for GLOBAL_PARAMS in GLOBAL_PARMS_TO_RUN:\n",
    "    \n",
    "    print '==================================== NEW PARAM SET ============================================'\n",
    "    print {k:v for k,v in GLOBAL_PARAMS.items() if k != 'classifications'}\n",
    "    \n",
    "    classifications = GLOBAL_PARAMS['classifications']\n",
    "    classifications_type = GLOBAL_PARAMS['classifications_type']\n",
    "    \n",
    "    PARTS_LEVEL = GLOBAL_PARAMS['parts_level']\n",
    "    \n",
    "    placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}'.format(DOC2VEC_SIZE, \n",
    "                                                                DOC2VEC_WINDOW, \n",
    "                                                                'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                                DOC2VEC_TRAIN_WORDS,\n",
    "                                                                DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                                str(DOC2VEC_MAX_VOCAB_SIZE))\n",
    "    GLOBAL_VARS.DOC2VEC_MODEL_NAME = placeholder_model_name\n",
    "    placeholder_model_name = os.path.join(placeholder_model_name, \"epoch_{}\")\n",
    "\n",
    "    epoch = GLOBAL_PARAMS['doc2vec_epoch']\n",
    "\n",
    "    GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "    print GLOBAL_VARS.MODEL_NAME\n",
    "    \n",
    "    time.sleep(0.2)\n",
    "    \n",
    "    info(\"Loading Training Documents\")\n",
    "    X, y = get_training_data(classifications_type, PARTS_LEVEL)\n",
    "    print X.shape\n",
    "    print y.shape\n",
    "    \n",
    "    info(\"Loading Validation Documents\")\n",
    "    Xv, yv = get_validation_data(classifications_type, PARTS_LEVEL)\n",
    "    print Xv.shape\n",
    "    print yv.shape\n",
    "    \n",
    "    info(\"Reshaping\")\n",
    "    X = np.reshape(X, (X.shape[0], X.shape[1]* X.shape[2]))\n",
    "    Xv = np.reshape(Xv, (Xv.shape[0], Xv.shape[1]* Xv.shape[2]))\n",
    "    print X.shape\n",
    "    print Xv.shape\n",
    "    \n",
    "    \n",
    "    NN_INPUT_NEURONS = X.shape[1]\n",
    "    NN_OUTPUT_NEURONS = len(classifications)\n",
    "    \n",
    "    EARLY_STOPPER_MIN_DELTA = early_stopper_deltas[classifications_type]\n",
    "    EARLY_STOPPER_PATIENCE = early_stopper_patience[classifications_type]\n",
    "\n",
    "    NN_MAX_EPOCHS = 200\n",
    "    NN_PARAM_SAMPLE_SEED = 1234\n",
    "\n",
    "    NN_BATCH_SIZE = GLOBAL_PARAMS['nn_batch_size']\n",
    "\n",
    "    MODEL_VERBOSITY = 1\n",
    "\n",
    "    NN_OPTIMIZER = 'rmsprop'\n",
    "    # NN_OPTIMIZER = 'adam'\n",
    "\n",
    "    to_skip = []\n",
    "\n",
    "    load_existing_results = True\n",
    "    save_results = True\n",
    "\n",
    "\n",
    "    first_hidden_layer_sizes = [100,200,500,1000]\n",
    "    second_hidden_layer_sizes = [None,500,1000,2000]\n",
    "    first_hidden_layer_activations = ['relu','sigmoid', 'tanh']\n",
    "    second_hidden_layer_activations = ['relu','sigmoid', 'tanh']\n",
    "    # first_hidden_layer_activations = ['relu']\n",
    "    # second_hidden_layer_activations = ['relu']\n",
    "    # input_dropout_options = [False, True]\n",
    "    # hidden_dropout_options = [False, True]\n",
    "    input_dropout_options = [False]\n",
    "    hidden_dropout_options = [True]\n",
    "    second_hidden_dropout_options = [False]\n",
    "\n",
    "    \n",
    "    first_hidden_layer_sizes = [500]\n",
    "    second_hidden_layer_sizes = [2000,]\n",
    "    first_hidden_layer_activations = ['tanh']\n",
    "    second_hidden_layer_activations = ['sigmoid']\n",
    "    # first_hidden_layer_activations = ['relu']\n",
    "    # second_hidden_layer_activations = ['relu']\n",
    "    # input_dropout_options = [False, True]\n",
    "    # hidden_dropout_options = [False, True]\n",
    "    input_dropout_options = [False]\n",
    "    hidden_dropout_options = [True]\n",
    "    second_hidden_dropout_options = [False]\n",
    "    NN_RANDOM_SEARCH_BUDGET = 1\n",
    "\n",
    "\n",
    "    np.random.seed(NN_SEED)\n",
    "    \n",
    "    \n",
    "    ################################################################################\n",
    "    ############### Actual Training\n",
    "\n",
    "\n",
    "    # load previous finshed results so we dont redo them\n",
    "    param_results_dict = {}\n",
    "\n",
    "    param_results_path = os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                           NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, PARTS_LEVEL, NN_BATCH_SIZE)))\n",
    "\n",
    "    if load_existing_results:\n",
    "        if os.path.exists(param_results_path):\n",
    "            info('Loading Previous results in {}'.format(param_results_path))\n",
    "            param_results_dict = pickle.load(open(param_results_path))\n",
    "        else:\n",
    "            info('No Previous results exist in {}'.format(param_results_path))\n",
    "\n",
    "    ensure_disk_location_exists(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME))\n",
    "    \n",
    "    param_sampler = ParameterSampler({\n",
    "        'first_hidden_layer_size':first_hidden_layer_sizes,\n",
    "        'first_hidden_layer_activation':first_hidden_layer_activations,\n",
    "        'second_hidden_layer_size':second_hidden_layer_sizes,\n",
    "        'second_hidden_layer_activation':second_hidden_layer_activations,\n",
    "        'input_dropout':input_dropout_options,\n",
    "        'hidden_dropout':hidden_dropout_options,\n",
    "        'second_hidden_dropout':second_hidden_dropout_options\n",
    "    }, n_iter=NN_RANDOM_SEARCH_BUDGET, random_state=NN_PARAM_SAMPLE_SEED)\n",
    "\n",
    "\n",
    "    for parameters in param_sampler:\n",
    "        start_time = time.time()\n",
    "        first_hidden_layer_size = parameters['first_hidden_layer_size']\n",
    "        first_hidden_layer_activation = parameters['first_hidden_layer_activation']\n",
    "        second_hidden_layer_size = parameters['second_hidden_layer_size']\n",
    "        second_hidden_layer_activation = parameters['second_hidden_layer_activation']\n",
    "        input_dropout_do = parameters['input_dropout']\n",
    "        hidden_dropout_do = parameters['hidden_dropout']\n",
    "        second_hidden_dropout_do = parameters['second_hidden_dropout']\n",
    "\n",
    "        GLOBAL_VARS.NN_MODEL_NAME = 'nn_1st-size_{}_1st-act_{}_2nd-size_{}_2nd-act_{}_in-drop_{}_hid-drop_{}'.format(\n",
    "            first_hidden_layer_size, first_hidden_layer_activation, second_hidden_layer_size, \n",
    "            second_hidden_layer_activation, input_dropout_do, hidden_dropout_do\n",
    "        )\n",
    "        if second_hidden_dropout_do:\n",
    "            GLOBAL_VARS.NN_MODEL_NAME = GLOBAL_VARS.NN_MODEL_NAME + '_2nd-hid-drop_{}'.format(str(second_hidden_dropout_do))\n",
    "\n",
    "        if GLOBAL_VARS.NN_MODEL_NAME in param_results_dict.keys() or GLOBAL_VARS.NN_MODEL_NAME in to_skip:\n",
    "            print \"skipping: {}\".format(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "            continue\n",
    "            \n",
    "\n",
    "        info('***************************************************************************************')\n",
    "        info(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "\n",
    "        model = create_keras_nn_model(NN_INPUT_NEURONS, NN_OUTPUT_NEURONS, \n",
    "                                      first_hidden_layer_size, first_hidden_layer_activation, \n",
    "                                      second_hidden_layer_size, second_hidden_layer_activation, \n",
    "                                      input_dropout_do, hidden_dropout_do, second_hidden_dropout_do)\n",
    "        model.summary()\n",
    "\n",
    "        early_stopper = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=EARLY_STOPPER_MIN_DELTA,\n",
    "                                                      patience=EARLY_STOPPER_PATIENCE, verbose=1, mode='auto')\n",
    "        metrics_callback = MetricsCallback()\n",
    "\n",
    "        # Model Fitting\n",
    "        %time history = model.fit(x=X, y=y, validation_data=(Xv,yv), batch_size=NN_BATCH_SIZE, \\\n",
    "                                  nb_epoch=NN_MAX_EPOCHS, verbose=MODEL_VERBOSITY, callbacks=[early_stopper, metrics_callback])\n",
    "\n",
    "\n",
    "        # using the recorded weights of the best recorded validation loss\n",
    "        last_model_weights = model.get_weights()\n",
    "        info('Evaluating on Validation Data using saved best weights')\n",
    "        model.set_weights(metrics_callback.best_weights)\n",
    "        yvp = model.predict(Xv)\n",
    "        yvp_binary = get_binary_0_5(yvp)\n",
    "        #print yvp\n",
    "        info('Generating Validation Metrics')\n",
    "        validation_metrics = get_metrics(yv, yvp, yvp_binary)\n",
    "        print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "            validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "            validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "        best_validation_metrics = validation_metrics\n",
    "        \n",
    "        time.sleep(0.2)\n",
    "        param_results_dict[GLOBAL_VARS.NN_MODEL_NAME] = dict()\n",
    "        param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['best_validation_metrics'] = best_validation_metrics\n",
    "        param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['epochs'] = len(history.history['val_loss'])\n",
    "        param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['best_weights'] = metrics_callback.best_weights\n",
    "        param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['best_val_loss'] = metrics_callback.best_val_loss\n",
    "        param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['training_loss'] = metrics_callback.losses\n",
    "        param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['validation_loss'] = metrics_callback.val_losses\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['duration'] =  duration\n",
    "\n",
    "        del history, last_model_weights, metrics_callback\n",
    "\n",
    "    if save_results:\n",
    "        if load_existing_results:\n",
    "            if os.path.exists(param_results_path):\n",
    "                info('Loading Previous results from {}'.format(param_results_path))\n",
    "                loaded_param_results_dict = pickle.load(open(param_results_path))\n",
    "                param_results_dict.update(loaded_param_results_dict)\n",
    "\n",
    "        pickle.dump(param_results_dict, open(param_results_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(param_results_dict, open(param_results_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nn_1st-size_100_1st-act_relu_2nd-size_1000_2nd-act_sigmoid_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_100_1st-act_tanh_2nd-size_500_2nd-act_sigmoid_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_200_1st-act_tanh_2nd-size_2000_2nd-act_relu_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_200_1st-act_sigmoid_2nd-size_2000_2nd-act_relu_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_200_1st-act_tanh_2nd-size_500_2nd-act_tanh_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_100_1st-act_tanh_2nd-size_1000_2nd-act_relu_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_1000_1st-act_relu_2nd-size_2000_2nd-act_sigmoid_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_500_1st-act_relu_2nd-size_2000_2nd-act_relu_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_1000_1st-act_relu_2nd-size_None_2nd-act_sigmoid_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_1000_1st-act_sigmoid_2nd-size_None_2nd-act_sigmoid_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_200_1st-act_sigmoid_2nd-size_1000_2nd-act_sigmoid_in-drop_False_hid-drop_True']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_results_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifications = sections\n",
    "classifications_type = 'sections'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_METRICS_FILENAME = '{}_level_{}_standard_nn_test_metrics.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8\n"
     ]
    }
   ],
   "source": [
    "placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}'.format(DOC2VEC_SIZE, \n",
    "                                                            DOC2VEC_WINDOW, \n",
    "                                                            'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                            DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                            DOC2VEC_TRAIN_WORDS,\n",
    "                                                            DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                            str(DOC2VEC_MAX_VOCAB_SIZE))\n",
    "GLOBAL_VARS.DOC2VEC_MODEL_NAME = placeholder_model_name\n",
    "placeholder_model_name = os.path.join(placeholder_model_name, \"epoch_{}\")\n",
    "\n",
    "epoch = DOC2VEC_EPOCH\n",
    "\n",
    "GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "print GLOBAL_VARS.MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_BATCH_SIZE = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PARTS_LEVEL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_results_path = os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                           NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, PARTS_LEVEL, NN_BATCH_SIZE)))\n",
    "param_results_dict = pickle.load(open(param_results_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-13 02:01:23,680 : INFO : Loading Test Data from file\n"
     ]
    }
   ],
   "source": [
    "Xt, yt = get_test_data(classifications_type, PARTS_LEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-13 02:01:31,361 : INFO : Reshaping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(401877, 800)\n"
     ]
    }
   ],
   "source": [
    "info(\"Reshaping\")\n",
    "Xt = np.reshape(Xt, (Xt.shape[0], Xt.shape[1]* Xt.shape[2]))\n",
    "print Xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_OUTPUT_NEURONS = len(classifications)\n",
    "NN_INPUT_NEURONS = Xt.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nn_1st-size_1000_1st-act_sigmoid_2nd-size_None_2nd-act_sigmoid_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_100_1st-act_relu_2nd-size_1000_2nd-act_sigmoid_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_100_1st-act_tanh_2nd-size_500_2nd-act_sigmoid_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_200_1st-act_tanh_2nd-size_2000_2nd-act_relu_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_200_1st-act_sigmoid_2nd-size_2000_2nd-act_relu_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_100_1st-act_sigmoid_2nd-size_1000_2nd-act_tanh_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_200_1st-act_tanh_2nd-size_500_2nd-act_tanh_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_100_1st-act_tanh_2nd-size_1000_2nd-act_relu_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_100_1st-act_sigmoid_2nd-size_2000_2nd-act_relu_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_500_1st-act_relu_2nd-size_2000_2nd-act_tanh_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_1000_1st-act_relu_2nd-size_2000_2nd-act_sigmoid_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_1000_1st-act_relu_2nd-size_None_2nd-act_relu_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_1000_1st-act_relu_2nd-size_1000_2nd-act_sigmoid_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_500_1st-act_sigmoid_2nd-size_2000_2nd-act_sigmoid_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_500_1st-act_relu_2nd-size_2000_2nd-act_relu_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_1000_1st-act_sigmoid_2nd-size_500_2nd-act_relu_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_1000_1st-act_tanh_2nd-size_500_2nd-act_sigmoid_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_1000_1st-act_relu_2nd-size_None_2nd-act_sigmoid_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_500_1st-act_relu_2nd-size_500_2nd-act_relu_in-drop_False_hid-drop_True',\n",
       " 'nn_1st-size_200_1st-act_sigmoid_2nd-size_1000_2nd-act_sigmoid_in-drop_False_hid-drop_True']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_results_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-13 02:01:37,955 : INFO : ***************************************************************************************\n",
      "2017-04-13 02:01:37,957 : INFO : nn_1st-size_1000_1st-act_sigmoid_2nd-size_500_2nd-act_relu_in-drop_False_hid-drop_True\n",
      "2017-04-13 02:01:38,245 : INFO : Evaluating on Test Data using best weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 800)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 1000)          801000      doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)             (None, 1000)          0           hidden_layer_sigmoid[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer2_relu (Dense)       (None, 500)           500500      dropout_50[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        hidden_layer2_relu[0][0]         \n",
      "====================================================================================================\n",
      "Total params: 1305508\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-13 02:01:46,750 : INFO : Generating Test Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Test Metrics: Cov Err: 1.380, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.851, Top 3: 0.978, Top 5: 0.997, \n",
      "\t\t F1 Micro: 0.814, F1 Macro: 0.766, Total Pos: 437,702\n"
     ]
    }
   ],
   "source": [
    "first_hidden_layer_size = 1000\n",
    "first_hidden_layer_activation = 'sigmoid'\n",
    "second_hidden_layer_size = 500\n",
    "second_hidden_layer_activation = 'relu'\n",
    "input_dropout_do = False\n",
    "hidden_dropout_do = True\n",
    "second_hidden_dropout_do = False\n",
    "\n",
    "GLOBAL_VARS.NN_MODEL_NAME = 'nn_1st-size_{}_1st-act_{}_2nd-size_{}_2nd-act_{}_in-drop_{}_hid-drop_{}'.format(\n",
    "    first_hidden_layer_size, first_hidden_layer_activation, second_hidden_layer_size, \n",
    "    second_hidden_layer_activation, input_dropout_do, hidden_dropout_do\n",
    ")\n",
    "if second_hidden_dropout_do:\n",
    "    GLOBAL_VARS.NN_MODEL_NAME = GLOBAL_VARS.NN_MODEL_NAME + '_2nd-hid-drop_{}'.format(str(second_hidden_dropout_do))\n",
    "\n",
    "if GLOBAL_VARS.NN_MODEL_NAME not in param_results_dict.keys():\n",
    "    print \"Can't find model: {}\".format(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "    raise Exception()\n",
    "\n",
    "    \n",
    "info('***************************************************************************************')\n",
    "info(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "\n",
    "model = create_keras_nn_model(NN_INPUT_NEURONS, NN_OUTPUT_NEURONS, \n",
    "                              first_hidden_layer_size, first_hidden_layer_activation, \n",
    "                              second_hidden_layer_size, second_hidden_layer_activation, \n",
    "                              input_dropout_do, hidden_dropout_do)\n",
    "model.summary()\n",
    "\n",
    "# get model best weights\n",
    "# weights = param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['metrics_callback'].best_weights\n",
    "weights = param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['best_weights']\n",
    "model.set_weights(weights)\n",
    "\n",
    "info('Evaluating on Test Data using best weights')\n",
    "ytp = model.predict(Xt)\n",
    "ytp_binary = get_binary_0_5(ytp)\n",
    "#print yvp\n",
    "info('Generating Test Metrics')\n",
    "test_metrics = get_metrics(yt, ytp, ytp_binary)\n",
    "print \"** Test Metrics: Cov Err: {:.3f}, Avg Labels: {:.3f}, \\n\\t\\t Top 1: {:.3f}, Top 3: {:.3f}, Top 5: {:.3f}, \\n\\t\\t F1 Micro: {:.3f}, F1 Macro: {:.3f}, Total Pos: {:,d}\".format(\n",
    "    test_metrics['coverage_error'], test_metrics['average_num_of_labels'], \n",
    "    test_metrics['top_1'], test_metrics['top_3'], test_metrics['top_5'], \n",
    "    test_metrics['f1_micro'], test_metrics['f1_macro'], test_metrics['total_positive'])\n",
    "\n",
    "ensure_disk_location_exists(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME))\n",
    "    \n",
    "pickle.dump(test_metrics, open(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                            TEST_METRICS_FILENAME.format(classifications_type, PARTS_LEVEL)), 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
