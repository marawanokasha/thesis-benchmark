{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K40m (CNMeM is disabled, cuDNN 5105)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "from functools import partial\n",
    "\n",
    "from thesis.utils.metrics import *\n",
    "\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IS_SAMPLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SVM_SEED = 1234\n",
    "DOC2VEC_SEED = 1234\n",
    "WORD2VEC_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUMBER_INDICATOR = \"number_inidicator\"\n",
    "CURRENCY_INDICATOR = \"currency_inidicator\"\n",
    "CHEMICAL_INDICATOR = \"chemical_inidicator\"\n",
    "MIN_WORD_COUNT = 100\n",
    "MIN_SIZE = 0\n",
    "NUM_CORES = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GLOBAL_VARS = namedtuple('GLOBAL_VARS', ['MODEL_NAME', 'DOC2VEC_MODEL_NAME', 'DOC2VEC_MODEL', \n",
    "                                         'SVM_MODEL_NAME', 'NN_MODEL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VOCAB_MODEL = \"vocab_model\"\n",
    "MODEL_PREFIX = \"model\"\n",
    "VALIDATION_MATRIX = \"validation_matrix.pkl\"\n",
    "TEST_MATRIX = \"test_matrix.pkl\"\n",
    "METRICS = \"metrics.pkl\"\n",
    "CLASSIFIER = \"classifier.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_PARAMETER_SEARCH_PREFIX = \"{}_batch_{}_nn_parameter_searches.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training_file = \"/home/local/shalaby/docs_output_sample_100.json\"\n",
    "\n",
    "root_location = \"/mnt/data2/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "doc2vec_model_save_location = os.path.join(root_location, \"parameter_search_doc2vec_models_new\", \"full\")\n",
    "nn_parameter_search_location = os.path.join(root_location, \"nn_parameter_search\")\n",
    "if not os.path.exists(doc2vec_model_save_location):\n",
    "    os.makedirs(doc2vec_model_save_location)\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL)):\n",
    "    os.makedirs(os.path.join(doc2vec_model_save_location, VOCAB_MODEL))\n",
    "\n",
    "training_file = root_location + \"docs_output.json\"\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "classification_index_file = exports_location + \"classification_index.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "valid_classes_file = exports_location + \"valid_classes.pkl\"\n",
    "valid_subclasses_file = exports_location + \"valid_subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "training_docs_list_file = exports_location + \"training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"validation_docs_list.pkl\"\n",
    "test_docs_list_file = exports_location + \"test_docs_list.pkl\"\n",
    "\n",
    "preprocessed_location = root_location + \"preprocessed_data/\"\n",
    "\n",
    "training_preprocessed_files_prefix = preprocessed_location + \"training_docs_merged_data_preprocessed-\"\n",
    "training_preprocessed_docids_files_prefix = preprocessed_location + \"training_docs_merged_docids_preprocessed-\"\n",
    "validation_preprocessed_files_prefix = preprocessed_location + \"validation_docs_merged_data_preprocessed-\"\n",
    "validation_preprocessed_docids_files_prefix = preprocessed_location + \"validation_docs_merged_docids_preprocessed-\"\n",
    "test_preprocessed_files_prefix = preprocessed_location + \"test_docs_merged_data_preprocessed-\"\n",
    "test_preprocessed_docids_files_prefix = preprocessed_location + \"test_docs_merged_docids_preprocessed-\"\n",
    "\n",
    "word2vec_questions_file = result = root_location + 'tensorflow/word2vec/questions-words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.6 s, sys: 812 ms, total: 17.4 s\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "valid_classes = pickle.load(open(valid_classes_file))\n",
    "valid_subclasses = pickle.load(open(valid_subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "# classifications_index = pickle.load(open(classification_index_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286325"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321473"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ensure_disk_location_exists(location):\n",
    "    if not os.path.exists(location):\n",
    "        os.makedirs(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_docs_with_inference(doc2vec_model, doc_classification_map, classifications,\n",
    "                                           docs_list, file_to_write, preprocessed_files_prefix,\n",
    "                                           preprocessed_docids_files_prefix):\n",
    "    \"\"\"\n",
    "    Use the trained doc2vec model to get the paragraph vector representations of the validation or test documents\n",
    "    \"\"\"\n",
    "\n",
    "    def infer_one_doc(doc_tuple):\n",
    "        # doc2vec_model.random = np.random.RandomState(DOC2VEC_SEED)\n",
    "        doc_id, doc_tokens = doc_tuple\n",
    "        rep = doc2vec_model.infer_vector(doc_tokens)\n",
    "        return (doc_id, rep)\n",
    "\n",
    "\n",
    "    one_hot_encoder = OneHotEncoder(classifications)\n",
    "    classifications_set = set(classifications)\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write)):\n",
    "        info(\"===== Loading inference vectors\")\n",
    "        inference_labels = []\n",
    "        inference_vectors_matrix = pickle.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write)))\n",
    "        info(\"Loaded inference vectors matrix\")\n",
    "        for i, doc_id in enumerate(docs_list):\n",
    "            curr_doc_labels = set(doc_classification_map[doc_id]) & classifications_set\n",
    "            inference_labels.append(one_hot_encoder.get_label_vector(curr_doc_labels))\n",
    "            if i % 100000 == 0:\n",
    "                info(\"Finished {} in validation loading\".format(i))\n",
    "        inference_labels = np.array(inference_labels, dtype=np.int8)\n",
    "    else:\n",
    "        inference_documents_reps = {}\n",
    "        inference_vectors = []\n",
    "        inference_labels = []\n",
    "        info(\"===== Getting vectors with inference\")\n",
    "\n",
    "\n",
    "        # Multi-threaded inference\n",
    "        inference_docs_iterator = DocumentBatchGenerator(preprocessed_files_prefix,\n",
    "                                                          preprocessed_docids_files_prefix, batch_size=None)\n",
    "        generator_func = inference_docs_iterator.__iter__()\n",
    "        pool = ThreadPool(NUM_CORES)\n",
    "        # map consumes the whole iterator on the spot, so we have to use itertools.islice to fake mini-batching\n",
    "        mini_batch_size = 1000\n",
    "        while True:\n",
    "            threaded_reps_partial = pool.map(infer_one_doc, itertools.islice(generator_func, mini_batch_size))\n",
    "            info(\"Finished: {}\".format(str(inference_docs_iterator.curr_index)))\n",
    "            if threaded_reps_partial:\n",
    "                # threaded_reps.extend(threaded_reps_partial)\n",
    "                inference_documents_reps.update(threaded_reps_partial)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # create matrix for the inferred vectors\n",
    "        for doc_id in docs_list:\n",
    "            inference_vectors.append(inference_documents_reps[doc_id])\n",
    "            curr_doc_labels = set(doc_classification_map[doc_id]) & classifications_set\n",
    "            inference_labels.append(one_hot_encoder.get_label_vector(curr_doc_labels))\n",
    "        inference_vectors_matrix = np.array(inference_vectors)\n",
    "        inference_labels = np.array(inference_labels, dtype=np.int8)\n",
    "        pickle.dump(inference_vectors_matrix,\n",
    "                    open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write), 'w'))\n",
    "\n",
    "    return inference_vectors_matrix, inference_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class OneHotEncoder():\n",
    "    \n",
    "    def __init__(self, classifications):\n",
    "        self.classifications = classifications\n",
    "        self.one_hot_indices = {}\n",
    "\n",
    "        # convert character classifications to bit vectors\n",
    "        for i, clssf in enumerate(classifications):\n",
    "            bits = [0] * len(classifications)\n",
    "            bits[i] = 1\n",
    "            self.one_hot_indices[clssf] = i\n",
    "    \n",
    "    def get_label_vector(self, labels):\n",
    "        \"\"\"\n",
    "        classes: array of string with the classes assigned to the instance\n",
    "        \"\"\"\n",
    "        output_vector = [0] * len(self.classifications)\n",
    "        for label in labels:\n",
    "            index = self.one_hot_indices[label]\n",
    "            output_vector[index] = 1\n",
    "            \n",
    "        return output_vector\n",
    "\n",
    "def get_training_data(doc2vec_model, classifications):\n",
    "    one_hot_encoder = OneHotEncoder(classifications)\n",
    "    classifications_set = set(classifications)\n",
    "    training_data = []\n",
    "    training_labels_mat = np.zeros((len(training_docs_list), len(classifications)), dtype=np.int8)\n",
    "#     training_data_mat = np.zeros((len(training_docs_list), DOC2VEC_SIZE), dtype=np.float32)\n",
    "    for i,doc_id in enumerate(training_docs_list):\n",
    "        # converting from memmap to a normal array\n",
    "#         normal_array = []\n",
    "#         normal_array[:] = doc2vec_model.docvecs[doc_id][:]\n",
    "        normal_array = doc2vec_model.docvecs[doc_id]\n",
    "        training_data.append(normal_array)\n",
    "#         eligible_classifications = [clssf for clssf in doc_classification_map[doc_id] if clssf in classifications]\n",
    "        eligible_classifications = set(doc_classification_map[doc_id]) & classifications_set\n",
    "        training_labels_mat[i][:] = one_hot_encoder.get_label_vector(eligible_classifications)\n",
    "        if i % 100000 == 0:\n",
    "            info(\"Finished {} in training\".format(i))\n",
    "    info(\"doing matrix creation\")\n",
    "    training_data_mat = np.array(training_data)\n",
    "#     training_labels_mat = np.array(training_labels, dtype=np.int8)\n",
    "    del training_data\n",
    "    return training_data_mat, training_labels_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DocumentBatchGenerator(object):\n",
    "    def __init__(self, filename_prefix, filename_docids_prefix, batch_size=10000 ):\n",
    "        \"\"\"\n",
    "        batch_size cant be > 10,000 due to a limitation in doc2vec training, \n",
    "        None means no batching (only use for inference)\n",
    "        \"\"\"\n",
    "        assert batch_size <= 10000 or batch_size is None\n",
    "        self.filename_prefix = filename_prefix\n",
    "        self.filename_docids_prefix = filename_docids_prefix\n",
    "        self.curr_lines = []\n",
    "        self.curr_docids = []\n",
    "        self.batch_size = batch_size\n",
    "        self.curr_index = 0\n",
    "        self.batch_end = -1\n",
    "    def load_new_batch_in_memory(self):\n",
    "        del self.curr_lines, self.curr_docids\n",
    "        self.curr_lines, self.docids = [], []\n",
    "        info(\"Loading new batch for index: {}\".format(self.curr_index) )\n",
    "        try:\n",
    "            with open(self.filename_prefix + str(self.curr_index)) as preproc_file:\n",
    "                for line in preproc_file:\n",
    "                    self.curr_lines.append(line.split(\" \"))\n",
    "#                     if i % 1000 == 0:\n",
    "#                         print i\n",
    "            self.curr_docids = pickle.load(open(self.filename_docids_prefix + str(self.curr_index), \"r\"))\n",
    "            self.batch_end = self.curr_index + len(self.curr_lines) -1 \n",
    "            info(\"Finished loading new batch\")\n",
    "        except IOError:\n",
    "            info(\"No more batches to load, exiting at index: {}\".format(self.curr_index))\n",
    "            raise StopIteration()\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            if self.curr_index > self.batch_end:\n",
    "                self.load_new_batch_in_memory()\n",
    "            for (doc_id, tokens) in zip(self.curr_docids, self.curr_lines):\n",
    "                if self.batch_size is not None:\n",
    "                    curr_batch_iter = 0\n",
    "                    # divide the document to batches according to the batch size\n",
    "                    while curr_batch_iter < len(tokens):\n",
    "                        yield LabeledSentence(words=tokens[curr_batch_iter: curr_batch_iter + self.batch_size], tags=[doc_id])\n",
    "                        curr_batch_iter += self.batch_size\n",
    "                else:\n",
    "                    yield doc_id, tokens\n",
    "                self.curr_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Specific Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_keras_nn_model(input_size, output_size, \n",
    "                          first_hidden_layer_size, first_hidden_layer_activation, \n",
    "                          second_hidden_layer_size, second_hidden_layer_activation, \n",
    "                          input_dropout_do, hidden_dropout_do):\n",
    "    \n",
    "    doc_input = Input(shape=(DOC2VEC_SIZE,), name='doc_input')\n",
    "    if input_dropout_do:\n",
    "        hidden = Dropout(0.7)(doc_input)\n",
    "    hidden = Dense(first_hidden_layer_size, activation=first_hidden_layer_activation, \n",
    "                   name='hidden_layer_{}'.format(first_hidden_layer_activation))(doc_input if not input_dropout_do else hidden)\n",
    "    if hidden_dropout_do:\n",
    "        hidden = Dropout(0.5)(hidden)\n",
    "    if second_hidden_layer_size is not None:\n",
    "        hidden = Dense(second_hidden_layer_size, activation=second_hidden_layer_activation, \n",
    "                       name='hidden_layer2_{}'.format(second_hidden_layer_activation))(hidden)\n",
    "    softmax_output = Dense(output_size, activation='sigmoid', name='softmax_output')(hidden)\n",
    "\n",
    "    model = Model(input=doc_input, output=softmax_output)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_binary_0_5 = lambda x: 1 if x > 0.5 else 0\n",
    "get_binary_0_5 = np.vectorize(get_binary_0_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stopper_deltas = {\n",
    "    'sections': 0.0001,\n",
    "    'classes': 0.00001,\n",
    "    'subclasses': 0.00001\n",
    "}\n",
    "early_stopper_patience = {\n",
    "    'sections': 5,\n",
    "    'classes': 10,\n",
    "    'subclasses': 10\n",
    "}\n",
    "epochs_before_validation = {\n",
    "    'sections': 10,\n",
    "    'classes': 50,\n",
    "    'subclasses': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2vec_methods_dict = {\n",
    "#     'doc2vec_size_50_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None' : 7,\n",
    "#     'doc2vec_size_50_w_8_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None' : 8,\n",
    "#     'doc2vec_size_50_w_8_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 8,\n",
    "#     'doc2vec_size_100_w_2_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 9,\n",
    "#     'doc2vec_size_100_w_5_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 18,\n",
    "#     'doc2vec_size_100_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 10,\n",
    "#     'doc2vec_size_100_w_8_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None': 4,\n",
    "#     'doc2vec_size_100_w_8_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 7,\n",
    "    'doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None': 8,\n",
    "#     'doc2vec_size_200_w_2_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 7,\n",
    "    'doc2vec_size_200_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None': 6,\n",
    "#     'doc2vec_size_200_w_4_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 8,\n",
    "    'doc2vec_size_200_w_8_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None': 14,\n",
    "#     'doc2vec_size_200_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 6,\n",
    "#     'doc2vec_size_200_w_8_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 8,\n",
    "#     'doc2vec_size_500_w_8_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None': 8,\n",
    "#     'doc2vec_size_500_w_8_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 8,\n",
    "#     'doc2vec_size_1000_w_8_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 6,\n",
    "#     'doc2vec_size_1000_w_8_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifications = valid_subclasses\n",
    "classifications_type = 'subclasses'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MetricsCallback(keras.callbacks.Callback):\n",
    "    \n",
    "    EPOCHS_BEFORE_VALIDATION = epochs_before_validation[classifications_type]\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.epoch_index = 0\n",
    "        self.val_loss_reductions = 0\n",
    "        self.metrics_dict = {}\n",
    "        self.best_val_loss = np.iinfo(np.int32).max\n",
    "        self.best_weights = None\n",
    "        self.best_validation_metrics = None\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epoch_index += 1\n",
    "        if logs['val_loss'] < self.best_val_loss:\n",
    "            self.val_loss_reductions += 1\n",
    "            self.best_val_loss = logs['val_loss']\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            print '\\r    \\r' # to remove the previous line of verbose output of model fit\n",
    "            time.sleep(0.2)\n",
    "            info('Found lower val loss for epoch {} => {}'.format(self.epoch_index, round(logs['val_loss'], 5)))\n",
    "            if self.val_loss_reductions % MetricsCallback.EPOCHS_BEFORE_VALIDATION == 0:\n",
    "                \n",
    "                info('Validation Loss Reduced {} times'.format(self.val_loss_reductions))\n",
    "                info('Evaluating on Validation Data')\n",
    "                yvp = self.model.predict(Xv)\n",
    "                yvp_binary = get_binary_0_5(yvp)\n",
    "                info('Generating Validation Metrics')\n",
    "                validation_metrics = get_metrics(yv, yvp, yvp_binary)\n",
    "                print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "                    validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "                    validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "                self.metrics_dict[self.epoch_index] = validation_metrics\n",
    "#                 self.best_validation_metrics = validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NN_OUTPUT_NEURONS = len(classifications)\n",
    "\n",
    "EARLY_STOPPER_MIN_DELTA = early_stopper_deltas[classifications_type]\n",
    "EARLY_STOPPER_PATIENCE = early_stopper_patience[classifications_type]\n",
    "\n",
    "NN_MAX_EPOCHS = 100\n",
    "NN_RANDOM_SEARCH_BUDGET = 20\n",
    "NN_PARAM_SAMPLE_SEED = 1234\n",
    "\n",
    "NN_BATCH_SIZE = 4096\n",
    "\n",
    "MODEL_VERBOSITY = 1\n",
    "\n",
    "to_skip = []\n",
    "\n",
    "load_existing_results = True\n",
    "save_results = True\n",
    "\n",
    "\n",
    "first_hidden_layer_sizes = [100,200,500]\n",
    "# first_hidden_layer_sizes = [1000,2000]\n",
    "# second_hidden_layer_sizes = [1000,2000,3000,4000]\n",
    "second_hidden_layer_sizes = [None,500,1000,2000]\n",
    "first_hidden_layer_activations = ['relu','sigmoid', 'tanh']\n",
    "second_hidden_layer_activations = ['relu','sigmoid', 'tanh']\n",
    "# first_hidden_layer_activations = ['relu']\n",
    "# second_hidden_layer_activations = ['relu']\n",
    "# input_dropout_options = [False, True]\n",
    "# hidden_dropout_options = [False, True]\n",
    "input_dropout_options = [False]\n",
    "hidden_dropout_options = [False, True]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Uncomment for Specific Configuration\n",
    "NN_RANDOM_SEARCH_BUDGET = 1\n",
    "first_hidden_layer_sizes = [500]\n",
    "second_hidden_layer_sizes = [2000]\n",
    "first_hidden_layer_activations = ['tanh']\n",
    "second_hidden_layer_activations = ['relu']\n",
    "input_dropout_options = [False]\n",
    "hidden_dropout_options = [True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-20 13:05:53,788 : INFO : loading Doc2Vec object from /mnt/data2/shalaby/parameter_search_doc2vec_models_new/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None->8\n"
     ]
    }
   ],
   "source": [
    "for (doc2vec_method_name, epoch) in sorted(doc2vec_methods_dict.items(), key=lambda x: x[0]):\n",
    "    print '********* {}->{}'.format(doc2vec_method_name, epoch)\n",
    "\n",
    "    TRAINING_METRICS_FILENAME = '{}_training_metrics.pkl'.format(classifications_type)\n",
    "    VALIDATION_METRICS_FILENAME= '{}_validation_metrics.pkl'.format(classifications_type)\n",
    "    TEST_METRICS_FILENAME = '{}_test_metrics.pkl'.format(classifications_type)\n",
    "\n",
    "    placeholder_model_name = doc2vec_method_name\n",
    "    placeholder_model_name = os.path.join(placeholder_model_name, \"epoch_{}\")\n",
    "    GLOBAL_VARS.DOC2VEC_MODEL_NAME = doc2vec_method_name\n",
    "    \n",
    "    GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "    \n",
    "    # if we have the model, just load it, otherwise train the previous model\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)):\n",
    "        doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "        GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "    else:\n",
    "        info(\"Couldnt find the doc2vec model with epoch {}\".format(epoch))\n",
    "        raise Exception()\n",
    "\n",
    "\n",
    "    info('Getting training Data')\n",
    "    X, y = get_training_data(doc2vec_model, classifications)\n",
    "\n",
    "    info('Getting Validation Embeddings')\n",
    "    Xv, yv = get_docs_with_inference(doc2vec_model, doc_classification_map, classifications, \n",
    "                                     validation_docs_list, VALIDATION_MATRIX, \n",
    "                                     validation_preprocessed_files_prefix, \n",
    "                                     validation_preprocessed_docids_files_prefix)\n",
    "    # create nn parameter search directory\n",
    "    if not os.path.exists(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME)):\n",
    "        os.makedirs(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME))\n",
    "        \n",
    "    DOC2VEC_SIZE = X.shape[1]\n",
    "        \n",
    "    param_sampler = ParameterSampler({\n",
    "        'first_hidden_layer_size':first_hidden_layer_sizes,\n",
    "        'first_hidden_layer_activation':first_hidden_layer_activations,\n",
    "        'second_hidden_layer_size':second_hidden_layer_sizes,\n",
    "        'second_hidden_layer_activation':second_hidden_layer_activations,\n",
    "        'input_dropout':input_dropout_options,\n",
    "        'hidden_dropout':hidden_dropout_options\n",
    "    }, n_iter=NN_RANDOM_SEARCH_BUDGET, random_state=NN_PARAM_SAMPLE_SEED)\n",
    "    \n",
    "    param_results_dict = {}\n",
    "    if load_existing_results:\n",
    "        param_results_path = os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                           NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE)))\n",
    "        if os.path.exists(param_results_path):\n",
    "            param_results_dict = pickle.load(open(param_results_path))\n",
    "        else:\n",
    "            info('No Previous results exist in {}'.format(param_results_path))\n",
    "    \n",
    "    for parameters in param_sampler:\n",
    "        start_time = time.time()\n",
    "        first_hidden_layer_size = parameters['first_hidden_layer_size']\n",
    "        first_hidden_layer_activation = parameters['first_hidden_layer_activation']\n",
    "        second_hidden_layer_size = parameters['second_hidden_layer_size']\n",
    "        second_hidden_layer_activation = parameters['second_hidden_layer_activation']\n",
    "        input_dropout_do = parameters['input_dropout']\n",
    "        hidden_dropout_do = parameters['hidden_dropout']\n",
    "\n",
    "        GLOBAL_VARS.NN_MODEL_NAME = 'nn_1st-size_{}_1st-act_{}_2nd-size_{}_2nd-act_{}_in-drop_{}_hid-drop_{}'.format(\n",
    "            first_hidden_layer_size, first_hidden_layer_activation, second_hidden_layer_size, \n",
    "            second_hidden_layer_activation, input_dropout_do, hidden_dropout_do\n",
    "        )\n",
    "        if GLOBAL_VARS.NN_MODEL_NAME in param_results_dict.keys() or GLOBAL_VARS.NN_MODEL_NAME in to_skip:\n",
    "            print \"skipping: {}\".format(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "            continue\n",
    "#         if first_hidden_layer_size < DOC2VEC_SIZE or second_hidden_layer_size < NN_OUTPUT_NEURONS:\n",
    "#             print \"skipping: {} due to 1st layer size {} < {} or 2nd layer size {} < {}\".format(GLOBAL_VARS.NN_MODEL_NAME,\n",
    "#                                                                                                 first_hidden_layer_size, DOC2VEC_SIZE, \n",
    "#                                                                                                 second_hidden_layer_size, NN_OUTPUT_NEURONS)\n",
    "#             continue\n",
    "            \n",
    "\n",
    "        info('***************************************************************************************')\n",
    "        info(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "\n",
    "        model = create_keras_nn_model(DOC2VEC_SIZE, NN_OUTPUT_NEURONS, \n",
    "                                      first_hidden_layer_size, first_hidden_layer_activation, \n",
    "                                      second_hidden_layer_size, second_hidden_layer_activation, \n",
    "                                      input_dropout_do, hidden_dropout_do)\n",
    "        model.summary()\n",
    "\n",
    "        early_stopper = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=EARLY_STOPPER_MIN_DELTA, \\\n",
    "                                                      patience=EARLY_STOPPER_PATIENCE, verbose=1, mode='auto')\n",
    "        metrics_callback = MetricsCallback()\n",
    "\n",
    "        # Model Fitting\n",
    "        %time history = model.fit(x=X, y=y, validation_data=(Xv,yv), batch_size=NN_BATCH_SIZE, \\\n",
    "                                  nb_epoch=NN_MAX_EPOCHS, verbose=MODEL_VERBOSITY, callbacks=[early_stopper, metrics_callback])\n",
    "\n",
    "        \n",
    "        # using the recorded weights of the best recorded validation loss\n",
    "        last_model_weights = model.get_weights()\n",
    "        info('Evaluating on Validation Data using saved best weights')\n",
    "        model.set_weights(metrics_callback.best_weights)\n",
    "        yvp = model.predict(Xv)\n",
    "        yvp_binary = get_binary_0_5(yvp)\n",
    "        #print yvp\n",
    "        info('Generating Validation Metrics')\n",
    "        validation_metrics = get_metrics(yv, yvp, yvp_binary)\n",
    "        print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "            validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "            validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "        best_validation_metrics = validation_metrics\n",
    "\n",
    "        param_results_dict[GLOBAL_VARS.NN_MODEL_NAME] = dict()\n",
    "        param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['best_validation_metrics'] = best_validation_metrics\n",
    "        param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['epochs'] = len(history.history['val_loss'])\n",
    "        param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['best_weights'] = metrics_callback.best_weights\n",
    "#         param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['last_weights'] = last_model_weights\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['duration'] =  duration\n",
    "\n",
    "        del history, last_model_weights, metrics_callback, model\n",
    "\n",
    "    del doc2vec_model\n",
    "    \n",
    "    if save_results:\n",
    "        pickle.dump(param_results_dict, open(os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                                       NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE))), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.34 s, sys: 44 ms, total: 1.39 s\n",
      "Wall time: 1.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pickle.dump(param_results_dict, open(os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                           NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE))), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== NN: nn_1st-size_500_1st-act_sigmoid_2nd-size_None_2nd-act_sigmoid_in-drop_False_hid-drop_True\n",
      "Epochs => 26\n",
      "Best Val: Coverage Error => 3.1476 | F1 Micro => 0.5901 | F1 Macro => 0.1052 | Top 3 => 0.8330\n",
      "========== NN: nn_1st-size_500_1st-act_sigmoid_2nd-size_2000_2nd-act_relu_in-drop_False_hid-drop_True\n",
      "Epochs => 44\n",
      "Best Val: Coverage Error => 2.9189 | F1 Micro => 0.6550 | F1 Macro => 0.1744 | Top 3 => 0.8492\n",
      "========== NN: nn_1st-size_500_1st-act_sigmoid_2nd-size_1000_2nd-act_relu_in-drop_False_hid-drop_True\n",
      "Epochs => 45\n",
      "Best Val: Coverage Error => 2.9413 | F1 Micro => 0.6516 | F1 Macro => 0.1737 | Top 3 => 0.8484\n",
      "========== NN: nn_1st-size_500_1st-act_relu_2nd-size_2000_2nd-act_relu_in-drop_False_hid-drop_True\n",
      "Epochs => 41\n",
      "Best Val: Coverage Error => 2.9182 | F1 Micro => 0.6500 | F1 Macro => 0.1717 | Top 3 => 0.8481\n",
      "========== NN: nn_1st-size_100_1st-act_sigmoid_2nd-size_1000_2nd-act_sigmoid_in-drop_False_hid-drop_True\n",
      "Epochs => 25\n",
      "Best Val: Coverage Error => 3.5353 | F1 Micro => 0.5666 | F1 Macro => 0.0768 | Top 3 => 0.8014\n",
      "========== NN: nn_1st-size_500_1st-act_tanh_2nd-size_1000_2nd-act_sigmoid_in-drop_False_hid-drop_True\n",
      "Epochs => 39\n",
      "Best Val: Coverage Error => 2.8958 | F1 Micro => 0.6604 | F1 Macro => 0.1869 | Top 3 => 0.8547\n",
      "========== NN: nn_1st-size_200_1st-act_sigmoid_2nd-size_500_2nd-act_tanh_in-drop_False_hid-drop_True\n",
      "Epochs => 46\n",
      "Best Val: Coverage Error => 3.1170 | F1 Micro => 0.6351 | F1 Macro => 0.1335 | Top 3 => 0.8329\n",
      "========== NN: nn_1st-size_500_1st-act_tanh_2nd-size_None_2nd-act_tanh_in-drop_False_hid-drop_True\n",
      "Epochs => 100\n",
      "Best Val: Coverage Error => 3.4469 | F1 Micro => 0.5866 | F1 Macro => 0.1396 | Top 3 => 0.8203\n",
      "========== NN: nn_1st-size_500_1st-act_tanh_2nd-size_None_2nd-act_relu_in-drop_False_hid-drop_True\n",
      "Epochs => 100\n",
      "Best Val: Coverage Error => 3.4553 | F1 Micro => 0.5853 | F1 Macro => 0.1370 | Top 3 => 0.8195\n",
      "========== NN: nn_1st-size_100_1st-act_tanh_2nd-size_500_2nd-act_tanh_in-drop_False_hid-drop_True\n",
      "Epochs => 57\n",
      "Best Val: Coverage Error => 3.1239 | F1 Micro => 0.6270 | F1 Macro => 0.1342 | Top 3 => 0.8312\n",
      "========== NN: nn_1st-size_200_1st-act_sigmoid_2nd-size_2000_2nd-act_tanh_in-drop_False_hid-drop_True\n",
      "Epochs => 62\n",
      "Best Val: Coverage Error => 3.1238 | F1 Micro => 0.6292 | F1 Macro => 0.1283 | Top 3 => 0.8316\n",
      "========== NN: nn_1st-size_200_1st-act_sigmoid_2nd-size_1000_2nd-act_sigmoid_in-drop_False_hid-drop_True\n",
      "Epochs => 25\n",
      "Best Val: Coverage Error => 3.1244 | F1 Micro => 0.5942 | F1 Macro => 0.1161 | Top 3 => 0.8308\n",
      "========== NN: nn_1st-size_500_1st-act_relu_2nd-size_500_2nd-act_tanh_in-drop_False_hid-drop_True\n",
      "Epochs => 59\n",
      "Best Val: Coverage Error => 2.7937 | F1 Micro => 0.6639 | F1 Macro => 0.1817 | Top 3 => 0.8574\n",
      "========== NN: nn_1st-size_500_1st-act_relu_2nd-size_500_2nd-act_sigmoid_in-drop_False_hid-drop_True\n",
      "Epochs => 28\n",
      "Best Val: Coverage Error => 2.8626 | F1 Micro => 0.6573 | F1 Macro => 0.1986 | Top 3 => 0.8520\n",
      "========== NN: nn_1st-size_200_1st-act_sigmoid_2nd-size_None_2nd-act_sigmoid_in-drop_False_hid-drop_True\n",
      "Epochs => 34\n",
      "Best Val: Coverage Error => 3.4741 | F1 Micro => 0.5511 | F1 Macro => 0.0667 | Top 3 => 0.8127\n",
      "========== NN: nn_1st-size_200_1st-act_sigmoid_2nd-size_2000_2nd-act_sigmoid_in-drop_False_hid-drop_True\n",
      "Epochs => 27\n",
      "Best Val: Coverage Error => 3.0726 | F1 Micro => 0.5982 | F1 Macro => 0.1229 | Top 3 => 0.8346\n",
      "========== NN: nn_1st-size_200_1st-act_tanh_2nd-size_2000_2nd-act_relu_in-drop_False_hid-drop_True\n",
      "Epochs => 23\n",
      "Best Val: Coverage Error => 2.9561 | F1 Micro => 0.6482 | F1 Macro => 0.1660 | Top 3 => 0.8486\n",
      "========== NN: nn_1st-size_500_1st-act_sigmoid_2nd-size_None_2nd-act_tanh_in-drop_False_hid-drop_True\n",
      "Epochs => 26\n",
      "Best Val: Coverage Error => 3.1507 | F1 Micro => 0.5906 | F1 Macro => 0.1046 | Top 3 => 0.8329\n",
      "========== NN: nn_1st-size_100_1st-act_tanh_2nd-size_1000_2nd-act_tanh_in-drop_False_hid-drop_True\n",
      "Epochs => 54\n",
      "Best Val: Coverage Error => 3.1339 | F1 Micro => 0.6268 | F1 Macro => 0.1379 | Top 3 => 0.8320\n",
      "========== NN: nn_1st-size_100_1st-act_sigmoid_2nd-size_None_2nd-act_sigmoid_in-drop_False_hid-drop_True\n",
      "Epochs => 31\n",
      "Best Val: Coverage Error => 3.9217 | F1 Micro => 0.5065 | F1 Macro => 0.0443 | Top 3 => 0.7862\n"
     ]
    }
   ],
   "source": [
    "for key in param_results_dict.keys():\n",
    "    print('========== NN: {}'.format(key))\n",
    "    val = param_results_dict[key]\n",
    "#     val_metrics = val['last_validation_metrics']\n",
    "    val_metrics2 =  val['best_validation_metrics']\n",
    "    \n",
    "    print('Epochs => {}'.format(val['epochs']))\n",
    "    print('Best Val: Coverage Error => {:.4f} | F1 Micro => {:.4f} | F1 Macro => {:.4f} | Top 3 => {:.4f}'.format(val_metrics2['coverage_error'], \n",
    "                                                                                        val_metrics2['f1_micro'], val_metrics2['f1_macro'],\n",
    "                                                                                        val_metrics2['top_3']))\n",
    "#     print('Best Val Loss => {}'.format(val[\"metrics_callback\"].best_val_loss))\n",
    "#     print('Last Val: Coverage Error => {:.4f} | F1 Micro => {:.4f} | F1 Macro => {:.4f} | Top 3 => {:.4f}'.format(val_metrics['coverage_error'], \n",
    "#                                                                                         val_metrics['f1_micro'], val_metrics['f1_macro'],\n",
    "#                                                                                         val_metrics['top_3']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run network for specific configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_BATCH_SIZE = 4096\n",
    "NN_OUTPUT_NEURONS = len(classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "first_hidden_layer_size = 500\n",
    "first_hidden_layer_activation = 'tanh'\n",
    "second_hidden_layer_size = 2000\n",
    "second_hidden_layer_activation = 'sigmoid'\n",
    "input_dropout_do = False\n",
    "hidden_dropout_do = True\n",
    "\n",
    "#     print \"===================================================================================\\n\" + \\\n",
    "#           \"========== 1st Layer Size: {}, 1st Layer Activation: {}, \\n 2nd Layer Size: {}, 2nd Layer Activation: {}, \\n\" + \\\n",
    "#           \"Input Dropout: {}, Hidden Dropout: {} \\n\" + \\\n",
    "#           \"==========================\".format(first_hidden_layer_size, first_hidden_layer_activation, \n",
    "#                                                 second_hidden_layer_size, second_hidden_layer_activation, \n",
    "#                                                 input_dropout_do, hidden_dropout_do)\n",
    "\n",
    "GLOBAL_VARS.NN_MODEL_NAME = 'nn_1st-size_{}_1st-act_{}_2nd-size_{}_2nd-act_{}_in-drop_{}_hid-drop_{}'.format(\n",
    "    first_hidden_layer_size, first_hidden_layer_activation, second_hidden_layer_size, \n",
    "    second_hidden_layer_activation, input_dropout_do, hidden_dropout_do\n",
    ")\n",
    "if GLOBAL_VARS.NN_MODEL_NAME in param_results_dict.keys():\n",
    "    print \"Should be skipping: {}\".format(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "\n",
    "info('***************************************************************************************')\n",
    "info(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "\n",
    "model = create_keras_nn_model(DOC2VEC_SIZE, NN_OUTPUT_NEURONS, \n",
    "                              first_hidden_layer_size, first_hidden_layer_activation, \n",
    "                              second_hidden_layer_size, second_hidden_layer_activation, \n",
    "                              input_dropout_do, hidden_dropout_do)\n",
    "model.summary()\n",
    "\n",
    "early_stopper = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=EARLY_STOPPER_MIN_DELTA, \\\n",
    "                                              patience=EARLY_STOPPER_PATIENCE, verbose=1, mode='auto')\n",
    "metrics_callback = MetricsCallback()\n",
    "\n",
    "# Model Fitting\n",
    "%time history = model.fit(x=X, y=y, validation_data=(Xv,yv), batch_size=NN_BATCH_SIZE, \\\n",
    "                          nb_epoch=NN_MAX_EPOCHS, verbose=0, callbacks=[early_stopper, metrics_callback])\n",
    "\n",
    "#     info('Evaluating on Training Data')\n",
    "#     yp = model.predict(X, batch_size=NN_BATCH_SIZE)\n",
    "#     yp_binary = get_binary_0_5(yp)\n",
    "#     #print yp\n",
    "#     info('Generating Training Metrics')\n",
    "#     training_metrics = get_metrics(y, yp, yp_binary)\n",
    "#     print \"** Training Metrics: Cov Err: {:.3f}, Avg Labels: {:.3f}, \\n\\t\\t Top 1: {:.3f}, Top 3: {:.3f}, Top 5: {:.3f}, \\n\\t\\t F1 Micro: {:.3f}, F1 Macro: {:.3f}, Total Pos: {:,d}\".format(\n",
    "#         training_metrics['coverage_error'], training_metrics['average_num_of_labels'], \n",
    "#         training_metrics['top_1'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "#         training_metrics['f1_micro'],training_metrics['f1_macro'],  training_metrics['total_positive'])\n",
    "\n",
    "info('Evaluating on Validation Data using last weights')\n",
    "yvp = model.predict(Xv)\n",
    "yvp_binary = get_binary_0_5(yvp)\n",
    "#print yvp\n",
    "info('Generating Validation Metrics')\n",
    "validation_metrics = get_metrics(yv, yvp, yvp_binary)\n",
    "print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "    validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "last_validation_metrics = validation_metrics\n",
    "\n",
    "# using the recorded weights of the best recorded validation loss\n",
    "last_model_weights = model.get_weights()\n",
    "info('Evaluating on Validation Data using saved best weights')\n",
    "model.set_weights(metrics_callback.best_weights)\n",
    "yvp = model.predict(Xv)\n",
    "yvp_binary = get_binary_0_5(yvp)\n",
    "#print yvp\n",
    "info('Generating Validation Metrics')\n",
    "validation_metrics = get_metrics_detailed(yv, yvp, yvp_binary)\n",
    "print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "    validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "best_validation_metrics = validation_metrics\n",
    "\n",
    "\n",
    "param_results_dict[GLOBAL_VARS.NN_MODEL_NAME] = dict()\n",
    "#     param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['training_metrics'] = training_metrics\n",
    "param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['last_validation_metrics'] = last_validation_metrics\n",
    "param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['best_validation_metrics'] = best_validation_metrics\n",
    "param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['history'] = history\n",
    "param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['metrics_callback'] = metrics_callback\n",
    "param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['last_weights'] = last_model_weights\n",
    "\n",
    "duration = time.time() - start_time\n",
    "param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['duration'] =  duration\n",
    "\n",
    "pickle.dump(param_results_dict, open(os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                       NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE))), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== NN: nn_1st-size_500_1st-act_softmax_2nd-size_None_2nd-act_softmax_in-drop_True_hid-drop_False\n",
      "Epochs => 100\n",
      "Best Val Loss => 0.166829405505\n",
      "Last Val: Coverage Error => 1.4960 | F1 Micro => 0.7434 | F1 Macro => 0.6293 | Top 3 => 0.9637\n",
      "Best Val: Coverage Error => 1.4960 | F1 Micro => 0.7434 | F1 Macro => 0.6293 | Top 3 => 0.9637\n",
      "========== NN: nn_1st-size_200_1st-act_tanh_2nd-size_500_2nd-act_relu_in-drop_False_hid-drop_True\n",
      "Epochs => 47\n",
      "Best Val Loss => 0.147426413828\n",
      "Last Val: Coverage Error => 1.4576 | F1 Micro => 0.7738 | F1 Macro => 0.7021 | Top 3 => 0.9697\n",
      "Best Val: Coverage Error => 1.4576 | F1 Micro => 0.7738 | F1 Macro => 0.7021 | Top 3 => 0.9697\n",
      "========== NN: nn_1st-size_200_1st-act_tanh_2nd-size_500_2nd-act_tanh_in-drop_True_hid-drop_True\n",
      "Epochs => 28\n",
      "Best Val Loss => 0.192695072037\n",
      "Last Val: Coverage Error => 1.6299 | F1 Micro => 0.6631 | F1 Macro => 0.4731 | Top 3 => 0.9455\n",
      "Best Val: Coverage Error => 1.6299 | F1 Micro => 0.6631 | F1 Macro => 0.4731 | Top 3 => 0.9455\n"
     ]
    }
   ],
   "source": [
    "for key in param_results_dict.keys():\n",
    "    print('========== NN: {}'.format(key))\n",
    "    val = param_results_dict[key]\n",
    "    val_metrics = val['last_validation_metrics']\n",
    "    val_metrics2 =  val['best_validation_metrics']\n",
    "    \n",
    "    print('Epochs => {}'.format(len(val['history'].history['val_loss'])))\n",
    "    print('Best Val Loss => {}'.format(val[\"metrics_callback\"].best_val_loss))\n",
    "    print('Last Val: Coverage Error => {:.4f} | F1 Micro => {:.4f} | F1 Macro => {:.4f} | Top 3 => {:.4f}'.format(val_metrics['coverage_error'], \n",
    "                                                                                        val_metrics['f1_micro'], val_metrics['f1_macro'],\n",
    "                                                                                        val_metrics['top_3']))\n",
    "    print('Best Val: Coverage Error => {:.4f} | F1 Micro => {:.4f} | F1 Macro => {:.4f} | Top 3 => {:.4f}'.format(val_metrics2['coverage_error'], \n",
    "                                                                                        val_metrics2['f1_micro'], val_metrics2['f1_macro'],\n",
    "                                                                                        val_metrics2['top_3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(param_results_dict, open(os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                           NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE))), 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
