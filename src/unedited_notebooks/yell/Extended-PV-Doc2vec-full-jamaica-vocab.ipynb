{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Creates the doc2vec vector embeddings for a specific configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc installation and try again.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "import gzip\n",
    "\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "from functools import partial\n",
    "\n",
    "from thesis.utils.metrics import *\n",
    "from thesis.utils.file import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Global variables used throughout the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SVM_SEED = 1234\n",
    "DOC2VEC_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MIN_WORD_COUNT = 100\n",
    "NUM_CORES = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "GLOBAL_VARS = namedtuple('GLOBAL_VARS', ['MODEL_NAME', 'DOC2VEC_MODEL_NAME', 'DOC2VEC_MODEL', \n",
    "                                         'SVM_MODEL_NAME', 'NN_MODEL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "VOCAB_MODEL = \"vocab_model\"\n",
    "MODEL_PREFIX = \"model\"\n",
    "VALIDATION_DICT = \"validation_dict.pkl\"\n",
    "TEST_MATRIX = \"test_matrix.pkl\"\n",
    "TEST_DICT = \"test_dict.pkl\"\n",
    "METRICS = \"metrics.pkl\"\n",
    "CLASSIFIER = \"classifier.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# root_location = \"/mnt/virtual-machines/data/\"\n",
    "root_location = \"/home/local/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "training_docs_list_file = exports_location + \"training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"validation_docs_list.pkl\"\n",
    "test_docs_list_file = exports_location + \"test_docs_list.pkl\"\n",
    "\n",
    "preprocessed_location = root_location + \"preprocessed_data/extended_pv_abs_desc_claims_full_chunks/\"\n",
    "\n",
    "training_preprocessed_files_prefix = preprocessed_location + \"extended_pv_training_docs_data_preprocessed-\"\n",
    "validation_preprocessed_files_prefix = preprocessed_location + \"extended_pv_validation_docs_data_preprocessed-\"\n",
    "test_preprocessed_files_prefix = preprocessed_location + \"extended_pv_test_docs_data_preprocessed-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load general data required for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30 s, sys: 1.33 s, total: 31.3 s\n",
      "Wall time: 31.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286325"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321473"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401877"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Utility functions for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ExtendedPVDocumentBatchGenerator(Process):\n",
    "    def __init__(self, filename_prefix, queue, batch_size=10000, start_file=0, offset=10000):\n",
    "        super(ExtendedPVDocumentBatchGenerator, self).__init__()\n",
    "        self.queue = queue\n",
    "        self.offset = offset\n",
    "        self.filename_prefix = filename_prefix\n",
    "        self.files_loaded = start_file - offset\n",
    "\n",
    "    def run(self):\n",
    "        cur_file = None\n",
    "        while True:\n",
    "            try:\n",
    "                if cur_file is None:\n",
    "                    info(\"Loading new file for index: {}\".format(str(self.files_loaded + self.offset)))\n",
    "#                     cur_file = gzip.open(self.filename_prefix + str(self.files_loaded + self.offset) + '.gz')\n",
    "                    cur_file = open(self.filename_prefix + str(self.files_loaded + self.offset))\n",
    "                    self.files_loaded += self.offset\n",
    "                for line in cur_file:\n",
    "                    self.queue.put(line)\n",
    "                cur_file.close()\n",
    "                cur_file = None\n",
    "            except IOError:\n",
    "                self.queue.put(False, block=True, timeout=None)\n",
    "                info(\"All files are loaded - last file: {}\".format(str(self.files_loaded + self.offset)))\n",
    "                return\n",
    "\n",
    "\n",
    "class BatchWrapper(object):\n",
    "    def __init__(self, training_preprocessed_files_prefix, buffer_size=10000, batch_size=10000, level=1, level_type=None):\n",
    "        assert batch_size <= 10000 or batch_size is None\n",
    "        self.level = level\n",
    "        self.level_type = level_type[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.q = Queue(maxsize=buffer_size)\n",
    "        self.p = ExtendedPVDocumentBatchGenerator(training_preprocessed_files_prefix, queue=self.q,\n",
    "                                                  batch_size=batch_size, start_file=0, offset=10000)\n",
    "        self.p.start()\n",
    "        self.cur_data = []\n",
    "\n",
    "    def is_correct_type(self, doc_id):\n",
    "        parts = doc_id.split(\"_\")\n",
    "        len_parts = len(parts)\n",
    "        if len_parts == self.level:\n",
    "            if len_parts == 1:\n",
    "                return True\n",
    "            if len_parts == self.level and (parts[1][0] == self.level_type or self.level_type is None):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def return_sentences(self, line):\n",
    "        line_array = tuple(line.split(\" \"))\n",
    "        doc_id = line_array[0]\n",
    "        if not self.is_correct_type(doc_id):\n",
    "            return False\n",
    "        line_array = line_array[1:]\n",
    "        len_line_array = len(line_array)\n",
    "        curr_batch_iter = 0\n",
    "        # divide the document to batches according to the batch size\n",
    "        sentences = []\n",
    "        while curr_batch_iter < len_line_array:\n",
    "            sentences.append(LabeledSentence(words=line_array[curr_batch_iter: curr_batch_iter + self.batch_size], tags=[doc_id]))\n",
    "            curr_batch_iter += self.batch_size\n",
    "        return tuple(sentences)\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            item = self.q.get(block=True)\n",
    "            if item is False:\n",
    "                raise StopIteration()\n",
    "            else:\n",
    "                sentences = self.return_sentences(item)\n",
    "                if not sentences:\n",
    "                    None\n",
    "                else:\n",
    "                    for sentence in sentences:\n",
    "                        yield sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Doc2vec and SVM Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DOC2VEC_SIZE = 200\n",
    "DOC2VEC_WINDOW = 2\n",
    "DOC2VEC_MAX_VOCAB_SIZE = None\n",
    "DOC2VEC_SAMPLE = 1e-3\n",
    "DOC2VEC_TYPE = 1\n",
    "DOC2VEC_HIERARCHICAL_SAMPLE = 0\n",
    "DOC2VEC_NEGATIVE_SAMPLE_SIZE = 10\n",
    "DOC2VEC_CONCAT = 0\n",
    "DOC2VEC_MEAN = 1\n",
    "DOC2VEC_TRAIN_WORDS = 0\n",
    "DOC2VEC_EPOCHS = 1 # we do our training manually one epoch at a time\n",
    "DOC2VEC_MAX_EPOCHS = 8\n",
    "REPORT_DELAY = 20 # report the progress every x seconds\n",
    "REPORT_VOCAB_PROGRESS = 100000 # report vocab progress every x documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    (3, 'description')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/local/shalaby/preprocessed_data/extended_pv_abs_desc_claims_full_chunks/extended_pv_training_docs_data_preprocessed-'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_preprocessed_files_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-09 13:00:06,452 : INFO : creating vocabulary for 3 description in \n",
      "2017-04-09 13:00:06,454 : INFO : FILE /home/local/shalaby/parameter_search_doc2vec_models_3_description/full/vocab_model/model\n",
      "2017-04-09 13:00:06,522 : INFO : collecting all words and their counts\n",
      "2017-04-09 13:00:06,528 : INFO : Loading new file for index: 0\n",
      "2017-04-09 13:00:06,542 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-04-09 13:00:29,673 : INFO : PROGRESS: at example #100000, processed 37461598 words (1619643/s), 369706 word types, 99977 tags\n",
      "2017-04-09 13:00:53,123 : INFO : PROGRESS: at example #200000, processed 74340693 words (1572851/s), 653348 word types, 199977 tags\n",
      "2017-04-09 13:00:58,173 : INFO : Loading new file for index: 10000\n",
      "2017-04-09 13:01:14,142 : INFO : PROGRESS: at example #300000, processed 109625390 words (1678943/s), 852964 word types, 299954 tags\n",
      "2017-04-09 13:01:36,039 : INFO : PROGRESS: at example #400000, processed 146980501 words (1706113/s), 1066351 word types, 399954 tags\n",
      "2017-04-09 13:01:47,663 : INFO : Loading new file for index: 20000\n",
      "2017-04-09 13:01:58,166 : INFO : PROGRESS: at example #500000, processed 184389433 words (1690835/s), 1255732 word types, 499954 tags\n",
      "2017-04-09 13:02:20,966 : INFO : PROGRESS: at example #600000, processed 223054815 words (1696132/s), 1446967 word types, 599885 tags\n",
      "2017-04-09 13:02:39,422 : INFO : Loading new file for index: 30000\n",
      "2017-04-09 13:02:42,965 : INFO : PROGRESS: at example #700000, processed 260452753 words (1700175/s), 1625874 word types, 699885 tags\n",
      "2017-04-09 13:03:07,517 : INFO : PROGRESS: at example #800000, processed 299067669 words (1572875/s), 1819141 word types, 799862 tags\n",
      "2017-04-09 13:03:29,929 : INFO : PROGRESS: at example #900000, processed 337020121 words (1693546/s), 1999971 word types, 899862 tags\n",
      "2017-04-09 13:03:32,983 : INFO : Loading new file for index: 40000\n",
      "2017-04-09 13:03:52,286 : INFO : PROGRESS: at example #1000000, processed 375068860 words (1701959/s), 2171279 word types, 999839 tags\n",
      "2017-04-09 13:04:15,058 : INFO : PROGRESS: at example #1100000, processed 413523981 words (1688957/s), 2344237 word types, 1099816 tags\n",
      "2017-04-09 13:04:24,575 : INFO : Loading new file for index: 50000\n",
      "2017-04-09 13:04:37,479 : INFO : PROGRESS: at example #1200000, processed 451674443 words (1701673/s), 2498536 word types, 1199793 tags\n",
      "2017-04-09 13:05:00,437 : INFO : PROGRESS: at example #1300000, processed 490626099 words (1696712/s), 2669816 word types, 1299747 tags\n",
      "2017-04-09 13:05:16,520 : INFO : Loading new file for index: 60000\n",
      "2017-04-09 13:05:22,720 : INFO : PROGRESS: at example #1400000, processed 527781118 words (1667571/s), 2813247 word types, 1399747 tags\n",
      "2017-04-09 13:05:44,707 : INFO : PROGRESS: at example #1500000, processed 564143764 words (1653958/s), 2947697 word types, 1499747 tags\n",
      "2017-04-09 13:06:09,213 : INFO : PROGRESS: at example #1600000, processed 601903043 words (1540932/s), 3090758 word types, 1599747 tags\n",
      "2017-04-09 13:06:09,871 : INFO : Loading new file for index: 70000\n",
      "2017-04-09 13:06:31,975 : INFO : PROGRESS: at example #1700000, processed 639750426 words (1662848/s), 3238959 word types, 1699724 tags\n",
      "2017-04-09 13:06:54,649 : INFO : PROGRESS: at example #1800000, processed 678073514 words (1690267/s), 3394701 word types, 1799724 tags\n",
      "2017-04-09 13:07:02,196 : INFO : Loading new file for index: 80000\n",
      "2017-04-09 13:07:17,330 : INFO : PROGRESS: at example #1900000, processed 716370426 words (1688710/s), 3533198 word types, 1899678 tags\n",
      "2017-04-09 13:07:39,252 : INFO : PROGRESS: at example #2000000, processed 754064624 words (1719606/s), 3651437 word types, 1999678 tags\n",
      "2017-04-09 13:07:52,857 : INFO : Loading new file for index: 90000\n",
      "2017-04-09 13:08:00,486 : INFO : PROGRESS: at example #2100000, processed 790917964 words (1735635/s), 3804999 word types, 2099632 tags\n",
      "2017-04-09 13:08:23,059 : INFO : PROGRESS: at example #2200000, processed 829183094 words (1695356/s), 3946121 word types, 2199609 tags\n",
      "2017-04-09 13:08:43,850 : INFO : Loading new file for index: 100000\n",
      "2017-04-09 13:08:45,150 : INFO : PROGRESS: at example #2300000, processed 866048191 words (1668853/s), 4086695 word types, 2299586 tags\n",
      "2017-04-09 13:09:07,843 : INFO : PROGRESS: at example #2400000, processed 903933987 words (1669616/s), 4215759 word types, 2399586 tags\n",
      "2017-04-09 13:09:33,415 : INFO : PROGRESS: at example #2500000, processed 942552275 words (1510273/s), 4346793 word types, 2499586 tags\n",
      "2017-04-09 13:09:38,593 : INFO : Loading new file for index: 110000\n",
      "2017-04-09 13:09:56,427 : INFO : PROGRESS: at example #2600000, processed 980427383 words (1645979/s), 4560250 word types, 2599494 tags\n",
      "2017-04-09 13:10:19,491 : INFO : PROGRESS: at example #2700000, processed 1018799757 words (1663905/s), 4685747 word types, 2699448 tags\n",
      "2017-04-09 13:10:31,844 : INFO : Loading new file for index: 120000\n",
      "2017-04-09 13:10:42,106 : INFO : PROGRESS: at example #2800000, processed 1056381125 words (1661898/s), 4813722 word types, 2799448 tags\n",
      "2017-04-09 13:11:04,607 : INFO : PROGRESS: at example #2900000, processed 1093900190 words (1667514/s), 4941023 word types, 2899379 tags\n",
      "2017-04-09 13:11:23,384 : INFO : Loading new file for index: 130000\n",
      "2017-04-09 13:11:26,772 : INFO : PROGRESS: at example #3000000, processed 1130759991 words (1663140/s), 5058242 word types, 2999379 tags\n",
      "2017-04-09 13:11:48,915 : INFO : PROGRESS: at example #3100000, processed 1167560195 words (1662039/s), 5169624 word types, 3099379 tags\n",
      "2017-04-09 13:12:11,220 : INFO : PROGRESS: at example #3200000, processed 1205306171 words (1692323/s), 5302199 word types, 3199379 tags\n",
      "2017-04-09 13:12:14,417 : INFO : Loading new file for index: 140000\n",
      "2017-04-09 13:12:34,029 : INFO : PROGRESS: at example #3300000, processed 1243512923 words (1675265/s), 5422313 word types, 3299356 tags\n",
      "2017-04-09 13:12:56,524 : INFO : PROGRESS: at example #3400000, processed 1281338251 words (1681589/s), 5543444 word types, 3399356 tags\n",
      "2017-04-09 13:13:05,758 : INFO : Loading new file for index: 150000\n",
      "2017-04-09 13:13:18,743 : INFO : PROGRESS: at example #3500000, processed 1317687854 words (1636141/s), 5649589 word types, 3499356 tags\n",
      "2017-04-09 13:13:41,292 : INFO : PROGRESS: at example #3600000, processed 1355575359 words (1680303/s), 5766684 word types, 3599287 tags\n",
      "2017-04-09 13:14:01,159 : INFO : Loading new file for index: 160000\n",
      "2017-04-09 13:14:06,830 : INFO : PROGRESS: at example #3700000, processed 1393101677 words (1469513/s), 5884599 word types, 3699287 tags\n",
      "2017-04-09 13:14:29,658 : INFO : PROGRESS: at example #3800000, processed 1430133694 words (1622339/s), 6001922 word types, 3799264 tags\n",
      "2017-04-09 13:14:52,388 : INFO : PROGRESS: at example #3900000, processed 1467477449 words (1643136/s), 6115525 word types, 3899218 tags\n",
      "2017-04-09 13:14:53,232 : INFO : Loading new file for index: 170000\n",
      "2017-04-09 13:15:15,080 : INFO : PROGRESS: at example #4000000, processed 1505063125 words (1656494/s), 6259721 word types, 3999172 tags\n",
      "2017-04-09 13:15:38,255 : INFO : PROGRESS: at example #4100000, processed 1543818297 words (1672433/s), 6373205 word types, 4099172 tags\n",
      "2017-04-09 13:15:46,090 : INFO : Loading new file for index: 180000\n",
      "2017-04-09 13:16:00,863 : INFO : PROGRESS: at example #4200000, processed 1580784056 words (1635188/s), 6483392 word types, 4199172 tags\n",
      "2017-04-09 13:16:23,104 : INFO : PROGRESS: at example #4300000, processed 1617163951 words (1635906/s), 6588627 word types, 4299172 tags\n",
      "2017-04-09 13:16:37,374 : INFO : Loading new file for index: 190000\n",
      "2017-04-09 13:16:45,549 : INFO : PROGRESS: at example #4400000, processed 1654141844 words (1647567/s), 6689807 word types, 4399126 tags\n",
      "2017-04-09 13:17:07,808 : INFO : PROGRESS: at example #4500000, processed 1690653363 words (1640454/s), 6797627 word types, 4499126 tags\n",
      "2017-04-09 13:17:29,071 : INFO : Loading new file for index: 200000\n",
      "2017-04-09 13:17:30,397 : INFO : PROGRESS: at example #4600000, processed 1727972171 words (1652146/s), 6902132 word types, 4599126 tags\n",
      "2017-04-09 13:17:52,852 : INFO : PROGRESS: at example #4700000, processed 1765001603 words (1649219/s), 6998907 word types, 4699126 tags\n",
      "2017-04-09 13:18:15,544 : INFO : PROGRESS: at example #4800000, processed 1803134217 words (1680559/s), 7105424 word types, 4799126 tags\n",
      "2017-04-09 13:18:21,023 : INFO : Loading new file for index: 210000\n",
      "2017-04-09 13:18:38,024 : INFO : PROGRESS: at example #4900000, processed 1839967569 words (1638540/s), 7209377 word types, 4899103 tags\n",
      "2017-04-09 13:19:00,541 : INFO : PROGRESS: at example #5000000, processed 1876789964 words (1635464/s), 7311743 word types, 4999103 tags\n",
      "2017-04-09 13:19:13,227 : INFO : Loading new file for index: 220000\n",
      "2017-04-09 13:19:27,991 : INFO : PROGRESS: at example #5100000, processed 1915523773 words (1411143/s), 7424225 word types, 5099080 tags\n",
      "2017-04-09 13:19:50,729 : INFO : PROGRESS: at example #5200000, processed 1952955049 words (1646300/s), 7520238 word types, 5199057 tags\n",
      "2017-04-09 13:20:09,382 : INFO : Loading new file for index: 230000\n",
      "2017-04-09 13:20:13,231 : INFO : PROGRESS: at example #5300000, processed 1990012348 words (1646972/s), 7623676 word types, 5299011 tags\n",
      "2017-04-09 13:20:36,163 : INFO : PROGRESS: at example #5400000, processed 2027933566 words (1653771/s), 7750675 word types, 5398988 tags\n",
      "2017-04-09 13:20:59,151 : INFO : PROGRESS: at example #5500000, processed 2066059689 words (1658718/s), 7853326 word types, 5498988 tags\n",
      "2017-04-09 13:21:02,107 : INFO : Loading new file for index: 240000\n",
      "2017-04-09 13:21:22,355 : INFO : PROGRESS: at example #5600000, processed 2103542208 words (1615421/s), 7960910 word types, 5598942 tags\n",
      "2017-04-09 13:21:45,061 : INFO : PROGRESS: at example #5700000, processed 2140579647 words (1631276/s), 8057325 word types, 5698942 tags\n",
      "2017-04-09 13:21:55,183 : INFO : Loading new file for index: 250000\n",
      "2017-04-09 13:22:07,988 : INFO : PROGRESS: at example #5800000, processed 2178820857 words (1668139/s), 8169003 word types, 5798896 tags\n",
      "2017-04-09 13:22:31,032 : INFO : PROGRESS: at example #5900000, processed 2216745603 words (1645842/s), 8264467 word types, 5898873 tags\n",
      "2017-04-09 13:22:48,308 : INFO : Loading new file for index: 260000\n",
      "2017-04-09 13:22:54,204 : INFO : PROGRESS: at example #6000000, processed 2254941062 words (1648486/s), 8355633 word types, 5998850 tags\n",
      "2017-04-09 13:23:16,654 : INFO : PROGRESS: at example #6100000, processed 2291171016 words (1613888/s), 8441054 word types, 6098850 tags\n",
      "2017-04-09 13:23:39,514 : INFO : PROGRESS: at example #6200000, processed 2328668033 words (1640387/s), 8528328 word types, 6198850 tags\n",
      "2017-04-09 13:23:40,433 : INFO : Loading new file for index: 270000\n",
      "2017-04-09 13:24:03,440 : INFO : PROGRESS: at example #6300000, processed 2366944642 words (1599960/s), 8651425 word types, 6298850 tags\n",
      "2017-04-09 13:24:27,066 : INFO : PROGRESS: at example #6400000, processed 2404327336 words (1582418/s), 8758060 word types, 6398804 tags\n",
      "2017-04-09 13:24:35,886 : INFO : Loading new file for index: 280000\n",
      "2017-04-09 13:24:50,708 : INFO : PROGRESS: at example #6500000, processed 2442632972 words (1620366/s), 8863716 word types, 6498781 tags\n",
      "2017-04-09 13:25:14,140 : INFO : PROGRESS: at example #6600000, processed 2480056531 words (1597247/s), 8957340 word types, 6598781 tags\n",
      "2017-04-09 13:25:28,793 : INFO : Loading new file for index: 290000\n",
      "2017-04-09 13:25:36,948 : INFO : PROGRESS: at example #6700000, processed 2516913479 words (1616160/s), 9038023 word types, 6698781 tags\n",
      "2017-04-09 13:25:59,880 : INFO : PROGRESS: at example #6800000, processed 2555067262 words (1663934/s), 9132092 word types, 6798758 tags\n",
      "2017-04-09 13:26:22,068 : INFO : Loading new file for index: 300000\n",
      "2017-04-09 13:26:23,256 : INFO : PROGRESS: at example #6900000, processed 2592991548 words (1622468/s), 9219849 word types, 6898735 tags\n",
      "2017-04-09 13:26:51,043 : INFO : PROGRESS: at example #7000000, processed 2630386357 words (1345852/s), 9311587 word types, 6998735 tags\n",
      "2017-04-09 13:27:14,047 : INFO : PROGRESS: at example #7100000, processed 2668662071 words (1663993/s), 9409408 word types, 7098712 tags\n",
      "2017-04-09 13:27:19,895 : INFO : Loading new file for index: 310000\n",
      "2017-04-09 13:27:37,179 : INFO : PROGRESS: at example #7200000, processed 2706494208 words (1635651/s), 9515765 word types, 7198689 tags\n",
      "2017-04-09 13:28:00,174 : INFO : PROGRESS: at example #7300000, processed 2743953568 words (1629166/s), 9610849 word types, 7298689 tags\n",
      "2017-04-09 13:28:13,509 : INFO : Loading new file for index: 320000\n",
      "2017-04-09 13:28:23,620 : INFO : PROGRESS: at example #7400000, processed 2781167092 words (1587336/s), 9704514 word types, 7398643 tags\n",
      "2017-04-09 13:28:46,628 : INFO : PROGRESS: at example #7500000, processed 2818566046 words (1625631/s), 9794741 word types, 7498620 tags\n",
      "2017-04-09 13:29:05,936 : INFO : Loading new file for index: 330000\n",
      "2017-04-09 13:29:09,657 : INFO : PROGRESS: at example #7600000, processed 2855849169 words (1619113/s), 9897450 word types, 7598597 tags\n",
      "2017-04-09 13:29:31,961 : INFO : PROGRESS: at example #7700000, processed 2892120104 words (1626343/s), 9983651 word types, 7698597 tags\n",
      "2017-04-09 13:29:54,710 : INFO : PROGRESS: at example #7800000, processed 2929074533 words (1624585/s), 10069538 word types, 7798597 tags\n",
      "2017-04-09 13:29:58,176 : INFO : Loading new file for index: 340000\n",
      "2017-04-09 13:30:17,818 : INFO : PROGRESS: at example #7900000, processed 2966461732 words (1617996/s), 10159005 word types, 7898597 tags\n",
      "2017-04-09 13:30:40,515 : INFO : PROGRESS: at example #8000000, processed 3003392590 words (1627282/s), 10242502 word types, 7998528 tags\n",
      "2017-04-09 13:30:50,525 : INFO : Loading new file for index: 350000\n",
      "2017-04-09 13:31:03,383 : INFO : PROGRESS: at example #8100000, processed 3040607717 words (1627446/s), 10323098 word types, 8098528 tags\n",
      "2017-04-09 13:31:27,435 : INFO : PROGRESS: at example #8200000, processed 3080048729 words (1639986/s), 10412206 word types, 8198505 tags\n",
      "2017-04-09 13:31:44,944 : INFO : Loading new file for index: 360000\n",
      "2017-04-09 13:31:50,492 : INFO : PROGRESS: at example #8300000, processed 3117025180 words (1603837/s), 10491028 word types, 8298482 tags\n",
      "2017-04-09 13:32:14,395 : INFO : PROGRESS: at example #8400000, processed 3155737543 words (1619599/s), 10588474 word types, 8398482 tags\n",
      "2017-04-09 13:32:37,301 : INFO : PROGRESS: at example #8500000, processed 3193254067 words (1638028/s), 10680801 word types, 8498459 tags\n",
      "2017-04-09 13:32:38,598 : INFO : Loading new file for index: 370000\n",
      "2017-04-09 13:33:00,055 : INFO : PROGRESS: at example #8600000, processed 3230361674 words (1630886/s), 10765171 word types, 8598459 tags\n",
      "2017-04-09 13:33:22,031 : INFO : PROGRESS: at example #8700000, processed 3266007766 words (1622151/s), 10840778 word types, 8698459 tags\n",
      "2017-04-09 13:33:30,175 : INFO : Loading new file for index: 380000\n",
      "2017-04-09 13:33:45,242 : INFO : PROGRESS: at example #8800000, processed 3303942527 words (1634478/s), 10935392 word types, 8798413 tags\n",
      "2017-04-09 13:34:08,318 : INFO : PROGRESS: at example #8900000, processed 3341632583 words (1633456/s), 11055637 word types, 8898344 tags\n",
      "2017-04-09 13:34:23,298 : INFO : Loading new file for index: 390000\n",
      "2017-04-09 13:34:31,075 : INFO : PROGRESS: at example #9000000, processed 3378685888 words (1628324/s), 11135038 word types, 8998321 tags\n",
      "2017-04-09 13:34:54,387 : INFO : PROGRESS: at example #9100000, processed 3415871528 words (1595204/s), 11213749 word types, 9098321 tags\n",
      "2017-04-09 13:35:16,592 : INFO : Loading new file for index: 400000\n",
      "2017-04-09 13:35:17,661 : INFO : PROGRESS: at example #9200000, processed 3453080671 words (1598837/s), 11305356 word types, 9198321 tags\n",
      "2017-04-09 13:35:47,736 : INFO : PROGRESS: at example #9300000, processed 3490829611 words (1255258/s), 11386160 word types, 9298275 tags\n",
      "2017-04-09 13:36:11,250 : INFO : PROGRESS: at example #9400000, processed 3528550580 words (1604298/s), 11469029 word types, 9398252 tags\n",
      "2017-04-09 13:36:17,516 : INFO : Loading new file for index: 410000\n",
      "2017-04-09 13:36:34,221 : INFO : PROGRESS: at example #9500000, processed 3565734510 words (1618854/s), 11545442 word types, 9498229 tags\n",
      "2017-04-09 13:36:57,673 : INFO : PROGRESS: at example #9600000, processed 3604361761 words (1647198/s), 11648761 word types, 9598160 tags\n",
      "2017-04-09 13:37:10,621 : INFO : Loading new file for index: 420000\n",
      "2017-04-09 13:37:21,235 : INFO : PROGRESS: at example #9700000, processed 3642644191 words (1624792/s), 11743660 word types, 9698160 tags\n",
      "2017-04-09 13:37:44,741 : INFO : PROGRESS: at example #9800000, processed 3681198850 words (1640365/s), 11835475 word types, 9798160 tags\n",
      "2017-04-09 13:38:04,356 : INFO : Loading new file for index: 430000\n",
      "2017-04-09 13:38:07,908 : INFO : PROGRESS: at example #9900000, processed 3718316883 words (1602306/s), 11927163 word types, 9898160 tags\n",
      "2017-04-09 13:38:30,834 : INFO : PROGRESS: at example #10000000, processed 3755645321 words (1628354/s), 12005477 word types, 9998160 tags\n",
      "2017-04-09 13:38:54,179 : INFO : PROGRESS: at example #10100000, processed 3792776733 words (1590663/s), 12074048 word types, 10098137 tags\n",
      "2017-04-09 13:38:57,763 : INFO : Loading new file for index: 440000\n",
      "2017-04-09 13:39:16,838 : INFO : PROGRESS: at example #10200000, processed 3829219897 words (1608460/s), 12172856 word types, 10198137 tags\n",
      "2017-04-09 13:39:40,250 : INFO : PROGRESS: at example #10300000, processed 3865923809 words (1567908/s), 12250391 word types, 10298137 tags\n",
      "2017-04-09 13:39:50,754 : INFO : Loading new file for index: 450000\n",
      "2017-04-09 13:40:03,849 : INFO : PROGRESS: at example #10400000, processed 3903250502 words (1581817/s), 12335708 word types, 10398137 tags\n",
      "2017-04-09 13:40:28,131 : INFO : PROGRESS: at example #10500000, processed 3941192634 words (1562726/s), 12414175 word types, 10498114 tags\n",
      "2017-04-09 13:40:45,660 : INFO : Loading new file for index: 460000\n",
      "2017-04-09 13:40:51,160 : INFO : PROGRESS: at example #10600000, processed 3977985801 words (1597840/s), 12498076 word types, 10598091 tags\n",
      "2017-04-09 13:41:14,199 : INFO : PROGRESS: at example #10700000, processed 4015536670 words (1629965/s), 12579376 word types, 10698091 tags\n",
      "2017-04-09 13:41:36,713 : INFO : PROGRESS: at example #10800000, processed 4051266995 words (1587199/s), 12642191 word types, 10798068 tags\n",
      "2017-04-09 13:41:37,835 : INFO : Loading new file for index: 470000\n",
      "2017-04-09 13:41:59,478 : INFO : PROGRESS: at example #10900000, processed 4087585378 words (1595477/s), 12725275 word types, 10898068 tags\n",
      "2017-04-09 13:42:22,567 : INFO : PROGRESS: at example #11000000, processed 4125015068 words (1621278/s), 12805694 word types, 10998068 tags\n",
      "2017-04-09 13:42:30,898 : INFO : Loading new file for index: 480000\n",
      "2017-04-09 13:42:45,896 : INFO : PROGRESS: at example #11100000, processed 4163124437 words (1633626/s), 12886487 word types, 11098045 tags\n",
      "2017-04-09 13:43:09,291 : INFO : PROGRESS: at example #11200000, processed 4199850707 words (1569928/s), 12975813 word types, 11198045 tags\n",
      "2017-04-09 13:43:24,158 : INFO : Loading new file for index: 490000\n",
      "2017-04-09 13:43:32,146 : INFO : PROGRESS: at example #11300000, processed 4236482316 words (1602960/s), 13042734 word types, 11298045 tags\n",
      "2017-04-09 13:43:54,867 : INFO : PROGRESS: at example #11400000, processed 4273150472 words (1613941/s), 13116717 word types, 11398022 tags\n",
      "2017-04-09 13:44:17,250 : INFO : Loading new file for index: 500000\n",
      "2017-04-09 13:44:18,397 : INFO : PROGRESS: at example #11500000, processed 4310298854 words (1578924/s), 13200507 word types, 11498022 tags\n",
      "2017-04-09 13:44:41,407 : INFO : PROGRESS: at example #11600000, processed 4347437590 words (1614119/s), 13273604 word types, 11598022 tags\n",
      "2017-04-09 13:45:04,851 : INFO : PROGRESS: at example #11700000, processed 4384229023 words (1569459/s), 13339469 word types, 11698022 tags\n",
      "2017-04-09 13:45:11,147 : INFO : Loading new file for index: 510000\n",
      "2017-04-09 13:45:29,555 : INFO : PROGRESS: at example #11800000, processed 4423214458 words (1578255/s), 13432319 word types, 11797976 tags\n",
      "2017-04-09 13:45:52,641 : INFO : PROGRESS: at example #11900000, processed 4459932757 words (1590641/s), 13506477 word types, 11897953 tags\n",
      "2017-04-09 13:46:05,650 : INFO : Loading new file for index: 520000\n",
      "2017-04-09 13:46:15,968 : INFO : PROGRESS: at example #12000000, processed 4496460931 words (1566061/s), 13592569 word types, 11997953 tags\n",
      "2017-04-09 13:46:39,141 : INFO : PROGRESS: at example #12100000, processed 4533499970 words (1598537/s), 13663836 word types, 12097953 tags\n",
      "2017-04-09 13:47:07,067 : INFO : Loading new file for index: 530000\n",
      "2017-04-09 13:47:10,486 : INFO : PROGRESS: at example #12200000, processed 4570063647 words (1166550/s), 13755720 word types, 12197953 tags\n",
      "2017-04-09 13:47:33,658 : INFO : PROGRESS: at example #12300000, processed 4607341602 words (1608872/s), 13862921 word types, 12297907 tags\n",
      "2017-04-09 13:47:56,803 : INFO : PROGRESS: at example #12400000, processed 4644356569 words (1599368/s), 13934472 word types, 12397907 tags\n",
      "2017-04-09 13:48:00,336 : INFO : Loading new file for index: 540000\n",
      "2017-04-09 13:48:19,945 : INFO : PROGRESS: at example #12500000, processed 4681695202 words (1613561/s), 14007202 word types, 12497884 tags\n",
      "2017-04-09 13:48:43,266 : INFO : PROGRESS: at example #12600000, processed 4718917207 words (1596194/s), 14080437 word types, 12597884 tags\n",
      "2017-04-09 13:48:53,939 : INFO : Loading new file for index: 550000\n",
      "2017-04-09 13:49:06,511 : INFO : PROGRESS: at example #12700000, processed 4755860531 words (1589455/s), 14146367 word types, 12697861 tags\n",
      "2017-04-09 13:49:29,617 : INFO : PROGRESS: at example #12800000, processed 4793281588 words (1619646/s), 14215609 word types, 12797861 tags\n",
      "2017-04-09 13:49:46,287 : INFO : Loading new file for index: 560000\n",
      "2017-04-09 13:49:52,104 : INFO : PROGRESS: at example #12900000, processed 4829641108 words (1617113/s), 14293406 word types, 12897861 tags\n",
      "2017-04-09 13:50:14,915 : INFO : PROGRESS: at example #13000000, processed 4866112781 words (1598957/s), 14377475 word types, 12997861 tags\n",
      "2017-04-09 13:50:38,081 : INFO : PROGRESS: at example #13100000, processed 4903448998 words (1611772/s), 14449807 word types, 13097861 tags\n",
      "2017-04-09 13:50:39,253 : INFO : Loading new file for index: 570000\n",
      "2017-04-09 13:51:01,353 : INFO : PROGRESS: at example #13200000, processed 4940519316 words (1593001/s), 14523612 word types, 13197861 tags\n",
      "2017-04-09 13:51:23,975 : INFO : PROGRESS: at example #13300000, processed 4976990557 words (1612324/s), 14589928 word types, 13297861 tags\n",
      "2017-04-09 13:51:32,145 : INFO : Loading new file for index: 580000\n",
      "2017-04-09 13:51:46,639 : INFO : PROGRESS: at example #13400000, processed 5013972129 words (1631858/s), 14659866 word types, 13397838 tags\n",
      "2017-04-09 13:52:09,920 : INFO : PROGRESS: at example #13500000, processed 5051656922 words (1618821/s), 14734288 word types, 13497838 tags\n",
      "2017-04-09 13:52:24,772 : INFO : Loading new file for index: 590000\n",
      "2017-04-09 13:52:32,651 : INFO : PROGRESS: at example #13600000, processed 5088724889 words (1630898/s), 14805312 word types, 13597838 tags\n",
      "2017-04-09 13:52:55,699 : INFO : PROGRESS: at example #13700000, processed 5126610284 words (1643818/s), 14882060 word types, 13697792 tags\n",
      "2017-04-09 13:53:17,582 : INFO : Loading new file for index: 600000\n",
      "2017-04-09 13:53:18,639 : INFO : PROGRESS: at example #13800000, processed 5163777634 words (1620307/s), 14956301 word types, 13797769 tags\n",
      "2017-04-09 13:53:41,394 : INFO : PROGRESS: at example #13900000, processed 5200364903 words (1608027/s), 15030524 word types, 13897769 tags\n",
      "2017-04-09 13:54:04,187 : INFO : PROGRESS: at example #14000000, processed 5237262982 words (1618938/s), 15094712 word types, 13997769 tags\n",
      "2017-04-09 13:54:10,230 : INFO : Loading new file for index: 610000\n",
      "2017-04-09 13:54:27,442 : INFO : PROGRESS: at example #14100000, processed 5274453477 words (1599376/s), 15159878 word types, 14097769 tags\n",
      "2017-04-09 13:54:50,306 : INFO : PROGRESS: at example #14200000, processed 5311427174 words (1617198/s), 15241385 word types, 14197746 tags\n",
      "2017-04-09 13:55:02,846 : INFO : Loading new file for index: 620000\n",
      "2017-04-09 13:55:13,016 : INFO : PROGRESS: at example #14300000, processed 5347922743 words (1607142/s), 15311085 word types, 14297746 tags\n",
      "2017-04-09 13:55:35,738 : INFO : PROGRESS: at example #14400000, processed 5384676796 words (1617661/s), 15378347 word types, 14397746 tags\n",
      "2017-04-09 13:55:55,124 : INFO : Loading new file for index: 630000\n",
      "2017-04-09 13:55:58,415 : INFO : PROGRESS: at example #14500000, processed 5421274085 words (1613951/s), 15459736 word types, 14497723 tags\n",
      "2017-04-09 13:56:21,530 : INFO : PROGRESS: at example #14600000, processed 5459021099 words (1633131/s), 15527640 word types, 14597723 tags\n",
      "2017-04-09 13:56:44,721 : INFO : PROGRESS: at example #14700000, processed 5496890300 words (1633081/s), 15601614 word types, 14697700 tags\n",
      "2017-04-09 13:56:48,365 : INFO : Loading new file for index: 640000\n",
      "2017-04-09 13:57:08,402 : INFO : PROGRESS: at example #14800000, processed 5534282998 words (1579085/s), 15679773 word types, 14797677 tags\n",
      "2017-04-09 13:57:34,962 : INFO : PROGRESS: at example #14900000, processed 5572560558 words (1441391/s), 15750722 word types, 14897654 tags\n",
      "2017-04-09 13:57:46,539 : INFO : Loading new file for index: 650000\n",
      "2017-04-09 13:57:59,819 : INFO : PROGRESS: at example #15000000, processed 5611572559 words (1569715/s), 15823345 word types, 14997654 tags\n",
      "2017-04-09 13:58:24,348 : INFO : PROGRESS: at example #15100000, processed 5649871593 words (1561624/s), 15898786 word types, 15097654 tags\n",
      "2017-04-09 13:58:42,467 : INFO : Loading new file for index: 660000\n",
      "2017-04-09 13:58:48,300 : INFO : PROGRESS: at example #15200000, processed 5687498045 words (1571058/s), 15966143 word types, 15197631 tags\n",
      "2017-04-09 13:59:11,791 : INFO : PROGRESS: at example #15300000, processed 5724661215 words (1582207/s), 16038506 word types, 15297631 tags\n",
      "2017-04-09 13:59:35,378 : INFO : PROGRESS: at example #15400000, processed 5762248717 words (1593646/s), 16118195 word types, 15397608 tags\n",
      "2017-04-09 13:59:36,631 : INFO : Loading new file for index: 670000\n",
      "2017-04-09 13:59:58,679 : INFO : PROGRESS: at example #15500000, processed 5799378386 words (1593608/s), 16186871 word types, 15497608 tags\n",
      "2017-04-09 14:00:22,014 : INFO : PROGRESS: at example #15600000, processed 5836750299 words (1601686/s), 16258295 word types, 15597608 tags\n",
      "2017-04-09 14:00:30,619 : INFO : Loading new file for index: 680000\n",
      "2017-04-09 14:01:00,147 : INFO : PROGRESS: at example #15700000, processed 5874519942 words (990507/s), 16335571 word types, 15697608 tags\n",
      "2017-04-09 14:01:23,394 : INFO : PROGRESS: at example #15800000, processed 5912487637 words (1633356/s), 16398777 word types, 15797539 tags\n",
      "2017-04-09 14:01:39,071 : INFO : Loading new file for index: 690000\n",
      "2017-04-09 14:01:47,029 : INFO : PROGRESS: at example #15900000, processed 5950223883 words (1596722/s), 16479522 word types, 15897539 tags\n",
      "2017-04-09 14:02:10,340 : INFO : PROGRESS: at example #16000000, processed 5988360401 words (1636072/s), 16545053 word types, 15997539 tags\n",
      "2017-04-09 14:02:32,259 : INFO : Loading new file for index: 700000\n",
      "2017-04-09 14:02:33,218 : INFO : PROGRESS: at example #16100000, processed 6025385904 words (1618521/s), 16608092 word types, 16097516 tags\n",
      "2017-04-09 14:02:56,747 : INFO : PROGRESS: at example #16200000, processed 6064219617 words (1650545/s), 16680439 word types, 16197447 tags\n",
      "2017-04-09 14:03:20,008 : INFO : PROGRESS: at example #16300000, processed 6101627386 words (1608277/s), 16750931 word types, 16297447 tags\n",
      "2017-04-09 14:03:26,239 : INFO : Loading new file for index: 710000\n",
      "2017-04-09 14:03:43,815 : INFO : PROGRESS: at example #16400000, processed 6140230480 words (1621605/s), 16828984 word types, 16397447 tags\n",
      "2017-04-09 14:04:07,578 : INFO : PROGRESS: at example #16500000, processed 6179379020 words (1647606/s), 16910776 word types, 16497401 tags\n",
      "2017-04-09 14:04:20,332 : INFO : Loading new file for index: 720000\n",
      "2017-04-09 14:04:30,608 : INFO : PROGRESS: at example #16600000, processed 6217242458 words (1644222/s), 16982413 word types, 16597332 tags\n",
      "2017-04-09 14:04:53,376 : INFO : PROGRESS: at example #16700000, processed 6254152203 words (1621188/s), 17048083 word types, 16697332 tags\n",
      "2017-04-09 14:05:12,881 : INFO : Loading new file for index: 730000\n",
      "2017-04-09 14:05:16,167 : INFO : PROGRESS: at example #16800000, processed 6291400002 words (1634497/s), 17114667 word types, 16797309 tags\n",
      "2017-04-09 14:05:39,494 : INFO : PROGRESS: at example #16900000, processed 6329100596 words (1616274/s), 17181355 word types, 16897309 tags\n",
      "2017-04-09 14:06:02,802 : INFO : PROGRESS: at example #17000000, processed 6367420942 words (1644226/s), 17258477 word types, 16997217 tags\n",
      "2017-04-09 14:06:06,390 : INFO : Loading new file for index: 740000\n",
      "2017-04-09 14:06:25,914 : INFO : PROGRESS: at example #17100000, processed 6405069541 words (1629079/s), 17322572 word types, 17097217 tags\n",
      "2017-04-09 14:06:49,164 : INFO : PROGRESS: at example #17200000, processed 6443070639 words (1634522/s), 17392478 word types, 17197171 tags\n",
      "2017-04-09 14:06:59,565 : INFO : Loading new file for index: 750000\n",
      "2017-04-09 14:07:11,558 : INFO : PROGRESS: at example #17300000, processed 6479367768 words (1621036/s), 17459077 word types, 17297171 tags\n",
      "2017-04-09 14:07:34,741 : INFO : PROGRESS: at example #17400000, processed 6517419891 words (1641463/s), 17577309 word types, 17397171 tags\n",
      "2017-04-09 14:07:52,844 : INFO : Loading new file for index: 760000\n",
      "2017-04-09 14:07:58,404 : INFO : PROGRESS: at example #17500000, processed 6555836438 words (1623532/s), 17653330 word types, 17497148 tags\n",
      "2017-04-09 14:08:21,046 : INFO : PROGRESS: at example #17600000, processed 6592533840 words (1620943/s), 17726277 word types, 17597148 tags\n",
      "2017-04-09 14:08:44,889 : INFO : PROGRESS: at example #17700000, processed 6631290248 words (1625577/s), 17799884 word types, 17697125 tags\n",
      "2017-04-09 14:08:46,284 : INFO : Loading new file for index: 770000\n",
      "2017-04-09 14:09:07,525 : INFO : PROGRESS: at example #17800000, processed 6667542380 words (1601689/s), 17863951 word types, 17797102 tags\n",
      "2017-04-09 14:09:29,980 : INFO : PROGRESS: at example #17900000, processed 6704711508 words (1655367/s), 17923436 word types, 17897056 tags\n",
      "2017-04-09 14:09:38,283 : INFO : Loading new file for index: 780000\n",
      "2017-04-09 14:09:53,511 : INFO : PROGRESS: at example #18000000, processed 6742877122 words (1622012/s), 17988506 word types, 17997010 tags\n",
      "2017-04-09 14:10:16,483 : INFO : PROGRESS: at example #18100000, processed 6780678224 words (1645632/s), 18054954 word types, 18096964 tags\n",
      "2017-04-09 14:10:32,463 : INFO : Loading new file for index: 790000\n",
      "2017-04-09 14:10:40,059 : INFO : PROGRESS: at example #18200000, processed 6819427824 words (1643741/s), 18130236 word types, 18196964 tags\n",
      "2017-04-09 14:11:02,861 : INFO : PROGRESS: at example #18300000, processed 6856424155 words (1622615/s), 18195051 word types, 18296964 tags\n",
      "2017-04-09 14:11:25,401 : INFO : Loading new file for index: 800000\n",
      "2017-04-09 14:11:26,193 : INFO : PROGRESS: at example #18400000, processed 6894239831 words (1620907/s), 18263590 word types, 18396964 tags\n",
      "2017-04-09 14:11:49,294 : INFO : PROGRESS: at example #18500000, processed 6931410974 words (1609166/s), 18336393 word types, 18496964 tags\n",
      "2017-04-09 14:12:13,009 : INFO : PROGRESS: at example #18600000, processed 6970942194 words (1667034/s), 18419479 word types, 18596895 tags\n",
      "2017-04-09 14:12:19,360 : INFO : Loading new file for index: 810000\n",
      "2017-04-09 14:12:36,573 : INFO : PROGRESS: at example #18700000, processed 7009661428 words (1643256/s), 18485710 word types, 18696872 tags\n",
      "2017-04-09 14:13:00,196 : INFO : PROGRESS: at example #18800000, processed 7046520661 words (1560448/s), 18551692 word types, 18796849 tags\n",
      "2017-04-09 14:13:13,491 : INFO : Loading new file for index: 820000\n",
      "2017-04-09 14:13:23,331 : INFO : PROGRESS: at example #18900000, processed 7083538236 words (1600265/s), 18614291 word types, 18896826 tags\n",
      "2017-04-09 14:13:47,088 : INFO : PROGRESS: at example #19000000, processed 7120565257 words (1558664/s), 18677456 word types, 18996803 tags\n",
      "2017-04-09 14:14:06,452 : INFO : Loading new file for index: 830000\n",
      "2017-04-09 14:14:09,380 : INFO : PROGRESS: at example #19100000, processed 7155841544 words (1582660/s), 18728460 word types, 19096803 tags\n",
      "2017-04-09 14:14:33,382 : INFO : PROGRESS: at example #19200000, processed 7194862643 words (1625887/s), 18805592 word types, 19196780 tags\n",
      "2017-04-09 14:14:56,260 : INFO : PROGRESS: at example #19300000, processed 7231837034 words (1616350/s), 18870190 word types, 19296757 tags\n",
      "2017-04-09 14:15:00,617 : INFO : Loading new file for index: 840000\n",
      "2017-04-09 14:15:19,888 : INFO : PROGRESS: at example #19400000, processed 7269239113 words (1583117/s), 18931956 word types, 19396711 tags\n",
      "2017-04-09 14:15:43,917 : INFO : PROGRESS: at example #19500000, processed 7307555137 words (1594746/s), 19043658 word types, 19496619 tags\n",
      "2017-04-09 14:15:55,027 : INFO : Loading new file for index: 850000\n",
      "2017-04-09 14:16:07,548 : INFO : PROGRESS: at example #19600000, processed 7346044440 words (1628920/s), 19111583 word types, 19596619 tags\n",
      "2017-04-09 14:16:30,850 : INFO : PROGRESS: at example #19700000, processed 7383557942 words (1610006/s), 19203250 word types, 19696596 tags\n",
      "2017-04-09 14:16:49,334 : INFO : Loading new file for index: 860000\n",
      "2017-04-09 14:16:54,936 : INFO : PROGRESS: at example #19800000, processed 7421477881 words (1574432/s), 19268962 word types, 19796573 tags\n",
      "2017-04-09 14:17:18,342 : INFO : PROGRESS: at example #19900000, processed 7459347022 words (1618119/s), 19329491 word types, 19896550 tags\n",
      "2017-04-09 14:17:41,808 : INFO : PROGRESS: at example #20000000, processed 7497080228 words (1608126/s), 19390307 word types, 19996527 tags\n",
      "2017-04-09 14:17:43,361 : INFO : Loading new file for index: 870000\n",
      "2017-04-09 14:18:04,980 : INFO : PROGRESS: at example #20100000, processed 7534391731 words (1610348/s), 19464027 word types, 20096504 tags\n",
      "2017-04-09 14:18:44,715 : INFO : PROGRESS: at example #20200000, processed 7571255093 words (927780/s), 19525420 word types, 20196481 tags\n",
      "2017-04-09 14:18:53,497 : INFO : Loading new file for index: 880000\n",
      "2017-04-09 14:19:07,793 : INFO : PROGRESS: at example #20300000, processed 7608631802 words (1619706/s), 19584801 word types, 20296481 tags\n",
      "2017-04-09 14:19:30,670 : INFO : PROGRESS: at example #20400000, processed 7645873417 words (1628009/s), 19651081 word types, 20396481 tags\n",
      "2017-04-09 14:19:45,569 : INFO : Loading new file for index: 890000\n",
      "2017-04-09 14:19:53,607 : INFO : PROGRESS: at example #20500000, processed 7683234389 words (1628942/s), 19719304 word types, 20496481 tags\n",
      "2017-04-09 14:20:17,003 : INFO : PROGRESS: at example #20600000, processed 7721521046 words (1636594/s), 19795675 word types, 20596458 tags\n",
      "2017-04-09 14:20:39,160 : INFO : Loading new file for index: 900000\n",
      "2017-04-09 14:20:39,809 : INFO : PROGRESS: at example #20700000, processed 7758754651 words (1632701/s), 19852085 word types, 20696458 tags\n",
      "2017-04-09 14:21:02,898 : INFO : PROGRESS: at example #20800000, processed 7796276380 words (1625193/s), 19916763 word types, 20796458 tags\n",
      "2017-04-09 14:21:25,740 : INFO : PROGRESS: at example #20900000, processed 7832764134 words (1597555/s), 19976066 word types, 20896458 tags\n",
      "2017-04-09 14:21:31,893 : INFO : Loading new file for index: 910000\n",
      "2017-04-09 14:21:48,689 : INFO : PROGRESS: at example #21000000, processed 7870365560 words (1638632/s), 20036592 word types, 20996435 tags\n",
      "2017-04-09 14:22:11,488 : INFO : PROGRESS: at example #21100000, processed 7907025633 words (1608011/s), 20101603 word types, 21096435 tags\n",
      "2017-04-09 14:22:24,299 : INFO : Loading new file for index: 920000\n",
      "2017-04-09 14:22:34,334 : INFO : PROGRESS: at example #21200000, processed 7943942393 words (1616074/s), 20157989 word types, 21196412 tags\n",
      "2017-04-09 14:22:57,284 : INFO : PROGRESS: at example #21300000, processed 7981184636 words (1622836/s), 20218778 word types, 21296412 tags\n",
      "2017-04-09 14:23:17,059 : INFO : Loading new file for index: 930000\n",
      "2017-04-09 14:23:19,973 : INFO : PROGRESS: at example #21400000, processed 8017920873 words (1619270/s), 20279647 word types, 21396412 tags\n",
      "2017-04-09 14:23:42,804 : INFO : PROGRESS: at example #21500000, processed 8054994113 words (1623886/s), 20330104 word types, 21496412 tags\n",
      "2017-04-09 14:24:05,658 : INFO : PROGRESS: at example #21600000, processed 8091974431 words (1618210/s), 20393324 word types, 21596389 tags\n",
      "2017-04-09 14:24:09,479 : INFO : Loading new file for index: 940000\n",
      "2017-04-09 14:24:28,275 : INFO : PROGRESS: at example #21700000, processed 8128542996 words (1617036/s), 20448977 word types, 21696389 tags\n",
      "2017-04-09 14:24:51,338 : INFO : PROGRESS: at example #21800000, processed 8165950036 words (1622044/s), 20511367 word types, 21796389 tags\n",
      "2017-04-09 14:25:02,284 : INFO : Loading new file for index: 950000\n",
      "2017-04-09 14:25:14,911 : INFO : PROGRESS: at example #21900000, processed 8202695808 words (1558909/s), 20566589 word types, 21896389 tags\n",
      "2017-04-09 14:25:38,395 : INFO : PROGRESS: at example #22000000, processed 8240287390 words (1600886/s), 20625759 word types, 21996366 tags\n",
      "2017-04-09 14:25:56,067 : INFO : Loading new file for index: 960000\n",
      "2017-04-09 14:26:01,533 : INFO : PROGRESS: at example #22100000, processed 8276888470 words (1581977/s), 20682889 word types, 22096366 tags\n",
      "2017-04-09 14:26:25,246 : INFO : PROGRESS: at example #22200000, processed 8314548939 words (1588300/s), 20745891 word types, 22196366 tags\n",
      "2017-04-09 14:26:49,109 : INFO : PROGRESS: at example #22300000, processed 8352705640 words (1599197/s), 20807308 word types, 22296343 tags\n",
      "2017-04-09 14:26:50,584 : INFO : Loading new file for index: 970000\n",
      "2017-04-09 14:27:13,973 : INFO : PROGRESS: at example #22400000, processed 8389726148 words (1489033/s), 20866726 word types, 22396343 tags\n",
      "2017-04-09 14:27:38,016 : INFO : PROGRESS: at example #22500000, processed 8427705055 words (1579752/s), 20938104 word types, 22496297 tags\n",
      "2017-04-09 14:27:46,697 : INFO : Loading new file for index: 980000\n",
      "2017-04-09 14:28:01,985 : INFO : PROGRESS: at example #22600000, processed 8465129031 words (1561462/s), 21009185 word types, 22596274 tags\n",
      "2017-04-09 14:28:25,931 : INFO : PROGRESS: at example #22700000, processed 8502580799 words (1564198/s), 21073126 word types, 22696251 tags\n",
      "2017-04-09 14:28:41,086 : INFO : Loading new file for index: 990000\n",
      "2017-04-09 14:28:48,382 : INFO : PROGRESS: at example #22800000, processed 8538029855 words (1579066/s), 21124456 word types, 22796228 tags\n",
      "2017-04-09 14:29:12,465 : INFO : PROGRESS: at example #22900000, processed 8576126663 words (1582032/s), 21198812 word types, 22896182 tags\n",
      "2017-04-09 14:29:35,694 : INFO : Loading new file for index: 1000000\n",
      "2017-04-09 14:29:36,407 : INFO : PROGRESS: at example #23000000, processed 8613507028 words (1561427/s), 21255757 word types, 22996182 tags\n",
      "2017-04-09 14:30:00,233 : INFO : PROGRESS: at example #23100000, processed 8651866754 words (1610114/s), 21320711 word types, 23096182 tags\n",
      "2017-04-09 14:30:23,379 : INFO : PROGRESS: at example #23200000, processed 8688036635 words (1562859/s), 21376452 word types, 23196159 tags\n",
      "2017-04-09 14:30:29,585 : INFO : Loading new file for index: 1010000\n",
      "2017-04-09 14:30:47,660 : INFO : PROGRESS: at example #23300000, processed 8727465552 words (1623987/s), 21500891 word types, 23296021 tags\n",
      "2017-04-09 14:31:10,718 : INFO : PROGRESS: at example #23400000, processed 8765028210 words (1629199/s), 21556676 word types, 23396021 tags\n",
      "2017-04-09 14:31:23,784 : INFO : Loading new file for index: 1020000\n",
      "2017-04-09 14:31:33,733 : INFO : PROGRESS: at example #23500000, processed 8802436841 words (1625559/s), 21615114 word types, 23496021 tags\n",
      "2017-04-09 14:31:56,867 : INFO : PROGRESS: at example #23600000, processed 8839611444 words (1606993/s), 21686612 word types, 23596021 tags\n",
      "2017-04-09 14:32:17,058 : INFO : Loading new file for index: 1030000\n",
      "2017-04-09 14:32:19,835 : INFO : PROGRESS: at example #23700000, processed 8877525804 words (1650931/s), 21747803 word types, 23696021 tags\n",
      "2017-04-09 14:32:43,950 : INFO : PROGRESS: at example #23800000, processed 8916839329 words (1630348/s), 21808730 word types, 23795975 tags\n",
      "2017-04-09 14:33:07,930 : INFO : PROGRESS: at example #23900000, processed 8955390446 words (1607779/s), 21867076 word types, 23895952 tags\n",
      "2017-04-09 14:33:11,628 : INFO : Loading new file for index: 1040000\n",
      "2017-04-09 14:33:30,655 : INFO : PROGRESS: at example #24000000, processed 8991006042 words (1567374/s), 21923132 word types, 23995952 tags\n",
      "2017-04-09 14:33:56,575 : INFO : PROGRESS: at example #24100000, processed 9032309090 words (1593657/s), 22034304 word types, 24095538 tags\n",
      "2017-04-09 14:34:07,363 : INFO : Loading new file for index: 1050000\n",
      "2017-04-09 14:34:18,916 : INFO : PROGRESS: at example #24200000, processed 9068678217 words (1628054/s), 22094031 word types, 24195515 tags\n",
      "2017-04-09 14:34:42,035 : INFO : PROGRESS: at example #24300000, processed 9106693831 words (1644475/s), 22167126 word types, 24295515 tags\n",
      "2017-04-09 14:35:00,253 : INFO : Loading new file for index: 1060000\n",
      "2017-04-09 14:35:05,291 : INFO : PROGRESS: at example #24400000, processed 9144284636 words (1616551/s), 22226179 word types, 24395492 tags\n",
      "2017-04-09 14:35:28,465 : INFO : PROGRESS: at example #24500000, processed 9181749265 words (1616785/s), 22289717 word types, 24495469 tags\n",
      "2017-04-09 14:35:50,763 : INFO : PROGRESS: at example #24600000, processed 9218519703 words (1649227/s), 22342161 word types, 24595469 tags\n",
      "2017-04-09 14:35:52,539 : INFO : Loading new file for index: 1070000\n",
      "2017-04-09 14:36:14,143 : INFO : PROGRESS: at example #24700000, processed 9254245856 words (1528163/s), 22407939 word types, 24695469 tags\n",
      "2017-04-09 14:36:36,728 : INFO : PROGRESS: at example #24800000, processed 9290707222 words (1614593/s), 22465438 word types, 24795446 tags\n",
      "2017-04-09 14:36:45,231 : INFO : Loading new file for index: 1080000\n",
      "2017-04-09 14:36:59,636 : INFO : PROGRESS: at example #24900000, processed 9327416887 words (1602643/s), 22518305 word types, 24895446 tags\n",
      "2017-04-09 14:37:22,327 : INFO : PROGRESS: at example #25000000, processed 9363930645 words (1609316/s), 22579321 word types, 24995400 tags\n",
      "2017-04-09 14:37:38,180 : INFO : Loading new file for index: 1090000\n",
      "2017-04-09 14:37:45,413 : INFO : PROGRESS: at example #25100000, processed 9402008390 words (1649563/s), 22646354 word types, 25095354 tags\n",
      "2017-04-09 14:38:08,144 : INFO : PROGRESS: at example #25200000, processed 9439353391 words (1643010/s), 22699805 word types, 25195331 tags\n",
      "2017-04-09 14:38:31,571 : INFO : Loading new file for index: 1100000\n",
      "2017-04-09 14:38:32,022 : INFO : PROGRESS: at example #25300000, processed 9478575400 words (1642755/s), 22783708 word types, 25295308 tags\n",
      "2017-04-09 14:38:54,358 : INFO : PROGRESS: at example #25400000, processed 9515364923 words (1647283/s), 22840010 word types, 25395262 tags\n",
      "2017-04-09 14:39:16,327 : INFO : PROGRESS: at example #25500000, processed 9551387694 words (1639845/s), 22894898 word types, 25495262 tags\n",
      "2017-04-09 14:39:22,905 : INFO : Loading new file for index: 1110000\n",
      "2017-04-09 14:39:38,960 : INFO : PROGRESS: at example #25600000, processed 9588497685 words (1639762/s), 22958722 word types, 25595193 tags\n",
      "2017-04-09 14:40:01,429 : INFO : PROGRESS: at example #25700000, processed 9625315762 words (1638774/s), 23013427 word types, 25695193 tags\n",
      "2017-04-09 14:40:14,561 : INFO : Loading new file for index: 1120000\n",
      "2017-04-09 14:40:42,511 : INFO : PROGRESS: at example #25800000, processed 9663631737 words (932713/s), 23065758 word types, 25795170 tags\n",
      "2017-04-09 14:41:04,577 : INFO : PROGRESS: at example #25900000, processed 9700823161 words (1685631/s), 23122313 word types, 25895170 tags\n",
      "2017-04-09 14:41:24,400 : INFO : Loading new file for index: 1130000\n",
      "2017-04-09 14:41:26,825 : INFO : PROGRESS: at example #26000000, processed 9738582802 words (1697350/s), 23180323 word types, 25995147 tags\n",
      "2017-04-09 14:41:48,948 : INFO : PROGRESS: at example #26100000, processed 9776383966 words (1708763/s), 23247999 word types, 26095124 tags\n",
      "2017-04-09 14:42:10,948 : INFO : PROGRESS: at example #26200000, processed 9813392795 words (1682304/s), 23303295 word types, 26195124 tags\n",
      "2017-04-09 14:42:14,584 : INFO : Loading new file for index: 1140000\n",
      "2017-04-09 14:42:32,199 : INFO : PROGRESS: at example #26300000, processed 9849337641 words (1691682/s), 23350929 word types, 26295124 tags\n",
      "2017-04-09 14:42:53,760 : INFO : PROGRESS: at example #26400000, processed 9885439960 words (1674492/s), 23400056 word types, 26395124 tags\n",
      "2017-04-09 14:43:04,942 : INFO : Loading new file for index: 1150000\n",
      "2017-04-09 14:43:16,906 : INFO : PROGRESS: at example #26500000, processed 9924399804 words (1683363/s), 23470753 word types, 26495124 tags\n",
      "2017-04-09 14:43:39,093 : INFO : PROGRESS: at example #26600000, processed 9961780791 words (1684907/s), 23529461 word types, 26595124 tags\n",
      "2017-04-09 14:43:56,771 : INFO : Loading new file for index: 1160000\n",
      "2017-04-09 14:44:01,738 : INFO : PROGRESS: at example #26700000, processed 10000243797 words (1698688/s), 23589744 word types, 26695124 tags\n",
      "2017-04-09 14:44:23,920 : INFO : PROGRESS: at example #26800000, processed 10037288304 words (1670158/s), 23638038 word types, 26795124 tags\n",
      "2017-04-09 14:44:46,283 : INFO : PROGRESS: at example #26900000, processed 10074913969 words (1682584/s), 23692468 word types, 26895124 tags\n",
      "2017-04-09 14:44:48,084 : INFO : Loading new file for index: 1170000\n",
      "2017-04-09 14:45:08,679 : INFO : PROGRESS: at example #27000000, processed 10111467512 words (1632232/s), 23748635 word types, 26995124 tags\n",
      "2017-04-09 14:45:30,455 : INFO : PROGRESS: at example #27100000, processed 10147693319 words (1663705/s), 23799975 word types, 27095078 tags\n",
      "2017-04-09 14:45:38,869 : INFO : Loading new file for index: 1180000\n",
      "2017-04-09 14:45:52,451 : INFO : PROGRESS: at example #27200000, processed 10184503420 words (1673696/s), 23856896 word types, 27195055 tags\n",
      "2017-04-09 14:46:15,353 : INFO : PROGRESS: at example #27300000, processed 10223187893 words (1689229/s), 23920064 word types, 27295032 tags\n",
      "2017-04-09 14:46:30,820 : INFO : Loading new file for index: 1190000\n",
      "2017-04-09 14:46:37,631 : INFO : PROGRESS: at example #27400000, processed 10260368400 words (1669102/s), 23970706 word types, 27395032 tags\n",
      "2017-04-09 14:47:00,239 : INFO : PROGRESS: at example #27500000, processed 10298082911 words (1668312/s), 24024015 word types, 27495032 tags\n",
      "2017-04-09 14:47:21,549 : INFO : Loading new file for index: 1200000\n",
      "2017-04-09 14:47:21,948 : INFO : PROGRESS: at example #27600000, processed 10334237216 words (1665568/s), 24077767 word types, 27595032 tags\n",
      "2017-04-09 14:47:44,167 : INFO : PROGRESS: at example #27700000, processed 10371685539 words (1685533/s), 24134672 word types, 27695009 tags\n",
      "2017-04-09 14:48:06,449 : INFO : PROGRESS: at example #27800000, processed 10409263851 words (1686617/s), 24187942 word types, 27795009 tags\n",
      "2017-04-09 14:48:12,701 : INFO : Loading new file for index: 1210000\n",
      "2017-04-09 14:48:28,884 : INFO : PROGRESS: at example #27900000, processed 10447483989 words (1703733/s), 24279386 word types, 27894986 tags\n",
      "2017-04-09 14:48:51,016 : INFO : PROGRESS: at example #28000000, processed 10485018802 words (1696074/s), 24336758 word types, 27994963 tags\n",
      "2017-04-09 14:49:04,112 : INFO : Loading new file for index: 1220000\n",
      "2017-04-09 14:49:13,121 : INFO : PROGRESS: at example #28100000, processed 10522332178 words (1688150/s), 24393746 word types, 28094963 tags\n",
      "2017-04-09 14:49:35,270 : INFO : PROGRESS: at example #28200000, processed 10559819111 words (1692611/s), 24450076 word types, 28194963 tags\n",
      "2017-04-09 14:49:54,646 : INFO : Loading new file for index: 1230000\n",
      "2017-04-09 14:49:57,346 : INFO : PROGRESS: at example #28300000, processed 10597000040 words (1684303/s), 24500110 word types, 28294963 tags\n",
      "2017-04-09 14:50:19,224 : INFO : PROGRESS: at example #28400000, processed 10633663337 words (1675988/s), 24558833 word types, 28394940 tags\n",
      "2017-04-09 14:50:40,432 : INFO : PROGRESS: at example #28500000, processed 10668744647 words (1654209/s), 24604318 word types, 28494940 tags\n",
      "2017-04-09 14:50:44,420 : INFO : Loading new file for index: 1240000\n",
      "2017-04-09 14:51:02,206 : INFO : PROGRESS: at example #28600000, processed 10705040792 words (1667147/s), 24657022 word types, 28594940 tags\n",
      "2017-04-09 14:51:24,538 : INFO : PROGRESS: at example #28700000, processed 10742701464 words (1686505/s), 24705977 word types, 28694917 tags\n",
      "2017-04-09 14:51:35,020 : INFO : Loading new file for index: 1250000\n",
      "2017-04-09 14:51:46,638 : INFO : PROGRESS: at example #28800000, processed 10779955425 words (1685803/s), 24765776 word types, 28794894 tags\n",
      "2017-04-09 14:52:08,106 : INFO : PROGRESS: at example #28900000, processed 10815703709 words (1665335/s), 24818660 word types, 28894894 tags\n",
      "2017-04-09 14:52:25,354 : INFO : Loading new file for index: 1260000\n",
      "2017-04-09 14:52:30,567 : INFO : PROGRESS: at example #29000000, processed 10854000916 words (1705117/s), 24876199 word types, 28994871 tags\n",
      "2017-04-09 14:52:51,802 : INFO : PROGRESS: at example #29100000, processed 10889115264 words (1653713/s), 24921346 word types, 29094848 tags\n",
      "2017-04-09 14:53:13,954 : INFO : PROGRESS: at example #29200000, processed 10926484354 words (1687147/s), 24974833 word types, 29194848 tags\n",
      "2017-04-09 14:53:15,739 : INFO : Loading new file for index: 1270000\n",
      "2017-04-09 14:53:35,255 : INFO : PROGRESS: at example #29300000, processed 10961945667 words (1664825/s), 25031848 word types, 29294825 tags\n",
      "2017-04-09 14:53:56,964 : INFO : PROGRESS: at example #29400000, processed 10998521832 words (1684977/s), 25086908 word types, 29394825 tags\n",
      "2017-04-09 14:54:05,337 : INFO : Loading new file for index: 1280000\n",
      "2017-04-09 14:54:18,882 : INFO : PROGRESS: at example #29500000, processed 11036023642 words (1711158/s), 25138971 word types, 29494802 tags\n",
      "2017-04-09 14:54:37,716 : INFO : Loading new file for index: 1290000\n",
      "2017-04-09 14:54:37,718 : INFO : All files are loaded - last file: 1290000\n",
      "2017-04-09 14:54:39,222 : INFO : collected 25198638 word types and 29585475 unique tags from a corpus of 29590673 examples and 11070832244 words\n",
      "2017-04-09 14:54:56,692 : INFO : min_count=100 retains 434313 unique words (drops 24764325)\n",
      "2017-04-09 14:54:56,693 : INFO : min_count leaves 10962375449 word corpus (99% of original 11070832244)\n",
      "2017-04-09 14:54:58,793 : INFO : deleting the raw counts dictionary of 25198638 items\n",
      "2017-04-09 14:55:04,721 : INFO : sample=0.001 downsamples 32 most-common words\n",
      "2017-04-09 14:55:04,722 : INFO : downsampling leaves estimated 7517729447 word corpus (68.6% of prior 10962375449)\n",
      "2017-04-09 14:55:04,723 : INFO : estimated required memory for 434313 words and 200 dimensions: 30497532300 bytes\n",
      "2017-04-09 14:55:07,163 : INFO : resetting layer weights\n",
      "2017-04-09 15:08:50,242 : INFO : saving Doc2Vec object under /home/local/shalaby/parameter_search_doc2vec_models_3_description/full/vocab_model/model, separately None\n",
      "2017-04-09 15:08:50,244 : INFO : storing numpy array 'doctag_syn0' to /home/local/shalaby/parameter_search_doc2vec_models_3_description/full/vocab_model/model.docvecs.doctag_syn0.npy\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "Not enough free space to write 23668380000 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-1fe685e7859b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc2vec_model_save_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVOCAB_MODEL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMODEL_PREFIX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mdoc2vec_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_docs_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mREPORT_VOCAB_PROGRESS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mdoc2vec_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc2vec_model_save_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVOCAB_MODEL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMODEL_PREFIX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mdoc2vec_model_vocab_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc2vec_model_save_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVOCAB_MODEL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMODEL_PREFIX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env2/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1641\u001b[0m         \u001b[1;31m# don't bother storing the cached normalized vectors, recalculable table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1642\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'syn0norm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'table'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cum_table'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1643\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1645\u001b[0m     \u001b[0msave\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaveLoad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env2/lib/python2.7/site-packages/gensim/utils.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# `fname_or_handle` does not have write attribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             self._smart_save(fname_or_handle, separately, sep_limit, ignore,\n\u001b[1;32m--> 478\u001b[1;33m                              pickle_protocol=pickle_protocol)\n\u001b[0m\u001b[0;32m    479\u001b[0m \u001b[1;31m#endclass SaveLoad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env2/lib/python2.7/site-packages/gensim/utils.pyc\u001b[0m in \u001b[0;36m_smart_save\u001b[1;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         restores = self._save_specials(fname, separately, sep_limit, ignore, pickle_protocol,\n\u001b[1;32m--> 349\u001b[1;33m                                        compress, subname)\n\u001b[0m\u001b[0;32m    350\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env2/lib/python2.7/site-packages/gensim/utils.pyc\u001b[0m in \u001b[0;36m_save_specials\u001b[1;34m(self, fname, separately, sep_limit, ignore, pickle_protocol, compress, subname)\u001b[0m\n\u001b[0;32m    390\u001b[0m                 \u001b[0mcfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mattrib\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m                 restores.extend(val._save_specials(cfname, None, sep_limit, ignore,\n\u001b[1;32m--> 392\u001b[1;33m                                                    pickle_protocol, compress, subname))\n\u001b[0m\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env2/lib/python2.7/site-packages/gensim/utils.pyc\u001b[0m in \u001b[0;36m_save_specials\u001b[1;34m(self, fname, separately, sep_limit, ignore, pickle_protocol, compress, subname)\u001b[0m\n\u001b[0;32m    403\u001b[0m                         \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavez_compressed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m                         \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mattrib\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mignore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env2/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    507\u001b[0m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[1;32m--> 509\u001b[1;33m                            pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[0;32m    510\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env2/lib/python2.7/site-packages/numpy/lib/format.pyc\u001b[0m in \u001b[0;36mwrite_array\u001b[1;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m             \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m             for chunk in numpy.nditer(\n",
      "\u001b[1;31mIOError\u001b[0m: Not enough free space to write 23668380000 bytes"
     ]
    }
   ],
   "source": [
    "for level, model_name in models:\n",
    "    info(\"creating vocabulary for \" + str(level) + ' ' + model_name + ' in ')\n",
    "    doc2vec_model_save_location = os.path.join(root_location,\n",
    "                                               \"parameter_search_doc2vec_models_\" + str(level) + '_' + model_name,\n",
    "                                               \"full\")\n",
    "    if not os.path.exists(doc2vec_model_save_location):\n",
    "        os.makedirs(doc2vec_model_save_location)\n",
    "    if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL)):\n",
    "        os.makedirs(os.path.join(doc2vec_model_save_location, VOCAB_MODEL))\n",
    "\n",
    "    placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}_model_{}'.format(DOC2VEC_SIZE,\n",
    "                                                                    DOC2VEC_WINDOW,\n",
    "                                                                    'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                    DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                                    DOC2VEC_TRAIN_WORDS,\n",
    "                                                                    DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                                    str(DOC2VEC_MAX_VOCAB_SIZE),\n",
    "                                                                    str(level) + '_' + model_name\n",
    "                                                                    )\n",
    "    GLOBAL_VARS.DOC2VEC_MODEL_NAME = placeholder_model_name\n",
    "    placeholder_model_name = os.path.join(placeholder_model_name, \"epoch_{}\")\n",
    "    info(\"FILE \" + os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "    doc2vec_model = Doc2Vec(size=DOC2VEC_SIZE, window=DOC2VEC_WINDOW, min_count=MIN_WORD_COUNT,\n",
    "                    max_vocab_size= DOC2VEC_MAX_VOCAB_SIZE,\n",
    "                    sample=DOC2VEC_SAMPLE, seed=DOC2VEC_SEED, workers=NUM_CORES,\n",
    "                    # doc2vec algorithm dm=1 => PV-DM, dm=2 => PV-DBOW, PV-DM dictates CBOW for words\n",
    "                    dm=DOC2VEC_TYPE,\n",
    "                    # hs=0 => negative sampling, hs=1 => hierarchical softmax\n",
    "                    hs=DOC2VEC_HIERARCHICAL_SAMPLE, negative=DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                    dm_concat=DOC2VEC_CONCAT,\n",
    "                    # would train words with skip-gram on top of cbow, we don't need that for now\n",
    "                    dbow_words=DOC2VEC_TRAIN_WORDS,\n",
    "                    iter=DOC2VEC_EPOCHS)\n",
    "\n",
    "    GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "\n",
    "    training_docs_iterator = BatchWrapper(training_preprocessed_files_prefix, batch_size=10000, level=level,\n",
    "                                          level_type=model_name)\n",
    "    if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX)):\n",
    "        doc2vec_model.build_vocab(sentences=training_docs_iterator, progress_per=REPORT_VOCAB_PROGRESS)\n",
    "        doc2vec_model.save(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "    else:\n",
    "        doc2vec_model_vocab_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "        doc2vec_model.reset_from(doc2vec_model_vocab_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-09 15:45:35,319 : INFO : saving Doc2Vec object under /home/local/shalaby/parameter_search_doc2vec_models_3_description/full/vocab_model/model, separately None\n",
      "2017-04-09 15:45:35,322 : INFO : storing numpy array 'doctag_syn0' to /home/local/shalaby/parameter_search_doc2vec_models_3_description/full/vocab_model/model.docvecs.doctag_syn0.npy\n",
      "2017-04-09 15:45:55,735 : INFO : storing numpy array 'doctag_syn0_lockf' to /home/local/shalaby/parameter_search_doc2vec_models_3_description/full/vocab_model/model.docvecs.doctag_syn0_lockf.npy\n"
     ]
    }
   ],
   "source": [
    "doc2vec_model.save(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "doc2vec_model = Doc2Vec(size=DOC2VEC_SIZE , window=DOC2VEC_WINDOW, min_count=MIN_WORD_COUNT, \n",
    "                max_vocab_size= DOC2VEC_MAX_VOCAB_SIZE,\n",
    "                sample=DOC2VEC_SAMPLE, seed=DOC2VEC_SEED, workers=NUM_CORES,\n",
    "                # doc2vec algorithm dm=1 => PV-DM, dm=2 => PV-DBOW, PV-DM dictates CBOW for words\n",
    "                dm=DOC2VEC_TYPE,\n",
    "                # hs=0 => negative sampling, hs=1 => hierarchical softmax\n",
    "                hs=DOC2VEC_HIERARCHICAL_SAMPLE, negative=DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                dm_concat=DOC2VEC_CONCAT,\n",
    "                # would train words with skip-gram on top of cbow, we don't need that for now\n",
    "                dbow_words=DOC2VEC_TRAIN_WORDS,\n",
    "                iter=DOC2VEC_EPOCHS)\n",
    "\n",
    "GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## First: construct (or load) the vocabulary\n",
    "Only needed to be run if you dont already haave at least one epoch computed, otherwise, just set the start_from (below) to the epoch you want to restart from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-06 05:57:37,601 : INFO : loading Doc2Vec object from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/vocab_model/model\n",
      "2017-04-06 05:57:37,602 : INFO : Loading new file for index: 0\n",
      "2017-04-06 06:00:49,254 : INFO : loading docvecs recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/vocab_model/model.docvecs.* with mmap=None\n",
      "2017-04-06 06:00:49,255 : INFO : loading doctag_syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/vocab_model/model.docvecs.doctag_syn0.npy with mmap=None\n",
      "2017-04-06 06:01:07,168 : INFO : loading doctag_syn0_lockf from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/vocab_model/model.docvecs.doctag_syn0_lockf.npy with mmap=None\n",
      "2017-04-06 06:01:07,265 : INFO : loading syn1neg from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/vocab_model/model.syn1neg.npy with mmap=None\n",
      "2017-04-06 06:01:07,441 : INFO : loading syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks/full/vocab_model/model.syn0.npy with mmap=None\n",
      "2017-04-06 06:01:07,617 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-06 06:01:07,618 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-06 06:01:08,595 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 50s, sys: 1min 9s, total: 14min\n",
      "Wall time: 14min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# training_docs_iterator = ExtendedPVDocumentBatchGenerator(training_preprocessed_files_prefix, batch_size=10000)\n",
    "training_docs_iterator = BatchClass(training_preprocessed_files_prefix, batch_size=10000)\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX)):\n",
    "    doc2vec_model.build_vocab(sentences=training_docs_iterator, progress_per=REPORT_VOCAB_PROGRESS)\n",
    "    doc2vec_model.save(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "else:\n",
    "    doc2vec_model_vocab_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "    doc2vec_model.reset_from(doc2vec_model_vocab_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# vocab_counts = {k:doc2vec_model.vocab[k].count for k in doc2vec_model.vocab.keys()}\n",
    "# dd = sorted(vocab_counts, key=vocab_counts.get)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
