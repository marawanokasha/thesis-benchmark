{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K40m (CNMeM is disabled, cuDNN 5105)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "from functools import partial\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "from thesis.utils.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IS_SAMPLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_SEED = 1234\n",
    "DOC2VEC_SEED = 1234\n",
    "WORD2VEC_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUMBER_INDICATOR = \"number_inidicator\"\n",
    "CURRENCY_INDICATOR = \"currency_inidicator\"\n",
    "CHEMICAL_INDICATOR = \"chemical_inidicator\"\n",
    "MIN_WORD_COUNT = 20\n",
    "MIN_SIZE = 0\n",
    "NUM_CORES = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GLOBAL_VARS = namedtuple('GLOBAL_VARS', ['MODEL_NAME', 'DOC2VEC_MODEL', 'SVM_MODEL_NAME', 'NN_MODEL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATIO = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_MODEL = \"vocab_model\"\n",
    "MODEL_PREFIX = \"model\"\n",
    "VALIDATION_MATRIX = \"validation_matrix.pkl\"\n",
    "METRICS = \"metrics.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training_file = \"/home/local/shalaby/docs_output_sample_100.json\"\n",
    "\n",
    "save_parent_location = \"hdfs://deka.cip.ifi.lmu.de/pg-vectors/\"\n",
    "if IS_SAMPLE: \n",
    "    save_parent_location = save_parent_location + \"sample_\" + str(SAMPLE_RATIO) + \"/\"\n",
    "\n",
    "\n",
    "root_location = \"/mnt/data2/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "doc2vec_model_save_location = os.path.join(root_location, \"parameter_search_doc2vec_models_new\", \"sample_\" + str(SAMPLE_RATIO))\n",
    "if not os.path.exists(doc2vec_model_save_location):\n",
    "    os.makedirs(doc2vec_model_save_location)\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL)):\n",
    "    os.makedirs(os.path.join(doc2vec_model_save_location, VOCAB_MODEL))\n",
    "\n",
    "#training_file = root_location + \"docs_output.json\"\n",
    "training_file = root_location + 'docs_output_training_validation_documents_' + str(SAMPLE_RATIO)\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "training_docs_list_file = exports_location + \"training_documents_\" + str(SAMPLE_RATIO) + \"_sample.pkl\"\n",
    "validation_docs_list_file = exports_location + \"validation_documents_\" + str(SAMPLE_RATIO) + \"_sample.pkl\"\n",
    "\n",
    "\n",
    "training_preprocessed_files_prefix = root_location + \"training_docs_sample_%s_data_preprocessed-\" % str(SAMPLE_RATIO)\n",
    "training_preprocessed_docids_files_prefix = root_location + \"training_docs_sample_%s_docids_preprocessed-\" % str(SAMPLE_RATIO)\n",
    "validation_preprocessed_files_prefix = root_location + \"validation_docs_sample_%s_data_preprocessed-\" % str(SAMPLE_RATIO)\n",
    "validation_preprocessed_docids_files_prefix = root_location + \"validation_docs_sample_%s_docids_preprocessed-\" % str(SAMPLE_RATIO)\n",
    "\n",
    "word2vec_questions_file = result = root_location + 'tensorflow/word2vec/questions-words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.9 s, sys: 612 ms, total: 15.5 s\n",
      "Wall time: 15.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SMALL_SAMPLE_RATIO = 0.0001\n",
    "\n",
    "validation_small_preprocessed_files_prefix = root_location + \"validation_docs_sample_%s_data_preprocessed-\" % str(SMALL_SAMPLE_RATIO)\n",
    "validation_small_preprocessed_docids_files_prefix = root_location + \"validation_docs_sample_%s_docids_preprocessed-\" % str(SMALL_SAMPLE_RATIO)\n",
    "\n",
    "validation_small_docs_list_file = exports_location + \"validation_documents_\" + str(SMALL_SAMPLE_RATIO) + \"_sample.pkl\"\n",
    "validation_small_docs_list = pickle.load(open(validation_small_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49789"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12412"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemtokenizer(text):\n",
    "    \"\"\" MAIN FUNCTION to get clean stems out of a text. A list of clean stems are returned \"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\s+', gaps=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stems = []  # result\n",
    "    for token in tokens:\n",
    "        stem = token.lower()\n",
    "        stem = stem.strip(string.punctuation)\n",
    "        if stem:\n",
    "            if is_number(stem):\n",
    "                stem = NUMBER_INDICATOR\n",
    "            elif is_currency(stem):\n",
    "                stem = CURRENCY_INDICATOR\n",
    "            elif is_chemical(stem):\n",
    "                stem = CHEMICAL_INDICATOR\n",
    "            else:\n",
    "                stem = stem.strip(string.punctuation)\n",
    "            if stem and len(stem) >= MIN_SIZE:\n",
    "                # extract uni-grams\n",
    "                stems.append(stem)\n",
    "    del tokens\n",
    "    return stems\n",
    "\n",
    "def is_number(str):\n",
    "    \"\"\" Returns true if given string is a number (float or int)\"\"\"\n",
    "    try:\n",
    "        float(str.replace(\",\", \"\"))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_currency(str):\n",
    "    return str[0] == \"$\"\n",
    "\n",
    "def is_chemical(str):\n",
    "    return str.count(\"-\") > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_training_vector(classification, term_list, classifications, number_of_terms):\n",
    "    clss = 1 if classification in classifications else 0\n",
    "    return LabeledPoint(clss, SparseVector(number_of_terms, term_list))\n",
    "\n",
    "def train_level_new(docs_index, classification, doc_classification_map, number_of_terms):\n",
    "    training_vectors = docs_index.map(\n",
    "        lambda (doc_id, postings): get_training_vector(classification, postings,\n",
    "                                                        doc_classification_map[doc_id], number_of_terms))\n",
    "    svm = SVMWithSGD.train(training_vectors, iterations=SVM_ITERATIONS, convergenceTol=SVM_CONVERGENCE, regParam=SVM_REG)\n",
    "    return training_vectors, svm\n",
    "\n",
    "def model_exists(path):\n",
    "    try:\n",
    "        model = SVMModel.load(sc, path)\n",
    "        return True;\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def get_training_vector(classification, dense_vector, classifications):\n",
    "    clss = 1 if classification in classifications else 0\n",
    "    return LabeledPoint(clss, dense_vector)\n",
    "\n",
    "def train_level_doc2vec(classification, doc_classification_map):\n",
    "    doc2vec_model = GLOBAL_VARS.DOC2VEC_MODEL\n",
    "    training_vectors = []\n",
    "    for doc_id in training_docs_list:\n",
    "        # converting from memmap to a normal array as spark is unable to convert memmap to a spark Vector\n",
    "        normal_array = []\n",
    "        normal_array[:] = doc2vec_model.docvecs[doc_id][:]\n",
    "        training_vectors.append(get_training_vector(classification, normal_array, \n",
    "                                                    doc_classification_map[doc_id]))\n",
    "    info(\"Finished getting training vectors\")\n",
    "    training_vectors = sc.parallelize(training_vectors)\n",
    "    info(\"Finished parallelization\")\n",
    "    svm = SVMWithSGD.train(training_vectors, iterations=SVM_ITERATIONS, convergenceTol=SVM_CONVERGENCE, regParam=SVM_REG)\n",
    "    return training_vectors, svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ensure_hdfs_location_exists(location):\n",
    "    parent = os.path.dirname(location)\n",
    "    os.system(\"hdfs dfs -mkdir -p \" + location)\n",
    "\n",
    "def ensure_disk_location_exists(location):\n",
    "    if not os.path.exists(location):\n",
    "        os.makedirs(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_classifications(classifications):\n",
    "    info(\"====== Doing Training\")\n",
    "    i=0\n",
    "    for classification in classifications:\n",
    "        print classification\n",
    "        try:\n",
    "            model_path = get_svm_model_path(GLOBAL_VARS.MODEL_NAME, classification)\n",
    "            if not model_exists(model_path):\n",
    "                training_vectors, svm = train_level_doc2vec(classification, doc_classification_map)\n",
    "                svm.save(sc, model_path)\n",
    "            else:\n",
    "                print \"Model Exists\"\n",
    "        except:\n",
    "            print \"Problem creating: %s: %s\" % (classification, GLOBAL_VARS.MODEL_NAME)\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_validation(validation_vectors_matrix, doc_classification_map, classifications, classifications_name):\n",
    "\n",
    "    info(\"====== Doing Validation\")\n",
    "    method = GLOBAL_VARS.MODEL_NAME\n",
    "    subset = classifications_name\n",
    "\n",
    "    doc_count = validation_vectors_matrix.shape[0]\n",
    "    y_score = np.zeros((doc_count, len(classifications)))\n",
    "    y_true = np.zeros((doc_count, len(classifications)))\n",
    "    i=0\n",
    "\n",
    "    for classification in classifications:\n",
    "        print classification\n",
    "\n",
    "        validation_vectors = get_validation_doc2vec_spark_vectors(validation_vectors_matrix, \n",
    "                                                                  classification, doc_classification_map)\n",
    "        #global binarySvm\n",
    "        binarySvm = SVMModel.load(sc, get_svm_model_path(GLOBAL_VARS.MODEL_NAME, classification))\n",
    "        info(\"Loaded the model, Doing the prediction now....\")\n",
    "        binarySvm.clearThreshold()\n",
    "        binarySvmB = sc.broadcast(binarySvm)\n",
    "        # using the broadcasted binarySvm variable, fixes global name 'binarySvm' is not defined as this variable was not\n",
    "        # available in the workers, so we pass it explicitly to the mapper using partial\n",
    "        labels_predictions = validation_vectors.map( \\\n",
    "            partial(lambda svm, p: (p.label, svm.value.predict(p.features)), binarySvmB) \\\n",
    "        ).collect()\n",
    "        #labels = test_labeled_points.map(lambda p: p.labels)\n",
    "        y_true[:,i] = [label_pred[0] for label_pred in labels_predictions]\n",
    "        y_score[:,i] = [label_pred[1] for label_pred in labels_predictions]\n",
    "        i+=1\n",
    "    y_binary_score = get_binary(y_score)\n",
    "    # results[method][\"y_true\"] = y_true\n",
    "    # results[method][\"y_score\"] = y_score\n",
    "    # results[method][\"y_binary_score\"] = y_binary_score\n",
    "    metrics = get_metrics(y_true, y_score, y_binary_score)\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_validation_docs_with_inference(doc2vec_model, doc_classification_map):\n",
    "    \"\"\"\n",
    "    Use the trained doc2vec model to get the paragraph vector representations of the validation documents\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX)):\n",
    "        info(\"===== Loading validation vectors\")\n",
    "        validation_vectors_matrix = pickle.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX)))\n",
    "    else:\n",
    "        validation_documents_reps = {}\n",
    "        validation_vectors = []\n",
    "        validation_labels = []\n",
    "        info(\"===== Getting validation vectors with inference\")\n",
    "\n",
    "        # do inference and store results in dict\n",
    "        i = 0\n",
    "        for (doc_id, doc_contents_array) in ValidationDocumentGenerator(training_file, validation_docs_list):\n",
    "            i += 1\n",
    "            if i % 1000 == 0: info(\"Finished: {}\".format(str(i)))\n",
    "            validation_documents_reps[doc_id] = doc2vec_model.infer_vector(doc_contents_array)\n",
    "\n",
    "        # create matrix for the validation vectors\n",
    "        for validation_doc_id in validation_docs_list:\n",
    "            validation_vectors.append(validation_documents_reps[validation_doc_id])\n",
    "            validation_labels.append([classf for classf in doc_classification_map[validation_doc_id] if classf in sections])\n",
    "        validation_vectors_matrix = np.array(validation_vectors)\n",
    "        pickle.dump(validation_vectors_matrix, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX), 'w'))\n",
    "    \n",
    "    return validation_vectors_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_docs_with_inference_new(doc2vec_model, doc_classification_map, classifications, \n",
    "                                           val_docs_list, val_preprocessed_files_prefix, val_preprocessed_docids_files_prefix):\n",
    "    \"\"\"\n",
    "    Use the trained doc2vec model to get the paragraph vector representations of the validation documents\n",
    "    \"\"\"\n",
    "\n",
    "    def infer_one_doc(doc_tuple):\n",
    "        #doc2vec_model.random = np.random.RandomState(DOC2VEC_SEED)\n",
    "        doc_id, doc_tokens = doc_tuple\n",
    "        rep = doc2vec_model.infer_vector(doc_tokens)\n",
    "        return (doc_id, rep)\n",
    "\n",
    "    one_hot_encoder = OneHotEncoder(classifications)\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX)):\n",
    "        info(\"===== Loading validation vectors\")\n",
    "        validation_labels = []\n",
    "        validation_vectors_matrix = pickle.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX)))\n",
    "        for validation_doc_id in val_docs_list:\n",
    "            val_labels = [classf for classf in doc_classification_map[validation_doc_id] if classf in classifications]\n",
    "            validation_labels.append(one_hot_encoder.get_label_vector(val_labels))\n",
    "        validation_labels = np.array(validation_labels)\n",
    "    else:\n",
    "        validation_documents_reps = {}\n",
    "        validation_vectors = []\n",
    "        validation_labels = []\n",
    "        info(\"===== Getting validation vectors with inference\")\n",
    "\n",
    "        # Single-threaded inference\n",
    "        # do inference and store results in dict\n",
    "#         i = 0\n",
    "        \n",
    "#         validation_docs_iterator = DocumentBatchGenerator(val_preprocessed_files_prefix, \n",
    "#                                                         val_preprocessed_docids_files_prefix, batch_size=None)\n",
    "#         for (doc_id, doc_contents_array) in validation_docs_iterator:\n",
    "#             i += 1\n",
    "#             if i % 1000 == 0: info(\"Finished: {}\".format(str(i)))\n",
    "#             validation_documents_reps[doc_id] = doc2vec_model.infer_vector(doc_contents_array)\n",
    "        \n",
    "        # Multi-threaded inference\n",
    "        validation_docs_iterator = DocumentBatchGenerator(validation_preprocessed_files_prefix, \n",
    "                                                          validation_preprocessed_docids_files_prefix, batch_size=None)\n",
    "        generator_func = validation_docs_iterator.__iter__()\n",
    "        pool = ThreadPool(NUM_CORES)\n",
    "        # map consumes the whole iterator on the spot, so we have to use itertools.islice to fake mini-batching\n",
    "        validation_documents_reps = {}\n",
    "        mini_batch_size = 1000\n",
    "        while True:\n",
    "            threaded_reps_partial = pool.map(infer_one_doc, itertools.islice(generator_func, mini_batch_size))\n",
    "            info(\"Finished: {}\".format(str(validation_docs_iterator.curr_index)))\n",
    "            if threaded_reps_partial:\n",
    "                #threaded_reps.extend(threaded_reps_partial)\n",
    "                validation_documents_reps.update(threaded_reps_partial)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "                \n",
    "        # create matrix for the validation vectors\n",
    "        for validation_doc_id in val_docs_list:\n",
    "            validation_vectors.append(validation_documents_reps[validation_doc_id])\n",
    "            val_labels = [classf for classf in doc_classification_map[validation_doc_id] if classf in classifications]\n",
    "            validation_labels.append(one_hot_encoder.get_label_vector(val_labels))\n",
    "        validation_vectors_matrix = np.array(validation_vectors)\n",
    "        validation_labels = np.array(validation_labels)\n",
    "        pickle.dump(validation_vectors_matrix, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX), 'w'))\n",
    "    \n",
    "    return validation_vectors_matrix, validation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_doc2vec_spark_vectors(validation_vectors_matrix, classification, doc_classification_map):\n",
    "    validation_vectors = []\n",
    "    for (index, doc_id) in enumerate(validation_docs_list):\n",
    "        # converting from memmap to a normal array as spark is unable to convert memmap to a spark Vector\n",
    "        validation_vector = validation_vectors_matrix[index]\n",
    "        validation_vectors.append(get_training_vector(classification, validation_vector, \n",
    "                                                    doc_classification_map[doc_id]))\n",
    "    validation_vectors = sc.parallelize(validation_vectors)\n",
    "    info(\"Finished getting validation vectors\")\n",
    "    return validation_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OneHotEncoder():\n",
    "    \n",
    "    def __init__(self, classifications):\n",
    "        self.classifications = classifications\n",
    "        self.one_hot_indices = {}\n",
    "\n",
    "        # convert character classifications to bit vectors\n",
    "        for i, clssf in enumerate(classifications):\n",
    "            bits = [0] * len(classifications)\n",
    "            bits[i] = 1\n",
    "            self.one_hot_indices[clssf] = i\n",
    "    \n",
    "    def get_label_vector(self, labels):\n",
    "        \"\"\"\n",
    "        classes: array of string with the classes assigned to the instance\n",
    "        \"\"\"\n",
    "        output_vector = [0] * len(self.classifications)\n",
    "        for label in labels:\n",
    "            index = self.one_hot_indices[label]\n",
    "            output_vector[index] = 1\n",
    "            \n",
    "        return output_vector\n",
    "\n",
    "def get_training_data(doc2vec_model, classifications):\n",
    "    one_hot_encoder = OneHotEncoder(classifications)\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    for doc_id in training_docs_list:\n",
    "        # converting from memmap to a normal array\n",
    "        normal_array = []\n",
    "        normal_array[:] = doc2vec_model.docvecs[doc_id][:]\n",
    "        training_data.append(normal_array)\n",
    "        eligible_classifications = [clssf for clssf in doc_classification_map[doc_id] if clssf in classifications]\n",
    "        training_labels.append(one_hot_encoder.get_label_vector(eligible_classifications))\n",
    "    training_labels = np.array(training_labels)\n",
    "    return training_data, training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_svm_model_path(method, classification, reg=SVM_REG, iterations=SVM_ITERATIONS):\n",
    "    location = os.path.join(save_parent_location, \"models\", method, \n",
    "                            \"iter_\" + str(iterations) + \"_reg_\" + str(reg),\n",
    "                            classification + \"_model.svm\")\n",
    "    ensure_hdfs_location_exists(location)\n",
    "    return location\n",
    "\n",
    "class TrainingDocumentGenerator(object):\n",
    "    def __init__(self, filename, training_docs_list):\n",
    "        self.filename = filename\n",
    "        self.training_docs_list = training_docs_list\n",
    "    def __iter__(self):\n",
    "        with open(self.filename) as file_obj:\n",
    "            for line in file_obj:\n",
    "                if not line.strip(): continue\n",
    "                (doc_id, text) = eval(line)\n",
    "                if doc_id in self.training_docs_list:\n",
    "                    yield LabeledSentence(words=stemtokenizer(text), tags=[doc_id])\n",
    "                    \n",
    "class DocumentBatchGenerator(object):\n",
    "    def __init__(self, filename_prefix, filename_docids_prefix, batch_size=10000 ):\n",
    "        \"\"\"\n",
    "        batch_size cant be > 10,000 due to a limitation in doc2vec training, \n",
    "        None means no batching (only use for inference)\n",
    "        \"\"\"\n",
    "        assert batch_size <= 10000 or batch_size is None\n",
    "        self.filename_prefix = filename_prefix\n",
    "        self.filename_docids_prefix = filename_docids_prefix\n",
    "        self.curr_lines = []\n",
    "        self.curr_docids = []\n",
    "        self.batch_size = batch_size\n",
    "        self.curr_index = 0\n",
    "        self.batch_end = -1\n",
    "    def load_new_batch_in_memory(self):\n",
    "        self.curr_lines, self.docids = [], []\n",
    "        info(\"Loading new batch for index: {}\".format(self.curr_index) )\n",
    "        try:\n",
    "            with open(self.filename_prefix + str(self.curr_index)) as preproc_file:\n",
    "                for line in preproc_file:\n",
    "                    self.curr_lines.append(line.split(\" \"))\n",
    "#                     if i % 1000 == 0:\n",
    "#                         print i\n",
    "            self.curr_docids = pickle.load(open(self.filename_docids_prefix + str(self.curr_index), \"r\"))\n",
    "            self.batch_end = self.curr_index + len(self.curr_lines) -1 \n",
    "            info(\"Finished loading new batch\")\n",
    "        except IOError:\n",
    "            info(\"No more batches to load, exiting at index: {}\".format(self.curr_index))\n",
    "            raise StopIteration()\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            if self.curr_index > self.batch_end:\n",
    "                self.load_new_batch_in_memory()\n",
    "            for (doc_id, tokens) in zip(self.curr_docids, self.curr_lines):\n",
    "                if self.batch_size is not None:\n",
    "                    curr_batch_iter = 0\n",
    "                    # divide the document to batches according to the batch size\n",
    "                    while curr_batch_iter < len(tokens):\n",
    "                        yield LabeledSentence(words=tokens[curr_batch_iter: curr_batch_iter + self.batch_size], tags=[doc_id])\n",
    "                        curr_batch_iter += self.batch_size\n",
    "                else:\n",
    "                    yield doc_id, tokens\n",
    "                self.curr_index += 1\n",
    "\n",
    "class Word2VecTrainingDocumentGenerator(object):\n",
    "    def __init__(self, filename, training_docs_list):\n",
    "        self.filename = filename\n",
    "        self.training_docs_list = training_docs_list\n",
    "    def __iter__(self):\n",
    "        with open(self.filename) as file_obj:\n",
    "            for line in file_obj:\n",
    "                if not line.strip(): continue\n",
    "                (doc_id, text) = eval(line)\n",
    "                if doc_id in self.training_docs_list:\n",
    "                    yield stemtokenizer(text)\n",
    "                \n",
    "class ValidationDocumentGenerator(object):\n",
    "    def __init__(self, filename, validation_docs_list):\n",
    "        self.filename = filename\n",
    "        self.validation_docs_list = validation_docs_list\n",
    "    def __iter__(self):\n",
    "        with open(self.filename) as file_obj:\n",
    "            for line in file_obj:\n",
    "                if not line.strip(): continue\n",
    "                (doc_id, text) = eval(line)\n",
    "                if doc_id in self.validation_docs_list:\n",
    "                    yield doc_id, stemtokenizer(text)\n",
    "                    \n",
    "class StochasticDocumentGenerator(object):\n",
    "    \"\"\"\n",
    "    Randomly shuffle rows while reading them\n",
    "    \"\"\"\n",
    "    def __init__(self, filename, training_docs_list, line_positions):\n",
    "        self.filename = filename\n",
    "        self.training_docs_list = training_docs_list\n",
    "        self.line_positions = line_positions\n",
    "        self.lines = set(line_positions.keys())\n",
    "    def __iter__(self):\n",
    "        with open(self.filename) as file_obj:\n",
    "            while len(self.lines) > 0:\n",
    "                random_line = random.sample(self.lines,1)[0]\n",
    "                self.lines.remove(random_line)\n",
    "                file_obj.seek(self.line_positions[random_line])\n",
    "                line = file_obj.readline()\n",
    "                if not line.strip(): continue\n",
    "#                 print random_line, self.line_positions[random_line], line[:30]\n",
    "                (doc_id, text) = eval(line)\n",
    "                # print random_line , doc_id\n",
    "                if doc_id in self.training_docs_list:\n",
    "                    yield LabeledSentence(words=stemtokenizer(text), tags=[doc_id])\n",
    "#                     yield doc_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get starting positions in bytes for every line to be able to do random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.62 s, sys: 592 ms, total: 7.22 s\n",
      "Wall time: 7.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "line_positions = dict()\n",
    "with open(training_file) as f:\n",
    "    \n",
    "    i = 0\n",
    "    line_positions[i] = f.tell()\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        i+=1\n",
    "        if not line.strip(): continue\n",
    "        line_positions[i] = f.tell()\n",
    "        line = f.readline()\n",
    "    del line_positions[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2vec and SVM Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOC2VEC_SIZE = 300\n",
    "DOC2VEC_WINDOW = 8\n",
    "DOC2VEC_MAX_VOCAB_SIZE = None\n",
    "DOC2VEC_SAMPLE = 1e-3\n",
    "DOC2VEC_TYPE = 1\n",
    "DOC2VEC_HIERARCHICAL_SAMPLE = 0\n",
    "DOC2VEC_NEGATIVE_SAMPLE_SIZE = 10\n",
    "DOC2VEC_CONCAT = 1\n",
    "DOC2VEC_MEAN = 0\n",
    "DOC2VEC_TRAIN_WORDS = 0\n",
    "DOC2VEC_EPOCHS = 1 # we do our training manually one epoch at a time\n",
    "DOC2VEC_MAX_EPOCHS = 20\n",
    "REPORT_DELAY = 20 # report the progress every x seconds\n",
    "REPORT_VOCAB_PROGRESS = 1000 # report the progress every x terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_ITERATIONS = 100\n",
    "SVM_CONVERGENCE = 0.001\n",
    "SVM_REG = 0.001\n",
    "GLOBAL_VARS.SVM_MODEL_NAME = 'iter_{}_reg_{}'.format(SVM_ITERATIONS, SVM_REG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_HIDDEN_NEURONS = 4500\n",
    "NN_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_{}'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}'.format(DOC2VEC_SIZE, \n",
    "                                                                DOC2VEC_WINDOW, \n",
    "                                                                'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                                DOC2VEC_TRAIN_WORDS,\n",
    "                                                                DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                                str(DOC2VEC_MAX_VOCAB_SIZE))\n",
    "placeholder_model_name = placeholder_model_name + \"_curriter_{}\"\n",
    "placeholder_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc2vec_model = Doc2Vec(size=DOC2VEC_SIZE , window=DOC2VEC_WINDOW, min_count=MIN_WORD_COUNT, \n",
    "                max_vocab_size= DOC2VEC_MAX_VOCAB_SIZE,\n",
    "                sample=DOC2VEC_SAMPLE, seed=DOC2VEC_SEED, workers=NUM_CORES,\n",
    "                # doc2vec algorithm dm=1 => PV-DM, dm=2 => PV-DBOW, PV-DM dictates CBOW for words\n",
    "                dm=DOC2VEC_TYPE,\n",
    "                # hs=0 => negative sampling, hs=1 => hierarchical softmax\n",
    "                hs=DOC2VEC_HIERARCHICAL_SAMPLE, negative=DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                dm_concat=DOC2VEC_CONCAT,\n",
    "                # would train words with skip-gram on top of cbow, we don't need that for now\n",
    "                dbow_words=DOC2VEC_TRAIN_WORDS,\n",
    "                iter=DOC2VEC_EPOCHS)\n",
    "\n",
    "GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 04:29:54,015 : INFO : loading Doc2Vec object from /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/vocab_model/model\n",
      "2016-12-29 04:29:58,435 : INFO : loading docvecs recursively from /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/vocab_model/model.docvecs.* with mmap=None\n",
      "2016-12-29 04:29:58,438 : INFO : loading syn1neg from /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/vocab_model/model.syn1neg.npy with mmap=None\n",
      "2016-12-29 04:29:59,748 : INFO : loading syn0 from /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/vocab_model/model.syn0.npy with mmap=None\n",
      "2016-12-29 04:29:59,767 : INFO : setting ignored attribute syn0norm to None\n",
      "2016-12-29 04:29:59,768 : INFO : setting ignored attribute cum_table to None\n",
      "2016-12-29 04:30:00,162 : INFO : using concatenative 5100-dimensional layer1\n",
      "2016-12-29 04:30:00,164 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.6 s, sys: 2.4 s, total: 9.99 s\n",
      "Wall time: 9.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_docs_iterator = DocumentBatchGenerator(training_preprocessed_files_prefix, \n",
    "                                                        training_preprocessed_docids_files_prefix, batch_size=10000)\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX)):\n",
    "    doc2vec_model.build_vocab(sentences=training_docs_iterator, progress_per=REPORT_VOCAB_PROGRESS)\n",
    "    doc2vec_model.save(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "else:\n",
    "    doc2vec_model_vocab_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "    doc2vec_model.reset_from(doc2vec_model_vocab_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_counts = {k:doc2vec_model.vocab[k].count for k in doc2vec_model.vocab.keys()}\n",
    "dd = sorted(vocab_counts, key=vocab_counts.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Training, validation and Metrics Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2vec_model.min_alpha = 0.025\n",
    "epoch_metrics = []\n",
    "word2vec_results = []\n",
    "classifications = sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        this.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width);\n",
       "        canvas.attr('height', height);\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'];\n",
       "    var y0 = fig.canvas.height - msg['y0'];\n",
       "    var x1 = msg['x1'];\n",
       "    var y1 = fig.canvas.height - msg['y1'];\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x;\n",
       "    var y = canvas_pos.y;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overriden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 04:30:05,355 : INFO : ****************** Epoch 1 --- Working on doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_1 *******************\n",
      "2016-12-29 04:30:05,356 : INFO : loading Doc2Vec object from /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_1/model\n",
      "2016-12-29 04:30:05,961 : INFO : loading docvecs recursively from /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_1/model.docvecs.* with mmap=None\n",
      "2016-12-29 04:30:05,964 : INFO : loading doctag_syn0 from /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_1/model.docvecs.doctag_syn0.npy with mmap=None\n",
      "2016-12-29 04:30:06,005 : INFO : loading syn1neg from /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_1/model.syn1neg.npy with mmap=None\n",
      "2016-12-29 04:30:10,869 : INFO : loading syn0 from /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_1/model.syn0.npy with mmap=None\n",
      "2016-12-29 04:30:11,218 : INFO : setting ignored attribute syn0norm to None\n",
      "2016-12-29 04:30:11,219 : INFO : setting ignored attribute cum_table to None\n",
      "2016-12-29 04:30:12,978 : INFO : Training Classifier\n",
      "2016-12-29 04:30:49,836 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 04:30:58,535 : INFO : Getting Validation Embeddings\n",
      "2016-12-29 04:30:58,536 : INFO : ===== Loading validation vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Training Metrics: Cov Err: 6.762, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.451, Top 3: 0.658, Top 5: 0.689, \n",
      "\t\t F1 Micro: 0.323, Total Pos: 16,749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 04:31:00,113 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [1 0 1 ..., 0 0 0]]\n",
      "** Validation Metrics: Cov Err: 6.129, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.451, Top 3: 0.679, Top 5: 0.724, \n",
      "\t\t F1 Micro: 0.401, Total Pos: 9,500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 04:31:00,947 : INFO : precomputing L2-norms of word weight vectors\n",
      "2016-12-29 04:31:04,944 : INFO : capital-common-countries: 0.0% (0/156)\n",
      "2016-12-29 04:31:07,994 : INFO : capital-world: 0.0% (0/152)\n",
      "2016-12-29 04:31:08,801 : INFO : currency: 0.0% (0/40)\n",
      "2016-12-29 04:31:33,403 : INFO : city-in-state: 0.0% (0/1248)\n",
      "2016-12-29 04:31:35,573 : INFO : family: 1.8% (2/110)\n",
      "2016-12-29 04:31:46,191 : INFO : gram1-adjective-to-adverb: 1.1% (6/552)\n",
      "2016-12-29 04:31:52,764 : INFO : gram2-opposite: 0.9% (3/342)\n",
      "2016-12-29 04:32:18,092 : INFO : gram3-comparative: 11.3% (150/1332)\n",
      "2016-12-29 04:32:32,401 : INFO : gram4-superlative: 1.3% (10/756)\n",
      "2016-12-29 04:32:50,275 : INFO : gram5-present-participle: 3.5% (33/930)\n",
      "2016-12-29 04:33:01,689 : INFO : gram6-nationality-adjective: 0.0% (0/584)\n",
      "2016-12-29 04:33:25,915 : INFO : gram7-past-tense: 2.1% (27/1260)\n",
      "2016-12-29 04:33:44,634 : INFO : gram8-plural: 8.2% (81/992)\n",
      "2016-12-29 04:33:58,060 : INFO : gram9-plural-verbs: 7.0% (49/702)\n",
      "2016-12-29 04:33:58,064 : INFO : total: 3.9% (361/9156)\n",
      "2016-12-29 04:33:58,077 : INFO : ****************** Epoch 2 --- Working on doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_2 *******************\n",
      "2016-12-29 04:33:58,081 : INFO : training model with 16 workers on 146034 vocabulary and 5100 features, using sg=0 hs=0 sample=0.001 negative=10\n",
      "2016-12-29 04:33:58,083 : INFO : expecting 65535 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-12-29 04:33:58,119 : INFO : Loading new batch for index: 0\n",
      "2016-12-29 04:34:03,920 : INFO : Finished loading new batch\n",
      "2016-12-29 04:34:04,808 : INFO : PROGRESS: at 0.00% examples, 340 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 04:34:24,817 : INFO : PROGRESS: at 1.04% examples, 111667 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 04:34:44,836 : INFO : PROGRESS: at 2.19% examples, 131102 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 04:35:04,860 : INFO : PROGRESS: at 3.39% examples, 140175 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:35:24,941 : INFO : PROGRESS: at 4.57% examples, 145622 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:35:44,967 : INFO : PROGRESS: at 5.74% examples, 149459 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:36:04,974 : INFO : PROGRESS: at 6.97% examples, 152280 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:36:25,024 : INFO : PROGRESS: at 8.20% examples, 154628 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:36:45,040 : INFO : PROGRESS: at 9.47% examples, 156292 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:37:05,070 : INFO : PROGRESS: at 10.71% examples, 157692 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:37:25,071 : INFO : PROGRESS: at 11.94% examples, 158792 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 04:37:45,100 : INFO : PROGRESS: at 13.19% examples, 159736 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:38:05,125 : INFO : PROGRESS: at 14.46% examples, 160444 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 04:38:25,144 : INFO : PROGRESS: at 15.68% examples, 161031 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 04:38:45,154 : INFO : PROGRESS: at 16.94% examples, 161626 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:39:05,184 : INFO : PROGRESS: at 18.20% examples, 162126 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:39:25,214 : INFO : PROGRESS: at 19.45% examples, 162580 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:39:36,232 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 04:39:44,320 : INFO : Finished loading new batch\n",
      "2016-12-29 04:39:45,246 : INFO : PROGRESS: at 20.03% examples, 158061 words/s, in_qsize 1, out_qsize 0\n",
      "2016-12-29 04:40:05,247 : INFO : PROGRESS: at 21.18% examples, 158564 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 04:40:25,298 : INFO : PROGRESS: at 22.42% examples, 159159 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:40:45,309 : INFO : PROGRESS: at 23.72% examples, 159675 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:41:05,361 : INFO : PROGRESS: at 24.97% examples, 160157 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:41:25,440 : INFO : PROGRESS: at 26.21% examples, 160619 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:41:45,464 : INFO : PROGRESS: at 27.51% examples, 161133 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:42:05,468 : INFO : PROGRESS: at 28.75% examples, 161534 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:42:25,593 : INFO : PROGRESS: at 29.96% examples, 161884 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:42:45,629 : INFO : PROGRESS: at 31.22% examples, 162243 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 04:43:05,643 : INFO : PROGRESS: at 32.47% examples, 162608 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:43:25,650 : INFO : PROGRESS: at 33.71% examples, 162857 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:43:45,691 : INFO : PROGRESS: at 34.95% examples, 163115 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 04:44:05,747 : INFO : PROGRESS: at 36.24% examples, 163338 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:44:25,762 : INFO : PROGRESS: at 37.46% examples, 163569 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:44:45,771 : INFO : PROGRESS: at 38.67% examples, 163761 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:45:05,792 : INFO : PROGRESS: at 39.88% examples, 163980 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:45:10,376 : INFO : Loading new batch for index: 20000\n",
      "2016-12-29 04:45:18,941 : INFO : Finished loading new batch\n",
      "2016-12-29 04:45:25,792 : INFO : PROGRESS: at 40.49% examples, 161638 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:45:45,806 : INFO : PROGRESS: at 41.68% examples, 161898 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:46:05,807 : INFO : PROGRESS: at 42.94% examples, 162198 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:46:25,818 : INFO : PROGRESS: at 44.12% examples, 162439 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:46:45,839 : INFO : PROGRESS: at 45.36% examples, 162650 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 04:47:05,853 : INFO : PROGRESS: at 46.67% examples, 162886 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 04:47:25,870 : INFO : PROGRESS: at 47.96% examples, 163115 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:47:45,900 : INFO : PROGRESS: at 49.22% examples, 163301 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:48:05,899 : INFO : PROGRESS: at 50.47% examples, 163520 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:48:25,906 : INFO : PROGRESS: at 51.68% examples, 163698 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:48:45,950 : INFO : PROGRESS: at 52.95% examples, 163900 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:49:05,998 : INFO : PROGRESS: at 54.22% examples, 164085 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:49:26,043 : INFO : PROGRESS: at 55.45% examples, 164283 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:49:46,055 : INFO : PROGRESS: at 56.77% examples, 164439 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:50:06,070 : INFO : PROGRESS: at 58.01% examples, 164590 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:50:26,080 : INFO : PROGRESS: at 59.25% examples, 164646 words/s, in_qsize 32, out_qsize 1\n",
      "2016-12-29 04:50:42,369 : INFO : Loading new batch for index: 30000\n",
      "2016-12-29 04:50:50,124 : INFO : Finished loading new batch\n",
      "2016-12-29 04:50:51,435 : INFO : PROGRESS: at 60.28% examples, 163207 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:51:11,439 : INFO : PROGRESS: at 61.46% examples, 163326 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:51:31,471 : INFO : PROGRESS: at 62.74% examples, 163450 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:51:51,506 : INFO : PROGRESS: at 63.97% examples, 163571 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:52:11,536 : INFO : PROGRESS: at 65.20% examples, 163695 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:52:31,541 : INFO : PROGRESS: at 66.49% examples, 163815 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:52:51,562 : INFO : PROGRESS: at 67.68% examples, 163900 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:53:11,631 : INFO : PROGRESS: at 68.92% examples, 163993 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:53:31,656 : INFO : PROGRESS: at 70.19% examples, 164087 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:53:51,697 : INFO : PROGRESS: at 71.37% examples, 164180 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 04:54:11,736 : INFO : PROGRESS: at 72.62% examples, 164290 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:54:31,791 : INFO : PROGRESS: at 73.93% examples, 164400 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:54:51,794 : INFO : PROGRESS: at 75.21% examples, 164483 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:55:11,849 : INFO : PROGRESS: at 76.47% examples, 164606 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:55:31,913 : INFO : PROGRESS: at 77.70% examples, 164701 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:55:51,930 : INFO : PROGRESS: at 78.95% examples, 164822 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 04:56:11,981 : INFO : PROGRESS: at 80.22% examples, 164941 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:56:14,654 : INFO : Loading new batch for index: 40000\n",
      "2016-12-29 04:56:21,919 : INFO : Finished loading new batch\n",
      "2016-12-29 04:56:32,005 : INFO : PROGRESS: at 80.96% examples, 164009 words/s, in_qsize 32, out_qsize 1\n",
      "2016-12-29 04:56:52,034 : INFO : PROGRESS: at 82.20% examples, 164120 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:57:12,043 : INFO : PROGRESS: at 83.47% examples, 164213 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:57:32,058 : INFO : PROGRESS: at 84.75% examples, 164314 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:57:52,103 : INFO : PROGRESS: at 86.00% examples, 164388 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 04:58:12,113 : INFO : PROGRESS: at 87.23% examples, 164489 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:58:32,117 : INFO : PROGRESS: at 88.47% examples, 164571 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:58:52,155 : INFO : PROGRESS: at 89.74% examples, 164651 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:59:12,160 : INFO : PROGRESS: at 90.92% examples, 164744 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:59:32,182 : INFO : PROGRESS: at 92.18% examples, 164811 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 04:59:52,202 : INFO : PROGRESS: at 93.45% examples, 164900 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:00:12,272 : INFO : PROGRESS: at 94.65% examples, 164947 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:00:32,323 : INFO : PROGRESS: at 95.86% examples, 164995 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:00:52,346 : INFO : PROGRESS: at 97.07% examples, 165051 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 05:01:12,409 : INFO : PROGRESS: at 98.28% examples, 165122 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:01:32,433 : INFO : PROGRESS: at 99.52% examples, 165173 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:01:40,032 : INFO : Loading new batch for index: 49789\n",
      "2016-12-29 05:01:40,100 : INFO : No more batches to load, exiting at index: 49789\n",
      "2016-12-29 05:01:40,894 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2016-12-29 05:01:40,965 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2016-12-29 05:01:41,013 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2016-12-29 05:01:41,030 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2016-12-29 05:01:41,058 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2016-12-29 05:01:41,068 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2016-12-29 05:01:41,116 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2016-12-29 05:01:41,123 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2016-12-29 05:01:41,130 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2016-12-29 05:01:41,144 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2016-12-29 05:01:41,162 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2016-12-29 05:01:41,168 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2016-12-29 05:01:41,175 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-12-29 05:01:41,187 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-12-29 05:01:41,203 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-12-29 05:01:41,204 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-12-29 05:01:41,205 : INFO : training on 390507860 raw words (274476081 effective words) took 1663.1s, 165040 effective words/s\n",
      "2016-12-29 05:01:41,214 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_2/model, separately None\n",
      "2016-12-29 05:01:41,215 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_2/model.docvecs.doctag_syn0.npy\n",
      "2016-12-29 05:01:41,257 : INFO : storing numpy array 'syn1neg' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_2/model.syn1neg.npy\n",
      "2016-12-29 05:01:42,960 : INFO : not storing attribute syn0norm\n",
      "2016-12-29 05:01:42,961 : INFO : storing numpy array 'syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_2/model.syn0.npy\n",
      "2016-12-29 05:01:43,089 : INFO : not storing attribute cum_table\n",
      "2016-12-29 05:01:50,323 : INFO : Training Classifier\n",
      "2016-12-29 05:02:28,231 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 05:02:36,895 : INFO : Getting Validation Embeddings\n",
      "2016-12-29 05:02:36,896 : INFO : ===== Getting validation vectors with inference\n",
      "2016-12-29 05:02:36,904 : INFO : Loading new batch for index: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Training Metrics: Cov Err: 6.646, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.460, Top 3: 0.662, Top 5: 0.694, \n",
      "\t\t F1 Micro: 0.345, Total Pos: 18,180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 05:02:44,878 : INFO : Finished loading new batch\n",
      "2016-12-29 05:05:01,905 : INFO : Finished: 999\n",
      "2016-12-29 05:07:21,462 : INFO : Finished: 1999\n",
      "2016-12-29 05:09:38,384 : INFO : Finished: 2999\n",
      "2016-12-29 05:11:51,755 : INFO : Finished: 3999\n",
      "2016-12-29 05:14:08,287 : INFO : Finished: 4999\n",
      "2016-12-29 05:16:25,851 : INFO : Finished: 5999\n",
      "2016-12-29 05:18:37,367 : INFO : Finished: 6999\n",
      "2016-12-29 05:20:49,273 : INFO : Finished: 7999\n",
      "2016-12-29 05:23:04,957 : INFO : Finished: 8999\n",
      "2016-12-29 05:25:18,472 : INFO : Finished: 9999\n",
      "2016-12-29 05:25:20,079 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 05:25:21,760 : INFO : Finished loading new batch\n",
      "2016-12-29 05:27:38,742 : INFO : Finished: 10999\n",
      "2016-12-29 05:29:53,952 : INFO : Finished: 11999\n",
      "2016-12-29 05:29:54,223 : INFO : Loading new batch for index: 12412\n",
      "2016-12-29 05:29:54,227 : INFO : No more batches to load, exiting at index: 12412\n",
      "2016-12-29 05:30:52,198 : INFO : Finished: 12412\n",
      "2016-12-29 05:30:52,200 : INFO : Finished: 12412\n",
      "2016-12-29 05:30:53,150 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]]\n",
      "** Validation Metrics: Cov Err: 6.687, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.420, Top 3: 0.651, Top 5: 0.689, \n",
      "\t\t F1 Micro: 0.317, Total Pos: 6,715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 05:30:54,198 : INFO : precomputing L2-norms of word weight vectors\n",
      "2016-12-29 05:30:58,145 : INFO : capital-common-countries: 0.0% (0/156)\n",
      "2016-12-29 05:31:01,504 : INFO : capital-world: 0.7% (1/152)\n",
      "2016-12-29 05:31:02,393 : INFO : currency: 0.0% (0/40)\n",
      "2016-12-29 05:31:27,137 : INFO : city-in-state: 0.3% (4/1248)\n",
      "2016-12-29 05:31:29,263 : INFO : family: 2.7% (3/110)\n",
      "2016-12-29 05:31:39,919 : INFO : gram1-adjective-to-adverb: 0.4% (2/552)\n",
      "2016-12-29 05:31:46,599 : INFO : gram2-opposite: 0.9% (3/342)\n",
      "2016-12-29 05:32:12,474 : INFO : gram3-comparative: 10.6% (141/1332)\n",
      "2016-12-29 05:32:27,103 : INFO : gram4-superlative: 1.7% (13/756)\n",
      "2016-12-29 05:32:45,257 : INFO : gram5-present-participle: 7.1% (66/930)\n",
      "2016-12-29 05:32:56,501 : INFO : gram6-nationality-adjective: 0.0% (0/584)\n",
      "2016-12-29 05:33:20,604 : INFO : gram7-past-tense: 2.1% (27/1260)\n",
      "2016-12-29 05:33:39,485 : INFO : gram8-plural: 8.5% (84/992)\n",
      "2016-12-29 05:33:52,831 : INFO : gram9-plural-verbs: 7.3% (51/702)\n",
      "2016-12-29 05:33:52,833 : INFO : total: 4.3% (395/9156)\n",
      "2016-12-29 05:33:52,854 : INFO : ****************** Epoch 3 --- Working on doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_3 *******************\n",
      "2016-12-29 05:33:52,872 : INFO : training model with 16 workers on 146034 vocabulary and 5100 features, using sg=0 hs=0 sample=0.001 negative=10\n",
      "2016-12-29 05:33:52,875 : INFO : expecting 65535 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-12-29 05:33:52,899 : INFO : Loading new batch for index: 0\n",
      "2016-12-29 05:33:59,670 : INFO : Finished loading new batch\n",
      "2016-12-29 05:34:00,705 : INFO : PROGRESS: at 0.00% examples, 295 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 05:34:20,750 : INFO : PROGRESS: at 1.19% examples, 121102 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:34:40,809 : INFO : PROGRESS: at 2.42% examples, 141483 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:35:00,816 : INFO : PROGRESS: at 3.71% examples, 150184 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:35:20,950 : INFO : PROGRESS: at 4.91% examples, 154642 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:35:40,977 : INFO : PROGRESS: at 6.14% examples, 157934 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:36:01,003 : INFO : PROGRESS: at 7.39% examples, 160198 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 05:36:21,019 : INFO : PROGRESS: at 8.64% examples, 161755 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:36:41,093 : INFO : PROGRESS: at 9.96% examples, 162983 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:37:01,124 : INFO : PROGRESS: at 11.20% examples, 163954 words/s, in_qsize 32, out_qsize 1\n",
      "2016-12-29 05:37:21,133 : INFO : PROGRESS: at 12.48% examples, 164855 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:37:41,156 : INFO : PROGRESS: at 13.77% examples, 165454 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:38:01,183 : INFO : PROGRESS: at 15.01% examples, 165920 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:38:21,228 : INFO : PROGRESS: at 16.28% examples, 166499 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:38:41,269 : INFO : PROGRESS: at 17.57% examples, 166895 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 05:39:01,360 : INFO : PROGRESS: at 18.88% examples, 167216 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:39:20,605 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 05:39:21,477 : INFO : PROGRESS: at 20.00% examples, 166609 words/s, in_qsize 1, out_qsize 1\n",
      "2016-12-29 05:39:28,561 : INFO : Finished loading new batch\n",
      "2016-12-29 05:39:41,495 : INFO : PROGRESS: at 20.73% examples, 163222 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:40:01,531 : INFO : PROGRESS: at 21.94% examples, 163682 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 05:40:21,546 : INFO : PROGRESS: at 23.25% examples, 164016 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:40:41,579 : INFO : PROGRESS: at 24.53% examples, 164393 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 05:41:01,583 : INFO : PROGRESS: at 25.77% examples, 164720 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:41:21,615 : INFO : PROGRESS: at 27.06% examples, 165008 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:41:41,649 : INFO : PROGRESS: at 28.30% examples, 165304 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:42:01,661 : INFO : PROGRESS: at 29.53% examples, 165547 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:42:21,668 : INFO : PROGRESS: at 30.78% examples, 165841 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:42:41,689 : INFO : PROGRESS: at 32.02% examples, 166071 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:43:01,696 : INFO : PROGRESS: at 33.28% examples, 166351 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:43:21,706 : INFO : PROGRESS: at 34.54% examples, 166583 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:43:41,911 : INFO : PROGRESS: at 35.88% examples, 166752 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:44:01,941 : INFO : PROGRESS: at 37.13% examples, 166996 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:44:22,006 : INFO : PROGRESS: at 38.33% examples, 167168 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:44:42,081 : INFO : PROGRESS: at 39.59% examples, 167381 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:44:51,255 : INFO : Loading new batch for index: 20000\n",
      "2016-12-29 05:45:00,509 : INFO : Finished loading new batch\n",
      "2016-12-29 05:45:02,089 : INFO : PROGRESS: at 40.20% examples, 164954 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:45:22,107 : INFO : PROGRESS: at 41.44% examples, 165178 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:45:42,109 : INFO : PROGRESS: at 42.67% examples, 165400 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:46:02,112 : INFO : PROGRESS: at 43.90% examples, 165619 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 05:46:22,127 : INFO : PROGRESS: at 45.12% examples, 165787 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:46:42,133 : INFO : PROGRESS: at 46.44% examples, 165975 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 05:47:02,151 : INFO : PROGRESS: at 47.74% examples, 166141 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:47:22,153 : INFO : PROGRESS: at 49.01% examples, 166334 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:47:42,177 : INFO : PROGRESS: at 50.25% examples, 166475 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:48:02,185 : INFO : PROGRESS: at 51.47% examples, 166626 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 05:48:22,206 : INFO : PROGRESS: at 52.74% examples, 166784 words/s, in_qsize 31, out_qsize 1\n",
      "2016-12-29 05:48:42,251 : INFO : PROGRESS: at 54.02% examples, 166917 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:49:02,295 : INFO : PROGRESS: at 55.25% examples, 167087 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:49:22,309 : INFO : PROGRESS: at 56.59% examples, 167208 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:49:42,330 : INFO : PROGRESS: at 57.85% examples, 167353 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:50:02,420 : INFO : PROGRESS: at 59.14% examples, 167446 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 05:50:20,696 : INFO : Loading new batch for index: 30000\n",
      "2016-12-29 05:50:28,696 : INFO : Finished loading new batch\n",
      "2016-12-29 05:50:29,177 : INFO : PROGRESS: at 60.28% examples, 166006 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:50:49,213 : INFO : PROGRESS: at 61.47% examples, 166087 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:51:09,238 : INFO : PROGRESS: at 62.75% examples, 166191 words/s, in_qsize 32, out_qsize 1\n",
      "2016-12-29 05:51:29,241 : INFO : PROGRESS: at 63.99% examples, 166302 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:51:49,311 : INFO : PROGRESS: at 65.25% examples, 166424 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:52:09,397 : INFO : PROGRESS: at 66.58% examples, 166535 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:52:29,439 : INFO : PROGRESS: at 67.78% examples, 166646 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 05:52:49,446 : INFO : PROGRESS: at 69.05% examples, 166757 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:53:09,450 : INFO : PROGRESS: at 70.34% examples, 166864 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:53:29,472 : INFO : PROGRESS: at 71.55% examples, 166983 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:53:49,526 : INFO : PROGRESS: at 72.83% examples, 167080 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:54:09,533 : INFO : PROGRESS: at 74.16% examples, 167194 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 05:54:29,598 : INFO : PROGRESS: at 75.44% examples, 167301 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:54:49,612 : INFO : PROGRESS: at 76.72% examples, 167417 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 05:55:09,701 : INFO : PROGRESS: at 77.97% examples, 167495 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:55:29,724 : INFO : PROGRESS: at 79.23% examples, 167569 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:55:48,335 : INFO : Loading new batch for index: 40000\n",
      "2016-12-29 05:55:55,787 : INFO : Finished loading new batch\n",
      "2016-12-29 05:55:56,454 : INFO : PROGRESS: at 80.36% examples, 166534 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:56:16,503 : INFO : PROGRESS: at 81.57% examples, 166604 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:56:36,512 : INFO : PROGRESS: at 82.85% examples, 166672 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:56:56,609 : INFO : PROGRESS: at 84.13% examples, 166754 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:57:16,639 : INFO : PROGRESS: at 85.40% examples, 166815 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 05:57:36,660 : INFO : PROGRESS: at 86.70% examples, 166894 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:57:56,669 : INFO : PROGRESS: at 87.89% examples, 166943 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 05:58:16,722 : INFO : PROGRESS: at 89.18% examples, 167024 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:58:36,784 : INFO : PROGRESS: at 90.39% examples, 167095 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:58:56,815 : INFO : PROGRESS: at 91.66% examples, 167154 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 05:59:16,903 : INFO : PROGRESS: at 92.94% examples, 167230 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:59:36,919 : INFO : PROGRESS: at 94.18% examples, 167299 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 05:59:56,932 : INFO : PROGRESS: at 95.39% examples, 167381 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:00:16,994 : INFO : PROGRESS: at 96.63% examples, 167446 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 06:00:37,007 : INFO : PROGRESS: at 97.88% examples, 167532 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:00:57,012 : INFO : PROGRESS: at 99.14% examples, 167607 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:01:10,457 : INFO : Loading new batch for index: 49789\n",
      "2016-12-29 06:01:10,516 : INFO : No more batches to load, exiting at index: 49789\n",
      "2016-12-29 06:01:11,287 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2016-12-29 06:01:11,331 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2016-12-29 06:01:11,406 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2016-12-29 06:01:11,418 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2016-12-29 06:01:11,430 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2016-12-29 06:01:11,457 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2016-12-29 06:01:11,494 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2016-12-29 06:01:11,506 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2016-12-29 06:01:11,519 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2016-12-29 06:01:11,521 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2016-12-29 06:01:11,531 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2016-12-29 06:01:11,545 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2016-12-29 06:01:11,547 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-12-29 06:01:11,560 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-12-29 06:01:11,592 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-12-29 06:01:11,597 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-12-29 06:01:11,598 : INFO : training on 390507860 raw words (274492032 effective words) took 1638.7s, 167506 effective words/s\n",
      "2016-12-29 06:01:11,609 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_3/model, separately None\n",
      "2016-12-29 06:01:11,610 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_3/model.docvecs.doctag_syn0.npy\n",
      "2016-12-29 06:01:11,654 : INFO : storing numpy array 'syn1neg' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_3/model.syn1neg.npy\n",
      "2016-12-29 06:01:13,490 : INFO : not storing attribute syn0norm\n",
      "2016-12-29 06:01:13,492 : INFO : storing numpy array 'syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_3/model.syn0.npy\n",
      "2016-12-29 06:01:13,613 : INFO : not storing attribute cum_table\n",
      "2016-12-29 06:01:17,690 : INFO : Training Classifier\n",
      "2016-12-29 06:01:56,176 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 06:02:04,693 : INFO : Getting Validation Embeddings\n",
      "2016-12-29 06:02:04,694 : INFO : ===== Getting validation vectors with inference\n",
      "2016-12-29 06:02:04,702 : INFO : Loading new batch for index: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Training Metrics: Cov Err: 6.590, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.464, Top 3: 0.665, Top 5: 0.696, \n",
      "\t\t F1 Micro: 0.355, Total Pos: 18,952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 06:02:16,424 : INFO : Finished loading new batch\n",
      "2016-12-29 06:04:31,651 : INFO : Finished: 999\n",
      "2016-12-29 06:06:51,233 : INFO : Finished: 1999\n",
      "2016-12-29 06:09:07,294 : INFO : Finished: 2999\n",
      "2016-12-29 06:11:20,533 : INFO : Finished: 3999\n",
      "2016-12-29 06:13:37,037 : INFO : Finished: 4999\n",
      "2016-12-29 06:15:52,499 : INFO : Finished: 5999\n",
      "2016-12-29 06:18:02,707 : INFO : Finished: 6999\n",
      "2016-12-29 06:20:13,075 : INFO : Finished: 7999\n",
      "2016-12-29 06:22:27,467 : INFO : Finished: 8999\n",
      "2016-12-29 06:24:39,260 : INFO : Finished: 9999\n",
      "2016-12-29 06:24:40,842 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 06:24:41,975 : INFO : Finished loading new batch\n",
      "2016-12-29 06:26:58,240 : INFO : Finished: 10999\n",
      "2016-12-29 06:29:12,000 : INFO : Finished: 11999\n",
      "2016-12-29 06:29:12,274 : INFO : Loading new batch for index: 12412\n",
      "2016-12-29 06:29:12,278 : INFO : No more batches to load, exiting at index: 12412\n",
      "2016-12-29 06:30:09,461 : INFO : Finished: 12412\n",
      "2016-12-29 06:30:09,464 : INFO : Finished: 12412\n",
      "2016-12-29 06:30:10,245 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "** Validation Metrics: Cov Err: 6.876, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.406, Top 3: 0.633, Top 5: 0.668, \n",
      "\t\t F1 Micro: 0.284, Total Pos: 5,717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 06:30:11,209 : INFO : precomputing L2-norms of word weight vectors\n",
      "2016-12-29 06:30:16,208 : INFO : capital-common-countries: 1.3% (2/156)\n",
      "2016-12-29 06:30:20,545 : INFO : capital-world: 1.3% (2/152)\n",
      "2016-12-29 06:30:21,465 : INFO : currency: 0.0% (0/40)\n",
      "2016-12-29 06:30:49,413 : INFO : city-in-state: 0.0% (0/1248)\n",
      "2016-12-29 06:30:51,597 : INFO : family: 3.6% (4/110)\n",
      "2016-12-29 06:31:02,462 : INFO : gram1-adjective-to-adverb: 1.1% (6/552)\n",
      "2016-12-29 06:31:09,181 : INFO : gram2-opposite: 1.8% (6/342)\n",
      "2016-12-29 06:31:35,294 : INFO : gram3-comparative: 12.6% (168/1332)\n",
      "2016-12-29 06:31:49,953 : INFO : gram4-superlative: 2.4% (18/756)\n",
      "2016-12-29 06:32:08,120 : INFO : gram5-present-participle: 6.5% (60/930)\n",
      "2016-12-29 06:32:19,873 : INFO : gram6-nationality-adjective: 0.0% (0/584)\n",
      "2016-12-29 06:32:44,498 : INFO : gram7-past-tense: 2.1% (26/1260)\n",
      "2016-12-29 06:33:03,861 : INFO : gram8-plural: 8.7% (86/992)\n",
      "2016-12-29 06:33:17,435 : INFO : gram9-plural-verbs: 5.4% (38/702)\n",
      "2016-12-29 06:33:17,439 : INFO : total: 4.5% (416/9156)\n",
      "2016-12-29 06:33:17,454 : INFO : ****************** Epoch 4 --- Working on doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_4 *******************\n",
      "2016-12-29 06:33:17,458 : INFO : training model with 16 workers on 146034 vocabulary and 5100 features, using sg=0 hs=0 sample=0.001 negative=10\n",
      "2016-12-29 06:33:17,460 : INFO : expecting 65535 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-12-29 06:33:17,477 : INFO : Loading new batch for index: 0\n",
      "2016-12-29 06:33:23,977 : INFO : Finished loading new batch\n",
      "2016-12-29 06:33:24,512 : INFO : PROGRESS: at 0.00% examples, 323 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:33:44,575 : INFO : PROGRESS: at 1.21% examples, 126612 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:34:04,577 : INFO : PROGRESS: at 2.46% examples, 145528 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:34:24,618 : INFO : PROGRESS: at 3.74% examples, 153508 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:34:44,650 : INFO : PROGRESS: at 4.97% examples, 157746 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:35:04,668 : INFO : PROGRESS: at 6.19% examples, 160475 words/s, in_qsize 31, out_qsize 1\n",
      "2016-12-29 06:35:24,675 : INFO : PROGRESS: at 7.43% examples, 162334 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:35:44,682 : INFO : PROGRESS: at 8.69% examples, 163679 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:36:04,710 : INFO : PROGRESS: at 10.00% examples, 164667 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:36:24,717 : INFO : PROGRESS: at 11.23% examples, 165414 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:36:44,728 : INFO : PROGRESS: at 12.51% examples, 166105 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:37:04,731 : INFO : PROGRESS: at 13.81% examples, 166693 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:37:24,757 : INFO : PROGRESS: at 15.06% examples, 167110 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:37:44,762 : INFO : PROGRESS: at 16.31% examples, 167445 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:38:04,770 : INFO : PROGRESS: at 17.60% examples, 167761 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:38:24,796 : INFO : PROGRESS: at 18.90% examples, 168088 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:38:43,794 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 06:38:44,838 : INFO : PROGRESS: at 20.01% examples, 167322 words/s, in_qsize 0, out_qsize 0\n",
      "2016-12-29 06:38:50,180 : INFO : Finished loading new batch\n",
      "2016-12-29 06:39:04,843 : INFO : PROGRESS: at 20.86% examples, 164906 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 06:39:24,845 : INFO : PROGRESS: at 22.07% examples, 165271 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:39:44,883 : INFO : PROGRESS: at 23.41% examples, 165602 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 06:40:04,877 : INFO : PROGRESS: at 24.69% examples, 165956 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:40:24,900 : INFO : PROGRESS: at 25.93% examples, 166227 words/s, in_qsize 31, out_qsize 2\n",
      "2016-12-29 06:40:44,922 : INFO : PROGRESS: at 27.24% examples, 166561 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:41:04,964 : INFO : PROGRESS: at 28.49% examples, 166861 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:41:24,966 : INFO : PROGRESS: at 29.72% examples, 167159 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:41:45,012 : INFO : PROGRESS: at 30.99% examples, 167414 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:42:05,028 : INFO : PROGRESS: at 32.25% examples, 167701 words/s, in_qsize 31, out_qsize 1\n",
      "2016-12-29 06:42:25,099 : INFO : PROGRESS: at 33.53% examples, 167942 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 06:42:45,104 : INFO : PROGRESS: at 34.80% examples, 168194 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:43:05,116 : INFO : PROGRESS: at 36.13% examples, 168379 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 06:43:25,140 : INFO : PROGRESS: at 37.38% examples, 168560 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:43:45,234 : INFO : PROGRESS: at 38.62% examples, 168728 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:44:05,245 : INFO : PROGRESS: at 39.86% examples, 168956 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:44:09,912 : INFO : Loading new batch for index: 20000\n",
      "2016-12-29 06:44:18,469 : INFO : Finished loading new batch\n",
      "2016-12-29 06:44:25,254 : INFO : PROGRESS: at 40.51% examples, 166594 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:44:45,280 : INFO : PROGRESS: at 41.72% examples, 166761 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 06:45:05,309 : INFO : PROGRESS: at 43.02% examples, 167011 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:45:25,314 : INFO : PROGRESS: at 44.22% examples, 167220 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:45:45,359 : INFO : PROGRESS: at 45.47% examples, 167408 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:46:05,366 : INFO : PROGRESS: at 46.81% examples, 167592 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:46:25,410 : INFO : PROGRESS: at 48.14% examples, 167777 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:46:45,461 : INFO : PROGRESS: at 49.40% examples, 167938 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:47:05,510 : INFO : PROGRESS: at 50.65% examples, 168098 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:47:25,658 : INFO : PROGRESS: at 51.92% examples, 168231 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:47:45,765 : INFO : PROGRESS: at 53.20% examples, 168381 words/s, in_qsize 32, out_qsize 1\n",
      "2016-12-29 06:48:05,779 : INFO : PROGRESS: at 54.47% examples, 168554 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:48:25,827 : INFO : PROGRESS: at 55.75% examples, 168711 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:48:45,835 : INFO : PROGRESS: at 57.09% examples, 168855 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:49:05,876 : INFO : PROGRESS: at 58.38% examples, 168970 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:49:25,884 : INFO : PROGRESS: at 59.67% examples, 169087 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:49:35,919 : INFO : Loading new batch for index: 30000\n",
      "2016-12-29 06:49:43,307 : INFO : Finished loading new batch\n",
      "2016-12-29 06:49:45,896 : INFO : PROGRESS: at 60.41% examples, 167647 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:50:05,918 : INFO : PROGRESS: at 61.61% examples, 167736 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:50:25,934 : INFO : PROGRESS: at 62.88% examples, 167799 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:50:45,964 : INFO : PROGRESS: at 64.12% examples, 167901 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:51:05,994 : INFO : PROGRESS: at 65.38% examples, 168010 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:51:26,018 : INFO : PROGRESS: at 66.71% examples, 168103 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:51:46,169 : INFO : PROGRESS: at 67.92% examples, 168176 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:52:06,180 : INFO : PROGRESS: at 69.20% examples, 168277 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:52:26,185 : INFO : PROGRESS: at 70.47% examples, 168382 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:52:46,186 : INFO : PROGRESS: at 71.70% examples, 168487 words/s, in_qsize 32, out_qsize 1\n",
      "2016-12-29 06:53:06,227 : INFO : PROGRESS: at 73.02% examples, 168572 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:53:26,230 : INFO : PROGRESS: at 74.31% examples, 168656 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:53:46,234 : INFO : PROGRESS: at 75.62% examples, 168754 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 06:54:06,253 : INFO : PROGRESS: at 76.89% examples, 168829 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:54:26,274 : INFO : PROGRESS: at 78.14% examples, 168930 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:54:46,305 : INFO : PROGRESS: at 79.42% examples, 169027 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:55:01,517 : INFO : Loading new batch for index: 40000\n",
      "2016-12-29 06:55:09,978 : INFO : Finished loading new batch\n",
      "2016-12-29 06:55:10,984 : INFO : PROGRESS: at 80.36% examples, 167808 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:55:31,011 : INFO : PROGRESS: at 81.60% examples, 167919 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:55:51,016 : INFO : PROGRESS: at 82.91% examples, 168029 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:56:11,064 : INFO : PROGRESS: at 84.20% examples, 168117 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 06:56:31,126 : INFO : PROGRESS: at 85.50% examples, 168211 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:56:51,154 : INFO : PROGRESS: at 86.80% examples, 168301 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:57:11,187 : INFO : PROGRESS: at 88.01% examples, 168372 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:57:31,198 : INFO : PROGRESS: at 89.34% examples, 168458 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:57:51,207 : INFO : PROGRESS: at 90.55% examples, 168535 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 06:58:11,213 : INFO : PROGRESS: at 91.85% examples, 168608 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:58:31,243 : INFO : PROGRESS: at 93.12% examples, 168683 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:58:51,260 : INFO : PROGRESS: at 94.36% examples, 168756 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:59:11,278 : INFO : PROGRESS: at 95.57% examples, 168821 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 06:59:31,285 : INFO : PROGRESS: at 96.83% examples, 168894 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 06:59:51,332 : INFO : PROGRESS: at 98.09% examples, 168967 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:00:11,333 : INFO : PROGRESS: at 99.35% examples, 169043 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 07:00:21,319 : INFO : Loading new batch for index: 49789\n",
      "2016-12-29 07:00:21,384 : INFO : No more batches to load, exiting at index: 49789\n",
      "2016-12-29 07:00:22,172 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2016-12-29 07:00:22,181 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2016-12-29 07:00:22,228 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2016-12-29 07:00:22,292 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2016-12-29 07:00:22,313 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2016-12-29 07:00:22,316 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2016-12-29 07:00:22,319 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2016-12-29 07:00:22,360 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2016-12-29 07:00:22,365 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2016-12-29 07:00:22,379 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2016-12-29 07:00:22,399 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2016-12-29 07:00:22,403 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2016-12-29 07:00:22,424 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-12-29 07:00:22,430 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-12-29 07:00:22,445 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-12-29 07:00:22,456 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-12-29 07:00:22,457 : INFO : training on 390507860 raw words (274489611 effective words) took 1625.0s, 168919 effective words/s\n",
      "2016-12-29 07:00:22,464 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_4/model, separately None\n",
      "2016-12-29 07:00:22,466 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_4/model.docvecs.doctag_syn0.npy\n",
      "2016-12-29 07:00:22,506 : INFO : storing numpy array 'syn1neg' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_4/model.syn1neg.npy\n",
      "2016-12-29 07:00:24,289 : INFO : not storing attribute syn0norm\n",
      "2016-12-29 07:00:24,290 : INFO : storing numpy array 'syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_4/model.syn0.npy\n",
      "2016-12-29 07:00:24,403 : INFO : not storing attribute cum_table\n",
      "2016-12-29 07:00:28,694 : INFO : Training Classifier\n",
      "2016-12-29 07:01:07,412 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 07:01:15,641 : INFO : Getting Validation Embeddings\n",
      "2016-12-29 07:01:15,643 : INFO : ===== Getting validation vectors with inference\n",
      "2016-12-29 07:01:15,653 : INFO : Loading new batch for index: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Training Metrics: Cov Err: 6.544, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.467, Top 3: 0.666, Top 5: 0.698, \n",
      "\t\t F1 Micro: 0.363, Total Pos: 19,476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 07:01:23,714 : INFO : Finished loading new batch\n",
      "2016-12-29 07:03:39,669 : INFO : Finished: 999\n",
      "2016-12-29 07:05:58,398 : INFO : Finished: 1999\n",
      "2016-12-29 07:08:14,107 : INFO : Finished: 2999\n",
      "2016-12-29 07:10:25,883 : INFO : Finished: 3999\n",
      "2016-12-29 07:12:42,256 : INFO : Finished: 4999\n",
      "2016-12-29 07:14:58,782 : INFO : Finished: 5999\n",
      "2016-12-29 07:17:09,790 : INFO : Finished: 6999\n",
      "2016-12-29 07:19:20,428 : INFO : Finished: 7999\n",
      "2016-12-29 07:21:36,435 : INFO : Finished: 8999\n",
      "2016-12-29 07:23:50,560 : INFO : Finished: 9999\n",
      "2016-12-29 07:23:52,252 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 07:23:54,125 : INFO : Finished loading new batch\n",
      "2016-12-29 07:26:10,548 : INFO : Finished: 10999\n",
      "2016-12-29 07:28:28,060 : INFO : Finished: 11999\n",
      "2016-12-29 07:28:28,371 : INFO : Loading new batch for index: 12412\n",
      "2016-12-29 07:28:28,377 : INFO : No more batches to load, exiting at index: 12412\n",
      "2016-12-29 07:29:26,116 : INFO : Finished: 12412\n",
      "2016-12-29 07:29:26,118 : INFO : Finished: 12412\n",
      "2016-12-29 07:29:27,074 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "** Validation Metrics: Cov Err: 7.050, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.398, Top 3: 0.628, Top 5: 0.663, \n",
      "\t\t F1 Micro: 0.251, Total Pos: 4,852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 07:29:28,016 : INFO : precomputing L2-norms of word weight vectors\n",
      "2016-12-29 07:29:31,887 : INFO : capital-common-countries: 0.6% (1/156)\n",
      "2016-12-29 07:29:35,284 : INFO : capital-world: 0.7% (1/152)\n",
      "2016-12-29 07:29:36,177 : INFO : currency: 0.0% (0/40)\n",
      "2016-12-29 07:30:01,233 : INFO : city-in-state: 0.0% (0/1248)\n",
      "2016-12-29 07:30:03,745 : INFO : family: 7.3% (8/110)\n",
      "2016-12-29 07:30:16,581 : INFO : gram1-adjective-to-adverb: 1.4% (8/552)\n",
      "2016-12-29 07:30:24,895 : INFO : gram2-opposite: 2.0% (7/342)\n",
      "2016-12-29 07:30:52,412 : INFO : gram3-comparative: 14.3% (190/1332)\n",
      "2016-12-29 07:31:06,832 : INFO : gram4-superlative: 1.6% (12/756)\n",
      "2016-12-29 07:31:24,630 : INFO : gram5-present-participle: 6.2% (58/930)\n",
      "2016-12-29 07:31:35,862 : INFO : gram6-nationality-adjective: 0.0% (0/584)\n",
      "2016-12-29 07:31:59,908 : INFO : gram7-past-tense: 2.1% (27/1260)\n",
      "2016-12-29 07:32:18,668 : INFO : gram8-plural: 11.6% (115/992)\n",
      "2016-12-29 07:32:32,034 : INFO : gram9-plural-verbs: 7.3% (51/702)\n",
      "2016-12-29 07:32:32,037 : INFO : total: 5.2% (478/9156)\n",
      "2016-12-29 07:32:32,053 : INFO : ****************** Epoch 5 --- Working on doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_5 *******************\n",
      "2016-12-29 07:32:32,057 : INFO : training model with 16 workers on 146034 vocabulary and 5100 features, using sg=0 hs=0 sample=0.001 negative=10\n",
      "2016-12-29 07:32:32,059 : INFO : expecting 65535 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-12-29 07:32:32,078 : INFO : Loading new batch for index: 0\n",
      "2016-12-29 07:32:38,436 : INFO : Finished loading new batch\n",
      "2016-12-29 07:32:39,259 : INFO : PROGRESS: at 0.00% examples, 520 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:32:59,275 : INFO : PROGRESS: at 1.19% examples, 124627 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:33:19,277 : INFO : PROGRESS: at 2.43% examples, 143866 words/s, in_qsize 31, out_qsize 1\n",
      "2016-12-29 07:33:39,284 : INFO : PROGRESS: at 3.71% examples, 151815 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:33:59,317 : INFO : PROGRESS: at 4.91% examples, 156145 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:34:19,330 : INFO : PROGRESS: at 6.14% examples, 159044 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:34:39,337 : INFO : PROGRESS: at 7.37% examples, 160867 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:34:59,444 : INFO : PROGRESS: at 8.63% examples, 162177 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:35:19,529 : INFO : PROGRESS: at 9.93% examples, 163075 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:35:39,563 : INFO : PROGRESS: at 11.14% examples, 163898 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 07:35:59,572 : INFO : PROGRESS: at 12.41% examples, 164526 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:36:19,602 : INFO : PROGRESS: at 13.69% examples, 165000 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:36:39,638 : INFO : PROGRESS: at 14.93% examples, 165407 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:36:59,670 : INFO : PROGRESS: at 16.18% examples, 165775 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:37:19,685 : INFO : PROGRESS: at 17.44% examples, 166108 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:37:39,703 : INFO : PROGRESS: at 18.72% examples, 166299 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:37:59,712 : INFO : PROGRESS: at 19.92% examples, 166463 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:38:02,391 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 07:38:10,845 : INFO : Finished loading new batch\n",
      "2016-12-29 07:38:19,719 : INFO : PROGRESS: at 20.23% examples, 159500 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 07:38:39,762 : INFO : PROGRESS: at 21.40% examples, 160006 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:38:59,776 : INFO : PROGRESS: at 22.65% examples, 160468 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:39:19,822 : INFO : PROGRESS: at 23.93% examples, 160885 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:39:39,864 : INFO : PROGRESS: at 25.17% examples, 161200 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:39:59,964 : INFO : PROGRESS: at 26.44% examples, 161598 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 07:40:19,968 : INFO : PROGRESS: at 27.68% examples, 161920 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 07:40:39,988 : INFO : PROGRESS: at 28.93% examples, 162315 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 07:41:00,020 : INFO : PROGRESS: at 30.11% examples, 162583 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:41:20,042 : INFO : PROGRESS: at 31.38% examples, 162869 words/s, in_qsize 32, out_qsize 1\n",
      "2016-12-29 07:41:40,054 : INFO : PROGRESS: at 32.60% examples, 163134 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:42:00,067 : INFO : PROGRESS: at 33.82% examples, 163330 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:42:20,080 : INFO : PROGRESS: at 35.09% examples, 163571 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 07:42:40,091 : INFO : PROGRESS: at 36.36% examples, 163791 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 07:43:00,094 : INFO : PROGRESS: at 37.56% examples, 163961 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:43:20,183 : INFO : PROGRESS: at 38.77% examples, 164138 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:43:40,195 : INFO : PROGRESS: at 39.98% examples, 164347 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:43:42,749 : INFO : Loading new batch for index: 20000\n",
      "2016-12-29 07:43:50,808 : INFO : Finished loading new batch\n",
      "2016-12-29 07:44:00,210 : INFO : PROGRESS: at 40.65% examples, 162212 words/s, in_qsize 31, out_qsize 2\n",
      "2016-12-29 07:44:20,207 : INFO : PROGRESS: at 41.82% examples, 162406 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:44:40,250 : INFO : PROGRESS: at 43.08% examples, 162598 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:45:00,279 : INFO : PROGRESS: at 44.25% examples, 162770 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 07:45:20,279 : INFO : PROGRESS: at 45.47% examples, 162960 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:45:40,314 : INFO : PROGRESS: at 46.76% examples, 163105 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:46:00,352 : INFO : PROGRESS: at 48.04% examples, 163230 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:46:20,355 : INFO : PROGRESS: at 49.27% examples, 163369 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:46:40,364 : INFO : PROGRESS: at 50.48% examples, 163482 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 07:47:00,375 : INFO : PROGRESS: at 51.69% examples, 163653 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:47:20,408 : INFO : PROGRESS: at 52.94% examples, 163777 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:47:40,435 : INFO : PROGRESS: at 54.18% examples, 163912 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:48:00,449 : INFO : PROGRESS: at 55.39% examples, 164045 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:48:20,452 : INFO : PROGRESS: at 56.71% examples, 164165 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:48:40,481 : INFO : PROGRESS: at 57.92% examples, 164267 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:49:00,491 : INFO : PROGRESS: at 59.17% examples, 164356 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:49:18,375 : INFO : Loading new batch for index: 30000\n",
      "2016-12-29 07:49:26,693 : INFO : Finished loading new batch\n",
      "2016-12-29 07:49:27,719 : INFO : PROGRESS: at 60.28% examples, 162839 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:49:47,733 : INFO : PROGRESS: at 61.45% examples, 162926 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 07:50:07,792 : INFO : PROGRESS: at 62.71% examples, 163007 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 07:50:27,824 : INFO : PROGRESS: at 63.92% examples, 163062 words/s, in_qsize 31, out_qsize 1\n",
      "2016-12-29 07:50:47,875 : INFO : PROGRESS: at 65.12% examples, 163130 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:51:07,949 : INFO : PROGRESS: at 66.38% examples, 163202 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 07:51:27,953 : INFO : PROGRESS: at 67.60% examples, 163282 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:51:47,981 : INFO : PROGRESS: at 68.78% examples, 163369 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 07:52:08,015 : INFO : PROGRESS: at 70.05% examples, 163440 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:52:28,025 : INFO : PROGRESS: at 71.22% examples, 163511 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:52:48,081 : INFO : PROGRESS: at 72.45% examples, 163603 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:53:08,111 : INFO : PROGRESS: at 73.76% examples, 163691 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:53:28,122 : INFO : PROGRESS: at 75.03% examples, 163780 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 07:53:48,169 : INFO : PROGRESS: at 76.24% examples, 163863 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:54:08,186 : INFO : PROGRESS: at 77.47% examples, 163936 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 07:54:28,195 : INFO : PROGRESS: at 78.69% examples, 164001 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:54:48,201 : INFO : PROGRESS: at 79.91% examples, 164054 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 07:54:55,937 : INFO : Loading new batch for index: 40000\n",
      "2016-12-29 07:55:02,255 : INFO : Finished loading new batch\n",
      "2016-12-29 07:55:08,216 : INFO : PROGRESS: at 80.66% examples, 163111 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:55:28,225 : INFO : PROGRESS: at 81.83% examples, 163175 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:55:48,236 : INFO : PROGRESS: at 83.09% examples, 163256 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:56:08,261 : INFO : PROGRESS: at 84.37% examples, 163345 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:56:28,314 : INFO : PROGRESS: at 85.62% examples, 163430 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:56:48,329 : INFO : PROGRESS: at 86.88% examples, 163536 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 07:57:08,331 : INFO : PROGRESS: at 88.07% examples, 163633 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 07:57:28,353 : INFO : PROGRESS: at 89.38% examples, 163730 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:57:48,363 : INFO : PROGRESS: at 90.57% examples, 163851 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:58:08,388 : INFO : PROGRESS: at 91.87% examples, 163964 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:58:28,418 : INFO : PROGRESS: at 93.11% examples, 164056 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:58:48,428 : INFO : PROGRESS: at 94.33% examples, 164145 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:59:08,428 : INFO : PROGRESS: at 95.53% examples, 164244 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:59:28,470 : INFO : PROGRESS: at 96.77% examples, 164325 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 07:59:48,478 : INFO : PROGRESS: at 98.01% examples, 164441 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 08:00:08,492 : INFO : PROGRESS: at 99.25% examples, 164513 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:00:20,251 : INFO : Loading new batch for index: 49789\n",
      "2016-12-29 08:00:20,298 : INFO : No more batches to load, exiting at index: 49789\n",
      "2016-12-29 08:00:21,094 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2016-12-29 08:00:21,098 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2016-12-29 08:00:21,215 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2016-12-29 08:00:21,230 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2016-12-29 08:00:21,265 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2016-12-29 08:00:21,278 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2016-12-29 08:00:21,287 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2016-12-29 08:00:21,306 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2016-12-29 08:00:21,348 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2016-12-29 08:00:21,350 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2016-12-29 08:00:21,355 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2016-12-29 08:00:21,356 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2016-12-29 08:00:21,370 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-12-29 08:00:21,375 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-12-29 08:00:21,401 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-12-29 08:00:21,417 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-12-29 08:00:21,417 : INFO : training on 390507860 raw words (274480864 effective words) took 1669.3s, 164425 effective words/s\n",
      "2016-12-29 08:00:21,427 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_5/model, separately None\n",
      "2016-12-29 08:00:21,428 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_5/model.docvecs.doctag_syn0.npy\n",
      "2016-12-29 08:00:21,470 : INFO : storing numpy array 'syn1neg' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_5/model.syn1neg.npy\n",
      "2016-12-29 08:00:23,223 : INFO : not storing attribute syn0norm\n",
      "2016-12-29 08:00:23,224 : INFO : storing numpy array 'syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_5/model.syn0.npy\n",
      "2016-12-29 08:00:23,340 : INFO : not storing attribute cum_table\n",
      "2016-12-29 08:00:27,002 : INFO : Training Classifier\n",
      "2016-12-29 08:01:04,918 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 08:01:13,490 : INFO : Getting Validation Embeddings\n",
      "2016-12-29 08:01:13,491 : INFO : ===== Getting validation vectors with inference\n",
      "2016-12-29 08:01:13,500 : INFO : Loading new batch for index: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Training Metrics: Cov Err: 6.492, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.471, Top 3: 0.668, Top 5: 0.700, \n",
      "\t\t F1 Micro: 0.372, Total Pos: 20,068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 08:01:21,680 : INFO : Finished loading new batch\n",
      "2016-12-29 08:03:39,759 : INFO : Finished: 999\n",
      "2016-12-29 08:06:00,088 : INFO : Finished: 1999\n",
      "2016-12-29 08:08:17,542 : INFO : Finished: 2999\n",
      "2016-12-29 08:10:32,770 : INFO : Finished: 3999\n",
      "2016-12-29 08:12:51,278 : INFO : Finished: 4999\n",
      "2016-12-29 08:15:10,046 : INFO : Finished: 5999\n",
      "2016-12-29 08:17:23,362 : INFO : Finished: 6999\n",
      "2016-12-29 08:19:36,721 : INFO : Finished: 7999\n",
      "2016-12-29 08:21:55,108 : INFO : Finished: 8999\n",
      "2016-12-29 08:24:11,089 : INFO : Finished: 9999\n",
      "2016-12-29 08:24:12,752 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 08:24:14,771 : INFO : Finished loading new batch\n",
      "2016-12-29 08:26:32,434 : INFO : Finished: 10999\n",
      "2016-12-29 08:28:46,309 : INFO : Finished: 11999\n",
      "2016-12-29 08:28:46,601 : INFO : Loading new batch for index: 12412\n",
      "2016-12-29 08:28:46,606 : INFO : No more batches to load, exiting at index: 12412\n",
      "2016-12-29 08:29:43,017 : INFO : Finished: 12412\n",
      "2016-12-29 08:29:43,019 : INFO : Finished: 12412\n",
      "2016-12-29 08:29:43,990 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "** Validation Metrics: Cov Err: 7.179, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.393, Top 3: 0.620, Top 5: 0.652, \n",
      "\t\t F1 Micro: 0.224, Total Pos: 4,151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 08:29:45,020 : INFO : precomputing L2-norms of word weight vectors\n",
      "2016-12-29 08:29:48,858 : INFO : capital-common-countries: 0.0% (0/156)\n",
      "2016-12-29 08:29:52,177 : INFO : capital-world: 0.7% (1/152)\n",
      "2016-12-29 08:29:53,067 : INFO : currency: 0.0% (0/40)\n",
      "2016-12-29 08:30:23,661 : INFO : city-in-state: 0.0% (0/1248)\n",
      "2016-12-29 08:30:26,130 : INFO : family: 7.3% (8/110)\n",
      "2016-12-29 08:30:37,396 : INFO : gram1-adjective-to-adverb: 2.0% (11/552)\n",
      "2016-12-29 08:30:44,167 : INFO : gram2-opposite: 0.6% (2/342)\n",
      "2016-12-29 08:31:10,016 : INFO : gram3-comparative: 14.3% (191/1332)\n",
      "2016-12-29 08:31:24,685 : INFO : gram4-superlative: 2.0% (15/756)\n",
      "2016-12-29 08:31:42,806 : INFO : gram5-present-participle: 5.7% (53/930)\n",
      "2016-12-29 08:31:54,181 : INFO : gram6-nationality-adjective: 0.0% (0/584)\n",
      "2016-12-29 08:32:18,416 : INFO : gram7-past-tense: 3.7% (47/1260)\n",
      "2016-12-29 08:32:37,788 : INFO : gram8-plural: 10.1% (100/992)\n",
      "2016-12-29 08:32:51,254 : INFO : gram9-plural-verbs: 7.7% (54/702)\n",
      "2016-12-29 08:32:51,258 : INFO : total: 5.3% (482/9156)\n",
      "2016-12-29 08:32:51,272 : INFO : ****************** Epoch 6 --- Working on doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_6 *******************\n",
      "2016-12-29 08:32:51,276 : INFO : training model with 16 workers on 146034 vocabulary and 5100 features, using sg=0 hs=0 sample=0.001 negative=10\n",
      "2016-12-29 08:32:51,279 : INFO : expecting 65535 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-12-29 08:32:51,313 : INFO : Loading new batch for index: 0\n",
      "2016-12-29 08:32:56,428 : INFO : Finished loading new batch\n",
      "2016-12-29 08:32:57,442 : INFO : PROGRESS: at 0.00% examples, 373 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:33:17,461 : INFO : PROGRESS: at 1.24% examples, 133666 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:33:37,462 : INFO : PROGRESS: at 2.52% examples, 152085 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:33:57,541 : INFO : PROGRESS: at 3.83% examples, 159119 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:34:17,560 : INFO : PROGRESS: at 5.07% examples, 162941 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:34:37,564 : INFO : PROGRESS: at 6.32% examples, 165610 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:34:57,564 : INFO : PROGRESS: at 7.59% examples, 167072 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:35:17,606 : INFO : PROGRESS: at 8.91% examples, 168464 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:35:37,643 : INFO : PROGRESS: at 10.23% examples, 169438 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:35:57,716 : INFO : PROGRESS: at 11.50% examples, 170089 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:36:17,729 : INFO : PROGRESS: at 12.82% examples, 170679 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:36:37,752 : INFO : PROGRESS: at 14.15% examples, 171291 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:36:57,767 : INFO : PROGRESS: at 15.42% examples, 171734 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:37:17,804 : INFO : PROGRESS: at 16.73% examples, 172101 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:37:37,835 : INFO : PROGRESS: at 18.06% examples, 172372 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:37:57,853 : INFO : PROGRESS: at 19.33% examples, 172541 words/s, in_qsize 32, out_qsize 1\n",
      "2016-12-29 08:38:09,701 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 08:38:17,331 : INFO : Finished loading new batch\n",
      "2016-12-29 08:38:18,218 : INFO : PROGRESS: at 20.03% examples, 167840 words/s, in_qsize 11, out_qsize 0\n",
      "2016-12-29 08:38:38,208 : INFO : PROGRESS: at 21.23% examples, 168168 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 08:38:58,285 : INFO : PROGRESS: at 22.50% examples, 168483 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:39:18,326 : INFO : PROGRESS: at 23.83% examples, 168816 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:39:38,361 : INFO : PROGRESS: at 25.13% examples, 169145 words/s, in_qsize 32, out_qsize 1\n",
      "2016-12-29 08:39:58,394 : INFO : PROGRESS: at 26.44% examples, 169468 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:40:18,400 : INFO : PROGRESS: at 27.71% examples, 169726 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:40:38,413 : INFO : PROGRESS: at 29.00% examples, 169976 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:40:58,411 : INFO : PROGRESS: at 30.22% examples, 170217 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:41:18,418 : INFO : PROGRESS: at 31.54% examples, 170450 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 08:41:38,431 : INFO : PROGRESS: at 32.83% examples, 170679 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:41:58,436 : INFO : PROGRESS: at 34.07% examples, 170861 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:42:18,466 : INFO : PROGRESS: at 35.41% examples, 171046 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:42:38,503 : INFO : PROGRESS: at 36.70% examples, 171210 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 08:42:58,553 : INFO : PROGRESS: at 37.94% examples, 171375 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:43:18,599 : INFO : PROGRESS: at 39.21% examples, 171529 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:43:33,558 : INFO : Loading new batch for index: 20000\n",
      "2016-12-29 08:43:42,668 : INFO : Finished loading new batch\n",
      "2016-12-29 08:43:43,661 : INFO : PROGRESS: at 40.13% examples, 168935 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:44:03,673 : INFO : PROGRESS: at 41.39% examples, 169109 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:44:23,683 : INFO : PROGRESS: at 42.63% examples, 169291 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 08:44:43,769 : INFO : PROGRESS: at 43.88% examples, 169423 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:45:03,791 : INFO : PROGRESS: at 45.13% examples, 169632 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 08:45:23,803 : INFO : PROGRESS: at 46.47% examples, 169778 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:45:43,850 : INFO : PROGRESS: at 47.79% examples, 169913 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:46:03,955 : INFO : PROGRESS: at 49.06% examples, 170034 words/s, in_qsize 30, out_qsize 2\n",
      "2016-12-29 08:46:24,023 : INFO : PROGRESS: at 50.34% examples, 170176 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 08:46:44,091 : INFO : PROGRESS: at 51.59% examples, 170329 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:47:04,103 : INFO : PROGRESS: at 52.90% examples, 170498 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:47:24,121 : INFO : PROGRESS: at 54.18% examples, 170603 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:47:44,143 : INFO : PROGRESS: at 55.43% examples, 170706 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:48:04,160 : INFO : PROGRESS: at 56.78% examples, 170790 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 08:48:24,195 : INFO : PROGRESS: at 58.05% examples, 170895 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:48:44,283 : INFO : PROGRESS: at 59.36% examples, 170966 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 08:48:58,758 : INFO : Loading new batch for index: 30000\n",
      "2016-12-29 08:49:07,071 : INFO : Finished loading new batch\n",
      "2016-12-29 08:49:07,468 : INFO : PROGRESS: at 60.28% examples, 169434 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:49:27,486 : INFO : PROGRESS: at 61.49% examples, 169519 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 08:49:47,517 : INFO : PROGRESS: at 62.79% examples, 169574 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:50:07,552 : INFO : PROGRESS: at 64.03% examples, 169636 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:50:27,554 : INFO : PROGRESS: at 65.27% examples, 169659 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:50:47,592 : INFO : PROGRESS: at 66.60% examples, 169717 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:51:07,635 : INFO : PROGRESS: at 67.80% examples, 169752 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:51:27,646 : INFO : PROGRESS: at 69.06% examples, 169804 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 08:51:47,698 : INFO : PROGRESS: at 70.34% examples, 169851 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:52:07,713 : INFO : PROGRESS: at 71.55% examples, 169915 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:52:27,756 : INFO : PROGRESS: at 72.83% examples, 169950 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:52:47,779 : INFO : PROGRESS: at 74.14% examples, 169969 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:53:07,845 : INFO : PROGRESS: at 75.40% examples, 169996 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:53:27,884 : INFO : PROGRESS: at 76.65% examples, 169994 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:53:47,972 : INFO : PROGRESS: at 77.89% examples, 170009 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:54:07,996 : INFO : PROGRESS: at 79.13% examples, 170029 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:54:28,118 : INFO : Loading new batch for index: 40000\n",
      "2016-12-29 08:54:28,215 : INFO : PROGRESS: at 80.27% examples, 169763 words/s, in_qsize 21, out_qsize 15\n",
      "2016-12-29 08:54:35,735 : INFO : Finished loading new batch\n",
      "2016-12-29 08:54:48,172 : INFO : PROGRESS: at 81.10% examples, 168945 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:55:08,172 : INFO : PROGRESS: at 82.37% examples, 169006 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:55:28,207 : INFO : PROGRESS: at 83.65% examples, 169064 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:55:48,242 : INFO : PROGRESS: at 84.95% examples, 169119 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 08:56:08,242 : INFO : PROGRESS: at 86.21% examples, 169167 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:56:28,245 : INFO : PROGRESS: at 87.41% examples, 169187 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:56:48,256 : INFO : PROGRESS: at 88.68% examples, 169238 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:57:08,269 : INFO : PROGRESS: at 89.92% examples, 169303 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 08:57:28,303 : INFO : PROGRESS: at 91.17% examples, 169343 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:57:48,321 : INFO : PROGRESS: at 92.46% examples, 169388 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 08:58:08,338 : INFO : PROGRESS: at 93.71% examples, 169435 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:58:28,411 : INFO : PROGRESS: at 94.94% examples, 169476 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:58:48,432 : INFO : PROGRESS: at 96.18% examples, 169526 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:59:08,441 : INFO : PROGRESS: at 97.44% examples, 169589 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:59:28,485 : INFO : PROGRESS: at 98.68% examples, 169615 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 08:59:49,582 : INFO : Loading new batch for index: 49789\n",
      "2016-12-29 08:59:49,595 : INFO : PROGRESS: at 99.89% examples, 169460 words/s, in_qsize 26, out_qsize 10\n",
      "2016-12-29 08:59:49,602 : INFO : No more batches to load, exiting at index: 49789\n",
      "2016-12-29 08:59:50,441 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2016-12-29 08:59:50,480 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2016-12-29 08:59:50,545 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2016-12-29 08:59:50,589 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2016-12-29 08:59:50,594 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2016-12-29 08:59:50,601 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2016-12-29 08:59:50,645 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2016-12-29 08:59:50,666 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2016-12-29 08:59:50,679 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2016-12-29 08:59:50,707 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2016-12-29 08:59:50,709 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2016-12-29 08:59:50,710 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2016-12-29 08:59:50,723 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-12-29 08:59:50,736 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-12-29 08:59:50,748 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-12-29 08:59:50,755 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-12-29 08:59:50,755 : INFO : training on 390507860 raw words (274500506 effective words) took 1619.4s, 169503 effective words/s\n",
      "2016-12-29 08:59:50,759 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_6/model, separately None\n",
      "2016-12-29 08:59:50,759 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_6/model.docvecs.doctag_syn0.npy\n",
      "2016-12-29 08:59:50,802 : INFO : storing numpy array 'syn1neg' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_6/model.syn1neg.npy\n",
      "2016-12-29 08:59:52,600 : INFO : not storing attribute syn0norm\n",
      "2016-12-29 08:59:52,601 : INFO : storing numpy array 'syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_6/model.syn0.npy\n",
      "2016-12-29 08:59:52,724 : INFO : not storing attribute cum_table\n",
      "2016-12-29 08:59:59,968 : INFO : Training Classifier\n",
      "2016-12-29 09:00:37,178 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 09:00:45,540 : INFO : Getting Validation Embeddings\n",
      "2016-12-29 09:00:45,541 : INFO : ===== Getting validation vectors with inference\n",
      "2016-12-29 09:00:45,548 : INFO : Loading new batch for index: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Training Metrics: Cov Err: 6.477, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.472, Top 3: 0.669, Top 5: 0.700, \n",
      "\t\t F1 Micro: 0.376, Total Pos: 20,322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 09:00:51,823 : INFO : Finished loading new batch\n",
      "2016-12-29 09:02:56,909 : INFO : Finished: 999\n",
      "2016-12-29 09:05:04,410 : INFO : Finished: 1999\n",
      "2016-12-29 09:07:09,930 : INFO : Finished: 2999\n",
      "2016-12-29 09:09:12,031 : INFO : Finished: 3999\n",
      "2016-12-29 09:11:16,534 : INFO : Finished: 4999\n",
      "2016-12-29 09:13:21,375 : INFO : Finished: 5999\n",
      "2016-12-29 09:15:22,387 : INFO : Finished: 6999\n",
      "2016-12-29 09:17:23,820 : INFO : Finished: 7999\n",
      "2016-12-29 09:19:29,414 : INFO : Finished: 8999\n",
      "2016-12-29 09:21:34,013 : INFO : Finished: 9999\n",
      "2016-12-29 09:21:35,507 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 09:21:37,438 : INFO : Finished loading new batch\n",
      "2016-12-29 09:23:47,976 : INFO : Finished: 10999\n",
      "2016-12-29 09:25:58,013 : INFO : Finished: 11999\n",
      "2016-12-29 09:25:58,264 : INFO : Loading new batch for index: 12412\n",
      "2016-12-29 09:25:58,268 : INFO : No more batches to load, exiting at index: 12412\n",
      "2016-12-29 09:26:53,416 : INFO : Finished: 12412\n",
      "2016-12-29 09:26:53,419 : INFO : Finished: 12412\n",
      "2016-12-29 09:26:54,299 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]]\n",
      "** Validation Metrics: Cov Err: 7.280, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.382, Top 3: 0.615, Top 5: 0.647, \n",
      "\t\t F1 Micro: 0.201, Total Pos: 3,795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 09:26:55,260 : INFO : precomputing L2-norms of word weight vectors\n",
      "2016-12-29 09:26:59,064 : INFO : capital-common-countries: 0.0% (0/156)\n",
      "2016-12-29 09:27:02,399 : INFO : capital-world: 0.0% (0/152)\n",
      "2016-12-29 09:27:03,277 : INFO : currency: 0.0% (0/40)\n",
      "2016-12-29 09:27:30,509 : INFO : city-in-state: 0.0% (0/1248)\n",
      "2016-12-29 09:27:32,974 : INFO : family: 8.2% (9/110)\n",
      "2016-12-29 09:27:45,043 : INFO : gram1-adjective-to-adverb: 2.0% (11/552)\n",
      "2016-12-29 09:27:52,491 : INFO : gram2-opposite: 0.9% (3/342)\n",
      "2016-12-29 09:28:21,763 : INFO : gram3-comparative: 14.8% (197/1332)\n",
      "2016-12-29 09:28:38,131 : INFO : gram4-superlative: 2.2% (17/756)\n",
      "2016-12-29 09:28:58,707 : INFO : gram5-present-participle: 6.1% (57/930)\n",
      "2016-12-29 09:29:11,773 : INFO : gram6-nationality-adjective: 0.0% (0/584)\n",
      "2016-12-29 09:29:39,636 : INFO : gram7-past-tense: 2.8% (35/1260)\n",
      "2016-12-29 09:30:01,328 : INFO : gram8-plural: 10.3% (102/992)\n",
      "2016-12-29 09:30:20,115 : INFO : gram9-plural-verbs: 9.0% (63/702)\n",
      "2016-12-29 09:30:20,117 : INFO : total: 5.4% (494/9156)\n",
      "2016-12-29 09:30:20,131 : INFO : ****************** Epoch 7 --- Working on doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_7 *******************\n",
      "2016-12-29 09:30:20,144 : INFO : training model with 16 workers on 146034 vocabulary and 5100 features, using sg=0 hs=0 sample=0.001 negative=10\n",
      "2016-12-29 09:30:20,147 : INFO : expecting 65535 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-12-29 09:30:20,191 : INFO : Loading new batch for index: 0\n",
      "2016-12-29 09:30:25,421 : INFO : Finished loading new batch\n",
      "2016-12-29 09:30:26,414 : INFO : PROGRESS: at 0.00% examples, 369 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 09:30:46,429 : INFO : PROGRESS: at 1.26% examples, 135404 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:31:06,433 : INFO : PROGRESS: at 2.55% examples, 153395 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 09:31:26,457 : INFO : PROGRESS: at 3.87% examples, 160537 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:31:46,493 : INFO : PROGRESS: at 5.12% examples, 164522 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:32:06,515 : INFO : PROGRESS: at 6.38% examples, 167038 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 09:32:26,537 : INFO : PROGRESS: at 7.68% examples, 168691 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:32:46,581 : INFO : PROGRESS: at 8.99% examples, 169772 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:33:06,632 : INFO : PROGRESS: at 10.33% examples, 170747 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 09:33:26,681 : INFO : PROGRESS: at 11.60% examples, 171538 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:33:46,727 : INFO : PROGRESS: at 12.94% examples, 172201 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:34:06,799 : INFO : PROGRESS: at 14.28% examples, 172784 words/s, in_qsize 31, out_qsize 2\n",
      "2016-12-29 09:34:26,830 : INFO : PROGRESS: at 15.57% examples, 173217 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:34:46,842 : INFO : PROGRESS: at 16.89% examples, 173455 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:35:06,890 : INFO : PROGRESS: at 18.21% examples, 173742 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:35:26,947 : INFO : PROGRESS: at 19.52% examples, 173974 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:35:36,068 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 09:35:43,944 : INFO : Finished loading new batch\n",
      "2016-12-29 09:35:46,974 : INFO : PROGRESS: at 20.17% examples, 169248 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:36:07,081 : INFO : PROGRESS: at 21.40% examples, 169636 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:36:27,098 : INFO : PROGRESS: at 22.73% examples, 170116 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:36:47,159 : INFO : PROGRESS: at 24.07% examples, 170454 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:37:07,172 : INFO : PROGRESS: at 25.36% examples, 170788 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:37:27,182 : INFO : PROGRESS: at 26.69% examples, 171115 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 09:37:47,205 : INFO : PROGRESS: at 27.98% examples, 171323 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:38:07,222 : INFO : PROGRESS: at 29.26% examples, 171524 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:38:27,285 : INFO : PROGRESS: at 30.51% examples, 171783 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:38:47,298 : INFO : PROGRESS: at 31.80% examples, 171971 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:39:07,320 : INFO : PROGRESS: at 33.09% examples, 172162 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:39:27,348 : INFO : PROGRESS: at 34.37% examples, 172332 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:39:47,385 : INFO : PROGRESS: at 35.75% examples, 172540 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:40:07,412 : INFO : PROGRESS: at 37.03% examples, 172719 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:40:27,498 : INFO : PROGRESS: at 38.27% examples, 172927 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 09:40:47,555 : INFO : PROGRESS: at 39.57% examples, 173138 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:40:56,801 : INFO : Loading new batch for index: 20000\n",
      "2016-12-29 09:41:05,761 : INFO : Finished loading new batch\n",
      "2016-12-29 09:41:07,596 : INFO : PROGRESS: at 40.23% examples, 170594 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:41:27,618 : INFO : PROGRESS: at 41.50% examples, 170856 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:41:47,662 : INFO : PROGRESS: at 42.77% examples, 171059 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:42:07,667 : INFO : PROGRESS: at 44.02% examples, 171285 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:42:27,678 : INFO : PROGRESS: at 45.31% examples, 171458 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:42:47,786 : INFO : PROGRESS: at 46.66% examples, 171619 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:43:07,795 : INFO : PROGRESS: at 48.01% examples, 171778 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 09:43:27,837 : INFO : PROGRESS: at 49.31% examples, 171954 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:43:47,865 : INFO : PROGRESS: at 50.58% examples, 172066 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:44:07,897 : INFO : PROGRESS: at 51.86% examples, 172219 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:44:27,959 : INFO : PROGRESS: at 53.17% examples, 172358 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 09:44:47,958 : INFO : PROGRESS: at 54.45% examples, 172473 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:45:07,982 : INFO : PROGRESS: at 55.75% examples, 172608 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:45:28,013 : INFO : PROGRESS: at 57.10% examples, 172711 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:45:48,026 : INFO : PROGRESS: at 58.41% examples, 172811 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 09:46:08,057 : INFO : PROGRESS: at 59.72% examples, 172897 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:46:17,037 : INFO : Loading new batch for index: 30000\n",
      "2016-12-29 09:46:25,620 : INFO : Finished loading new batch\n",
      "2016-12-29 09:46:28,074 : INFO : PROGRESS: at 60.42% examples, 171226 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:46:48,098 : INFO : PROGRESS: at 61.65% examples, 171331 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:47:08,103 : INFO : PROGRESS: at 62.98% examples, 171477 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:47:28,111 : INFO : PROGRESS: at 64.24% examples, 171571 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:47:48,119 : INFO : PROGRESS: at 65.51% examples, 171649 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:48:08,220 : INFO : PROGRESS: at 66.86% examples, 171726 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:48:28,279 : INFO : PROGRESS: at 68.10% examples, 171790 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:48:48,402 : INFO : PROGRESS: at 69.37% examples, 171828 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:49:08,499 : INFO : PROGRESS: at 70.65% examples, 171906 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:49:28,508 : INFO : PROGRESS: at 71.92% examples, 171984 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:49:48,518 : INFO : PROGRESS: at 73.26% examples, 172051 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:50:08,541 : INFO : PROGRESS: at 74.55% examples, 172114 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 09:50:28,558 : INFO : PROGRESS: at 75.87% examples, 172138 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:50:48,592 : INFO : PROGRESS: at 77.12% examples, 172174 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:51:08,596 : INFO : PROGRESS: at 78.39% examples, 172231 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 09:51:28,602 : INFO : PROGRESS: at 79.67% examples, 172300 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:51:39,819 : INFO : Loading new batch for index: 40000\n",
      "2016-12-29 09:51:47,932 : INFO : Finished loading new batch\n",
      "2016-12-29 09:51:48,667 : INFO : PROGRESS: at 80.36% examples, 171081 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:52:08,679 : INFO : PROGRESS: at 81.62% examples, 171173 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:52:28,704 : INFO : PROGRESS: at 82.95% examples, 171281 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 09:52:48,733 : INFO : PROGRESS: at 84.27% examples, 171380 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:53:08,758 : INFO : PROGRESS: at 85.57% examples, 171441 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:53:28,782 : INFO : PROGRESS: at 86.89% examples, 171532 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:53:48,828 : INFO : PROGRESS: at 88.14% examples, 171611 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:54:08,831 : INFO : PROGRESS: at 89.49% examples, 171697 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 09:54:28,881 : INFO : PROGRESS: at 90.71% examples, 171770 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:54:48,949 : INFO : PROGRESS: at 92.00% examples, 171827 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:55:09,008 : INFO : PROGRESS: at 93.31% examples, 171869 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:55:29,019 : INFO : PROGRESS: at 94.55% examples, 171914 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:55:49,117 : INFO : PROGRESS: at 95.79% examples, 171934 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:56:09,136 : INFO : PROGRESS: at 97.05% examples, 171995 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 09:56:29,149 : INFO : PROGRESS: at 98.29% examples, 172025 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:56:49,198 : INFO : PROGRESS: at 99.57% examples, 172062 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 09:56:55,801 : INFO : Loading new batch for index: 49789\n",
      "2016-12-29 09:56:55,826 : INFO : No more batches to load, exiting at index: 49789\n",
      "2016-12-29 09:56:56,708 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2016-12-29 09:56:56,738 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2016-12-29 09:56:56,763 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2016-12-29 09:56:56,817 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2016-12-29 09:56:56,848 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2016-12-29 09:56:56,853 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2016-12-29 09:56:56,875 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2016-12-29 09:56:56,888 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2016-12-29 09:56:56,898 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2016-12-29 09:56:56,902 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2016-12-29 09:56:56,918 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2016-12-29 09:56:56,920 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2016-12-29 09:56:56,928 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-12-29 09:56:56,950 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-12-29 09:56:56,968 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-12-29 09:56:56,973 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-12-29 09:56:56,973 : INFO : training on 390507860 raw words (274497925 effective words) took 1596.8s, 171907 effective words/s\n",
      "2016-12-29 09:56:56,976 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_7/model, separately None\n",
      "2016-12-29 09:56:56,977 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_7/model.docvecs.doctag_syn0.npy\n",
      "2016-12-29 09:56:57,020 : INFO : storing numpy array 'syn1neg' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_7/model.syn1neg.npy\n",
      "2016-12-29 09:56:58,817 : INFO : not storing attribute syn0norm\n",
      "2016-12-29 09:56:58,819 : INFO : storing numpy array 'syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_7/model.syn0.npy\n",
      "2016-12-29 09:56:58,952 : INFO : not storing attribute cum_table\n",
      "2016-12-29 09:57:06,139 : INFO : Training Classifier\n",
      "2016-12-29 09:57:42,631 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 09:57:50,695 : INFO : Getting Validation Embeddings\n",
      "2016-12-29 09:57:50,696 : INFO : ===== Getting validation vectors with inference\n",
      "2016-12-29 09:57:50,703 : INFO : Loading new batch for index: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Training Metrics: Cov Err: 6.455, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.474, Top 3: 0.670, Top 5: 0.702, \n",
      "\t\t F1 Micro: 0.380, Total Pos: 20,703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 09:57:59,771 : INFO : Finished loading new batch\n",
      "2016-12-29 10:00:14,016 : INFO : Finished: 999\n",
      "2016-12-29 10:02:33,009 : INFO : Finished: 1999\n",
      "2016-12-29 10:04:48,661 : INFO : Finished: 2999\n",
      "2016-12-29 10:07:00,509 : INFO : Finished: 3999\n",
      "2016-12-29 10:09:16,434 : INFO : Finished: 4999\n",
      "2016-12-29 10:11:33,019 : INFO : Finished: 5999\n",
      "2016-12-29 10:13:43,439 : INFO : Finished: 6999\n",
      "2016-12-29 10:15:54,802 : INFO : Finished: 7999\n",
      "2016-12-29 10:18:09,871 : INFO : Finished: 8999\n",
      "2016-12-29 10:20:24,739 : INFO : Finished: 9999\n",
      "2016-12-29 10:20:26,197 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 10:20:27,982 : INFO : Finished loading new batch\n",
      "2016-12-29 10:22:42,172 : INFO : Finished: 10999\n",
      "2016-12-29 10:24:57,265 : INFO : Finished: 11999\n",
      "2016-12-29 10:24:57,550 : INFO : Loading new batch for index: 12412\n",
      "2016-12-29 10:24:57,556 : INFO : No more batches to load, exiting at index: 12412\n",
      "2016-12-29 10:25:54,744 : INFO : Finished: 12412\n",
      "2016-12-29 10:25:54,746 : INFO : Finished: 12412\n",
      "2016-12-29 10:25:55,540 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "** Validation Metrics: Cov Err: 7.298, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.379, Top 3: 0.611, Top 5: 0.645, \n",
      "\t\t F1 Micro: 0.199, Total Pos: 3,717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 10:25:56,461 : INFO : precomputing L2-norms of word weight vectors\n",
      "2016-12-29 10:26:00,315 : INFO : capital-common-countries: 0.0% (0/156)\n",
      "2016-12-29 10:26:03,753 : INFO : capital-world: 0.7% (1/152)\n",
      "2016-12-29 10:26:04,647 : INFO : currency: 0.0% (0/40)\n",
      "2016-12-29 10:26:32,461 : INFO : city-in-state: 0.0% (0/1248)\n",
      "2016-12-29 10:26:34,944 : INFO : family: 8.2% (9/110)\n",
      "2016-12-29 10:26:47,364 : INFO : gram1-adjective-to-adverb: 1.6% (9/552)\n",
      "2016-12-29 10:26:55,160 : INFO : gram2-opposite: 1.5% (5/342)\n",
      "2016-12-29 10:27:24,632 : INFO : gram3-comparative: 15.7% (209/1332)\n",
      "2016-12-29 10:27:41,548 : INFO : gram4-superlative: 2.4% (18/756)\n",
      "2016-12-29 10:28:02,821 : INFO : gram5-present-participle: 6.7% (62/930)\n",
      "2016-12-29 10:28:16,043 : INFO : gram6-nationality-adjective: 0.0% (0/584)\n",
      "2016-12-29 10:28:44,496 : INFO : gram7-past-tense: 1.7% (22/1260)\n",
      "2016-12-29 10:29:06,513 : INFO : gram8-plural: 9.9% (98/992)\n",
      "2016-12-29 10:29:21,950 : INFO : gram9-plural-verbs: 9.5% (67/702)\n",
      "2016-12-29 10:29:21,953 : INFO : total: 5.5% (500/9156)\n",
      "2016-12-29 10:29:21,978 : INFO : ****************** Epoch 8 --- Working on doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_8 *******************\n",
      "2016-12-29 10:29:21,981 : INFO : training model with 16 workers on 146034 vocabulary and 5100 features, using sg=0 hs=0 sample=0.001 negative=10\n",
      "2016-12-29 10:29:21,983 : INFO : expecting 65535 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-12-29 10:29:22,014 : INFO : Loading new batch for index: 0\n",
      "2016-12-29 10:29:30,288 : INFO : Finished loading new batch\n",
      "2016-12-29 10:29:30,974 : INFO : PROGRESS: at 0.00% examples, 255 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 10:29:50,992 : INFO : PROGRESS: at 1.18% examples, 115685 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:30:11,076 : INFO : PROGRESS: at 2.40% examples, 136852 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 10:30:31,089 : INFO : PROGRESS: at 3.67% examples, 146116 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:30:51,186 : INFO : PROGRESS: at 4.85% examples, 151157 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:31:11,191 : INFO : PROGRESS: at 6.07% examples, 154562 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:31:31,252 : INFO : PROGRESS: at 7.30% examples, 156690 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:31:51,272 : INFO : PROGRESS: at 8.54% examples, 158489 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:32:11,324 : INFO : PROGRESS: at 9.85% examples, 159825 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 10:32:31,338 : INFO : PROGRESS: at 11.05% examples, 160941 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 10:32:51,435 : INFO : PROGRESS: at 12.32% examples, 161772 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:33:11,451 : INFO : PROGRESS: at 13.60% examples, 162550 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:33:31,462 : INFO : PROGRESS: at 14.85% examples, 163273 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 10:33:51,465 : INFO : PROGRESS: at 16.10% examples, 163795 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:34:11,477 : INFO : PROGRESS: at 17.36% examples, 164327 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:34:31,496 : INFO : PROGRESS: at 18.65% examples, 164721 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:34:51,613 : INFO : PROGRESS: at 19.87% examples, 165055 words/s, in_qsize 32, out_qsize 1\n",
      "2016-12-29 10:34:54,800 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 10:35:01,636 : INFO : Finished loading new batch\n",
      "2016-12-29 10:35:11,620 : INFO : PROGRESS: at 20.57% examples, 161454 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:35:31,807 : INFO : PROGRESS: at 21.77% examples, 161948 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:35:51,834 : INFO : PROGRESS: at 23.08% examples, 162479 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:36:11,899 : INFO : PROGRESS: at 24.38% examples, 162930 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:36:31,954 : INFO : PROGRESS: at 25.62% examples, 163307 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:36:52,002 : INFO : PROGRESS: at 26.91% examples, 163717 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:37:12,042 : INFO : PROGRESS: at 28.17% examples, 164087 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:37:32,084 : INFO : PROGRESS: at 29.42% examples, 164454 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 10:37:52,135 : INFO : PROGRESS: at 30.67% examples, 164807 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:38:12,207 : INFO : PROGRESS: at 31.94% examples, 165168 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 10:38:32,187 : INFO : PROGRESS: at 33.19% examples, 165430 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:38:52,197 : INFO : PROGRESS: at 34.44% examples, 165732 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:39:12,199 : INFO : PROGRESS: at 35.79% examples, 166004 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:39:32,207 : INFO : PROGRESS: at 37.03% examples, 166233 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:39:52,240 : INFO : PROGRESS: at 38.23% examples, 166474 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:40:12,324 : INFO : PROGRESS: at 39.49% examples, 166706 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 10:40:22,757 : INFO : Loading new batch for index: 20000\n",
      "2016-12-29 10:40:30,058 : INFO : Finished loading new batch\n",
      "2016-12-29 10:40:32,430 : INFO : PROGRESS: at 40.24% examples, 164756 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:40:52,480 : INFO : PROGRESS: at 41.47% examples, 165024 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 10:41:12,493 : INFO : PROGRESS: at 42.70% examples, 165245 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 10:41:32,487 : INFO : PROGRESS: at 43.93% examples, 165461 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:41:52,491 : INFO : PROGRESS: at 45.16% examples, 165649 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:42:12,519 : INFO : PROGRESS: at 46.48% examples, 165812 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:42:32,538 : INFO : PROGRESS: at 47.78% examples, 166008 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:42:52,588 : INFO : PROGRESS: at 49.03% examples, 166141 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:43:12,597 : INFO : PROGRESS: at 50.28% examples, 166307 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:43:32,601 : INFO : PROGRESS: at 51.50% examples, 166450 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:43:52,612 : INFO : PROGRESS: at 52.77% examples, 166609 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:44:12,648 : INFO : PROGRESS: at 54.04% examples, 166751 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:44:32,655 : INFO : PROGRESS: at 55.27% examples, 166907 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:44:52,723 : INFO : PROGRESS: at 56.61% examples, 167027 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:45:12,754 : INFO : PROGRESS: at 57.87% examples, 167170 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:45:32,760 : INFO : PROGRESS: at 59.15% examples, 167285 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:45:50,552 : INFO : Loading new batch for index: 30000\n",
      "2016-12-29 10:45:57,670 : INFO : Finished loading new batch\n",
      "2016-12-29 10:45:58,405 : INFO : PROGRESS: at 60.28% examples, 165987 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 10:46:18,411 : INFO : PROGRESS: at 61.48% examples, 166107 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 10:46:38,427 : INFO : PROGRESS: at 62.76% examples, 166204 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:46:58,548 : INFO : PROGRESS: at 64.00% examples, 166286 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:47:18,569 : INFO : PROGRESS: at 65.25% examples, 166394 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 10:47:38,576 : INFO : PROGRESS: at 66.56% examples, 166503 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:47:58,623 : INFO : PROGRESS: at 67.78% examples, 166613 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 10:48:18,656 : INFO : PROGRESS: at 69.04% examples, 166691 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:48:38,683 : INFO : PROGRESS: at 70.32% examples, 166791 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:48:58,696 : INFO : PROGRESS: at 71.50% examples, 166872 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:49:18,712 : INFO : PROGRESS: at 72.78% examples, 166964 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:49:38,718 : INFO : PROGRESS: at 74.11% examples, 167075 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 10:49:58,713 : INFO : PROGRESS: at 75.38% examples, 167173 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:50:18,737 : INFO : PROGRESS: at 76.63% examples, 167225 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:50:38,739 : INFO : PROGRESS: at 77.87% examples, 167287 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:50:58,744 : INFO : PROGRESS: at 79.12% examples, 167370 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:51:18,613 : INFO : Loading new batch for index: 40000\n",
      "2016-12-29 10:51:18,767 : INFO : PROGRESS: at 80.27% examples, 167191 words/s, in_qsize 17, out_qsize 15\n",
      "2016-12-29 10:51:26,733 : INFO : Finished loading new batch\n",
      "2016-12-29 10:51:38,757 : INFO : PROGRESS: at 81.05% examples, 166313 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 10:51:58,774 : INFO : PROGRESS: at 82.29% examples, 166383 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:52:18,784 : INFO : PROGRESS: at 83.57% examples, 166462 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:52:38,820 : INFO : PROGRESS: at 84.86% examples, 166538 words/s, in_qsize 32, out_qsize 2\n",
      "2016-12-29 10:52:58,898 : INFO : PROGRESS: at 86.11% examples, 166609 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 10:53:18,930 : INFO : PROGRESS: at 87.35% examples, 166678 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:53:38,965 : INFO : PROGRESS: at 88.61% examples, 166761 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:53:58,990 : INFO : PROGRESS: at 89.84% examples, 166831 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:54:18,993 : INFO : PROGRESS: at 91.08% examples, 166903 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:54:39,017 : INFO : PROGRESS: at 92.36% examples, 166972 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:54:59,046 : INFO : PROGRESS: at 93.61% examples, 167027 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:55:19,132 : INFO : PROGRESS: at 94.83% examples, 167078 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:55:39,156 : INFO : PROGRESS: at 96.06% examples, 167162 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:55:59,145 : INFO : PROGRESS: at 97.30% examples, 167227 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:56:19,150 : INFO : PROGRESS: at 98.53% examples, 167290 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:56:39,198 : INFO : PROGRESS: at 99.80% examples, 167348 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 10:56:42,488 : INFO : Loading new batch for index: 49789\n",
      "2016-12-29 10:56:42,562 : INFO : No more batches to load, exiting at index: 49789\n",
      "2016-12-29 10:56:43,341 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2016-12-29 10:56:43,356 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2016-12-29 10:56:43,493 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2016-12-29 10:56:43,506 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2016-12-29 10:56:43,517 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2016-12-29 10:56:43,528 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2016-12-29 10:56:43,531 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2016-12-29 10:56:43,574 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2016-12-29 10:56:43,577 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2016-12-29 10:56:43,596 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2016-12-29 10:56:43,598 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2016-12-29 10:56:43,618 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2016-12-29 10:56:43,631 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-12-29 10:56:43,634 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-12-29 10:56:43,642 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-12-29 10:56:43,665 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-12-29 10:56:43,668 : INFO : training on 390507860 raw words (274492274 effective words) took 1641.7s, 167205 effective words/s\n",
      "2016-12-29 10:56:43,670 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_8/model, separately None\n",
      "2016-12-29 10:56:43,670 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_8/model.docvecs.doctag_syn0.npy\n",
      "2016-12-29 10:56:43,743 : INFO : storing numpy array 'syn1neg' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_8/model.syn1neg.npy\n",
      "2016-12-29 10:56:45,568 : INFO : not storing attribute syn0norm\n",
      "2016-12-29 10:56:45,570 : INFO : storing numpy array 'syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_8/model.syn0.npy\n",
      "2016-12-29 10:56:45,706 : INFO : not storing attribute cum_table\n",
      "2016-12-29 10:56:49,157 : INFO : Training Classifier\n",
      "2016-12-29 10:57:27,095 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 10:57:35,905 : INFO : Getting Validation Embeddings\n",
      "2016-12-29 10:57:35,906 : INFO : ===== Getting validation vectors with inference\n",
      "2016-12-29 10:57:35,918 : INFO : Loading new batch for index: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Training Metrics: Cov Err: 6.441, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.474, Top 3: 0.670, Top 5: 0.701, \n",
      "\t\t F1 Micro: 0.381, Total Pos: 20,765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 10:57:43,845 : INFO : Finished loading new batch\n",
      "2016-12-29 10:59:58,996 : INFO : Finished: 999\n",
      "2016-12-29 11:02:07,671 : INFO : Finished: 1999\n",
      "2016-12-29 11:04:14,837 : INFO : Finished: 2999\n",
      "2016-12-29 11:06:18,262 : INFO : Finished: 3999\n",
      "2016-12-29 11:08:25,627 : INFO : Finished: 4999\n",
      "2016-12-29 11:10:33,234 : INFO : Finished: 5999\n",
      "2016-12-29 11:12:35,254 : INFO : Finished: 6999\n",
      "2016-12-29 11:14:38,635 : INFO : Finished: 7999\n",
      "2016-12-29 11:16:44,923 : INFO : Finished: 8999\n",
      "2016-12-29 11:18:49,457 : INFO : Finished: 9999\n",
      "2016-12-29 11:18:50,990 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 11:18:52,931 : INFO : Finished loading new batch\n",
      "2016-12-29 11:21:00,839 : INFO : Finished: 10999\n",
      "2016-12-29 11:23:09,821 : INFO : Finished: 11999\n",
      "2016-12-29 11:23:10,157 : INFO : Loading new batch for index: 12412\n",
      "2016-12-29 11:23:10,162 : INFO : No more batches to load, exiting at index: 12412\n",
      "2016-12-29 11:24:05,873 : INFO : Finished: 12412\n",
      "2016-12-29 11:24:05,876 : INFO : Finished: 12412\n",
      "2016-12-29 11:24:06,662 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "** Validation Metrics: Cov Err: 7.342, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.377, Top 3: 0.608, Top 5: 0.640, \n",
      "\t\t F1 Micro: 0.185, Total Pos: 3,466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 11:24:07,586 : INFO : precomputing L2-norms of word weight vectors\n",
      "2016-12-29 11:24:11,420 : INFO : capital-common-countries: 0.0% (0/156)\n",
      "2016-12-29 11:24:14,794 : INFO : capital-world: 0.7% (1/152)\n",
      "2016-12-29 11:24:15,690 : INFO : currency: 0.0% (0/40)\n",
      "2016-12-29 11:24:43,586 : INFO : city-in-state: 0.0% (0/1248)\n",
      "2016-12-29 11:24:46,052 : INFO : family: 5.5% (6/110)\n",
      "2016-12-29 11:24:58,352 : INFO : gram1-adjective-to-adverb: 1.8% (10/552)\n",
      "2016-12-29 11:25:05,856 : INFO : gram2-opposite: 0.6% (2/342)\n",
      "2016-12-29 11:25:35,110 : INFO : gram3-comparative: 15.9% (212/1332)\n",
      "2016-12-29 11:25:51,596 : INFO : gram4-superlative: 2.2% (17/756)\n",
      "2016-12-29 11:26:11,931 : INFO : gram5-present-participle: 7.5% (70/930)\n",
      "2016-12-29 11:26:24,941 : INFO : gram6-nationality-adjective: 0.0% (0/584)\n",
      "2016-12-29 11:26:52,679 : INFO : gram7-past-tense: 2.8% (35/1260)\n",
      "2016-12-29 11:27:13,949 : INFO : gram8-plural: 9.8% (97/992)\n",
      "2016-12-29 11:27:29,040 : INFO : gram9-plural-verbs: 11.7% (82/702)\n",
      "2016-12-29 11:27:29,042 : INFO : total: 5.8% (532/9156)\n",
      "2016-12-29 11:27:29,065 : INFO : ****************** Epoch 9 --- Working on doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_9 *******************\n",
      "2016-12-29 11:27:29,078 : INFO : training model with 16 workers on 146034 vocabulary and 5100 features, using sg=0 hs=0 sample=0.001 negative=10\n",
      "2016-12-29 11:27:29,082 : INFO : expecting 65535 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-12-29 11:27:29,111 : INFO : Loading new batch for index: 0\n",
      "2016-12-29 11:27:36,642 : INFO : Finished loading new batch\n",
      "2016-12-29 11:27:37,519 : INFO : PROGRESS: at 0.00% examples, 274 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 11:27:57,530 : INFO : PROGRESS: at 1.18% examples, 118279 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:28:17,538 : INFO : PROGRESS: at 2.43% examples, 140128 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:28:37,559 : INFO : PROGRESS: at 3.72% examples, 149625 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:28:57,569 : INFO : PROGRESS: at 4.96% examples, 155279 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:29:17,593 : INFO : PROGRESS: at 6.20% examples, 158867 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 11:29:37,597 : INFO : PROGRESS: at 7.45% examples, 161066 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:29:57,615 : INFO : PROGRESS: at 8.72% examples, 162759 words/s, in_qsize 31, out_qsize 2\n",
      "2016-12-29 11:30:17,709 : INFO : PROGRESS: at 10.03% examples, 163792 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:30:37,742 : INFO : PROGRESS: at 11.27% examples, 164771 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:30:57,773 : INFO : PROGRESS: at 12.56% examples, 165643 words/s, in_qsize 32, out_qsize 1\n",
      "2016-12-29 11:31:17,780 : INFO : PROGRESS: at 13.88% examples, 166509 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:31:37,928 : INFO : PROGRESS: at 15.12% examples, 166798 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:31:57,960 : INFO : PROGRESS: at 16.39% examples, 167235 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:32:17,973 : INFO : PROGRESS: at 17.68% examples, 167519 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:32:37,984 : INFO : PROGRESS: at 18.99% examples, 167826 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 11:32:56,294 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 11:33:04,970 : INFO : Finished loading new batch\n",
      "2016-12-29 11:33:05,797 : INFO : PROGRESS: at 20.03% examples, 162958 words/s, in_qsize 7, out_qsize 0\n",
      "2016-12-29 11:33:25,808 : INFO : PROGRESS: at 21.20% examples, 163367 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:33:45,828 : INFO : PROGRESS: at 22.46% examples, 163829 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:34:05,860 : INFO : PROGRESS: at 23.77% examples, 164289 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:34:25,877 : INFO : PROGRESS: at 25.04% examples, 164664 words/s, in_qsize 31, out_qsize 1\n",
      "2016-12-29 11:34:45,908 : INFO : PROGRESS: at 26.31% examples, 165012 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:35:05,966 : INFO : PROGRESS: at 27.60% examples, 165390 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:35:25,981 : INFO : PROGRESS: at 28.87% examples, 165717 words/s, in_qsize 32, out_qsize 1\n",
      "2016-12-29 11:35:46,086 : INFO : PROGRESS: at 30.09% examples, 166026 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:36:06,121 : INFO : PROGRESS: at 31.38% examples, 166329 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:36:26,122 : INFO : PROGRESS: at 32.64% examples, 166618 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 11:36:46,139 : INFO : PROGRESS: at 33.89% examples, 166881 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:37:06,186 : INFO : PROGRESS: at 35.17% examples, 167072 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:37:26,189 : INFO : PROGRESS: at 36.46% examples, 167277 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:37:46,191 : INFO : PROGRESS: at 37.69% examples, 167442 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:38:06,203 : INFO : PROGRESS: at 38.92% examples, 167613 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:38:26,329 : INFO : Loading new batch for index: 20000\n",
      "2016-12-29 11:38:26,353 : INFO : PROGRESS: at 40.02% examples, 167250 words/s, in_qsize 26, out_qsize 11\n",
      "2016-12-29 11:38:32,616 : INFO : Finished loading new batch\n",
      "2016-12-29 11:38:46,464 : INFO : PROGRESS: at 40.62% examples, 164680 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:39:06,465 : INFO : PROGRESS: at 41.84% examples, 164971 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:39:26,503 : INFO : PROGRESS: at 43.15% examples, 165272 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:39:46,510 : INFO : PROGRESS: at 44.33% examples, 165492 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:40:06,521 : INFO : PROGRESS: at 45.60% examples, 165720 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:40:26,546 : INFO : PROGRESS: at 46.95% examples, 165947 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:40:46,548 : INFO : PROGRESS: at 48.25% examples, 166153 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:41:06,553 : INFO : PROGRESS: at 49.51% examples, 166366 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 11:41:26,557 : INFO : PROGRESS: at 50.76% examples, 166577 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:41:46,557 : INFO : PROGRESS: at 52.05% examples, 166779 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:42:06,560 : INFO : PROGRESS: at 53.32% examples, 166991 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:42:26,628 : INFO : PROGRESS: at 54.61% examples, 167224 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:42:46,639 : INFO : PROGRESS: at 55.90% examples, 167431 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:43:06,687 : INFO : PROGRESS: at 57.24% examples, 167597 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:43:26,711 : INFO : PROGRESS: at 58.52% examples, 167745 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 11:43:46,714 : INFO : PROGRESS: at 59.83% examples, 167899 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:43:54,402 : INFO : Loading new batch for index: 30000\n",
      "2016-12-29 11:44:02,810 : INFO : Finished loading new batch\n",
      "2016-12-29 11:44:06,739 : INFO : PROGRESS: at 60.47% examples, 166241 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:44:26,751 : INFO : PROGRESS: at 61.68% examples, 166389 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:44:46,763 : INFO : PROGRESS: at 62.98% examples, 166525 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:45:06,797 : INFO : PROGRESS: at 64.21% examples, 166663 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 11:45:26,825 : INFO : PROGRESS: at 65.48% examples, 166812 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:45:46,859 : INFO : PROGRESS: at 66.82% examples, 166935 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:46:06,891 : INFO : PROGRESS: at 68.04% examples, 167098 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 11:46:26,897 : INFO : PROGRESS: at 69.32% examples, 167241 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:46:46,932 : INFO : PROGRESS: at 70.59% examples, 167347 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 11:47:06,955 : INFO : PROGRESS: at 71.83% examples, 167464 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:47:26,996 : INFO : PROGRESS: at 73.15% examples, 167594 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 11:47:47,081 : INFO : PROGRESS: at 74.46% examples, 167708 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:48:07,084 : INFO : PROGRESS: at 75.79% examples, 167839 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:48:27,109 : INFO : PROGRESS: at 77.05% examples, 167948 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:48:47,133 : INFO : PROGRESS: at 78.32% examples, 168076 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:49:07,158 : INFO : PROGRESS: at 79.61% examples, 168205 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:49:19,391 : INFO : Loading new batch for index: 40000\n",
      "2016-12-29 11:49:27,502 : INFO : Finished loading new batch\n",
      "2016-12-29 11:49:28,693 : INFO : PROGRESS: at 80.36% examples, 167033 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:49:48,698 : INFO : PROGRESS: at 81.60% examples, 167159 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 11:50:08,700 : INFO : PROGRESS: at 82.90% examples, 167253 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:50:28,707 : INFO : PROGRESS: at 84.16% examples, 167310 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:50:48,726 : INFO : PROGRESS: at 85.45% examples, 167390 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 11:51:08,739 : INFO : PROGRESS: at 86.75% examples, 167486 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:51:28,755 : INFO : PROGRESS: at 87.95% examples, 167564 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:51:48,770 : INFO : PROGRESS: at 89.26% examples, 167643 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:52:08,815 : INFO : PROGRESS: at 90.47% examples, 167722 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:52:28,833 : INFO : PROGRESS: at 91.76% examples, 167793 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:52:48,859 : INFO : PROGRESS: at 93.02% examples, 167843 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:53:08,898 : INFO : PROGRESS: at 94.25% examples, 167890 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:53:28,913 : INFO : PROGRESS: at 95.46% examples, 167948 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:53:48,980 : INFO : PROGRESS: at 96.69% examples, 168000 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:54:09,015 : INFO : PROGRESS: at 97.94% examples, 168047 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 11:54:29,023 : INFO : PROGRESS: at 99.16% examples, 168087 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 11:54:42,060 : INFO : Loading new batch for index: 49789\n",
      "2016-12-29 11:54:42,108 : INFO : No more batches to load, exiting at index: 49789\n",
      "2016-12-29 11:54:42,887 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2016-12-29 11:54:42,921 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2016-12-29 11:54:42,946 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2016-12-29 11:54:43,010 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2016-12-29 11:54:43,033 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2016-12-29 11:54:43,041 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2016-12-29 11:54:43,060 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2016-12-29 11:54:43,100 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2016-12-29 11:54:43,106 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2016-12-29 11:54:43,117 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2016-12-29 11:54:43,122 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2016-12-29 11:54:43,144 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2016-12-29 11:54:43,153 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-12-29 11:54:43,175 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-12-29 11:54:43,176 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-12-29 11:54:43,185 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-12-29 11:54:43,186 : INFO : training on 390507860 raw words (274486159 effective words) took 1634.1s, 167977 effective words/s\n",
      "2016-12-29 11:54:43,188 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_9/model, separately None\n",
      "2016-12-29 11:54:43,189 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_9/model.docvecs.doctag_syn0.npy\n",
      "2016-12-29 11:54:43,231 : INFO : storing numpy array 'syn1neg' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_9/model.syn1neg.npy\n",
      "2016-12-29 11:54:44,904 : INFO : not storing attribute syn0norm\n",
      "2016-12-29 11:54:44,905 : INFO : storing numpy array 'syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_9/model.syn0.npy\n",
      "2016-12-29 11:54:45,026 : INFO : not storing attribute cum_table\n",
      "2016-12-29 11:54:49,278 : INFO : Training Classifier\n",
      "2016-12-29 11:55:26,941 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 11:55:35,405 : INFO : Getting Validation Embeddings\n",
      "2016-12-29 11:55:35,406 : INFO : ===== Getting validation vectors with inference\n",
      "2016-12-29 11:55:35,417 : INFO : Loading new batch for index: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Training Metrics: Cov Err: 6.429, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.475, Top 3: 0.671, Top 5: 0.702, \n",
      "\t\t F1 Micro: 0.383, Total Pos: 20,924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 11:55:41,441 : INFO : Finished loading new batch\n",
      "2016-12-29 11:57:57,460 : INFO : Finished: 999\n",
      "2016-12-29 12:00:16,908 : INFO : Finished: 1999\n",
      "2016-12-29 12:02:33,473 : INFO : Finished: 2999\n",
      "2016-12-29 12:04:46,680 : INFO : Finished: 3999\n",
      "2016-12-29 12:07:02,471 : INFO : Finished: 4999\n",
      "2016-12-29 12:09:19,418 : INFO : Finished: 5999\n",
      "2016-12-29 12:11:30,328 : INFO : Finished: 6999\n",
      "2016-12-29 12:13:41,801 : INFO : Finished: 7999\n",
      "2016-12-29 12:15:56,939 : INFO : Finished: 8999\n",
      "2016-12-29 12:18:10,998 : INFO : Finished: 9999\n",
      "2016-12-29 12:18:12,499 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 12:18:13,952 : INFO : Finished loading new batch\n",
      "2016-12-29 12:20:28,749 : INFO : Finished: 10999\n",
      "2016-12-29 12:22:41,680 : INFO : Finished: 11999\n",
      "2016-12-29 12:22:42,000 : INFO : Loading new batch for index: 12412\n",
      "2016-12-29 12:22:42,005 : INFO : No more batches to load, exiting at index: 12412\n",
      "2016-12-29 12:23:37,090 : INFO : Finished: 12412\n",
      "2016-12-29 12:23:37,092 : INFO : Finished: 12412\n",
      "2016-12-29 12:23:37,856 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "** Validation Metrics: Cov Err: 7.383, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.374, Top 3: 0.607, Top 5: 0.638, \n",
      "\t\t F1 Micro: 0.174, Total Pos: 3,169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 12:23:38,814 : INFO : precomputing L2-norms of word weight vectors\n",
      "2016-12-29 12:23:42,552 : INFO : capital-common-countries: 0.0% (0/156)\n",
      "2016-12-29 12:23:46,001 : INFO : capital-world: 0.7% (1/152)\n",
      "2016-12-29 12:23:46,870 : INFO : currency: 0.0% (0/40)\n",
      "2016-12-29 12:24:14,388 : INFO : city-in-state: 0.0% (0/1248)\n",
      "2016-12-29 12:24:16,900 : INFO : family: 5.5% (6/110)\n",
      "2016-12-29 12:24:29,198 : INFO : gram1-adjective-to-adverb: 1.6% (9/552)\n",
      "2016-12-29 12:24:36,732 : INFO : gram2-opposite: 0.3% (1/342)\n",
      "2016-12-29 12:25:05,939 : INFO : gram3-comparative: 15.2% (202/1332)\n",
      "2016-12-29 12:25:22,581 : INFO : gram4-superlative: 2.4% (18/756)\n",
      "2016-12-29 12:25:42,929 : INFO : gram5-present-participle: 8.2% (76/930)\n",
      "2016-12-29 12:25:55,628 : INFO : gram6-nationality-adjective: 0.0% (0/584)\n",
      "2016-12-29 12:26:23,639 : INFO : gram7-past-tense: 3.2% (40/1260)\n",
      "2016-12-29 12:26:45,209 : INFO : gram8-plural: 9.4% (93/992)\n",
      "2016-12-29 12:27:00,605 : INFO : gram9-plural-verbs: 10.7% (75/702)\n",
      "2016-12-29 12:27:00,607 : INFO : total: 5.7% (521/9156)\n",
      "2016-12-29 12:27:00,624 : INFO : ****************** Epoch 10 --- Working on doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_10 *******************\n",
      "2016-12-29 12:27:00,628 : INFO : training model with 16 workers on 146034 vocabulary and 5100 features, using sg=0 hs=0 sample=0.001 negative=10\n",
      "2016-12-29 12:27:00,629 : INFO : expecting 65535 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-12-29 12:27:00,656 : INFO : Loading new batch for index: 0\n",
      "2016-12-29 12:27:07,246 : INFO : Finished loading new batch\n",
      "2016-12-29 12:27:08,292 : INFO : PROGRESS: at 0.00% examples, 297 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 12:27:28,301 : INFO : PROGRESS: at 1.27% examples, 129920 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:27:48,307 : INFO : PROGRESS: at 2.60% examples, 151236 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:28:08,382 : INFO : PROGRESS: at 3.93% examples, 159981 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:28:28,485 : INFO : PROGRESS: at 5.21% examples, 164826 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:28:48,495 : INFO : PROGRESS: at 6.52% examples, 168029 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:29:08,511 : INFO : PROGRESS: at 7.83% examples, 170029 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 12:29:28,567 : INFO : PROGRESS: at 9.20% examples, 171520 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:29:48,594 : INFO : PROGRESS: at 10.53% examples, 172834 words/s, in_qsize 31, out_qsize 3\n",
      "2016-12-29 12:30:08,727 : INFO : PROGRESS: at 11.87% examples, 173655 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:30:28,738 : INFO : PROGRESS: at 13.20% examples, 174278 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:30:48,743 : INFO : PROGRESS: at 14.56% examples, 174915 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:31:08,754 : INFO : PROGRESS: at 15.87% examples, 175418 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:31:28,785 : INFO : PROGRESS: at 17.23% examples, 175904 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:31:48,791 : INFO : PROGRESS: at 18.57% examples, 176178 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 12:32:08,837 : INFO : PROGRESS: at 19.88% examples, 176525 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:32:12,169 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 12:32:20,249 : INFO : Finished loading new batch\n",
      "2016-12-29 12:32:28,881 : INFO : PROGRESS: at 20.49% examples, 171300 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:32:48,909 : INFO : PROGRESS: at 21.76% examples, 171883 words/s, in_qsize 30, out_qsize 1\n",
      "2016-12-29 12:33:08,942 : INFO : PROGRESS: at 23.13% examples, 172292 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:33:28,953 : INFO : PROGRESS: at 24.47% examples, 172647 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:33:48,953 : INFO : PROGRESS: at 25.78% examples, 173025 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:34:08,957 : INFO : PROGRESS: at 27.13% examples, 173375 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:34:28,980 : INFO : PROGRESS: at 28.45% examples, 173713 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:34:48,988 : INFO : PROGRESS: at 29.72% examples, 173973 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:35:08,991 : INFO : PROGRESS: at 31.03% examples, 174234 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:35:29,017 : INFO : PROGRESS: at 32.32% examples, 174387 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 12:35:49,077 : INFO : PROGRESS: at 33.66% examples, 174622 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 12:36:09,092 : INFO : PROGRESS: at 34.96% examples, 174782 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:36:29,142 : INFO : PROGRESS: at 36.32% examples, 174950 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 12:36:49,146 : INFO : PROGRESS: at 37.58% examples, 175079 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:37:09,153 : INFO : PROGRESS: at 38.86% examples, 175229 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 12:37:29,393 : INFO : Loading new batch for index: 20000\n",
      "2016-12-29 12:37:29,437 : INFO : PROGRESS: at 40.02% examples, 174811 words/s, in_qsize 24, out_qsize 12\n",
      "2016-12-29 12:37:37,832 : INFO : Finished loading new batch\n",
      "2016-12-29 12:37:49,502 : INFO : PROGRESS: at 40.82% examples, 172758 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:38:09,502 : INFO : PROGRESS: at 42.08% examples, 172983 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 12:38:29,548 : INFO : PROGRESS: at 43.41% examples, 173182 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:38:49,572 : INFO : PROGRESS: at 44.67% examples, 173407 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 12:39:09,579 : INFO : PROGRESS: at 46.01% examples, 173623 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:39:29,588 : INFO : PROGRESS: at 47.37% examples, 173807 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:39:49,593 : INFO : PROGRESS: at 48.71% examples, 173945 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:40:09,613 : INFO : PROGRESS: at 49.99% examples, 174079 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:40:29,625 : INFO : PROGRESS: at 51.28% examples, 174210 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:40:49,734 : INFO : PROGRESS: at 52.58% examples, 174308 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:41:09,766 : INFO : PROGRESS: at 53.90% examples, 174432 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 12:41:29,790 : INFO : PROGRESS: at 55.15% examples, 174518 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:41:49,801 : INFO : PROGRESS: at 56.54% examples, 174599 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:42:09,850 : INFO : PROGRESS: at 57.83% examples, 174673 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:42:29,856 : INFO : PROGRESS: at 59.14% examples, 174737 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:42:47,213 : INFO : Loading new batch for index: 30000\n",
      "2016-12-29 12:42:55,438 : INFO : Finished loading new batch\n",
      "2016-12-29 12:42:55,875 : INFO : PROGRESS: at 60.28% examples, 173138 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:43:15,924 : INFO : PROGRESS: at 61.53% examples, 173241 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:43:35,941 : INFO : PROGRESS: at 62.87% examples, 173362 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:43:55,943 : INFO : PROGRESS: at 64.16% examples, 173483 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:44:15,945 : INFO : PROGRESS: at 65.45% examples, 173585 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 12:44:35,947 : INFO : PROGRESS: at 66.82% examples, 173676 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 12:44:55,965 : INFO : PROGRESS: at 68.06% examples, 173756 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:45:16,116 : INFO : PROGRESS: at 69.37% examples, 173820 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 12:45:36,141 : INFO : PROGRESS: at 70.67% examples, 173917 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:45:56,187 : INFO : PROGRESS: at 71.95% examples, 173972 words/s, in_qsize 32, out_qsize 2\n",
      "2016-12-29 12:46:16,195 : INFO : PROGRESS: at 73.30% examples, 174064 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:46:36,230 : INFO : PROGRESS: at 74.62% examples, 174142 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:46:56,277 : INFO : PROGRESS: at 75.99% examples, 174235 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:47:16,306 : INFO : PROGRESS: at 77.26% examples, 174307 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:47:36,322 : INFO : PROGRESS: at 78.57% examples, 174397 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:47:56,337 : INFO : PROGRESS: at 79.86% examples, 174467 words/s, in_qsize 31, out_qsize 1\n",
      "2016-12-29 12:48:04,352 : INFO : Loading new batch for index: 40000\n",
      "2016-12-29 12:48:10,742 : INFO : Finished loading new batch\n",
      "2016-12-29 12:48:16,353 : INFO : PROGRESS: at 80.70% examples, 173484 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:48:36,378 : INFO : PROGRESS: at 81.96% examples, 173560 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:48:56,410 : INFO : PROGRESS: at 83.32% examples, 173652 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:49:16,421 : INFO : PROGRESS: at 84.66% examples, 173733 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 12:49:36,436 : INFO : PROGRESS: at 85.95% examples, 173778 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:49:56,457 : INFO : PROGRESS: at 87.22% examples, 173828 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 12:50:16,501 : INFO : PROGRESS: at 88.49% examples, 173843 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:50:36,542 : INFO : PROGRESS: at 89.78% examples, 173878 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:50:56,551 : INFO : PROGRESS: at 91.03% examples, 173922 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:51:16,551 : INFO : PROGRESS: at 92.33% examples, 173938 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:51:36,623 : INFO : PROGRESS: at 93.64% examples, 173976 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:51:56,632 : INFO : PROGRESS: at 94.89% examples, 174012 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:52:16,655 : INFO : PROGRESS: at 96.16% examples, 174059 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:52:36,660 : INFO : PROGRESS: at 97.44% examples, 174127 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:52:56,678 : INFO : PROGRESS: at 98.72% examples, 174169 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 12:53:16,959 : INFO : Loading new batch for index: 49789\n",
      "2016-12-29 12:53:17,027 : INFO : No more batches to load, exiting at index: 49789\n",
      "2016-12-29 12:53:17,032 : INFO : PROGRESS: at 99.89% examples, 173956 words/s, in_qsize 24, out_qsize 10\n",
      "2016-12-29 12:53:17,757 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2016-12-29 12:53:17,789 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2016-12-29 12:53:17,840 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2016-12-29 12:53:17,902 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2016-12-29 12:53:17,907 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2016-12-29 12:53:17,948 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2016-12-29 12:53:17,975 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2016-12-29 12:53:17,986 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2016-12-29 12:53:18,019 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2016-12-29 12:53:18,028 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2016-12-29 12:53:18,036 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2016-12-29 12:53:18,041 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2016-12-29 12:53:18,045 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-12-29 12:53:18,060 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-12-29 12:53:18,081 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-12-29 12:53:18,087 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-12-29 12:53:18,088 : INFO : training on 390507860 raw words (274486250 effective words) took 1577.4s, 174008 effective words/s\n",
      "2016-12-29 12:53:18,090 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_10/model, separately None\n",
      "2016-12-29 12:53:18,090 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_10/model.docvecs.doctag_syn0.npy\n",
      "2016-12-29 12:53:18,134 : INFO : storing numpy array 'syn1neg' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_10/model.syn1neg.npy\n",
      "2016-12-29 12:53:19,883 : INFO : not storing attribute syn0norm\n",
      "2016-12-29 12:53:19,884 : INFO : storing numpy array 'syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_10/model.syn0.npy\n",
      "2016-12-29 12:53:20,010 : INFO : not storing attribute cum_table\n",
      "2016-12-29 12:53:26,917 : INFO : Training Classifier\n",
      "2016-12-29 12:54:05,748 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 12:54:14,085 : INFO : Getting Validation Embeddings\n",
      "2016-12-29 12:54:14,086 : INFO : ===== Getting validation vectors with inference\n",
      "2016-12-29 12:54:14,093 : INFO : Loading new batch for index: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Training Metrics: Cov Err: 6.420, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.475, Top 3: 0.671, Top 5: 0.702, \n",
      "\t\t F1 Micro: 0.385, Total Pos: 21,053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 12:54:22,667 : INFO : Finished loading new batch\n",
      "2016-12-29 12:56:35,201 : INFO : Finished: 999\n",
      "2016-12-29 12:58:50,080 : INFO : Finished: 1999\n",
      "2016-12-29 13:01:02,908 : INFO : Finished: 2999\n",
      "2016-12-29 13:03:12,135 : INFO : Finished: 3999\n",
      "2016-12-29 13:05:23,987 : INFO : Finished: 4999\n",
      "2016-12-29 13:07:36,869 : INFO : Finished: 5999\n",
      "2016-12-29 13:09:44,133 : INFO : Finished: 6999\n",
      "2016-12-29 13:11:51,853 : INFO : Finished: 7999\n",
      "2016-12-29 13:14:02,456 : INFO : Finished: 8999\n",
      "2016-12-29 13:16:11,373 : INFO : Finished: 9999\n",
      "2016-12-29 13:16:13,088 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 13:16:15,111 : INFO : Finished loading new batch\n",
      "2016-12-29 13:18:26,984 : INFO : Finished: 10999\n",
      "2016-12-29 13:20:37,503 : INFO : Finished: 11999\n",
      "2016-12-29 13:20:37,762 : INFO : Loading new batch for index: 12412\n",
      "2016-12-29 13:20:37,766 : INFO : No more batches to load, exiting at index: 12412\n",
      "2016-12-29 13:21:32,496 : INFO : Finished: 12412\n",
      "2016-12-29 13:21:32,499 : INFO : Finished: 12412\n",
      "2016-12-29 13:21:33,287 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [1 0 0 ..., 0 0 0]]\n",
      "** Validation Metrics: Cov Err: 7.387, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.374, Top 3: 0.604, Top 5: 0.635, \n",
      "\t\t F1 Micro: 0.174, Total Pos: 3,120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 13:21:34,372 : INFO : precomputing L2-norms of word weight vectors\n",
      "2016-12-29 13:21:38,179 : INFO : capital-common-countries: 0.0% (0/156)\n",
      "2016-12-29 13:21:41,573 : INFO : capital-world: 0.7% (1/152)\n",
      "2016-12-29 13:21:42,447 : INFO : currency: 0.0% (0/40)\n",
      "2016-12-29 13:22:09,837 : INFO : city-in-state: 0.0% (0/1248)\n",
      "2016-12-29 13:22:12,227 : INFO : family: 4.5% (5/110)\n",
      "2016-12-29 13:22:24,294 : INFO : gram1-adjective-to-adverb: 1.6% (9/552)\n",
      "2016-12-29 13:22:31,873 : INFO : gram2-opposite: 0.6% (2/342)\n",
      "2016-12-29 13:23:00,888 : INFO : gram3-comparative: 15.7% (209/1332)\n",
      "2016-12-29 13:23:17,350 : INFO : gram4-superlative: 2.2% (17/756)\n",
      "2016-12-29 13:23:37,562 : INFO : gram5-present-participle: 8.4% (78/930)\n",
      "2016-12-29 13:23:50,172 : INFO : gram6-nationality-adjective: 0.0% (0/584)\n",
      "2016-12-29 13:24:17,340 : INFO : gram7-past-tense: 3.8% (48/1260)\n",
      "2016-12-29 13:24:38,740 : INFO : gram8-plural: 9.2% (91/992)\n",
      "2016-12-29 13:24:53,797 : INFO : gram9-plural-verbs: 11.1% (78/702)\n",
      "2016-12-29 13:24:53,801 : INFO : total: 5.9% (538/9156)\n",
      "2016-12-29 13:24:53,818 : INFO : ****************** Epoch 11 --- Working on doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_11 *******************\n",
      "2016-12-29 13:24:53,822 : INFO : training model with 16 workers on 146034 vocabulary and 5100 features, using sg=0 hs=0 sample=0.001 negative=10\n",
      "2016-12-29 13:24:53,824 : INFO : expecting 65535 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-12-29 13:24:53,843 : INFO : Loading new batch for index: 0\n",
      "2016-12-29 13:24:59,470 : INFO : Finished loading new batch\n",
      "2016-12-29 13:25:00,639 : INFO : PROGRESS: at 0.00% examples, 337 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 13:25:20,664 : INFO : PROGRESS: at 1.24% examples, 130448 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:25:40,675 : INFO : PROGRESS: at 2.51% examples, 149079 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 13:26:00,681 : INFO : PROGRESS: at 3.82% examples, 157412 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:26:20,719 : INFO : PROGRESS: at 5.07% examples, 161655 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:26:40,726 : INFO : PROGRESS: at 6.31% examples, 164398 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 13:27:00,752 : INFO : PROGRESS: at 7.59% examples, 166230 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:27:20,763 : INFO : PROGRESS: at 8.91% examples, 167674 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:27:40,796 : INFO : PROGRESS: at 10.23% examples, 168747 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:28:00,810 : INFO : PROGRESS: at 11.51% examples, 169723 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:28:20,878 : INFO : PROGRESS: at 12.83% examples, 170430 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:28:40,887 : INFO : PROGRESS: at 14.16% examples, 171057 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:29:00,901 : INFO : PROGRESS: at 15.45% examples, 171597 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:29:20,905 : INFO : PROGRESS: at 16.77% examples, 172111 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:29:40,915 : INFO : PROGRESS: at 18.10% examples, 172476 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:30:00,915 : INFO : PROGRESS: at 19.40% examples, 172839 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:30:11,806 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 13:30:20,218 : INFO : Finished loading new batch\n",
      "2016-12-29 13:30:20,954 : INFO : PROGRESS: at 20.04% examples, 167806 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:30:40,963 : INFO : PROGRESS: at 21.28% examples, 168460 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 13:31:00,980 : INFO : PROGRESS: at 22.57% examples, 168913 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:31:20,998 : INFO : PROGRESS: at 23.92% examples, 169350 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:31:41,002 : INFO : PROGRESS: at 25.23% examples, 169749 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:32:01,018 : INFO : PROGRESS: at 26.55% examples, 170079 words/s, in_qsize 31, out_qsize 1\n",
      "2016-12-29 13:32:21,043 : INFO : PROGRESS: at 27.82% examples, 170405 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:32:41,053 : INFO : PROGRESS: at 29.13% examples, 170703 words/s, in_qsize 31, out_qsize 1\n",
      "2016-12-29 13:33:01,102 : INFO : PROGRESS: at 30.39% examples, 171005 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:33:21,119 : INFO : PROGRESS: at 31.69% examples, 171266 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:33:41,137 : INFO : PROGRESS: at 32.99% examples, 171500 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:34:01,137 : INFO : PROGRESS: at 34.26% examples, 171740 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:34:21,163 : INFO : PROGRESS: at 35.63% examples, 171953 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 13:34:41,172 : INFO : PROGRESS: at 36.90% examples, 172135 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:35:01,217 : INFO : PROGRESS: at 38.14% examples, 172299 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:35:21,239 : INFO : PROGRESS: at 39.43% examples, 172492 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:35:32,633 : INFO : Loading new batch for index: 20000\n",
      "2016-12-29 13:35:42,320 : INFO : Finished loading new batch\n",
      "2016-12-29 13:35:42,788 : INFO : PROGRESS: at 40.13% examples, 169819 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:36:02,798 : INFO : PROGRESS: at 41.41% examples, 170048 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:36:22,829 : INFO : PROGRESS: at 42.67% examples, 170293 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:36:42,860 : INFO : PROGRESS: at 43.94% examples, 170502 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:37:02,897 : INFO : PROGRESS: at 45.21% examples, 170701 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:37:22,923 : INFO : PROGRESS: at 46.59% examples, 170951 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:37:42,924 : INFO : PROGRESS: at 47.93% examples, 171177 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:38:02,934 : INFO : PROGRESS: at 49.23% examples, 171372 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 13:38:22,937 : INFO : PROGRESS: at 50.53% examples, 171561 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:38:42,962 : INFO : PROGRESS: at 51.81% examples, 171773 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:39:02,968 : INFO : PROGRESS: at 53.13% examples, 171955 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:39:22,970 : INFO : PROGRESS: at 54.43% examples, 172119 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:39:42,974 : INFO : PROGRESS: at 55.73% examples, 172268 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:40:03,071 : INFO : PROGRESS: at 57.09% examples, 172411 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:40:23,109 : INFO : PROGRESS: at 58.41% examples, 172542 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:40:43,132 : INFO : PROGRESS: at 59.74% examples, 172665 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:40:51,810 : INFO : Loading new batch for index: 30000\n",
      "2016-12-29 13:41:00,235 : INFO : Finished loading new batch\n",
      "2016-12-29 13:41:03,147 : INFO : PROGRESS: at 60.45% examples, 171063 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:41:23,150 : INFO : PROGRESS: at 61.69% examples, 171207 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:41:43,159 : INFO : PROGRESS: at 63.02% examples, 171344 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:42:03,216 : INFO : PROGRESS: at 64.30% examples, 171464 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 13:42:23,227 : INFO : PROGRESS: at 65.58% examples, 171574 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:42:43,359 : INFO : PROGRESS: at 66.95% examples, 171684 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:43:03,380 : INFO : PROGRESS: at 68.19% examples, 171793 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:43:23,388 : INFO : PROGRESS: at 69.47% examples, 171877 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 13:43:43,399 : INFO : PROGRESS: at 70.75% examples, 171977 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:44:03,401 : INFO : PROGRESS: at 72.02% examples, 172048 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:44:23,406 : INFO : PROGRESS: at 73.36% examples, 172135 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:44:43,409 : INFO : PROGRESS: at 74.68% examples, 172210 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:45:03,423 : INFO : PROGRESS: at 76.01% examples, 172270 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:45:23,462 : INFO : PROGRESS: at 77.26% examples, 172321 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:45:43,477 : INFO : PROGRESS: at 78.55% examples, 172403 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:46:03,504 : INFO : PROGRESS: at 79.84% examples, 172463 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:46:12,136 : INFO : Loading new batch for index: 40000\n",
      "2016-12-29 13:46:20,651 : INFO : Finished loading new batch\n",
      "2016-12-29 13:46:23,553 : INFO : PROGRESS: at 80.51% examples, 171211 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:46:43,553 : INFO : PROGRESS: at 81.77% examples, 171313 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:47:03,557 : INFO : PROGRESS: at 83.09% examples, 171415 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:47:23,607 : INFO : PROGRESS: at 84.44% examples, 171496 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:47:43,609 : INFO : PROGRESS: at 85.73% examples, 171588 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:48:03,650 : INFO : PROGRESS: at 87.02% examples, 171661 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:48:23,668 : INFO : PROGRESS: at 88.28% examples, 171748 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:48:43,677 : INFO : PROGRESS: at 89.63% examples, 171814 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:49:03,717 : INFO : PROGRESS: at 90.84% examples, 171890 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:49:23,736 : INFO : PROGRESS: at 92.14% examples, 171947 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 13:49:43,744 : INFO : PROGRESS: at 93.44% examples, 172006 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:50:03,754 : INFO : PROGRESS: at 94.71% examples, 172065 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:50:23,773 : INFO : PROGRESS: at 95.94% examples, 172079 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:50:43,838 : INFO : PROGRESS: at 97.19% examples, 172123 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:51:03,866 : INFO : PROGRESS: at 98.44% examples, 172160 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:51:23,911 : INFO : PROGRESS: at 99.73% examples, 172196 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 13:51:28,370 : INFO : Loading new batch for index: 49789\n",
      "2016-12-29 13:51:28,500 : INFO : No more batches to load, exiting at index: 49789\n",
      "2016-12-29 13:51:29,257 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2016-12-29 13:51:29,269 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2016-12-29 13:51:29,352 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2016-12-29 13:51:29,379 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2016-12-29 13:51:29,383 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2016-12-29 13:51:29,391 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2016-12-29 13:51:29,394 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2016-12-29 13:51:29,415 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2016-12-29 13:51:29,429 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2016-12-29 13:51:29,452 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2016-12-29 13:51:29,455 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2016-12-29 13:51:29,457 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2016-12-29 13:51:29,471 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-12-29 13:51:29,481 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-12-29 13:51:29,510 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-12-29 13:51:29,520 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-12-29 13:51:29,521 : INFO : training on 390507860 raw words (274491576 effective words) took 1595.7s, 172022 effective words/s\n",
      "2016-12-29 13:51:29,523 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_11/model, separately None\n",
      "2016-12-29 13:51:29,525 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_11/model.docvecs.doctag_syn0.npy\n",
      "2016-12-29 13:51:29,568 : INFO : storing numpy array 'syn1neg' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_11/model.syn1neg.npy\n",
      "2016-12-29 13:51:31,492 : INFO : not storing attribute syn0norm\n",
      "2016-12-29 13:51:31,493 : INFO : storing numpy array 'syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_11/model.syn0.npy\n",
      "2016-12-29 13:51:31,676 : INFO : not storing attribute cum_table\n",
      "2016-12-29 13:51:36,030 : INFO : Training Classifier\n",
      "2016-12-29 13:52:14,200 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 13:52:22,578 : INFO : Getting Validation Embeddings\n",
      "2016-12-29 13:52:22,579 : INFO : ===== Getting validation vectors with inference\n",
      "2016-12-29 13:52:22,605 : INFO : Loading new batch for index: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Training Metrics: Cov Err: 6.414, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.476, Top 3: 0.671, Top 5: 0.702, \n",
      "\t\t F1 Micro: 0.386, Total Pos: 21,079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 13:52:35,415 : INFO : Finished loading new batch\n",
      "2016-12-29 13:54:47,713 : INFO : Finished: 999\n",
      "2016-12-29 13:57:03,143 : INFO : Finished: 1999\n",
      "2016-12-29 13:59:15,802 : INFO : Finished: 2999\n",
      "2016-12-29 14:01:24,662 : INFO : Finished: 3999\n",
      "2016-12-29 14:03:36,851 : INFO : Finished: 4999\n",
      "2016-12-29 14:05:49,325 : INFO : Finished: 5999\n",
      "2016-12-29 14:07:56,169 : INFO : Finished: 6999\n",
      "2016-12-29 14:10:02,724 : INFO : Finished: 7999\n",
      "2016-12-29 14:12:14,655 : INFO : Finished: 8999\n",
      "2016-12-29 14:14:24,561 : INFO : Finished: 9999\n",
      "2016-12-29 14:14:26,263 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 14:14:27,657 : INFO : Finished loading new batch\n",
      "2016-12-29 14:16:41,531 : INFO : Finished: 10999\n",
      "2016-12-29 14:18:56,514 : INFO : Finished: 11999\n",
      "2016-12-29 14:18:56,798 : INFO : Loading new batch for index: 12412\n",
      "2016-12-29 14:18:56,803 : INFO : No more batches to load, exiting at index: 12412\n",
      "2016-12-29 14:19:54,293 : INFO : Finished: 12412\n",
      "2016-12-29 14:19:54,295 : INFO : Finished: 12412\n",
      "2016-12-29 14:19:55,092 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 1 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "** Validation Metrics: Cov Err: 7.417, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.373, Top 3: 0.604, Top 5: 0.634, \n",
      "\t\t F1 Micro: 0.168, Total Pos: 3,038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 14:19:56,057 : INFO : precomputing L2-norms of word weight vectors\n",
      "2016-12-29 14:20:00,193 : INFO : capital-common-countries: 0.0% (0/156)\n",
      "2016-12-29 14:20:03,601 : INFO : capital-world: 0.7% (1/152)\n",
      "2016-12-29 14:20:04,501 : INFO : currency: 0.0% (0/40)\n",
      "2016-12-29 14:20:31,671 : INFO : city-in-state: 0.0% (0/1248)\n",
      "2016-12-29 14:20:34,151 : INFO : family: 2.7% (3/110)\n",
      "2016-12-29 14:20:46,315 : INFO : gram1-adjective-to-adverb: 2.0% (11/552)\n",
      "2016-12-29 14:20:53,702 : INFO : gram2-opposite: 1.2% (4/342)\n",
      "2016-12-29 14:21:22,122 : INFO : gram3-comparative: 15.9% (212/1332)\n",
      "2016-12-29 14:21:37,638 : INFO : gram4-superlative: 2.6% (20/756)\n",
      "2016-12-29 14:21:56,854 : INFO : gram5-present-participle: 7.8% (73/930)\n",
      "2016-12-29 14:22:09,033 : INFO : gram6-nationality-adjective: 0.0% (0/584)\n",
      "2016-12-29 14:22:35,073 : INFO : gram7-past-tense: 3.5% (44/1260)\n",
      "2016-12-29 14:22:55,664 : INFO : gram8-plural: 9.4% (93/992)\n",
      "2016-12-29 14:23:10,136 : INFO : gram9-plural-verbs: 10.4% (73/702)\n",
      "2016-12-29 14:23:10,138 : INFO : total: 5.8% (534/9156)\n",
      "2016-12-29 14:23:10,152 : INFO : ****************** Epoch 12 --- Working on doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_12 *******************\n",
      "2016-12-29 14:23:10,156 : INFO : training model with 16 workers on 146034 vocabulary and 5100 features, using sg=0 hs=0 sample=0.001 negative=10\n",
      "2016-12-29 14:23:10,157 : INFO : expecting 65535 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-12-29 14:23:10,200 : INFO : Loading new batch for index: 0\n",
      "2016-12-29 14:23:17,741 : INFO : Finished loading new batch\n",
      "2016-12-29 14:23:18,280 : INFO : PROGRESS: at 0.00% examples, 285 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 14:23:38,328 : INFO : PROGRESS: at 1.18% examples, 119273 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:23:58,334 : INFO : PROGRESS: at 2.42% examples, 140103 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:24:18,344 : INFO : PROGRESS: at 3.69% examples, 148924 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:24:38,394 : INFO : PROGRESS: at 4.89% examples, 153697 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:24:58,401 : INFO : PROGRESS: at 6.10% examples, 156687 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 14:25:18,416 : INFO : PROGRESS: at 7.33% examples, 158808 words/s, in_qsize 31, out_qsize 2\n",
      "2016-12-29 14:25:38,505 : INFO : PROGRESS: at 8.58% examples, 160325 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:25:58,527 : INFO : PROGRESS: at 9.88% examples, 161448 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:26:18,654 : INFO : PROGRESS: at 11.09% examples, 162265 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:26:38,665 : INFO : PROGRESS: at 12.36% examples, 163038 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:26:58,686 : INFO : PROGRESS: at 13.64% examples, 163727 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:27:18,694 : INFO : PROGRESS: at 14.88% examples, 164302 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:27:38,724 : INFO : PROGRESS: at 16.14% examples, 164816 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:27:58,754 : INFO : PROGRESS: at 17.40% examples, 165227 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:28:18,756 : INFO : PROGRESS: at 18.69% examples, 165579 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:28:38,791 : INFO : PROGRESS: at 19.91% examples, 165891 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 14:28:41,571 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 14:28:48,745 : INFO : Finished loading new batch\n",
      "2016-12-29 14:28:58,812 : INFO : PROGRESS: at 20.57% examples, 161990 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:29:18,833 : INFO : PROGRESS: at 21.77% examples, 162459 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:29:38,836 : INFO : PROGRESS: at 23.07% examples, 162885 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 14:29:58,927 : INFO : PROGRESS: at 24.37% examples, 163309 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:30:18,938 : INFO : PROGRESS: at 25.59% examples, 163569 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:30:38,989 : INFO : PROGRESS: at 26.88% examples, 163881 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:30:59,034 : INFO : PROGRESS: at 28.11% examples, 164198 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:31:19,041 : INFO : PROGRESS: at 29.37% examples, 164448 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:31:39,075 : INFO : PROGRESS: at 30.57% examples, 164734 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:31:59,152 : INFO : PROGRESS: at 31.82% examples, 164967 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 14:32:19,220 : INFO : PROGRESS: at 33.07% examples, 165137 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:32:39,240 : INFO : PROGRESS: at 34.29% examples, 165353 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:32:59,306 : INFO : PROGRESS: at 35.61% examples, 165491 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:33:19,306 : INFO : PROGRESS: at 36.83% examples, 165670 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:33:39,326 : INFO : PROGRESS: at 38.04% examples, 165817 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:33:59,355 : INFO : PROGRESS: at 39.27% examples, 166008 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 14:34:14,352 : INFO : Loading new batch for index: 20000\n",
      "2016-12-29 14:34:22,604 : INFO : Finished loading new batch\n",
      "2016-12-29 14:34:23,282 : INFO : PROGRESS: at 40.13% examples, 163732 words/s, in_qsize 22, out_qsize 0\n",
      "2016-12-29 14:34:43,324 : INFO : PROGRESS: at 41.36% examples, 163895 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:35:03,332 : INFO : PROGRESS: at 42.56% examples, 164114 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:35:23,397 : INFO : PROGRESS: at 43.80% examples, 164299 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:35:43,421 : INFO : PROGRESS: at 44.99% examples, 164478 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:36:03,469 : INFO : PROGRESS: at 46.29% examples, 164636 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:36:23,493 : INFO : PROGRESS: at 47.60% examples, 164793 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:36:43,507 : INFO : PROGRESS: at 48.84% examples, 164907 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 14:37:03,571 : INFO : PROGRESS: at 50.07% examples, 165029 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:37:23,593 : INFO : PROGRESS: at 51.28% examples, 165157 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:37:43,686 : INFO : PROGRESS: at 52.52% examples, 165269 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:38:03,696 : INFO : PROGRESS: at 53.78% examples, 165399 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:38:23,721 : INFO : PROGRESS: at 54.97% examples, 165469 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:38:43,744 : INFO : PROGRESS: at 56.29% examples, 165624 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:39:03,755 : INFO : PROGRESS: at 57.54% examples, 165693 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:39:23,771 : INFO : PROGRESS: at 58.78% examples, 165773 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:39:43,772 : INFO : PROGRESS: at 60.08% examples, 165856 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 14:39:47,738 : INFO : Loading new batch for index: 30000\n",
      "2016-12-29 14:39:55,695 : INFO : Finished loading new batch\n",
      "2016-12-29 14:40:03,796 : INFO : PROGRESS: at 60.72% examples, 164382 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:40:23,822 : INFO : PROGRESS: at 61.92% examples, 164495 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:40:43,903 : INFO : PROGRESS: at 63.20% examples, 164595 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 14:41:03,918 : INFO : PROGRESS: at 64.44% examples, 164734 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 14:41:23,918 : INFO : PROGRESS: at 65.67% examples, 164843 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:41:43,949 : INFO : PROGRESS: at 66.98% examples, 164936 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:42:03,962 : INFO : PROGRESS: at 68.15% examples, 165000 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:42:24,006 : INFO : PROGRESS: at 69.38% examples, 165061 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:42:44,024 : INFO : PROGRESS: at 70.62% examples, 165150 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 14:43:04,138 : INFO : PROGRESS: at 71.82% examples, 165211 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:43:24,168 : INFO : PROGRESS: at 73.12% examples, 165283 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 14:43:44,213 : INFO : PROGRESS: at 74.37% examples, 165336 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:44:04,271 : INFO : PROGRESS: at 75.64% examples, 165402 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:44:24,326 : INFO : PROGRESS: at 76.88% examples, 165453 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:44:44,342 : INFO : PROGRESS: at 78.09% examples, 165517 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:45:04,380 : INFO : PROGRESS: at 79.34% examples, 165586 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:45:21,440 : INFO : Loading new batch for index: 40000\n",
      "2016-12-29 14:45:30,039 : INFO : Finished loading new batch\n",
      "2016-12-29 14:45:31,206 : INFO : PROGRESS: at 80.36% examples, 164368 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:45:51,267 : INFO : PROGRESS: at 81.56% examples, 164452 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:46:11,329 : INFO : PROGRESS: at 82.84% examples, 164544 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 14:46:31,333 : INFO : PROGRESS: at 84.09% examples, 164606 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:46:51,341 : INFO : PROGRESS: at 85.37% examples, 164703 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:47:11,347 : INFO : PROGRESS: at 86.64% examples, 164785 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:47:31,392 : INFO : PROGRESS: at 87.83% examples, 164848 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 14:47:51,420 : INFO : PROGRESS: at 89.10% examples, 164915 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:48:11,472 : INFO : PROGRESS: at 90.32% examples, 165009 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 14:48:31,505 : INFO : PROGRESS: at 91.57% examples, 165069 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:48:51,539 : INFO : PROGRESS: at 92.84% examples, 165148 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:49:11,542 : INFO : PROGRESS: at 94.07% examples, 165238 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:49:31,542 : INFO : PROGRESS: at 95.25% examples, 165312 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 14:49:51,583 : INFO : PROGRESS: at 96.49% examples, 165395 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 14:50:11,584 : INFO : PROGRESS: at 97.73% examples, 165460 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 14:50:31,631 : INFO : PROGRESS: at 98.95% examples, 165501 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 14:50:48,671 : INFO : Loading new batch for index: 49789\n",
      "2016-12-29 14:50:48,696 : INFO : No more batches to load, exiting at index: 49789\n",
      "2016-12-29 14:50:49,506 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2016-12-29 14:50:49,514 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2016-12-29 14:50:49,621 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2016-12-29 14:50:49,655 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2016-12-29 14:50:49,679 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2016-12-29 14:50:49,685 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2016-12-29 14:50:49,749 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2016-12-29 14:50:49,761 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2016-12-29 14:50:49,768 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2016-12-29 14:50:49,772 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2016-12-29 14:50:49,776 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2016-12-29 14:50:49,787 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2016-12-29 14:50:49,800 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-12-29 14:50:49,803 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-12-29 14:50:49,812 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-12-29 14:50:49,819 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-12-29 14:50:49,820 : INFO : training on 390507860 raw words (274489637 effective words) took 1659.6s, 165393 effective words/s\n",
      "2016-12-29 14:50:49,825 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_12/model, separately None\n",
      "2016-12-29 14:50:49,825 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_12/model.docvecs.doctag_syn0.npy\n",
      "2016-12-29 14:50:49,867 : INFO : storing numpy array 'syn1neg' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_12/model.syn1neg.npy\n",
      "2016-12-29 14:50:51,645 : INFO : not storing attribute syn0norm\n",
      "2016-12-29 14:50:51,646 : INFO : storing numpy array 'syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_12/model.syn0.npy\n",
      "2016-12-29 14:50:51,766 : INFO : not storing attribute cum_table\n",
      "2016-12-29 14:50:55,446 : INFO : Training Classifier\n",
      "2016-12-29 14:51:32,727 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 14:51:41,372 : INFO : Getting Validation Embeddings\n",
      "2016-12-29 14:51:41,374 : INFO : ===== Getting validation vectors with inference\n",
      "2016-12-29 14:51:41,383 : INFO : Loading new batch for index: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Training Metrics: Cov Err: 6.398, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.478, Top 3: 0.671, Top 5: 0.702, \n",
      "\t\t F1 Micro: 0.388, Total Pos: 21,220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 14:51:48,894 : INFO : Finished loading new batch\n",
      "2016-12-29 14:54:06,729 : INFO : Finished: 999\n",
      "2016-12-29 14:56:28,896 : INFO : Finished: 1999\n",
      "2016-12-29 14:58:46,982 : INFO : Finished: 2999\n",
      "2016-12-29 15:01:01,233 : INFO : Finished: 3999\n",
      "2016-12-29 15:03:20,270 : INFO : Finished: 4999\n",
      "2016-12-29 15:05:39,777 : INFO : Finished: 5999\n",
      "2016-12-29 15:07:53,731 : INFO : Finished: 6999\n",
      "2016-12-29 15:10:08,111 : INFO : Finished: 7999\n",
      "2016-12-29 15:12:26,136 : INFO : Finished: 8999\n",
      "2016-12-29 15:14:42,538 : INFO : Finished: 9999\n",
      "2016-12-29 15:14:44,261 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 15:14:45,609 : INFO : Finished loading new batch\n",
      "2016-12-29 15:17:03,521 : INFO : Finished: 10999\n",
      "2016-12-29 15:19:19,378 : INFO : Finished: 11999\n",
      "2016-12-29 15:19:19,723 : INFO : Loading new batch for index: 12412\n",
      "2016-12-29 15:19:19,727 : INFO : No more batches to load, exiting at index: 12412\n",
      "2016-12-29 15:20:16,823 : INFO : Finished: 12412\n",
      "2016-12-29 15:20:16,827 : INFO : Finished: 12412\n",
      "2016-12-29 15:20:17,777 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "** Validation Metrics: Cov Err: 7.438, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.371, Top 3: 0.604, Top 5: 0.635, \n",
      "\t\t F1 Micro: 0.162, Total Pos: 2,903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 15:20:18,734 : INFO : precomputing L2-norms of word weight vectors\n",
      "2016-12-29 15:20:22,571 : INFO : capital-common-countries: 0.0% (0/156)\n",
      "2016-12-29 15:20:26,025 : INFO : capital-world: 0.0% (0/152)\n",
      "2016-12-29 15:20:26,933 : INFO : currency: 0.0% (0/40)\n",
      "2016-12-29 15:20:54,546 : INFO : city-in-state: 0.0% (0/1248)\n",
      "2016-12-29 15:20:56,947 : INFO : family: 5.5% (6/110)\n",
      "2016-12-29 15:21:09,168 : INFO : gram1-adjective-to-adverb: 3.1% (17/552)\n",
      "2016-12-29 15:21:16,745 : INFO : gram2-opposite: 0.9% (3/342)\n",
      "2016-12-29 15:21:46,102 : INFO : gram3-comparative: 15.2% (202/1332)\n",
      "2016-12-29 15:22:02,957 : INFO : gram4-superlative: 2.2% (17/756)\n",
      "2016-12-29 15:22:23,563 : INFO : gram5-present-participle: 7.6% (71/930)\n",
      "2016-12-29 15:22:36,385 : INFO : gram6-nationality-adjective: 0.0% (0/584)\n",
      "2016-12-29 15:23:04,073 : INFO : gram7-past-tense: 3.3% (41/1260)\n",
      "2016-12-29 15:23:25,835 : INFO : gram8-plural: 10.2% (101/992)\n",
      "2016-12-29 15:23:41,096 : INFO : gram9-plural-verbs: 10.8% (76/702)\n",
      "2016-12-29 15:23:41,100 : INFO : total: 5.8% (534/9156)\n",
      "2016-12-29 15:23:41,127 : INFO : ****************** Epoch 13 --- Working on doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_13 *******************\n",
      "2016-12-29 15:23:41,131 : INFO : training model with 16 workers on 146034 vocabulary and 5100 features, using sg=0 hs=0 sample=0.001 negative=10\n",
      "2016-12-29 15:23:41,133 : INFO : expecting 65535 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-12-29 15:23:41,163 : INFO : Loading new batch for index: 0\n",
      "2016-12-29 15:23:46,991 : INFO : Finished loading new batch\n",
      "2016-12-29 15:23:47,857 : INFO : PROGRESS: at 0.00% examples, 343 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:24:07,859 : INFO : PROGRESS: at 1.17% examples, 125177 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:24:27,956 : INFO : PROGRESS: at 2.40% examples, 143320 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:24:47,983 : INFO : PROGRESS: at 3.67% examples, 150896 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:25:07,998 : INFO : PROGRESS: at 4.85% examples, 155023 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:25:28,002 : INFO : PROGRESS: at 6.05% examples, 157354 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:25:48,056 : INFO : PROGRESS: at 7.28% examples, 159223 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:26:08,072 : INFO : PROGRESS: at 8.51% examples, 160485 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 15:26:28,094 : INFO : PROGRESS: at 9.81% examples, 161534 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:26:48,105 : INFO : PROGRESS: at 11.02% examples, 162330 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:27:08,155 : INFO : PROGRESS: at 12.27% examples, 162970 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:27:28,177 : INFO : PROGRESS: at 13.54% examples, 163535 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:27:48,214 : INFO : PROGRESS: at 14.78% examples, 163974 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:28:08,296 : INFO : PROGRESS: at 16.01% examples, 164402 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:28:28,300 : INFO : PROGRESS: at 17.27% examples, 164660 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:28:48,307 : INFO : PROGRESS: at 18.54% examples, 164972 words/s, in_qsize 32, out_qsize 1\n",
      "2016-12-29 15:29:08,377 : INFO : PROGRESS: at 19.76% examples, 165192 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:29:13,677 : INFO : Loading new batch for index: 10000\n",
      "2016-12-29 15:29:21,622 : INFO : Finished loading new batch\n",
      "2016-12-29 15:29:28,387 : INFO : PROGRESS: at 20.36% examples, 160818 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:29:48,389 : INFO : PROGRESS: at 21.52% examples, 161227 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:30:08,433 : INFO : PROGRESS: at 22.79% examples, 161592 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:30:28,447 : INFO : PROGRESS: at 24.06% examples, 161842 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 15:30:48,459 : INFO : PROGRESS: at 25.30% examples, 162237 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:31:08,489 : INFO : PROGRESS: at 26.58% examples, 162610 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:31:28,508 : INFO : PROGRESS: at 27.81% examples, 162975 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:31:48,519 : INFO : PROGRESS: at 29.07% examples, 163256 words/s, in_qsize 32, out_qsize 1\n",
      "2016-12-29 15:32:08,539 : INFO : PROGRESS: at 30.25% examples, 163590 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:32:28,569 : INFO : PROGRESS: at 31.54% examples, 163911 words/s, in_qsize 28, out_qsize 2\n",
      "2016-12-29 15:32:48,646 : INFO : PROGRESS: at 32.78% examples, 164132 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 15:33:08,651 : INFO : PROGRESS: at 34.00% examples, 164329 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:33:28,669 : INFO : PROGRESS: at 35.28% examples, 164554 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:33:48,695 : INFO : PROGRESS: at 36.54% examples, 164710 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:34:08,710 : INFO : PROGRESS: at 37.74% examples, 164882 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:34:28,721 : INFO : PROGRESS: at 38.96% examples, 165092 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:34:48,412 : INFO : Loading new batch for index: 20000\n",
      "2016-12-29 15:34:48,745 : INFO : PROGRESS: at 40.06% examples, 164782 words/s, in_qsize 17, out_qsize 0\n",
      "2016-12-29 15:34:56,047 : INFO : Finished loading new batch\n",
      "2016-12-29 15:35:08,784 : INFO : PROGRESS: at 40.58% examples, 162054 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:35:28,874 : INFO : PROGRESS: at 41.78% examples, 162321 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:35:48,880 : INFO : PROGRESS: at 43.05% examples, 162573 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:36:08,983 : INFO : PROGRESS: at 44.23% examples, 162777 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:36:29,051 : INFO : PROGRESS: at 45.46% examples, 163004 words/s, in_qsize 31, out_qsize 1\n",
      "2016-12-29 15:36:49,045 : INFO : PROGRESS: at 46.77% examples, 163199 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 15:37:09,057 : INFO : PROGRESS: at 48.06% examples, 163400 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:37:29,113 : INFO : PROGRESS: at 49.31% examples, 163568 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:37:49,117 : INFO : PROGRESS: at 50.53% examples, 163724 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:38:09,171 : INFO : PROGRESS: at 51.74% examples, 163863 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:38:29,180 : INFO : PROGRESS: at 53.00% examples, 163991 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 15:38:49,224 : INFO : PROGRESS: at 54.24% examples, 164117 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:39:09,256 : INFO : PROGRESS: at 55.45% examples, 164254 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:39:29,291 : INFO : PROGRESS: at 56.77% examples, 164374 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:39:49,356 : INFO : PROGRESS: at 57.98% examples, 164475 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:40:09,392 : INFO : PROGRESS: at 59.24% examples, 164567 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:40:26,304 : INFO : Loading new batch for index: 30000\n",
      "2016-12-29 15:40:33,720 : INFO : Finished loading new batch\n",
      "2016-12-29 15:40:34,680 : INFO : PROGRESS: at 60.28% examples, 163177 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:40:54,682 : INFO : PROGRESS: at 61.46% examples, 163295 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 15:41:14,724 : INFO : PROGRESS: at 62.74% examples, 163429 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 15:41:34,744 : INFO : PROGRESS: at 63.97% examples, 163551 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 15:41:54,752 : INFO : PROGRESS: at 65.20% examples, 163680 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:42:14,830 : INFO : PROGRESS: at 66.50% examples, 163790 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:42:34,856 : INFO : PROGRESS: at 67.69% examples, 163878 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:42:54,882 : INFO : PROGRESS: at 68.92% examples, 163971 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:43:14,916 : INFO : PROGRESS: at 70.18% examples, 164057 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:43:34,920 : INFO : PROGRESS: at 71.36% examples, 164136 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:43:54,919 : INFO : PROGRESS: at 72.58% examples, 164209 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:44:14,922 : INFO : PROGRESS: at 73.89% examples, 164299 words/s, in_qsize 32, out_qsize 1\n",
      "2016-12-29 15:44:34,944 : INFO : PROGRESS: at 75.18% examples, 164372 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:44:54,982 : INFO : PROGRESS: at 76.40% examples, 164456 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:45:15,116 : INFO : PROGRESS: at 77.62% examples, 164501 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:45:35,196 : INFO : PROGRESS: at 78.85% examples, 164572 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:45:55,231 : INFO : PROGRESS: at 80.08% examples, 164637 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:46:00,317 : INFO : Loading new batch for index: 40000\n",
      "2016-12-29 15:46:07,358 : INFO : Finished loading new batch\n",
      "2016-12-29 15:46:15,238 : INFO : PROGRESS: at 80.80% examples, 163650 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 15:46:35,267 : INFO : PROGRESS: at 82.00% examples, 163738 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:46:55,274 : INFO : PROGRESS: at 83.29% examples, 163831 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:47:15,280 : INFO : PROGRESS: at 84.56% examples, 163942 words/s, in_qsize 31, out_qsize 1\n",
      "2016-12-29 15:47:35,297 : INFO : PROGRESS: at 85.82% examples, 164032 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:47:55,302 : INFO : PROGRESS: at 87.05% examples, 164111 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:48:15,320 : INFO : PROGRESS: at 88.27% examples, 164218 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 15:48:35,332 : INFO : PROGRESS: at 89.57% examples, 164305 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 15:48:55,398 : INFO : PROGRESS: at 90.74% examples, 164391 words/s, in_qsize 32, out_qsize 1\n",
      "2016-12-29 15:49:15,415 : INFO : PROGRESS: at 92.00% examples, 164469 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:49:35,452 : INFO : PROGRESS: at 93.26% examples, 164535 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:49:55,482 : INFO : PROGRESS: at 94.46% examples, 164597 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:50:15,500 : INFO : PROGRESS: at 95.66% examples, 164646 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 15:50:35,516 : INFO : PROGRESS: at 96.86% examples, 164688 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 15:50:55,510 : INFO : PROGRESS: at 98.07% examples, 164735 words/s, in_qsize 32, out_qsize 0\n",
      "2016-12-29 15:51:15,549 : INFO : PROGRESS: at 99.28% examples, 164777 words/s, in_qsize 31, out_qsize 0\n",
      "2016-12-29 15:51:27,089 : INFO : Loading new batch for index: 49789\n",
      "2016-12-29 15:51:27,148 : INFO : No more batches to load, exiting at index: 49789\n",
      "2016-12-29 15:51:27,926 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2016-12-29 15:51:27,950 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2016-12-29 15:51:28,025 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2016-12-29 15:51:28,076 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2016-12-29 15:51:28,113 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2016-12-29 15:51:28,117 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2016-12-29 15:51:28,143 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2016-12-29 15:51:28,147 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2016-12-29 15:51:28,154 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2016-12-29 15:51:28,179 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2016-12-29 15:51:28,182 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2016-12-29 15:51:28,185 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2016-12-29 15:51:28,202 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-12-29 15:51:28,230 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-12-29 15:51:28,244 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-12-29 15:51:28,251 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-12-29 15:51:28,252 : INFO : training on 390507860 raw words (274481592 effective words) took 1667.1s, 164647 effective words/s\n",
      "2016-12-29 15:51:28,254 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_13/model, separately None\n",
      "2016-12-29 15:51:28,255 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_13/model.docvecs.doctag_syn0.npy\n",
      "2016-12-29 15:51:28,297 : INFO : storing numpy array 'syn1neg' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_13/model.syn1neg.npy\n",
      "2016-12-29 15:51:30,011 : INFO : not storing attribute syn0norm\n",
      "2016-12-29 15:51:30,013 : INFO : storing numpy array 'syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new/sample_0.01/doc2vec_size_300_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_13/model.syn0.npy\n",
      "2016-12-29 15:51:30,157 : INFO : not storing attribute cum_table\n",
      "2016-12-29 15:51:34,487 : INFO : Training Classifier\n",
      "2016-12-29 15:52:13,026 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 15:52:21,322 : INFO : Getting Validation Embeddings\n",
      "2016-12-29 15:52:21,323 : INFO : ===== Getting validation vectors with inference\n",
      "2016-12-29 15:52:21,332 : INFO : Loading new batch for index: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Training Metrics: Cov Err: 6.388, Avg Labels: 1.000, \n",
      "\t\t Top 1: 0.479, Top 3: 0.673, Top 5: 0.704, \n",
      "\t\t F1 Micro: 0.390, Total Pos: 21,501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 15:52:29,150 : INFO : Finished loading new batch\n",
      "2016-12-29 15:54:47,534 : INFO : Finished: 999\n",
      "2016-12-29 15:57:09,901 : INFO : Finished: 1999\n",
      "2016-12-29 15:59:29,448 : INFO : Finished: 2999\n",
      "2016-12-29 16:01:45,265 : INFO : Finished: 3999\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-192-fac19e5b9532>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'%matplotlib notebook\\ngraph = MetricsGraph()\\ngraph.init_graph()\\n# when resuming, resume from an epoch with a previously created doc2vec model to get the learning rate right\\nstart_from = 1\\nfor epoch in range(start_from, DOC2VEC_MAX_EPOCHS+1):\\n    GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\\n    info(\"****************** Epoch {} --- Working on {} *******************\".format(epoch, GLOBAL_VARS.MODEL_NAME))\\n    \\n    # if we have the model, just load it, otherwise train the previous model\\n    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)):\\n        doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\\n        GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\\n    else:\\n        # train the doc2vec model\\n        training_docs_iterator = DocumentBatchGenerator(training_preprocessed_files_prefix, \\n                                                        training_preprocessed_docids_files_prefix, batch_size=10000)\\n        doc2vec_model.train(sentences=training_docs_iterator, report_delay=REPORT_DELAY)\\n        doc2vec_model.alpha -= 0.001  # decrease the learning rate\\n        doc2vec_model.min_alpha = doc2vec_model.alpha  # fix the learning rate, no decay\\n        ensure_disk_location_exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME))\\n        doc2vec_model.save(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\\n        GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\\n        \\n    X, y = get_training_data(doc2vec_model, classifications)\\n    \\n    # try class weights\\n    # try warm start and evaluate after every iter\\n    \\n    info(\\'Training Classifier\\')\\n    clf = OneVsRestClassifier(linear_model.SGDClassifier(loss=\\'hinge\\', penalty=\\'l2\\', \\n                                                         #alpha is the 1/C parameter\\n                                                         alpha=SVM_REG, fit_intercept=True, n_iter=SVM_ITERATIONS,\\n                                                         #n_jobs=-1 means use all cpus\\n                                                         shuffle=True, verbose=0, n_jobs=-1,\\n                                                         #eta0 is the learning rate when we use constant configuration\\n                                                         random_state=SVM_SEED, learning_rate=\\'optimal\\', eta0=0.0, \\n                                                         class_weight=None, warm_start=False), n_jobs=1)\\n    \\n    # Training Metrics\\n    clf.fit(X,y)\\n    info(\\'Evaluating on Training Data\\')\\n    yp = clf.predict(X)\\n    print yp\\n    training_metrics = get_metrics(y, yp, yp)\\n    print \"** Training Metrics: Cov Err: {:.3f}, Avg Labels: {:.3f}, \\\\n\\\\t\\\\t Top 1: {:.3f}, Top 3: {:.3f}, Top 5: {:.3f}, \\\\n\\\\t\\\\t F1 Micro: {:.3f}, Total Pos: {:,d}\".format(\\n        training_metrics[\\'coverage_error\\'], training_metrics[\\'average_num_of_labels\\'], \\n        training_metrics[\\'top_1\\'], training_metrics[\\'top_3\\'], training_metrics[\\'top_5\\'], \\n        training_metrics[\\'f1_micro\\'], training_metrics[\\'total_positive\\'])\\n    \\n    # Validation Metrics\\n    info(\\'Getting Validation Embeddings\\')\\n    Xv, yv = get_validation_docs_with_inference_new(doc2vec_model, doc_classification_map, classifications, \\n                                                    validation_docs_list, validation_preprocessed_files_prefix,\\n                                                    validation_preprocessed_docids_files_prefix)\\n    info(\\'Evaluating on Validation Data\\')\\n    yvp = clf.predict(Xv)\\n    print yvp\\n    validation_metrics = get_metrics(yv, yvp, yvp)\\n    print \"** Validation Metrics: Cov Err: {:.3f}, Avg Labels: {:.3f}, \\\\n\\\\t\\\\t Top 1: {:.3f}, Top 3: {:.3f}, Top 5: {:.3f}, \\\\n\\\\t\\\\t F1 Micro: {:.3f}, Total Pos: {:,d}\".format(\\n        validation_metrics[\\'coverage_error\\'], validation_metrics[\\'average_num_of_labels\\'], \\n        validation_metrics[\\'top_1\\'], validation_metrics[\\'top_3\\'], validation_metrics[\\'top_5\\'], \\n        validation_metrics[\\'f1_micro\\'], validation_metrics[\\'total_positive\\'])\\n    \\n    graph.add_metrics_to_graph(validation_metrics, epoch)\\n    \\n    epoch_metrics.append(validation_metrics)\\n    \\n    word2vec_result = doc2vec_model.accuracy(word2vec_questions_file, restrict_vocab=None)\\n    \\n    word2vec_results.append(word2vec_result)\\n    \\n    \\n    \\n\\n#     # Training and validation of SVMs using those docvecs\\n#     train_classifications(sections)\\n#     validation_vectors_matrix = get_validation_docs_with_inference(doc2vec_model, doc_classification_map)\\n#     metrics = do_validation(validation_vectors_matrix, doc_classification_map, sections, \"sections\")\\n#     ensure_disk_location_exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \\n#                                              GLOBAL_VARS.SVM_MODEL_NAME))\\n#     pickle.dump(metrics, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, GLOBAL_VARS.SVM_MODEL_NAME, METRICS), \\'w\\'))\\n#     print \"Coverage Error: {}, Average No of Labels: {}, Top 1: {}, Top 3: {}, Top 5: {}, F1 Micro: {}, Total Positive: {}\".format(\\n#         metrics[\\'coverage_error\\'], metrics[\\'average_num_of_labels\\'], metrics[\\'top_1\\'], metrics[\\'top_3\\'], metrics[\\'top_5\\'], \\n#         metrics[\\'f1_micro\\'], metrics[\\'total_positive\\'])\\n                                                                                     \\n#     epoch_metrics.append(metrics)\\n#     graph.add_metrics_to_graph(metrics, epoch)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-161-7cba380f9175>\u001b[0m in \u001b[0;36mget_validation_docs_with_inference_new\u001b[1;34m(doc2vec_model, doc_classification_map, classifications, val_docs_list, val_preprocessed_files_prefix, val_preprocessed_docids_files_prefix)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mmini_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0mthreaded_reps_partial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfer_one_doc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m             \u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Finished: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_docs_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurr_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mthreaded_reps_partial\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    249\u001b[0m         '''\n\u001b[0;32m    250\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mRUN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mimap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s.wait(): got it\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%matplotlib notebook\n",
    "graph = MetricsGraph()\n",
    "graph.init_graph()\n",
    "# when resuming, resume from an epoch with a previously created doc2vec model to get the learning rate right\n",
    "start_from = 1\n",
    "for epoch in range(start_from, DOC2VEC_MAX_EPOCHS+1):\n",
    "    GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "    info(\"****************** Epoch {} --- Working on {} *******************\".format(epoch, GLOBAL_VARS.MODEL_NAME))\n",
    "    \n",
    "    # if we have the model, just load it, otherwise train the previous model\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)):\n",
    "        doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "        GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "    else:\n",
    "        # train the doc2vec model\n",
    "        training_docs_iterator = DocumentBatchGenerator(training_preprocessed_files_prefix, \n",
    "                                                        training_preprocessed_docids_files_prefix, batch_size=10000)\n",
    "        doc2vec_model.train(sentences=training_docs_iterator, report_delay=REPORT_DELAY)\n",
    "        doc2vec_model.alpha -= 0.001  # decrease the learning rate\n",
    "        doc2vec_model.min_alpha = doc2vec_model.alpha  # fix the learning rate, no decay\n",
    "        ensure_disk_location_exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME))\n",
    "        doc2vec_model.save(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "        GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "        \n",
    "    X, y = get_training_data(doc2vec_model, classifications)\n",
    "    \n",
    "    # try class weights\n",
    "    # try warm start and evaluate after every iter\n",
    "    \n",
    "    info('Training Classifier')\n",
    "    clf = OneVsRestClassifier(linear_model.SGDClassifier(loss='hinge', penalty='l2', \n",
    "                                                         #alpha is the 1/C parameter\n",
    "                                                         alpha=SVM_REG, fit_intercept=True, n_iter=SVM_ITERATIONS,\n",
    "                                                         #n_jobs=-1 means use all cpus\n",
    "                                                         shuffle=True, verbose=0, n_jobs=-1,\n",
    "                                                         #eta0 is the learning rate when we use constant configuration\n",
    "                                                         random_state=SVM_SEED, learning_rate='optimal', eta0=0.0, \n",
    "                                                         class_weight=None, warm_start=False), n_jobs=1)\n",
    "    \n",
    "    # Training Metrics\n",
    "    clf.fit(X,y)\n",
    "    info('Evaluating on Training Data')\n",
    "    yp = clf.predict(X)\n",
    "    print yp\n",
    "    training_metrics = get_metrics(y, yp, yp)\n",
    "    print \"** Training Metrics: Cov Err: {:.3f}, Avg Labels: {:.3f}, \\n\\t\\t Top 1: {:.3f}, Top 3: {:.3f}, Top 5: {:.3f}, \\n\\t\\t F1 Micro: {:.3f}, Total Pos: {:,d}\".format(\n",
    "        training_metrics['coverage_error'], training_metrics['average_num_of_labels'], \n",
    "        training_metrics['top_1'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "        training_metrics['f1_micro'], training_metrics['total_positive'])\n",
    "    \n",
    "    # Validation Metrics\n",
    "    info('Getting Validation Embeddings')\n",
    "    Xv, yv = get_validation_docs_with_inference_new(doc2vec_model, doc_classification_map, classifications, \n",
    "                                                    validation_docs_list, validation_preprocessed_files_prefix,\n",
    "                                                    validation_preprocessed_docids_files_prefix)\n",
    "    info('Evaluating on Validation Data')\n",
    "    yvp = clf.predict(Xv)\n",
    "    print yvp\n",
    "    validation_metrics = get_metrics(yv, yvp, yvp)\n",
    "    print \"** Validation Metrics: Cov Err: {:.3f}, Avg Labels: {:.3f}, \\n\\t\\t Top 1: {:.3f}, Top 3: {:.3f}, Top 5: {:.3f}, \\n\\t\\t F1 Micro: {:.3f}, Total Pos: {:,d}\".format(\n",
    "        validation_metrics['coverage_error'], validation_metrics['average_num_of_labels'], \n",
    "        validation_metrics['top_1'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "        validation_metrics['f1_micro'], validation_metrics['total_positive'])\n",
    "    \n",
    "    graph.add_metrics_to_graph(validation_metrics, epoch)\n",
    "    \n",
    "    epoch_metrics.append(validation_metrics)\n",
    "    \n",
    "    word2vec_result = doc2vec_model.accuracy(word2vec_questions_file, restrict_vocab=None)\n",
    "    \n",
    "    word2vec_results.append(word2vec_result)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#     # Training and validation of SVMs using those docvecs\n",
    "#     train_classifications(sections)\n",
    "#     validation_vectors_matrix = get_validation_docs_with_inference(doc2vec_model, doc_classification_map)\n",
    "#     metrics = do_validation(validation_vectors_matrix, doc_classification_map, sections, \"sections\")\n",
    "#     ensure_disk_location_exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "#                                              GLOBAL_VARS.SVM_MODEL_NAME))\n",
    "#     pickle.dump(metrics, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, GLOBAL_VARS.SVM_MODEL_NAME, METRICS), 'w'))\n",
    "#     print \"Coverage Error: {}, Average No of Labels: {}, Top 1: {}, Top 3: {}, Top 5: {}, F1 Micro: {}, Total Positive: {}\".format(\n",
    "#         metrics['coverage_error'], metrics['average_num_of_labels'], metrics['top_1'], metrics['top_3'], metrics['top_5'], \n",
    "#         metrics['f1_micro'], metrics['total_positive'])\n",
    "                                                                                     \n",
    "#     epoch_metrics.append(metrics)\n",
    "#     graph.add_metrics_to_graph(metrics, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing.dummy import Pool as ThreadPool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-28 22:22:21,536 : INFO : Loading new batch\n",
      "2016-12-28 22:22:27,109 : INFO : Finished loading new batch\n"
     ]
    }
   ],
   "source": [
    "validation_docs_iterator = DocumentBatchGenerator(validation_preprocessed_files_prefix, \n",
    "                                                  validation_preprocessed_docids_files_prefix, batch_size=None)\n",
    "i=0\n",
    "doc_contents = []\n",
    "for (doc_id, doc_contents_array) in validation_docs_iterator:\n",
    "    i += 1\n",
    "    doc_contents.append((doc_id, doc_contents_array))\n",
    "    if i > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def infer_one_doc(doc):\n",
    "    #doc2vec_model.random = np.random.RandomState(DOC2VEC_SEED)\n",
    "    rep = doc2vec_model.infer_vector(doc[1])\n",
    "    return (doc[0], rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threaded Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 41s, sys: 620 ms, total: 1min 41s\n",
      "Wall time: 9.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool = ThreadPool(16)\n",
    "doc2vec_model.random = np.random.RandomState(DOC2VEC_SEED)\n",
    "threaded_reps = pool.map(infer_one_doc, doc_contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-Threaded Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 s, sys: 44 ms, total: 14.1 s\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reps = []\n",
    "doc2vec_model.random = np.random.RandomState(DOC2VEC_SEED)\n",
    "for doc in doc_contents:\n",
    "    reps.append((doc[0], doc2vec_model.infer_vector(doc[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'08825480',\n",
       " u'08774433',\n",
       " u'08791071',\n",
       " u'08912011',\n",
       " u'08678092',\n",
       " u'08859194',\n",
       " u'08635554',\n",
       " u'08914715',\n",
       " u'08740442',\n",
       " u'08740792',\n",
       " u'08741891',\n",
       " u'08889791',\n",
       " u'08845058',\n",
       " u'08675352',\n",
       " u'08910298',\n",
       " u'08908470',\n",
       " u'07336611',\n",
       " u'07370801',\n",
       " u'08923495',\n",
       " u'08730828']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d[0] for d in threaded_reps][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'08825480',\n",
       " u'08774433',\n",
       " u'08791071',\n",
       " u'08912011',\n",
       " u'08678092',\n",
       " u'08859194',\n",
       " u'08635554',\n",
       " u'08914715',\n",
       " u'08740442',\n",
       " u'08740792',\n",
       " u'08741891',\n",
       " u'08889791',\n",
       " u'08845058',\n",
       " u'08675352',\n",
       " u'08910298',\n",
       " u'08908470',\n",
       " u'07336611',\n",
       " u'07370801',\n",
       " u'08923495',\n",
       " u'08730828']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d[0] for d in reps][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal([d[0] for d in reps], [d[0] for d in threaded_reps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30695215,  0.62732637, -0.16665867,  0.21510798, -0.31053102,\n",
       "        0.24912256,  0.27854046, -0.18195362, -0.2556676 , -0.5964365 ,\n",
       "        0.21105912, -0.23973639, -0.03185667, -0.07150706,  0.34752986,\n",
       "       -0.10195051, -0.21096784, -0.16357803, -0.36328176,  0.69572109,\n",
       "        0.56532162, -0.2350243 ,  0.29052514,  0.08191228,  0.35617095,\n",
       "       -0.04608935, -0.22245102, -0.2092436 ,  0.03193387,  0.20119652,\n",
       "        0.41143674, -0.00198068,  0.2738685 ,  0.53701001, -0.1117554 ,\n",
       "       -0.03540101, -0.34937236, -0.79319656, -0.24756837,  0.25518459,\n",
       "       -0.13143119, -0.28934225,  0.40138   , -0.98963302,  0.13317154,\n",
       "       -0.78089136,  0.02822817,  0.09919885,  0.06839398,  1.14812255,\n",
       "       -0.35712692,  0.03212974,  0.31967002,  0.01885306,  0.32403627,\n",
       "        0.06881366, -0.36663699, -0.06164655, -0.50977266,  0.13202219,\n",
       "        0.34584206, -0.23481339, -0.26995379, -0.05701207,  0.09176121,\n",
       "        0.05095135, -0.33242008,  0.24291369,  0.01117826,  0.10993928,\n",
       "       -0.1800983 ,  0.49444726,  0.26564318,  0.7361095 , -0.06239768,\n",
       "       -0.21320428, -0.20742436, -0.03807143, -0.12843417, -0.48612225,\n",
       "        0.14675111,  0.1267944 , -0.44746301, -0.68673682, -0.54249072,\n",
       "       -0.07855206, -0.43142584,  0.35044146,  0.15806454, -0.81718534,\n",
       "       -0.24564786, -0.41462311,  0.05346218,  0.13060793, -0.52602261,\n",
       "       -0.68961495, -0.35983112, -0.10904109, -0.70629472, -0.74079585], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.44398868,  0.72295773, -0.1431679 ,  0.12140259, -0.35566956,\n",
       "        0.25285348,  0.25987589, -0.25635543, -0.27375746, -0.68261814,\n",
       "        0.2418227 , -0.31993562, -0.04068605, -0.19011304,  0.25827357,\n",
       "       -0.23827454, -0.14665996, -0.24676035, -0.31205919,  0.7144047 ,\n",
       "        0.51341397, -0.27819353,  0.2297533 ,  0.21143083,  0.21526954,\n",
       "       -0.12819654, -0.23137507, -0.18031113, -0.0302615 ,  0.25606248,\n",
       "        0.46331209,  0.03231328,  0.29240945,  0.55419838, -0.08406049,\n",
       "       -0.02534914, -0.30697635, -0.89490503, -0.25361407, -0.0620365 ,\n",
       "       -0.10279118, -0.24122375,  0.2858358 , -0.89883715,  0.2349384 ,\n",
       "       -0.77614403,  0.07704009,  0.00219567,  0.05001302,  1.00935435,\n",
       "       -0.43593982,  0.03736721,  0.49705869, -0.04218138,  0.37695912,\n",
       "        0.0875724 , -0.41761422,  0.01351045, -0.63688326,  0.19897321,\n",
       "        0.29641187, -0.23571339, -0.14795278, -0.12061672,  0.1306804 ,\n",
       "        0.2521036 , -0.36068025,  0.26970887, -0.00897445,  0.17607778,\n",
       "       -0.101063  ,  0.44033691,  0.19987908,  0.65593952, -0.2147298 ,\n",
       "       -0.10094108, -0.20588517, -0.08825132, -0.08381121, -0.44989595,\n",
       "        0.17535064,  0.10233553, -0.57504725, -0.80179036, -0.56347519,\n",
       "       -0.09048223, -0.4018501 ,  0.54773515,  0.20243937, -0.86586028,\n",
       "       -0.30235139, -0.51399952,  0.10769948, -0.03541334, -0.45822388,\n",
       "       -0.72284496, -0.29005697, -0.11217777, -0.75840199, -0.75367755], dtype=float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threaded_reps[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple threading, but problem is that pool.map exhausts the whole iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-28 23:52:07,981 : INFO : Loading new batch for index: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 ms, sys: 12 ms, total: 36 ms\n",
      "Wall time: 26.4 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-28 23:52:14,764 : INFO : Finished loading new batch\n",
      "2016-12-28 23:52:16,320 : INFO : Loading new batch for index: 10000\n",
      "2016-12-28 23:52:36,742 : INFO : Finished loading new batch\n",
      "2016-12-28 23:52:37,438 : INFO : Loading new batch for index: 12412\n",
      "2016-12-28 23:52:37,487 : INFO : No more batches to load, exiting at index: 12412\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "validation_docs_iterator = DocumentBatchGenerator(validation_preprocessed_files_prefix, \n",
    "                                                  validation_preprocessed_docids_files_prefix, batch_size=None)\n",
    "pool = ThreadPool(16)\n",
    "threaded_reps = pool.map(infer_one_doc, validation_docs_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More advanced threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nothing_func(doc):\n",
    "    1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-29 02:08:42,347 : INFO : Loading new batch for index: 0\n",
      "2016-12-29 02:08:49,943 : INFO : Finished loading new batch\n",
      "2016-12-29 02:09:13,387 : INFO : Finished: 1000\n",
      "2016-12-29 02:09:36,895 : INFO : Finished: 2000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-158-20d2bff78aed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'validation_docs_iterator = DocumentBatchGenerator2(validation_preprocessed_files_prefix, \\n                                                  validation_preprocessed_docids_files_prefix, batch_size=None)\\ngenerator_func = validation_docs_iterator.__iter__()\\npool = ThreadPool(16)\\n# map consumes the whole iterator on the spot, so we have to use itertools.islice to fake mini-batching\\nthreaded_reps = {}\\nmini_batch_size = 1000\\nwhile True:\\n    threaded_reps_partial = pool.map(infer_one_doc, itertools.islice(generator_func, mini_batch_size))\\n    info(\"Finished: {}\".format(str(validation_docs_iterator.curr_index)))\\n    if threaded_reps_partial:\\n        #threaded_reps.extend(threaded_reps_partial)\\n        threaded_reps.update(threaded_reps_partial)\\n    else:\\n        break'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    249\u001b[0m         '''\n\u001b[0;32m    250\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mRUN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mimap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s.wait(): got it\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "validation_docs_iterator = DocumentBatchGenerator2(validation_preprocessed_files_prefix, \n",
    "                                                  validation_preprocessed_docids_files_prefix, batch_size=None)\n",
    "generator_func = validation_docs_iterator.__iter__()\n",
    "pool = ThreadPool(16)\n",
    "# map consumes the whole iterator on the spot, so we have to use itertools.islice to fake mini-batching\n",
    "threaded_reps = {}\n",
    "mini_batch_size = 1000\n",
    "while True:\n",
    "    threaded_reps_partial = pool.map(infer_one_doc, itertools.islice(generator_func, mini_batch_size))\n",
    "    info(\"Finished: {}\".format(str(validation_docs_iterator.curr_index)))\n",
    "    if threaded_reps_partial:\n",
    "        #threaded_reps.extend(threaded_reps_partial)\n",
    "        threaded_reps.update(threaded_reps_partial)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DocumentBatchGenerator2(object):\n",
    "    def __init__(self, filename_prefix, filename_docids_prefix, batch_size=10000 ):\n",
    "        \"\"\"\n",
    "        batch_size cant be > 10,000 due to a limitation in doc2vec training, \n",
    "        None means no batching (only use for inference)\n",
    "        \"\"\"\n",
    "        assert batch_size <= 10000 or batch_size is None\n",
    "        self.filename_prefix = filename_prefix\n",
    "        self.filename_docids_prefix = filename_docids_prefix\n",
    "        self.curr_lines = []\n",
    "        self.curr_docids = []\n",
    "        self.batch_size = batch_size\n",
    "        self.curr_index = 0\n",
    "        self.batch_end = -1\n",
    "    def load_new_batch_in_memory(self):\n",
    "        self.curr_lines, self.docids = [], []\n",
    "        info(\"Loading new batch for index: {}\".format(self.curr_index) )\n",
    "        try:\n",
    "            with open(self.filename_prefix + str(self.curr_index)) as preproc_file:\n",
    "                for line in preproc_file:\n",
    "                    self.curr_lines.append(line.split(\" \"))\n",
    "#                     if i % 1000 == 0:\n",
    "#                         print i\n",
    "            self.curr_docids = pickle.load(open(self.filename_docids_prefix + str(self.curr_index), \"r\"))\n",
    "            self.batch_end = self.curr_index + len(self.curr_lines) -1 \n",
    "            info(\"Finished loading new batch\")\n",
    "        except IOError:\n",
    "            info(\"No more batches to load, exiting at index: {}\".format(self.curr_index))\n",
    "            raise StopIteration()\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            if self.curr_index > self.batch_end:\n",
    "                self.load_new_batch_in_memory()\n",
    "            for (doc_id, tokens) in zip(self.curr_docids, self.curr_lines):\n",
    "                if self.batch_size is not None:\n",
    "                    curr_batch_iter = 0\n",
    "                    # divide the document to batches according to the batch size\n",
    "                    while curr_batch_iter < len(tokens):\n",
    "                        self.curr_index += 1\n",
    "                        yield LabeledSentence(words=tokens[curr_batch_iter: curr_batch_iter + self.batch_size], tags=[doc_id])\n",
    "                        curr_batch_iter += self.batch_size\n",
    "                else:\n",
    "                    self.curr_index += 1\n",
    "                    yield doc_id, tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12412"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13000"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(threaded_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "validation_docs_iterator = DocumentBatchGenerator(validation_preprocessed_files_prefix, \n",
    "                                                  validation_preprocessed_docids_files_prefix, batch_size=None)\n",
    "pool = ThreadPool(20)\n",
    "# map consumes the whole iterator on the spot, so we have to use itertools.islice to fake mini-batching\n",
    "threaded_reps = []\n",
    "mini_batch_size = 1000\n",
    "while True:\n",
    "    threaded_reps_partial = pool.map(infer_one_doc, itertools.islice(validation_docs_iterator, mini_batch_size))\n",
    "    info(\"Finished: {}\".format(str(validation_docs_iterator.curr_index)))\n",
    "    if threaded_reps_partial:\n",
    "        threaded_reps.extend(threaded_reps_partial)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "def g():\n",
    "    for el in xrange(50):\n",
    "        yield el\n",
    "\n",
    "go = g()\n",
    "result = []\n",
    "N = 10\n",
    "for i in itertools.islice(go, N):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-28 18:35:17,009 : INFO : Loading new batch\n",
      "2016-12-28 18:35:21,915 : INFO : Finished loading new batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 848 ms, sys: 8 ms, total: 856 ms\n",
      "Wall time: 852 ms\n",
      "CPU times: user 448 ms, sys: 12 ms, total: 460 ms\n",
      "Wall time: 457 ms\n",
      "CPU times: user 248 ms, sys: 4 ms, total: 252 ms\n",
      "Wall time: 250 ms\n",
      "{u'08774433': array([-0.24957576, -0.44971526, -0.56399536,  0.26340374, -0.72280848,\n",
      "        0.07701001,  0.53518784,  0.47400331,  0.00103833, -0.21523826,\n",
      "        0.5090881 , -0.66132861, -0.54403561,  0.44076878,  0.00470332,\n",
      "       -0.60674977,  0.25171441,  0.01955804, -0.42058256,  0.12502177,\n",
      "       -0.16908602, -0.77621526,  0.66039973,  0.22638585, -0.14937432,\n",
      "        0.18073724, -0.22520301, -0.01612019, -0.94866085, -0.56705993,\n",
      "       -0.31372947, -0.61444628,  0.36096638,  0.05321291,  0.31520829,\n",
      "       -0.78794104, -0.02634728,  0.27075273, -0.69757801, -0.11887208,\n",
      "        0.24548931, -0.37358913,  0.30241317, -0.02301121, -0.16444607,\n",
      "        0.32210201, -0.49894542,  0.47786587, -0.7696104 ,  0.57316011,\n",
      "        0.75851572, -0.29875955, -0.39299953,  0.29592195,  0.01002928,\n",
      "       -0.00714504, -0.45345017, -0.45329556, -0.15423954,  0.0956117 ,\n",
      "       -0.17815629,  0.00485223, -0.47434196,  0.8352198 ,  0.29984775,\n",
      "       -0.75325739, -0.2932708 ,  0.22751437, -0.72489858, -0.23059492,\n",
      "       -0.62555069,  1.02729392, -0.00715734, -0.35845065, -0.04130894,\n",
      "       -0.63375956,  0.13670547, -0.35168752, -0.120276  ,  0.06363766,\n",
      "       -0.14529508, -0.73692763, -0.10244129,  0.24102579, -0.39304346,\n",
      "       -0.44905126, -0.02963378,  0.40327483,  0.09173202, -0.20075732,\n",
      "        0.25192481, -0.40484259,  0.03583284,  0.08575827, -0.71413624,\n",
      "       -0.12856877,  0.00697137,  0.17350946, -0.09800411, -0.2387124 ], dtype=float32), u'08825480': array([ 0.43869719,  0.69493228, -0.15423374,  0.18415831, -0.27953076,\n",
      "        0.32741979,  0.25916466, -0.2263912 , -0.30463687, -0.68594444,\n",
      "        0.23562942, -0.26695064, -0.03071095, -0.19515269,  0.16481824,\n",
      "       -0.26835197, -0.1237495 , -0.21595523, -0.31863457,  0.67200935,\n",
      "        0.54566181, -0.3022787 ,  0.21178976,  0.16891097,  0.25907236,\n",
      "       -0.13429914, -0.29047617, -0.19215892, -0.03019565,  0.23971696,\n",
      "        0.4794547 , -0.0125326 ,  0.27816984,  0.5431518 , -0.08415657,\n",
      "       -0.06213364, -0.31795123, -0.95289969, -0.25565225, -0.03096373,\n",
      "       -0.0847759 , -0.239768  ,  0.34742916, -0.92078727,  0.13093236,\n",
      "       -0.80080444,  0.06648453, -0.03016538,  0.06161455,  1.04044044,\n",
      "       -0.43327764,  0.07086919,  0.35355189,  0.01039008,  0.4170292 ,\n",
      "        0.08228578, -0.45204273, -0.00495607, -0.65024465,  0.18624543,\n",
      "        0.29581559, -0.26705703, -0.08137546, -0.06691577,  0.14972705,\n",
      "        0.151612  , -0.31241465,  0.24215488,  0.00248444,  0.1801302 ,\n",
      "       -0.09848733,  0.43015948,  0.1988074 ,  0.64442545, -0.18052702,\n",
      "       -0.18566328, -0.22178149, -0.0649749 , -0.09142254, -0.46278423,\n",
      "        0.15451705,  0.1908915 , -0.52935004, -0.82046568, -0.49538887,\n",
      "       -0.10716466, -0.36556813,  0.5163148 ,  0.19676271, -0.84946609,\n",
      "       -0.29985458, -0.52276957,  0.03275018,  0.06643863, -0.43565878,\n",
      "       -0.76278096, -0.28564611, -0.09855174, -0.72650057, -0.71671903], dtype=float32), u'08791071': array([-0.23259643,  0.41450036,  0.51434827,  0.00787312, -0.2284483 ,\n",
      "       -0.52937728, -0.19830056,  0.03082488,  1.05058038,  0.17992359,\n",
      "        0.31625026, -0.21843682, -0.37463692, -0.30913776,  0.3170667 ,\n",
      "       -0.36334237, -0.70675725,  0.52395636, -0.26112491,  1.05543351,\n",
      "       -1.10443592, -0.21194471, -0.54147512,  0.09155747, -0.32934853,\n",
      "        0.01652185,  0.04093302,  0.08362015,  0.54133213,  0.03441263,\n",
      "       -0.88398618, -0.10849927,  0.56919503, -0.60503298,  0.12571461,\n",
      "        0.18873298,  0.17914939,  0.04950287, -0.28671712, -0.10798603,\n",
      "       -0.744412  ,  0.31562504, -0.27431366,  0.28065825,  0.23825864,\n",
      "       -0.1323806 ,  0.06025767, -0.22927827, -0.44928807,  0.21095365,\n",
      "        0.57495964, -0.1029187 , -0.00365091,  0.61913794, -0.88101172,\n",
      "        0.60874623, -0.38232478,  0.47158134, -0.05255252,  0.4324486 ,\n",
      "        0.3947401 ,  1.06982732,  0.19763152, -0.52621794,  0.30451465,\n",
      "       -0.41999158, -0.23451024,  0.31681204,  0.28471184, -0.22745852,\n",
      "       -0.60245752, -0.37300718, -0.06770302,  0.0346529 ,  0.36362028,\n",
      "       -0.08779243, -0.16030943, -0.11415948, -0.3266314 ,  0.30446157,\n",
      "        0.11385298, -0.21830294,  0.14570464, -0.71273166, -0.22124635,\n",
      "        0.22901915, -0.69352126, -0.81168574,  0.120683  ,  0.68962598,\n",
      "        0.1142956 ,  0.08250535, -0.01027837,  0.10909576,  0.22610265,\n",
      "        0.06197856, -0.4596194 ,  0.27084136,  0.23881529,  0.26482731], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "validation_docs_iterator = DocumentBatchGenerator(validation_preprocessed_files_prefix, \n",
    "                                                  validation_preprocessed_docids_files_prefix, batch_size=None)\n",
    "doc2vec_model.random = np.random.RandomState(DOC2VEC_SEED)\n",
    "i = 0\n",
    "val_docs_reps = {}\n",
    "for (doc_id, doc_contents_array) in validation_docs_iterator:\n",
    "    i += 1\n",
    "    %time val_docs_reps[doc_id] = doc2vec_model.infer_vector(doc_contents_array, steps=15)\n",
    "    if i > 2:\n",
    "        break\n",
    "    \n",
    "print val_docs_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = get_training_data(doc2vec_model, classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yc = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49789, 8)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = OneVsRestClassifier(estimator=linear_model.SGDClassifier(loss='hinge', penalty='l2', \n",
    "                                                         #alpha is the 1/C parameter\n",
    "                                                         alpha=0.001, fit_intercept=True, n_iter=10,\n",
    "                                                         #n_jobs=-1 means use all cpus\n",
    "                                                         shuffle=True, verbose=1, epsilon=0.1, n_jobs=-1,\n",
    "                                                         #eta0 is the learning rate when we use constant configuration\n",
    "                                                         random_state=SVM_SEED, learning_rate='optimal', eta0=0.0, \n",
    "                                                         class_weight=None, warm_start=False), n_jobs=1)\n",
    "\n",
    "# clf = OneVsRestClassifier(estimator=SVC(kernel='linear'), n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "yp = clf.predict(X)\n",
    "\n",
    "print yp\n",
    "\n",
    "training_metrics = get_metrics(np.array(y), yp, yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
       "       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    "Y = np.array(['a', 'a', 'b', 'b'])\n",
    "clf = linear_model.SGDClassifier()\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.predict([[-0.8, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_svm_epoch = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-11-26 18:25:14,192 : INFO : loading Doc2Vec object from /mnt/data2/shalaby/parameter_search_doc2vec_models/sample_0.0001/doc2vec_size_3000_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_13/model\n",
      "2016-11-26 18:25:14,584 : INFO : loading docvecs recursively from /mnt/data2/shalaby/parameter_search_doc2vec_models/sample_0.0001/doc2vec_size_3000_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_13/model.docvecs.* with mmap=None\n",
      "2016-11-26 18:25:14,585 : INFO : loading doctag_syn0 from /mnt/data2/shalaby/parameter_search_doc2vec_models/sample_0.0001/doc2vec_size_3000_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_13/model.docvecs.doctag_syn0.npy with mmap=None\n",
      "2016-11-26 18:25:14,814 : INFO : loading syn1neg from /mnt/data2/shalaby/parameter_search_doc2vec_models/sample_0.0001/doc2vec_size_3000_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_13/model.syn1neg.npy with mmap=None\n",
      "2016-11-26 18:26:37,586 : INFO : loading syn0 from /mnt/data2/shalaby/parameter_search_doc2vec_models/sample_0.0001/doc2vec_size_3000_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_13/model.syn0.npy with mmap=None\n",
      "2016-11-26 18:26:41,876 : INFO : setting ignored attribute syn0norm to None\n",
      "2016-11-26 18:26:41,877 : INFO : setting ignored attribute cum_table to None\n"
     ]
    }
   ],
   "source": [
    "GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(best_svm_epoch)\n",
    "doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.61 s, sys: 520 ms, total: 2.13 s\n",
      "Wall time: 1.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classifications = sections\n",
    "\n",
    "NN_OUTPUT_NEURONS = len(classifications)\n",
    "one_hot_encoder = OneHotEncoder(classifications)\n",
    "training_data = []\n",
    "training_labels = []\n",
    "for doc_id in training_docs_list:\n",
    "    # converting from memmap to a normal array\n",
    "    normal_array = []\n",
    "    normal_array[:] = doc2vec_model.docvecs[doc_id][:]\n",
    "    training_data.append(normal_array)\n",
    "    eligible_classifications = [clssf for clssf in doc_classification_map[doc_id] if clssf in classifications]\n",
    "    training_labels.append(one_hot_encoder.get_label_vector(eligible_classifications))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1969, 3000)\n",
      "CPU times: user 2.58 s, sys: 360 ms, total: 2.94 s\n",
      "Wall time: 2.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "validation_labels = []\n",
    "validation_data = pickle.load(open(\n",
    "        os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_MATRIX)\n",
    "))\n",
    "print validation_data.shape\n",
    "\n",
    "for validation_doc_id in validation_docs_list:\n",
    "    eligible_classifications = [clssf for clssf in doc_classification_map[validation_doc_id] if clssf in classifications]\n",
    "    validation_labels.append(one_hot_encoder.get_label_vector(eligible_classifications))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Keras NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================\n",
      "========== Layer Size: 500, Activation: relu, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_relu (Dense)        (None, 500)           1500500     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        hidden_layer_relu[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 1504508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 500, Activation: relu, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_relu (Dense)        (None, 500)           1500500     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_64 (Dropout)             (None, 500)           0           hidden_layer_relu[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        dropout_64[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1504508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 500, Activation: relu, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)             (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_relu (Dense)        (None, 500)           1500500     dropout_65[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        hidden_layer_relu[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 1504508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 500, Activation: relu, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)             (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_relu (Dense)        (None, 500)           1500500     dropout_66[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)             (None, 500)           0           hidden_layer_relu[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        dropout_67[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1504508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 500, Activation: sigmoid, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 500)           1500500     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        hidden_layer_sigmoid[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 1504508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 500, Activation: sigmoid, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 500)           1500500     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)             (None, 500)           0           hidden_layer_sigmoid[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        dropout_68[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1504508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 500, Activation: sigmoid, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)             (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 500)           1500500     dropout_69[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        hidden_layer_sigmoid[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 1504508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 500, Activation: sigmoid, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_70 (Dropout)             (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 500)           1500500     dropout_70[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)             (None, 500)           0           hidden_layer_sigmoid[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        dropout_71[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1504508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 500, Activation: tanh, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 500)           1500500     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        hidden_layer_tanh[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 1504508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 500, Activation: tanh, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 500)           1500500     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)             (None, 500)           0           hidden_layer_tanh[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        dropout_72[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1504508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 500, Activation: tanh, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)             (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 500)           1500500     dropout_73[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        hidden_layer_tanh[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 1504508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 500, Activation: tanh, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)             (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 500)           1500500     dropout_74[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_75 (Dropout)             (None, 500)           0           hidden_layer_tanh[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        dropout_75[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1504508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 500, Activation: linear, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_linear (Dense)      (None, 500)           1500500     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        hidden_layer_linear[0][0]        \n",
      "====================================================================================================\n",
      "Total params: 1504508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 500, Activation: linear, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_linear (Dense)      (None, 500)           1500500     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_76 (Dropout)             (None, 500)           0           hidden_layer_linear[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        dropout_76[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1504508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 500, Activation: linear, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_77 (Dropout)             (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_linear (Dense)      (None, 500)           1500500     dropout_77[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        hidden_layer_linear[0][0]        \n",
      "====================================================================================================\n",
      "Total params: 1504508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 500, Activation: linear, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_78 (Dropout)             (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_linear (Dense)      (None, 500)           1500500     dropout_78[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_79 (Dropout)             (None, 500)           0           hidden_layer_linear[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        dropout_79[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1504508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 500, Activation: softmax, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_softmax (Dense)     (None, 500)           1500500     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        hidden_layer_softmax[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 1504508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 500, Activation: softmax, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_softmax (Dense)     (None, 500)           1500500     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_80 (Dropout)             (None, 500)           0           hidden_layer_softmax[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        dropout_80[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1504508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 500, Activation: softmax, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_81 (Dropout)             (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_softmax (Dense)     (None, 500)           1500500     dropout_81[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        hidden_layer_softmax[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 1504508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 500, Activation: softmax, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_82 (Dropout)             (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_softmax (Dense)     (None, 500)           1500500     dropout_82[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_83 (Dropout)             (None, 500)           0           hidden_layer_softmax[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             4008        dropout_83[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1504508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 1500, Activation: relu, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_relu (Dense)        (None, 1500)          4501500     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             12008       hidden_layer_relu[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 4513508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 1500, Activation: relu, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_relu (Dense)        (None, 1500)          4501500     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_84 (Dropout)             (None, 1500)          0           hidden_layer_relu[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             12008       dropout_84[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 4513508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 1500, Activation: relu, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_85 (Dropout)             (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_relu (Dense)        (None, 1500)          4501500     dropout_85[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             12008       hidden_layer_relu[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 4513508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 1500, Activation: relu, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_86 (Dropout)             (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_relu (Dense)        (None, 1500)          4501500     dropout_86[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_87 (Dropout)             (None, 1500)          0           hidden_layer_relu[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             12008       dropout_87[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 4513508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 1500, Activation: sigmoid, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 1500)          4501500     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             12008       hidden_layer_sigmoid[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 4513508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 1500, Activation: sigmoid, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 1500)          4501500     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_88 (Dropout)             (None, 1500)          0           hidden_layer_sigmoid[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             12008       dropout_88[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 4513508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 1500, Activation: sigmoid, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_89 (Dropout)             (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 1500)          4501500     dropout_89[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             12008       hidden_layer_sigmoid[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 4513508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 1500, Activation: sigmoid, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_90 (Dropout)             (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 1500)          4501500     dropout_90[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_91 (Dropout)             (None, 1500)          0           hidden_layer_sigmoid[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             12008       dropout_91[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 4513508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 1500, Activation: tanh, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 1500)          4501500     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             12008       hidden_layer_tanh[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 4513508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 1500, Activation: tanh, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 1500)          4501500     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_92 (Dropout)             (None, 1500)          0           hidden_layer_tanh[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             12008       dropout_92[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 4513508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 1500, Activation: tanh, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_93 (Dropout)             (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 1500)          4501500     dropout_93[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             12008       hidden_layer_tanh[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 4513508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 1500, Activation: tanh, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_94 (Dropout)             (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 1500)          4501500     dropout_94[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_95 (Dropout)             (None, 1500)          0           hidden_layer_tanh[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             12008       dropout_95[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 4513508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 1500, Activation: linear, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_linear (Dense)      (None, 1500)          4501500     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             12008       hidden_layer_linear[0][0]        \n",
      "====================================================================================================\n",
      "Total params: 4513508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 1500, Activation: linear, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_linear (Dense)      (None, 1500)          4501500     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_96 (Dropout)             (None, 1500)          0           hidden_layer_linear[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             12008       dropout_96[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 4513508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 1500, Activation: linear, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_97 (Dropout)             (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_linear (Dense)      (None, 1500)          4501500     dropout_97[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             12008       hidden_layer_linear[0][0]        \n",
      "====================================================================================================\n",
      "Total params: 4513508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 1500, Activation: linear, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_98 (Dropout)             (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_linear (Dense)      (None, 1500)          4501500     dropout_98[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_99 (Dropout)             (None, 1500)          0           hidden_layer_linear[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             12008       dropout_99[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 4513508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 1500, Activation: softmax, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_softmax (Dense)     (None, 1500)          4501500     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             12008       hidden_layer_softmax[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 4513508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 1500, Activation: softmax, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_softmax (Dense)     (None, 1500)          4501500     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_100 (Dropout)            (None, 1500)          0           hidden_layer_softmax[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             12008       dropout_100[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 4513508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 1500, Activation: softmax, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_101 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_softmax (Dense)     (None, 1500)          4501500     dropout_101[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             12008       hidden_layer_softmax[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 4513508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 1500, Activation: softmax, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_102 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_softmax (Dense)     (None, 1500)          4501500     dropout_102[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_103 (Dropout)            (None, 1500)          0           hidden_layer_softmax[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             12008       dropout_103[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 4513508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 3000, Activation: relu, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_relu (Dense)        (None, 3000)          9003000     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             24008       hidden_layer_relu[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 9027008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 3000, Activation: relu, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_relu (Dense)        (None, 3000)          9003000     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_104 (Dropout)            (None, 3000)          0           hidden_layer_relu[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             24008       dropout_104[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 9027008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 3000, Activation: relu, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_105 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_relu (Dense)        (None, 3000)          9003000     dropout_105[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             24008       hidden_layer_relu[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 9027008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 3000, Activation: relu, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_106 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_relu (Dense)        (None, 3000)          9003000     dropout_106[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_107 (Dropout)            (None, 3000)          0           hidden_layer_relu[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             24008       dropout_107[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 9027008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 3000, Activation: sigmoid, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 3000)          9003000     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             24008       hidden_layer_sigmoid[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 9027008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 3000, Activation: sigmoid, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 3000)          9003000     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_108 (Dropout)            (None, 3000)          0           hidden_layer_sigmoid[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             24008       dropout_108[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 9027008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 3000, Activation: sigmoid, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_109 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 3000)          9003000     dropout_109[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             24008       hidden_layer_sigmoid[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 9027008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 3000, Activation: sigmoid, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_110 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 3000)          9003000     dropout_110[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_111 (Dropout)            (None, 3000)          0           hidden_layer_sigmoid[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             24008       dropout_111[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 9027008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 3000, Activation: tanh, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 3000)          9003000     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             24008       hidden_layer_tanh[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 9027008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 3000, Activation: tanh, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 3000)          9003000     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_112 (Dropout)            (None, 3000)          0           hidden_layer_tanh[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             24008       dropout_112[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 9027008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 3000, Activation: tanh, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_113 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 3000)          9003000     dropout_113[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             24008       hidden_layer_tanh[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 9027008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 3000, Activation: tanh, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_114 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 3000)          9003000     dropout_114[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_115 (Dropout)            (None, 3000)          0           hidden_layer_tanh[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             24008       dropout_115[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 9027008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 3000, Activation: linear, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_linear (Dense)      (None, 3000)          9003000     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             24008       hidden_layer_linear[0][0]        \n",
      "====================================================================================================\n",
      "Total params: 9027008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 3000, Activation: linear, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_linear (Dense)      (None, 3000)          9003000     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_116 (Dropout)            (None, 3000)          0           hidden_layer_linear[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             24008       dropout_116[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 9027008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 3000, Activation: linear, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_117 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_linear (Dense)      (None, 3000)          9003000     dropout_117[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             24008       hidden_layer_linear[0][0]        \n",
      "====================================================================================================\n",
      "Total params: 9027008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 3000, Activation: linear, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_118 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_linear (Dense)      (None, 3000)          9003000     dropout_118[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_119 (Dropout)            (None, 3000)          0           hidden_layer_linear[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             24008       dropout_119[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 9027008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 3000, Activation: softmax, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_softmax (Dense)     (None, 3000)          9003000     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             24008       hidden_layer_softmax[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 9027008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 3000, Activation: softmax, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_softmax (Dense)     (None, 3000)          9003000     doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_120 (Dropout)            (None, 3000)          0           hidden_layer_softmax[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             24008       dropout_120[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 9027008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 3000, Activation: softmax, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_121 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_softmax (Dense)     (None, 3000)          9003000     dropout_121[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             24008       hidden_layer_softmax[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 9027008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 3000, Activation: softmax, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_122 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_softmax (Dense)     (None, 3000)          9003000     dropout_122[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_123 (Dropout)            (None, 3000)          0           hidden_layer_softmax[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             24008       dropout_123[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 9027008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 4500, Activation: relu, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_relu (Dense)        (None, 4500)          13504500    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       hidden_layer_relu[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 4500, Activation: relu, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_relu (Dense)        (None, 4500)          13504500    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_124 (Dropout)            (None, 4500)          0           hidden_layer_relu[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       dropout_124[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 4500, Activation: relu, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_125 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_relu (Dense)        (None, 4500)          13504500    dropout_125[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       hidden_layer_relu[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 4500, Activation: relu, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_126 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_relu (Dense)        (None, 4500)          13504500    dropout_126[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_127 (Dropout)            (None, 4500)          0           hidden_layer_relu[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       dropout_127[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 4500, Activation: sigmoid, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 4500)          13504500    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       hidden_layer_sigmoid[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 4500, Activation: sigmoid, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 4500)          13504500    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_128 (Dropout)            (None, 4500)          0           hidden_layer_sigmoid[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       dropout_128[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 4500, Activation: sigmoid, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_129 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 4500)          13504500    dropout_129[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       hidden_layer_sigmoid[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 4500, Activation: sigmoid, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_130 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 4500)          13504500    dropout_130[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_131 (Dropout)            (None, 4500)          0           hidden_layer_sigmoid[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       dropout_131[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 4500, Activation: tanh, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 4500)          13504500    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       hidden_layer_tanh[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 4500, Activation: tanh, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 4500)          13504500    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_132 (Dropout)            (None, 4500)          0           hidden_layer_tanh[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       dropout_132[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 4500, Activation: tanh, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_133 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 4500)          13504500    dropout_133[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       hidden_layer_tanh[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 4500, Activation: tanh, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_134 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 4500)          13504500    dropout_134[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_135 (Dropout)            (None, 4500)          0           hidden_layer_tanh[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       dropout_135[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 4500, Activation: linear, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_linear (Dense)      (None, 4500)          13504500    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       hidden_layer_linear[0][0]        \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 4500, Activation: linear, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_linear (Dense)      (None, 4500)          13504500    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_136 (Dropout)            (None, 4500)          0           hidden_layer_linear[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       dropout_136[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 4500, Activation: linear, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_137 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_linear (Dense)      (None, 4500)          13504500    dropout_137[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       hidden_layer_linear[0][0]        \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 4500, Activation: linear, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_138 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_linear (Dense)      (None, 4500)          13504500    dropout_138[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_139 (Dropout)            (None, 4500)          0           hidden_layer_linear[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       dropout_139[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 4500, Activation: softmax, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_softmax (Dense)     (None, 4500)          13504500    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       hidden_layer_softmax[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 4500, Activation: softmax, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_softmax (Dense)     (None, 4500)          13504500    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_140 (Dropout)            (None, 4500)          0           hidden_layer_softmax[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       dropout_140[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 4500, Activation: softmax, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_141 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_softmax (Dense)     (None, 4500)          13504500    dropout_141[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       hidden_layer_softmax[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 4500, Activation: softmax, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_142 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_softmax (Dense)     (None, 4500)          13504500    dropout_142[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_143 (Dropout)            (None, 4500)          0           hidden_layer_softmax[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       dropout_143[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 6000, Activation: relu, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_relu (Dense)        (None, 6000)          18006000    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             48008       hidden_layer_relu[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 18054008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 6000, Activation: relu, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_relu (Dense)        (None, 6000)          18006000    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_144 (Dropout)            (None, 6000)          0           hidden_layer_relu[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             48008       dropout_144[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 18054008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 6000, Activation: relu, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_145 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_relu (Dense)        (None, 6000)          18006000    dropout_145[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             48008       hidden_layer_relu[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 18054008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 6000, Activation: relu, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_146 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_relu (Dense)        (None, 6000)          18006000    dropout_146[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_147 (Dropout)            (None, 6000)          0           hidden_layer_relu[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             48008       dropout_147[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 18054008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 6000, Activation: sigmoid, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 6000)          18006000    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             48008       hidden_layer_sigmoid[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 18054008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 6000, Activation: sigmoid, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 6000)          18006000    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_148 (Dropout)            (None, 6000)          0           hidden_layer_sigmoid[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             48008       dropout_148[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 18054008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 6000, Activation: sigmoid, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_149 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 6000)          18006000    dropout_149[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             48008       hidden_layer_sigmoid[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 18054008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 6000, Activation: sigmoid, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_150 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_sigmoid (Dense)     (None, 6000)          18006000    dropout_150[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_151 (Dropout)            (None, 6000)          0           hidden_layer_sigmoid[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             48008       dropout_151[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 18054008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 6000, Activation: tanh, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 6000)          18006000    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             48008       hidden_layer_tanh[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 18054008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 6000, Activation: tanh, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 6000)          18006000    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_152 (Dropout)            (None, 6000)          0           hidden_layer_tanh[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             48008       dropout_152[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 18054008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 6000, Activation: tanh, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_153 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 6000)          18006000    dropout_153[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             48008       hidden_layer_tanh[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 18054008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 6000, Activation: tanh, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_154 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 6000)          18006000    dropout_154[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_155 (Dropout)            (None, 6000)          0           hidden_layer_tanh[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             48008       dropout_155[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 18054008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 6000, Activation: linear, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_linear (Dense)      (None, 6000)          18006000    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             48008       hidden_layer_linear[0][0]        \n",
      "====================================================================================================\n",
      "Total params: 18054008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 6000, Activation: linear, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_linear (Dense)      (None, 6000)          18006000    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_156 (Dropout)            (None, 6000)          0           hidden_layer_linear[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             48008       dropout_156[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 18054008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 6000, Activation: linear, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_157 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_linear (Dense)      (None, 6000)          18006000    dropout_157[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             48008       hidden_layer_linear[0][0]        \n",
      "====================================================================================================\n",
      "Total params: 18054008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 6000, Activation: linear, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_158 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_linear (Dense)      (None, 6000)          18006000    dropout_158[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_159 (Dropout)            (None, 6000)          0           hidden_layer_linear[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             48008       dropout_159[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 18054008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 6000, Activation: softmax, Input Dropout: False, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_softmax (Dense)     (None, 6000)          18006000    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             48008       hidden_layer_softmax[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 18054008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 6000, Activation: softmax, Input Dropout: False, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_softmax (Dense)     (None, 6000)          18006000    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_160 (Dropout)            (None, 6000)          0           hidden_layer_softmax[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             48008       dropout_160[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 18054008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 6000, Activation: softmax, Input Dropout: True, Hidden Dropout: False ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_161 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_softmax (Dense)     (None, 6000)          18006000    dropout_161[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             48008       hidden_layer_softmax[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 18054008\n",
      "____________________________________________________________________________________________________\n",
      "===================================================================================\n",
      "========== Layer Size: 6000, Activation: softmax, Input Dropout: True, Hidden Dropout: True ==========================\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_162 (Dropout)            (None, 3000)          0           doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_softmax (Dense)     (None, 6000)          18006000    dropout_162[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_163 (Dropout)            (None, 6000)          0           hidden_layer_softmax[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             48008       dropout_163[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 18054008\n",
      "____________________________________________________________________________________________________\n",
      "CPU times: user 1h 52min 19s, sys: 2h 41min 12s, total: 4h 33min 31s\n",
      "Wall time: 4h 34min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history_list = []\n",
    "hidden_layer_sizes = [500,1500,3000,4500,6000]\n",
    "activations = ['relu','sigmoid', 'tanh', 'linear', 'softmax']\n",
    "input_dropout_options = [False, True]\n",
    "hidden_dropout_options = [False, True]\n",
    "\n",
    "params = list(itertools.product(hidden_layer_sizes, activations, input_dropout_options, hidden_dropout_options))\n",
    "for layer_size, activation_func, input_dropout_do, hidden_dropout_do in params:\n",
    "    print \"===================================================================================\\n\" + \\\n",
    "          \"========== Layer Size: {}, Activation: {}, Input Dropout: {}, Hidden Dropout: {} ==========================\"\"\".format(layer_size, activation_func, input_dropout_do, hidden_dropout_do)\n",
    "    doc_input = Input(shape=(DOC2VEC_SIZE,), name='doc_input')\n",
    "    if input_dropout_do:\n",
    "        hidden = Dropout(0.7)(doc_input)\n",
    "    hidden = Dense(layer_size, activation=activation_func, name='hidden_layer_{}'.format(activation_func))(doc_input if not input_dropout_do else hidden)\n",
    "    if hidden_dropout_do:\n",
    "        hidden = Dropout(0.5)(hidden)\n",
    "    softmax_output = Dense(NN_OUTPUT_NEURONS, activation='sigmoid', name='softmax_output')(hidden)\n",
    "    model = Model(input=doc_input, output=softmax_output)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', \n",
    "                  metrics=['accuracy', 'fbeta_score', theano_coverage_error])\n",
    "    model.summary()\n",
    "    history = model.fit(x=training_data, y=training_labels, \n",
    "          validation_data=(validation_data, validation_labels), \n",
    "          nb_epoch=NN_EPOCHS, verbose=0)\n",
    "    history_list.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(history_list, open('/mnt/data2/shalaby/history_list_sample_0.0001.pickle','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print len(history_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.66835957339\n",
      "2.51193499238\n",
      "2.40680548502\n",
      "2.281361097\n",
      "2.76536312849\n",
      "2.42001015744\n",
      "2.3844591163\n",
      "2.3092940579\n",
      "3.03910614525\n",
      "2.50482478415\n",
      "2.52412392077\n",
      "2.38547486034\n",
      "2.89588623667\n",
      "2.85017775521\n",
      "2.46267140681\n",
      "2.45962417471\n",
      "2.91010665312\n",
      "2.81513458608\n",
      "3.01218892839\n",
      "3.13052310818\n",
      "2.61097003555\n",
      "2.64144235653\n",
      "2.39918740477\n",
      "2.29456576943\n",
      "2.89893346877\n",
      "2.5876079228\n",
      "2.49212798375\n",
      "2.42153377349\n",
      "3.01422041646\n",
      "2.63484002031\n",
      "2.50837988827\n",
      "2.39969527679\n",
      "2.87455561199\n",
      "2.94261046216\n",
      "2.50431691214\n",
      "2.52615540884\n",
      "2.71203656679\n",
      "2.96901980701\n",
      "2.96597257491\n",
      "3.17013712544\n",
      "2.81665820213\n",
      "2.87760284408\n",
      "2.39918740477\n",
      "2.3001523616\n",
      "3.03098019299\n",
      "2.67394616557\n",
      "2.64347384459\n",
      "2.6719146775\n",
      "2.95226003047\n",
      "2.68359573388\n",
      "2.50279329609\n",
      "2.49771457593\n",
      "2.89233113255\n",
      "2.9939055358\n",
      "2.54647028949\n",
      "2.63941086846\n",
      "2.93143727781\n",
      "2.97206703911\n",
      "3.06094464195\n",
      "3.22346368715\n",
      "2.93905535805\n",
      "3.12341289995\n",
      "2.39969527679\n",
      "2.32453021838\n",
      "3.03859827324\n",
      "2.76231589639\n",
      "2.66124936516\n",
      "2.64550533266\n",
      "2.96800406298\n",
      "2.75825292026\n",
      "2.5281868969\n",
      "2.60589131539\n",
      "2.92432706958\n",
      "2.97257491112\n",
      "2.59878110716\n",
      "2.71051295074\n",
      "2.7374301676\n",
      "3.09192483494\n",
      "2.88725241239\n",
      "3.50380904012\n",
      "3.09446419502\n",
      "3.2092432707\n",
      "2.43219908583\n",
      "2.34890807517\n",
      "3.05688166582\n",
      "2.80700863382\n",
      "2.84255967496\n",
      "2.86084306755\n",
      "2.92737430168\n",
      "2.84560690706\n",
      "2.55510411376\n",
      "2.64093448451\n",
      "2.8938547486\n",
      "3.13610970036\n",
      "2.63026917217\n",
      "2.81056373794\n",
      "2.83646521077\n",
      "3.1188420518\n",
      "3.19857795835\n",
      "3.59268664297\n"
     ]
    }
   ],
   "source": [
    "for history in history_list:\n",
    "    hist = history.history\n",
    "    max_val_fbeta = max(hist['val_coverage error'])\n",
    "    print max_val_fbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_input = Input(shape=(DOC2VEC_SIZE,), name='doc_input')\n",
    "hidden = Dense(NN_HIDDEN_NEURONS, activation='relu', name='hidden_layer')(doc_input)\n",
    "softmax_output = Dense(NN_OUTPUT_NEURONS, activation='sigmoid', name='softmax_output')(hidden)\n",
    "model = Model(input=doc_input, output=softmax_output)\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', 'fbeta_score', theano_coverage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 3000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer (Dense)             (None, 4500)          13504500    doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             36008       hidden_layer[0][0]               \n",
      "====================================================================================================\n",
      "Total params: 13540508\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8979 samples, validate on 1969 samples\n",
      "Epoch 1/1\n",
      "8979/8979 [==============================] - 4s - loss: 0.0427 - acc: 0.9835 - fbeta_score: 0.9504 - coverage error: 1.4936 - val_loss: 2.1309 - val_acc: 0.8531 - val_fbeta_score: 0.4738 - val_coverage error: 3.4474\n"
     ]
    }
   ],
   "source": [
    "model.fit(x=training_data, y=training_labels, \n",
    "          validation_data=(validation_data, validation_labels), \n",
    "          nb_epoch=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_prediction = model.predict(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.70263344e-01,   3.49998474e-02,   3.13610617e-05,\n",
       "          1.08150870e-03,   3.11665332e-07,   7.09001958e-01,\n",
       "          4.97711152e-02,   2.63609409e-01],\n",
       "       [  1.70166213e-02,   5.36046147e-01,   2.61311390e-04,\n",
       "          4.87348643e-06,   1.27638310e-01,   9.57404263e-03,\n",
       "          3.39831635e-02,   2.13784515e-03]], dtype=float32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_prediction[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 1, 1], [0, 1, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_labels[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = T.matrix('y_true')\n",
    "y_score = T.matrix('y_score')\n",
    "\n",
    "y_score_true = y_true * y_score # mark the scores of actually true labels\n",
    "zero_true_elem = T.eq(y_true, 0).nonzero()\n",
    "y_score_masked = T.set_subtensor(y_score_true[zero_true_elem], 100)\n",
    "#zero_elements = T.eq(true_scores,0)\n",
    "min_true_scores = T.min(y_score_masked, axis=1, keepdims=True) # we do keepdims in order to keep the broadcastable columns\n",
    "coverage_per_row = (y_score >= min_true_scores).sum(axis=1)\n",
    "coverage = T.mean(coverage_per_row)\n",
    "theano_coverage_err_func = function(inputs=[y_true, y_score], outputs=coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uu = np.array([[1,0,1],[0,0,1]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yc = T.set_subtensor(y_true[T.eq(y_true,0).nonzero()], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.,  100.,    1.],\n",
       "       [ 100.,  100.,    1.]], dtype=float32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yc.eval({y_true: uu})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T.set_subtensor(y_true[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 1, 1]]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_labels[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.08178342e-08,   1.02713175e-05,   4.41441728e-28,\n",
       "          1.19779872e-36,   0.00000000e+00,   1.00000000e+00,\n",
       "          2.75591228e-05,   2.31643662e-06]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_prediction[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 8.01 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(2.5129507364144237)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "theano_coverage_err_func(validation_labels, val_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.08178342e-08,   1.02713175e-05,   4.41441728e-28,\n",
       "          1.19779872e-36,   0.00000000e+00,   1.00000000e+00,\n",
       "          1.00000028e+02,   1.00000002e+02]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(validation_labels[:1] , 100) + val_prediction[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.00000231643662"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_prediction[0,7] + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-99.99999768])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.multiply(validation_labels[:1] , -100) + val_prediction[:1]).min(axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 5.51 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.5129507364144237"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "coverage_error(validation_labels, val_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.43536258e-08,   2.79983366e-03,   1.15342544e-08,\n",
       "          7.79373941e-17,   5.66347130e-03,   9.99988794e-01,\n",
       "          6.53333089e-04,   5.85054120e-20],\n",
       "       [  2.85919495e-02,   4.81873751e-02,   1.16070651e-05,\n",
       "          2.11304723e-04,   9.95886266e-01,   7.23218254e-05,\n",
       "          2.44005350e-03,   9.50191250e-08],\n",
       "       [  1.57324195e-01,   4.55876261e-01,   1.18607618e-01,\n",
       "          1.59025192e-02,   5.29134236e-02,   3.10803294e-01,\n",
       "          7.60788023e-02,   4.09732945e-02],\n",
       "       [  8.80629957e-01,   8.06344330e-01,   8.66507888e-01,\n",
       "          8.60296586e-06,   8.48201476e-03,   3.66728357e-03,\n",
       "          4.01142472e-03,   6.63176891e-09],\n",
       "       [  3.67009136e-07,   2.43742179e-04,   4.91571154e-05,\n",
       "          3.91778943e-16,   2.10585220e-08,   9.19317733e-03,\n",
       "          2.35959844e-04,   9.99994278e-01]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(training_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.07762730e-06,   4.37068702e-05,   8.50037267e-16,\n",
       "          3.26617112e-20,   5.99386426e-25,   9.99954700e-01,\n",
       "          4.03022398e-08,   5.06598099e-07],\n",
       "       [  7.84522370e-02,   7.51568982e-03,   3.02292941e-10,\n",
       "          1.27776785e-27,   8.58146071e-01,   5.19246235e-02,\n",
       "          3.96136660e-03,   9.01197339e-09],\n",
       "       [  8.13455582e-01,   3.37661535e-04,   1.86206713e-01,\n",
       "          2.93652289e-25,   1.42611062e-11,   8.11548398e-11,\n",
       "          1.56509191e-13,   3.87454735e-10],\n",
       "       [  9.48790824e-10,   4.76847440e-02,   2.83465356e-01,\n",
       "          2.84471139e-27,   8.79647612e-15,   5.65862817e-15,\n",
       "          4.19551939e-01,   2.49298021e-01],\n",
       "       [  1.14569569e-10,   1.01953819e-12,   1.14190914e-01,\n",
       "          4.46660243e-30,   3.54068044e-18,   8.85809124e-01,\n",
       "          1.19672533e-14,   2.22166152e-09]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(validation_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 1, 1],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 0]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        this.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width);\n",
       "        canvas.attr('height', height);\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'];\n",
       "    var y0 = fig.canvas.height - msg['y0'];\n",
       "    var x1 = msg['x1'];\n",
       "    var y1 = fig.canvas.height - msg['y1'];\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x;\n",
       "    var y = canvas_pos.y;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overriden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='529ae006-9318-4a66-885e-c1acfd8626d5'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-11-27 12:06:37,744 : INFO : ****************** Epoch 1 --- Working on doc2vec_size_3000_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_1 *******************\n",
      "2016-11-27 12:06:37,755 : INFO : training model with 12 workers on 243681 vocabulary and 51000 features, using sg=0 hs=0 sample=1e-05 negative=10\n",
      "2016-11-27 12:06:37,756 : INFO : expecting 49789 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-11-27 12:07:05,285 : INFO : PROGRESS: at 0.00% examples, 40 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 12:08:05,390 : INFO : PROGRESS: at 0.18% examples, 1894 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:09:05,994 : INFO : PROGRESS: at 0.37% examples, 2579 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:10:06,043 : INFO : PROGRESS: at 0.61% examples, 2862 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:11:06,643 : INFO : PROGRESS: at 0.83% examples, 2995 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:12:07,595 : INFO : PROGRESS: at 1.07% examples, 3100 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:13:08,002 : INFO : PROGRESS: at 1.29% examples, 3203 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:14:09,096 : INFO : PROGRESS: at 1.50% examples, 3232 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:15:09,907 : INFO : PROGRESS: at 1.71% examples, 3282 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:16:09,978 : INFO : PROGRESS: at 1.90% examples, 3288 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:17:10,530 : INFO : PROGRESS: at 2.13% examples, 3349 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:18:14,161 : INFO : PROGRESS: at 2.34% examples, 3345 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:19:14,616 : INFO : PROGRESS: at 2.61% examples, 3389 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:20:15,627 : INFO : PROGRESS: at 2.84% examples, 3394 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:21:15,663 : INFO : PROGRESS: at 3.08% examples, 3417 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 12:22:15,936 : INFO : PROGRESS: at 3.30% examples, 3423 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:23:16,886 : INFO : PROGRESS: at 3.57% examples, 3448 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:24:17,161 : INFO : PROGRESS: at 3.80% examples, 3465 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:25:17,872 : INFO : PROGRESS: at 4.04% examples, 3477 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:26:19,350 : INFO : PROGRESS: at 4.23% examples, 3478 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:27:19,567 : INFO : PROGRESS: at 4.46% examples, 3486 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 12:28:19,671 : INFO : PROGRESS: at 4.66% examples, 3494 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:29:20,065 : INFO : PROGRESS: at 4.91% examples, 3506 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 12:30:20,207 : INFO : PROGRESS: at 5.13% examples, 3509 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:31:20,227 : INFO : PROGRESS: at 5.37% examples, 3519 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 12:32:21,132 : INFO : PROGRESS: at 5.60% examples, 3519 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:33:21,471 : INFO : PROGRESS: at 5.83% examples, 3531 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:34:22,513 : INFO : PROGRESS: at 6.10% examples, 3534 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 12:35:23,635 : INFO : PROGRESS: at 6.32% examples, 3539 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:36:23,744 : INFO : PROGRESS: at 6.57% examples, 3544 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:37:25,751 : INFO : PROGRESS: at 6.81% examples, 3540 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:38:25,994 : INFO : PROGRESS: at 7.05% examples, 3554 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:39:26,711 : INFO : PROGRESS: at 7.26% examples, 3550 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:40:27,694 : INFO : PROGRESS: at 7.49% examples, 3556 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:41:27,976 : INFO : PROGRESS: at 7.73% examples, 3560 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:42:27,996 : INFO : PROGRESS: at 7.97% examples, 3568 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:43:28,208 : INFO : PROGRESS: at 8.21% examples, 3572 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:44:28,550 : INFO : PROGRESS: at 8.44% examples, 3575 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:45:28,867 : INFO : PROGRESS: at 8.65% examples, 3578 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:46:29,758 : INFO : PROGRESS: at 8.88% examples, 3582 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:47:30,399 : INFO : PROGRESS: at 9.11% examples, 3584 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:48:32,137 : INFO : PROGRESS: at 9.34% examples, 3589 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:49:32,723 : INFO : PROGRESS: at 9.58% examples, 3588 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:50:34,411 : INFO : PROGRESS: at 9.79% examples, 3587 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:51:36,420 : INFO : PROGRESS: at 9.99% examples, 3589 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:52:37,124 : INFO : PROGRESS: at 10.19% examples, 3599 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:53:37,138 : INFO : PROGRESS: at 10.40% examples, 3602 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:54:37,311 : INFO : PROGRESS: at 10.63% examples, 3607 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:55:39,995 : INFO : PROGRESS: at 10.87% examples, 3607 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:56:40,373 : INFO : PROGRESS: at 11.09% examples, 3611 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:57:41,091 : INFO : PROGRESS: at 11.33% examples, 3614 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:58:41,147 : INFO : PROGRESS: at 11.55% examples, 3616 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 12:59:42,045 : INFO : PROGRESS: at 11.79% examples, 3618 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:00:42,422 : INFO : PROGRESS: at 12.05% examples, 3619 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:01:42,940 : INFO : PROGRESS: at 12.28% examples, 3620 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:02:43,099 : INFO : PROGRESS: at 12.51% examples, 3626 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 13:03:43,389 : INFO : PROGRESS: at 12.76% examples, 3626 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 13:04:43,883 : INFO : PROGRESS: at 13.00% examples, 3629 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:05:44,325 : INFO : PROGRESS: at 13.24% examples, 3631 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:06:44,716 : INFO : PROGRESS: at 13.49% examples, 3634 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:07:45,235 : INFO : PROGRESS: at 13.71% examples, 3639 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:08:45,909 : INFO : PROGRESS: at 13.94% examples, 3642 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:09:45,937 : INFO : PROGRESS: at 14.17% examples, 3642 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:10:46,236 : INFO : PROGRESS: at 14.41% examples, 3645 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 13:11:46,816 : INFO : PROGRESS: at 14.64% examples, 3652 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:12:47,090 : INFO : PROGRESS: at 14.86% examples, 3652 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 13:13:48,900 : INFO : PROGRESS: at 15.11% examples, 3654 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:14:49,108 : INFO : PROGRESS: at 15.35% examples, 3655 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:15:50,781 : INFO : PROGRESS: at 15.57% examples, 3658 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:16:51,094 : INFO : PROGRESS: at 15.79% examples, 3662 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:17:51,147 : INFO : PROGRESS: at 15.98% examples, 3664 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:18:52,657 : INFO : PROGRESS: at 16.17% examples, 3663 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:19:53,850 : INFO : PROGRESS: at 16.41% examples, 3670 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:20:54,106 : INFO : PROGRESS: at 16.65% examples, 3670 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:21:54,118 : INFO : PROGRESS: at 16.88% examples, 3673 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:22:54,360 : INFO : PROGRESS: at 17.12% examples, 3672 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:23:55,945 : INFO : PROGRESS: at 17.33% examples, 3680 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:24:55,966 : INFO : PROGRESS: at 17.57% examples, 3678 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 13:25:56,032 : INFO : PROGRESS: at 17.81% examples, 3684 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:26:57,736 : INFO : PROGRESS: at 18.10% examples, 3683 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:27:58,649 : INFO : PROGRESS: at 18.37% examples, 3687 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:29:00,395 : INFO : PROGRESS: at 18.61% examples, 3686 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:30:00,548 : INFO : PROGRESS: at 18.84% examples, 3690 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:31:01,628 : INFO : PROGRESS: at 19.08% examples, 3692 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:32:02,692 : INFO : PROGRESS: at 19.34% examples, 3695 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:33:03,441 : INFO : PROGRESS: at 19.57% examples, 3696 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:34:03,603 : INFO : PROGRESS: at 19.81% examples, 3696 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:35:04,129 : INFO : PROGRESS: at 20.05% examples, 3698 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:36:04,342 : INFO : PROGRESS: at 20.30% examples, 3701 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:37:04,515 : INFO : PROGRESS: at 20.55% examples, 3705 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 13:38:04,606 : INFO : PROGRESS: at 20.77% examples, 3705 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 13:39:07,558 : INFO : PROGRESS: at 21.01% examples, 3707 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:40:07,760 : INFO : PROGRESS: at 21.25% examples, 3709 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 13:41:08,162 : INFO : PROGRESS: at 21.51% examples, 3711 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:42:08,511 : INFO : PROGRESS: at 21.74% examples, 3715 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:43:09,995 : INFO : PROGRESS: at 21.98% examples, 3715 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:44:12,443 : INFO : PROGRESS: at 22.22% examples, 3716 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:45:12,783 : INFO : PROGRESS: at 22.50% examples, 3719 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:46:12,856 : INFO : PROGRESS: at 22.72% examples, 3720 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:47:12,912 : INFO : PROGRESS: at 22.92% examples, 3721 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:48:13,354 : INFO : PROGRESS: at 23.15% examples, 3724 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:49:14,160 : INFO : PROGRESS: at 23.37% examples, 3726 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:50:14,577 : INFO : PROGRESS: at 23.61% examples, 3729 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:51:15,438 : INFO : PROGRESS: at 23.90% examples, 3729 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:52:15,861 : INFO : PROGRESS: at 24.15% examples, 3732 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:53:16,073 : INFO : PROGRESS: at 24.37% examples, 3733 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 13:54:16,314 : INFO : PROGRESS: at 24.61% examples, 3734 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:55:16,996 : INFO : PROGRESS: at 24.89% examples, 3735 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:56:17,309 : INFO : PROGRESS: at 25.11% examples, 3737 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:57:18,897 : INFO : PROGRESS: at 25.35% examples, 3738 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:58:19,271 : INFO : PROGRESS: at 25.59% examples, 3740 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 13:59:20,328 : INFO : PROGRESS: at 25.84% examples, 3742 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 14:00:20,444 : INFO : PROGRESS: at 26.08% examples, 3742 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:01:21,553 : INFO : PROGRESS: at 26.33% examples, 3743 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:02:21,901 : INFO : PROGRESS: at 26.55% examples, 3749 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:03:22,303 : INFO : PROGRESS: at 26.79% examples, 3749 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:04:22,445 : INFO : PROGRESS: at 27.06% examples, 3750 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 14:05:22,795 : INFO : PROGRESS: at 27.31% examples, 3750 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:06:23,435 : INFO : PROGRESS: at 27.56% examples, 3752 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:07:23,547 : INFO : PROGRESS: at 27.80% examples, 3752 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:08:23,668 : INFO : PROGRESS: at 28.07% examples, 3754 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:09:24,668 : INFO : PROGRESS: at 28.32% examples, 3754 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:10:25,781 : INFO : PROGRESS: at 28.54% examples, 3756 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:11:26,837 : INFO : PROGRESS: at 28.78% examples, 3758 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 14:12:28,205 : INFO : PROGRESS: at 28.99% examples, 3759 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:13:29,199 : INFO : PROGRESS: at 29.25% examples, 3760 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:14:30,036 : INFO : PROGRESS: at 29.49% examples, 3761 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:15:31,272 : INFO : PROGRESS: at 29.71% examples, 3762 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:16:32,173 : INFO : PROGRESS: at 29.92% examples, 3764 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:17:32,589 : INFO : PROGRESS: at 30.16% examples, 3761 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:18:32,648 : INFO : PROGRESS: at 30.44% examples, 3766 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:19:33,214 : INFO : PROGRESS: at 30.66% examples, 3765 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:20:33,301 : INFO : PROGRESS: at 30.88% examples, 3766 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:21:33,306 : INFO : PROGRESS: at 31.12% examples, 3769 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:22:33,772 : INFO : PROGRESS: at 31.36% examples, 3769 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:23:33,884 : INFO : PROGRESS: at 31.60% examples, 3771 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 14:24:33,907 : INFO : PROGRESS: at 31.83% examples, 3772 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:25:34,438 : INFO : PROGRESS: at 32.08% examples, 3770 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:26:35,310 : INFO : PROGRESS: at 32.30% examples, 3771 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 14:27:36,287 : INFO : PROGRESS: at 32.55% examples, 3776 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:28:36,659 : INFO : PROGRESS: at 32.81% examples, 3776 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:29:38,235 : INFO : PROGRESS: at 33.05% examples, 3778 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:30:39,040 : INFO : PROGRESS: at 33.29% examples, 3777 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:31:39,149 : INFO : PROGRESS: at 33.53% examples, 3778 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:32:39,746 : INFO : PROGRESS: at 33.73% examples, 3780 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:33:40,393 : INFO : PROGRESS: at 33.97% examples, 3781 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:34:41,110 : INFO : PROGRESS: at 34.20% examples, 3782 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:35:41,428 : INFO : PROGRESS: at 34.46% examples, 3784 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:36:41,802 : INFO : PROGRESS: at 34.72% examples, 3785 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:37:41,945 : INFO : PROGRESS: at 34.97% examples, 3787 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:38:42,424 : INFO : PROGRESS: at 35.21% examples, 3788 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:39:42,632 : INFO : PROGRESS: at 35.44% examples, 3789 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:40:42,650 : INFO : PROGRESS: at 35.67% examples, 3788 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:41:42,729 : INFO : PROGRESS: at 35.90% examples, 3792 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:42:43,062 : INFO : PROGRESS: at 36.11% examples, 3792 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:43:44,539 : INFO : PROGRESS: at 36.36% examples, 3793 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:44:45,034 : INFO : PROGRESS: at 36.61% examples, 3795 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:45:45,860 : INFO : PROGRESS: at 36.85% examples, 3794 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:46:46,532 : INFO : PROGRESS: at 37.07% examples, 3795 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:47:47,010 : INFO : PROGRESS: at 37.32% examples, 3798 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:48:47,158 : INFO : PROGRESS: at 37.57% examples, 3799 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:49:47,192 : INFO : PROGRESS: at 37.81% examples, 3798 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:50:47,381 : INFO : PROGRESS: at 38.04% examples, 3799 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:51:47,770 : INFO : PROGRESS: at 38.25% examples, 3802 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:52:48,094 : INFO : PROGRESS: at 38.48% examples, 3803 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:53:48,834 : INFO : PROGRESS: at 38.73% examples, 3803 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:54:49,413 : INFO : PROGRESS: at 38.96% examples, 3804 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:55:49,470 : INFO : PROGRESS: at 39.22% examples, 3805 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:56:49,600 : INFO : PROGRESS: at 39.44% examples, 3806 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:57:50,476 : INFO : PROGRESS: at 39.71% examples, 3807 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:58:50,657 : INFO : PROGRESS: at 39.94% examples, 3807 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 14:59:51,537 : INFO : PROGRESS: at 40.19% examples, 3809 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:00:52,562 : INFO : PROGRESS: at 40.42% examples, 3808 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:01:53,476 : INFO : PROGRESS: at 40.64% examples, 3810 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:02:55,361 : INFO : PROGRESS: at 40.90% examples, 3809 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 15:03:55,573 : INFO : PROGRESS: at 41.15% examples, 3811 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:04:56,214 : INFO : PROGRESS: at 41.39% examples, 3811 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:05:57,202 : INFO : PROGRESS: at 41.65% examples, 3813 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:06:58,459 : INFO : PROGRESS: at 41.89% examples, 3812 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:07:59,897 : INFO : PROGRESS: at 42.13% examples, 3812 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:09:00,216 : INFO : PROGRESS: at 42.38% examples, 3815 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:10:01,291 : INFO : PROGRESS: at 42.60% examples, 3814 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:11:01,798 : INFO : PROGRESS: at 42.84% examples, 3816 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:12:02,237 : INFO : PROGRESS: at 43.10% examples, 3815 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:13:02,729 : INFO : PROGRESS: at 43.38% examples, 3817 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:14:02,965 : INFO : PROGRESS: at 43.61% examples, 3817 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:15:03,094 : INFO : PROGRESS: at 43.81% examples, 3818 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 15:16:04,555 : INFO : PROGRESS: at 44.07% examples, 3819 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:17:05,176 : INFO : PROGRESS: at 44.31% examples, 3820 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:18:05,499 : INFO : PROGRESS: at 44.57% examples, 3820 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:19:05,809 : INFO : PROGRESS: at 44.80% examples, 3822 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:20:06,080 : INFO : PROGRESS: at 45.02% examples, 3822 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:21:07,820 : INFO : PROGRESS: at 45.25% examples, 3822 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 15:22:08,802 : INFO : PROGRESS: at 45.46% examples, 3823 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:23:09,905 : INFO : PROGRESS: at 45.72% examples, 3825 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:24:11,410 : INFO : PROGRESS: at 45.93% examples, 3825 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:25:12,732 : INFO : PROGRESS: at 46.17% examples, 3826 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:26:13,796 : INFO : PROGRESS: at 46.41% examples, 3825 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:27:14,710 : INFO : PROGRESS: at 46.65% examples, 3825 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:28:15,152 : INFO : PROGRESS: at 46.89% examples, 3826 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:29:15,300 : INFO : PROGRESS: at 47.15% examples, 3827 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:30:15,568 : INFO : PROGRESS: at 47.39% examples, 3827 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:31:15,937 : INFO : PROGRESS: at 47.61% examples, 3828 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:32:15,997 : INFO : PROGRESS: at 47.84% examples, 3829 words/s, in_qsize 22, out_qsize 0\n",
      "2016-11-27 15:33:16,769 : INFO : PROGRESS: at 48.08% examples, 3830 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:34:17,608 : INFO : PROGRESS: at 48.32% examples, 3830 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:35:17,871 : INFO : PROGRESS: at 48.52% examples, 3832 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 15:36:18,551 : INFO : PROGRESS: at 48.78% examples, 3833 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:37:19,257 : INFO : PROGRESS: at 49.06% examples, 3834 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:38:19,846 : INFO : PROGRESS: at 49.31% examples, 3834 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:39:19,872 : INFO : PROGRESS: at 49.56% examples, 3834 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:40:20,518 : INFO : PROGRESS: at 49.81% examples, 3835 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:41:21,740 : INFO : PROGRESS: at 50.05% examples, 3834 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:42:21,806 : INFO : PROGRESS: at 50.27% examples, 3835 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:43:21,861 : INFO : PROGRESS: at 50.51% examples, 3836 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:44:21,950 : INFO : PROGRESS: at 50.75% examples, 3837 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:45:22,252 : INFO : PROGRESS: at 51.00% examples, 3838 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:46:22,377 : INFO : PROGRESS: at 51.22% examples, 3839 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:47:22,727 : INFO : PROGRESS: at 51.46% examples, 3840 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 15:48:22,741 : INFO : PROGRESS: at 51.67% examples, 3841 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 15:49:23,232 : INFO : PROGRESS: at 51.91% examples, 3840 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 15:50:23,718 : INFO : PROGRESS: at 52.15% examples, 3841 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:51:23,847 : INFO : PROGRESS: at 52.40% examples, 3840 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:52:24,316 : INFO : PROGRESS: at 52.68% examples, 3842 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:53:24,843 : INFO : PROGRESS: at 52.90% examples, 3842 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:54:25,804 : INFO : PROGRESS: at 53.14% examples, 3843 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:55:26,238 : INFO : PROGRESS: at 53.34% examples, 3842 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:56:26,612 : INFO : PROGRESS: at 53.61% examples, 3843 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:57:26,654 : INFO : PROGRESS: at 53.85% examples, 3844 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:58:27,203 : INFO : PROGRESS: at 54.10% examples, 3844 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 15:59:27,465 : INFO : PROGRESS: at 54.34% examples, 3845 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:00:27,535 : INFO : PROGRESS: at 54.58% examples, 3846 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 16:01:27,717 : INFO : PROGRESS: at 54.80% examples, 3846 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:02:28,146 : INFO : PROGRESS: at 55.04% examples, 3846 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:03:28,536 : INFO : PROGRESS: at 55.28% examples, 3847 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:04:28,978 : INFO : PROGRESS: at 55.56% examples, 3847 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 16:05:29,040 : INFO : PROGRESS: at 55.77% examples, 3848 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 16:06:29,192 : INFO : PROGRESS: at 56.01% examples, 3849 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:07:29,843 : INFO : PROGRESS: at 56.24% examples, 3849 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:08:30,027 : INFO : PROGRESS: at 56.50% examples, 3850 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:09:30,213 : INFO : PROGRESS: at 56.75% examples, 3850 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:10:31,597 : INFO : PROGRESS: at 57.01% examples, 3850 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:11:31,816 : INFO : PROGRESS: at 57.29% examples, 3850 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:12:31,941 : INFO : PROGRESS: at 57.55% examples, 3851 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 16:13:32,174 : INFO : PROGRESS: at 57.77% examples, 3851 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:14:32,200 : INFO : PROGRESS: at 58.00% examples, 3852 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:15:32,302 : INFO : PROGRESS: at 58.22% examples, 3852 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:16:32,923 : INFO : PROGRESS: at 58.46% examples, 3852 words/s, in_qsize 20, out_qsize 0\n",
      "2016-11-27 16:17:33,586 : INFO : PROGRESS: at 58.71% examples, 3853 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:18:34,611 : INFO : PROGRESS: at 58.97% examples, 3853 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:19:35,200 : INFO : PROGRESS: at 59.21% examples, 3853 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:20:35,371 : INFO : PROGRESS: at 59.41% examples, 3853 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:21:35,707 : INFO : PROGRESS: at 59.66% examples, 3854 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:22:35,715 : INFO : PROGRESS: at 59.93% examples, 3853 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:23:35,949 : INFO : PROGRESS: at 60.15% examples, 3853 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 16:24:36,451 : INFO : PROGRESS: at 60.38% examples, 3854 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:25:36,661 : INFO : PROGRESS: at 60.65% examples, 3853 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 16:26:37,261 : INFO : PROGRESS: at 60.87% examples, 3855 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:27:37,316 : INFO : PROGRESS: at 61.10% examples, 3854 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:28:37,458 : INFO : PROGRESS: at 61.36% examples, 3855 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 16:29:38,067 : INFO : PROGRESS: at 61.60% examples, 3855 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:30:39,788 : INFO : PROGRESS: at 61.86% examples, 3855 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:31:41,442 : INFO : PROGRESS: at 62.08% examples, 3855 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:32:41,788 : INFO : PROGRESS: at 62.29% examples, 3856 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:33:41,878 : INFO : PROGRESS: at 62.53% examples, 3856 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:34:42,541 : INFO : PROGRESS: at 62.78% examples, 3856 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:35:42,541 : INFO : PROGRESS: at 63.02% examples, 3856 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:36:43,306 : INFO : PROGRESS: at 63.27% examples, 3856 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:37:43,331 : INFO : PROGRESS: at 63.50% examples, 3856 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:38:43,466 : INFO : PROGRESS: at 63.73% examples, 3856 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:39:44,227 : INFO : PROGRESS: at 63.95% examples, 3857 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:40:44,430 : INFO : PROGRESS: at 64.18% examples, 3857 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:41:47,124 : INFO : PROGRESS: at 64.43% examples, 3857 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:42:47,589 : INFO : PROGRESS: at 64.67% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:43:47,628 : INFO : PROGRESS: at 64.94% examples, 3857 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:44:47,669 : INFO : PROGRESS: at 65.17% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:45:47,680 : INFO : PROGRESS: at 65.41% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:46:47,687 : INFO : PROGRESS: at 65.62% examples, 3857 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:47:49,221 : INFO : PROGRESS: at 65.85% examples, 3857 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:48:49,277 : INFO : PROGRESS: at 66.08% examples, 3857 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:49:50,278 : INFO : PROGRESS: at 66.34% examples, 3857 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:50:51,221 : INFO : PROGRESS: at 66.58% examples, 3857 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:51:52,038 : INFO : PROGRESS: at 66.81% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:52:52,907 : INFO : PROGRESS: at 67.06% examples, 3858 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 16:53:53,344 : INFO : PROGRESS: at 67.31% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:54:54,263 : INFO : PROGRESS: at 67.55% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:55:54,376 : INFO : PROGRESS: at 67.80% examples, 3858 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 16:56:55,078 : INFO : PROGRESS: at 68.01% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:57:55,290 : INFO : PROGRESS: at 68.25% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:58:55,506 : INFO : PROGRESS: at 68.50% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 16:59:55,739 : INFO : PROGRESS: at 68.73% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:00:56,353 : INFO : PROGRESS: at 68.97% examples, 3859 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:01:57,413 : INFO : PROGRESS: at 69.21% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:02:59,446 : INFO : PROGRESS: at 69.45% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:04:00,869 : INFO : PROGRESS: at 69.68% examples, 3859 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:05:01,259 : INFO : PROGRESS: at 69.93% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:06:03,301 : INFO : PROGRESS: at 70.18% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:07:05,741 : INFO : PROGRESS: at 70.43% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:08:06,579 : INFO : PROGRESS: at 70.70% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:09:07,149 : INFO : PROGRESS: at 70.96% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:10:07,340 : INFO : PROGRESS: at 71.19% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:11:08,053 : INFO : PROGRESS: at 71.44% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:12:08,346 : INFO : PROGRESS: at 71.65% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:13:08,403 : INFO : PROGRESS: at 71.88% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:14:08,715 : INFO : PROGRESS: at 72.11% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:15:09,343 : INFO : PROGRESS: at 72.35% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:16:09,701 : INFO : PROGRESS: at 72.58% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:17:09,770 : INFO : PROGRESS: at 72.83% examples, 3857 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:18:09,948 : INFO : PROGRESS: at 73.07% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:19:10,260 : INFO : PROGRESS: at 73.27% examples, 3857 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:20:10,457 : INFO : PROGRESS: at 73.51% examples, 3857 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:21:12,006 : INFO : PROGRESS: at 73.75% examples, 3857 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 17:22:12,423 : INFO : PROGRESS: at 74.02% examples, 3858 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:23:12,864 : INFO : PROGRESS: at 74.28% examples, 3857 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:24:13,619 : INFO : PROGRESS: at 74.50% examples, 3857 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:25:13,633 : INFO : PROGRESS: at 74.74% examples, 3856 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 17:26:13,792 : INFO : PROGRESS: at 74.93% examples, 3857 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:27:15,185 : INFO : PROGRESS: at 75.16% examples, 3857 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:28:16,374 : INFO : PROGRESS: at 75.38% examples, 3857 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:29:17,040 : INFO : PROGRESS: at 75.61% examples, 3856 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:30:18,028 : INFO : PROGRESS: at 75.83% examples, 3856 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:31:18,620 : INFO : PROGRESS: at 76.07% examples, 3857 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:32:19,137 : INFO : PROGRESS: at 76.31% examples, 3856 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:33:19,177 : INFO : PROGRESS: at 76.56% examples, 3857 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 17:34:20,893 : INFO : PROGRESS: at 76.81% examples, 3856 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:35:21,113 : INFO : PROGRESS: at 77.04% examples, 3856 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:36:21,423 : INFO : PROGRESS: at 77.29% examples, 3856 words/s, in_qsize 22, out_qsize 0\n",
      "2016-11-27 17:37:23,028 : INFO : PROGRESS: at 77.53% examples, 3856 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 17:38:23,085 : INFO : PROGRESS: at 77.76% examples, 3856 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:39:23,474 : INFO : PROGRESS: at 78.02% examples, 3856 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:40:24,945 : INFO : PROGRESS: at 78.22% examples, 3855 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:41:25,386 : INFO : PROGRESS: at 78.47% examples, 3856 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:42:25,656 : INFO : PROGRESS: at 78.72% examples, 3856 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:43:25,679 : INFO : PROGRESS: at 78.97% examples, 3856 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:44:25,724 : INFO : PROGRESS: at 79.17% examples, 3855 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:45:26,503 : INFO : PROGRESS: at 79.39% examples, 3856 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:46:26,751 : INFO : PROGRESS: at 79.63% examples, 3855 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:47:27,402 : INFO : PROGRESS: at 79.87% examples, 3855 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:48:27,790 : INFO : PROGRESS: at 80.12% examples, 3855 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:49:27,908 : INFO : PROGRESS: at 80.33% examples, 3855 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:50:30,324 : INFO : PROGRESS: at 80.54% examples, 3854 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:51:30,580 : INFO : PROGRESS: at 80.80% examples, 3855 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:52:30,974 : INFO : PROGRESS: at 80.99% examples, 3855 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 17:53:31,603 : INFO : PROGRESS: at 81.21% examples, 3855 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:54:31,727 : INFO : PROGRESS: at 81.46% examples, 3855 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 17:55:31,852 : INFO : PROGRESS: at 81.68% examples, 3855 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:56:33,918 : INFO : PROGRESS: at 81.94% examples, 3855 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:57:35,239 : INFO : PROGRESS: at 82.18% examples, 3855 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:58:35,671 : INFO : PROGRESS: at 82.41% examples, 3854 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 17:59:36,050 : INFO : PROGRESS: at 82.65% examples, 3853 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:00:36,300 : INFO : PROGRESS: at 82.89% examples, 3854 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:01:38,453 : INFO : PROGRESS: at 83.13% examples, 3853 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:02:39,104 : INFO : PROGRESS: at 83.38% examples, 3853 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:03:39,910 : INFO : PROGRESS: at 83.59% examples, 3853 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:04:41,486 : INFO : PROGRESS: at 83.82% examples, 3853 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:05:41,765 : INFO : PROGRESS: at 84.03% examples, 3853 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:06:42,376 : INFO : PROGRESS: at 84.29% examples, 3853 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:07:42,776 : INFO : PROGRESS: at 84.52% examples, 3853 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 18:08:42,903 : INFO : PROGRESS: at 84.79% examples, 3853 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:09:44,183 : INFO : PROGRESS: at 85.01% examples, 3852 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:10:44,538 : INFO : PROGRESS: at 85.25% examples, 3852 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:11:45,248 : INFO : PROGRESS: at 85.51% examples, 3852 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:12:45,373 : INFO : PROGRESS: at 85.74% examples, 3852 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:13:46,019 : INFO : PROGRESS: at 85.96% examples, 3852 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:14:46,761 : INFO : PROGRESS: at 86.20% examples, 3851 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:15:46,898 : INFO : PROGRESS: at 86.42% examples, 3852 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:16:46,987 : INFO : PROGRESS: at 86.65% examples, 3851 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 18:17:47,811 : INFO : PROGRESS: at 86.86% examples, 3851 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 18:18:47,842 : INFO : PROGRESS: at 87.10% examples, 3851 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:19:47,897 : INFO : PROGRESS: at 87.33% examples, 3851 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:20:48,647 : INFO : PROGRESS: at 87.56% examples, 3851 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:21:48,829 : INFO : PROGRESS: at 87.80% examples, 3850 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:22:48,845 : INFO : PROGRESS: at 88.03% examples, 3850 words/s, in_qsize 22, out_qsize 0\n",
      "2016-11-27 18:23:50,486 : INFO : PROGRESS: at 88.25% examples, 3850 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:24:50,848 : INFO : PROGRESS: at 88.46% examples, 3850 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 18:25:50,931 : INFO : PROGRESS: at 88.69% examples, 3849 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:26:51,107 : INFO : PROGRESS: at 88.91% examples, 3849 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:27:51,168 : INFO : PROGRESS: at 89.13% examples, 3849 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:28:51,448 : INFO : PROGRESS: at 89.38% examples, 3848 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 18:29:51,753 : INFO : PROGRESS: at 89.58% examples, 3848 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:30:52,853 : INFO : PROGRESS: at 89.83% examples, 3847 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:31:52,974 : INFO : PROGRESS: at 90.07% examples, 3847 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 18:32:54,092 : INFO : PROGRESS: at 90.28% examples, 3846 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:33:54,281 : INFO : PROGRESS: at 90.51% examples, 3846 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:34:55,199 : INFO : PROGRESS: at 90.72% examples, 3846 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 18:35:56,721 : INFO : PROGRESS: at 90.91% examples, 3845 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 18:36:57,442 : INFO : PROGRESS: at 91.12% examples, 3845 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:37:59,098 : INFO : PROGRESS: at 91.34% examples, 3845 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:38:59,783 : INFO : PROGRESS: at 91.56% examples, 3844 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:39:59,822 : INFO : PROGRESS: at 91.80% examples, 3844 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 18:40:59,917 : INFO : PROGRESS: at 92.01% examples, 3844 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:42:01,200 : INFO : PROGRESS: at 92.23% examples, 3843 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 18:43:02,262 : INFO : PROGRESS: at 92.44% examples, 3842 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:44:02,292 : INFO : PROGRESS: at 92.64% examples, 3842 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:45:02,311 : INFO : PROGRESS: at 92.91% examples, 3842 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:46:02,583 : INFO : PROGRESS: at 93.14% examples, 3842 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:47:02,596 : INFO : PROGRESS: at 93.39% examples, 3841 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:48:03,333 : INFO : PROGRESS: at 93.61% examples, 3841 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:49:03,370 : INFO : PROGRESS: at 93.86% examples, 3840 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:50:05,308 : INFO : PROGRESS: at 94.11% examples, 3840 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:51:05,472 : INFO : PROGRESS: at 94.34% examples, 3839 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:52:05,722 : INFO : PROGRESS: at 94.57% examples, 3839 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:53:05,875 : INFO : PROGRESS: at 94.77% examples, 3839 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 18:54:06,951 : INFO : PROGRESS: at 94.99% examples, 3838 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:55:07,928 : INFO : PROGRESS: at 95.22% examples, 3838 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:56:08,759 : INFO : PROGRESS: at 95.41% examples, 3837 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 18:57:08,926 : INFO : PROGRESS: at 95.64% examples, 3837 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:58:09,117 : INFO : PROGRESS: at 95.84% examples, 3836 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 18:59:10,087 : INFO : PROGRESS: at 96.07% examples, 3836 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:00:10,102 : INFO : PROGRESS: at 96.29% examples, 3835 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 19:01:10,634 : INFO : PROGRESS: at 96.48% examples, 3834 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:02:11,459 : INFO : PROGRESS: at 96.72% examples, 3835 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:03:11,719 : INFO : PROGRESS: at 96.94% examples, 3834 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:04:11,985 : INFO : PROGRESS: at 97.17% examples, 3834 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:05:12,982 : INFO : PROGRESS: at 97.39% examples, 3834 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:06:13,099 : INFO : PROGRESS: at 97.66% examples, 3833 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:07:13,105 : INFO : PROGRESS: at 97.86% examples, 3832 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:08:13,454 : INFO : PROGRESS: at 98.11% examples, 3832 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:09:13,568 : INFO : PROGRESS: at 98.32% examples, 3832 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:10:13,978 : INFO : PROGRESS: at 98.53% examples, 3831 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:11:14,467 : INFO : PROGRESS: at 98.75% examples, 3830 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:12:14,471 : INFO : PROGRESS: at 98.99% examples, 3830 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:13:15,543 : INFO : PROGRESS: at 99.21% examples, 3830 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:14:15,651 : INFO : PROGRESS: at 99.42% examples, 3829 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:15:15,673 : INFO : PROGRESS: at 99.66% examples, 3829 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:16:16,457 : INFO : PROGRESS: at 99.87% examples, 3828 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:16:44,065 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2016-11-27 19:16:44,870 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2016-11-27 19:16:45,467 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2016-11-27 19:16:46,775 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2016-11-27 19:16:46,851 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2016-11-27 19:16:46,951 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2016-11-27 19:16:47,279 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2016-11-27 19:16:47,761 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2016-11-27 19:16:48,298 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2016-11-27 19:16:48,667 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-11-27 19:16:48,896 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-11-27 19:16:49,649 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-11-27 19:16:49,651 : INFO : training on 390507860 raw words (98830815 effective words) took 25811.9s, 3829 effective words/s\n",
      "2016-11-27 19:16:49,652 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models/sample_0.01/doc2vec_size_3000_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_1/model, separately None\n",
      "2016-11-27 19:16:49,654 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models/sample_0.01/doc2vec_size_3000_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_1/model.docvecs.doctag_syn0.npy\n",
      "2016-11-27 19:16:50,043 : INFO : storing numpy array 'syn1neg' to /mnt/data2/shalaby/parameter_search_doc2vec_models/sample_0.01/doc2vec_size_3000_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_1/model.syn1neg.npy\n",
      "2016-11-27 19:18:13,911 : INFO : not storing attribute syn0norm\n",
      "2016-11-27 19:18:13,912 : INFO : storing numpy array 'syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models/sample_0.01/doc2vec_size_3000_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_1/model.syn0.npy\n",
      "2016-11-27 19:18:19,424 : INFO : not storing attribute cum_table\n",
      "2016-11-27 19:18:27,517 : INFO : ****************** Epoch 2 --- Working on doc2vec_size_3000_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_2 *******************\n",
      "2016-11-27 19:18:27,526 : INFO : training model with 12 workers on 243681 vocabulary and 51000 features, using sg=0 hs=0 sample=1e-05 negative=10\n",
      "2016-11-27 19:18:27,527 : INFO : expecting 49789 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-11-27 19:18:30,758 : INFO : PROGRESS: at 0.00% examples, 670 words/s, in_qsize 2, out_qsize 0\n",
      "2016-11-27 19:19:31,559 : INFO : PROGRESS: at 0.27% examples, 3909 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:20:31,739 : INFO : PROGRESS: at 0.52% examples, 4063 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:21:31,795 : INFO : PROGRESS: at 0.78% examples, 4211 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:22:32,265 : INFO : PROGRESS: at 1.05% examples, 4206 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:23:32,379 : INFO : PROGRESS: at 1.30% examples, 4237 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:24:32,547 : INFO : PROGRESS: at 1.55% examples, 4284 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 19:25:32,624 : INFO : PROGRESS: at 1.85% examples, 4297 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:26:33,040 : INFO : PROGRESS: at 2.11% examples, 4291 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:27:34,585 : INFO : PROGRESS: at 2.40% examples, 4330 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:28:34,900 : INFO : PROGRESS: at 2.66% examples, 4342 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:29:35,379 : INFO : PROGRESS: at 2.93% examples, 4355 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:30:35,647 : INFO : PROGRESS: at 3.19% examples, 4338 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:31:36,400 : INFO : PROGRESS: at 3.44% examples, 4364 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:32:36,517 : INFO : PROGRESS: at 3.75% examples, 4390 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:33:36,765 : INFO : PROGRESS: at 4.07% examples, 4401 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:34:36,920 : INFO : PROGRESS: at 4.37% examples, 4399 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:35:37,033 : INFO : PROGRESS: at 4.65% examples, 4414 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:36:37,544 : INFO : PROGRESS: at 4.92% examples, 4413 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:37:39,340 : INFO : PROGRESS: at 5.21% examples, 4442 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:38:39,396 : INFO : PROGRESS: at 5.46% examples, 4445 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:39:39,440 : INFO : PROGRESS: at 5.77% examples, 4469 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:40:39,934 : INFO : PROGRESS: at 6.07% examples, 4472 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:41:40,065 : INFO : PROGRESS: at 6.35% examples, 4486 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:42:41,252 : INFO : PROGRESS: at 6.66% examples, 4485 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:43:41,969 : INFO : PROGRESS: at 6.94% examples, 4499 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:44:42,204 : INFO : PROGRESS: at 7.20% examples, 4513 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 19:45:42,909 : INFO : PROGRESS: at 7.46% examples, 4518 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:46:43,340 : INFO : PROGRESS: at 7.77% examples, 4527 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 19:47:45,937 : INFO : PROGRESS: at 8.04% examples, 4530 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:48:46,229 : INFO : PROGRESS: at 8.30% examples, 4538 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:49:46,284 : INFO : PROGRESS: at 8.57% examples, 4551 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 19:50:48,122 : INFO : PROGRESS: at 8.90% examples, 4554 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:51:48,272 : INFO : PROGRESS: at 9.19% examples, 4562 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:52:48,459 : INFO : PROGRESS: at 9.47% examples, 4566 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 19:53:48,875 : INFO : PROGRESS: at 9.78% examples, 4583 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:54:49,025 : INFO : PROGRESS: at 10.06% examples, 4589 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:55:49,470 : INFO : PROGRESS: at 10.35% examples, 4593 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 19:56:50,296 : INFO : PROGRESS: at 10.65% examples, 4601 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:57:51,032 : INFO : PROGRESS: at 10.94% examples, 4605 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:58:51,285 : INFO : PROGRESS: at 11.25% examples, 4614 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 19:59:51,754 : INFO : PROGRESS: at 11.56% examples, 4620 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:00:51,874 : INFO : PROGRESS: at 11.86% examples, 4631 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:01:52,014 : INFO : PROGRESS: at 12.18% examples, 4640 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 20:02:52,564 : INFO : PROGRESS: at 12.47% examples, 4639 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:03:52,904 : INFO : PROGRESS: at 12.79% examples, 4650 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:04:53,246 : INFO : PROGRESS: at 13.10% examples, 4654 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:05:53,455 : INFO : PROGRESS: at 13.39% examples, 4664 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:06:53,934 : INFO : PROGRESS: at 13.68% examples, 4666 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:07:54,607 : INFO : PROGRESS: at 13.96% examples, 4673 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:08:55,063 : INFO : PROGRESS: at 14.27% examples, 4675 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:09:55,757 : INFO : PROGRESS: at 14.58% examples, 4684 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:10:57,225 : INFO : PROGRESS: at 14.87% examples, 4684 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:11:57,498 : INFO : PROGRESS: at 15.20% examples, 4693 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:12:57,937 : INFO : PROGRESS: at 15.53% examples, 4699 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:13:58,220 : INFO : PROGRESS: at 15.84% examples, 4705 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:14:58,854 : INFO : PROGRESS: at 16.15% examples, 4711 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 20:15:59,174 : INFO : PROGRESS: at 16.45% examples, 4720 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:16:59,188 : INFO : PROGRESS: at 16.78% examples, 4724 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:17:59,987 : INFO : PROGRESS: at 17.08% examples, 4730 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:19:00,127 : INFO : PROGRESS: at 17.36% examples, 4733 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:20:00,206 : INFO : PROGRESS: at 17.66% examples, 4743 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:21:00,225 : INFO : PROGRESS: at 18.00% examples, 4744 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:22:00,645 : INFO : PROGRESS: at 18.26% examples, 4748 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:23:01,336 : INFO : PROGRESS: at 18.58% examples, 4757 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:24:01,439 : INFO : PROGRESS: at 18.89% examples, 4763 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:25:01,622 : INFO : PROGRESS: at 19.21% examples, 4769 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:26:02,045 : INFO : PROGRESS: at 19.52% examples, 4775 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:27:02,147 : INFO : PROGRESS: at 19.89% examples, 4779 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:28:02,507 : INFO : PROGRESS: at 20.20% examples, 4785 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:29:04,324 : INFO : PROGRESS: at 20.53% examples, 4786 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:30:05,600 : INFO : PROGRESS: at 20.83% examples, 4794 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:31:06,919 : INFO : PROGRESS: at 21.10% examples, 4794 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 20:32:06,982 : INFO : PROGRESS: at 21.39% examples, 4792 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 20:33:07,113 : INFO : PROGRESS: at 21.66% examples, 4789 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:34:08,404 : INFO : PROGRESS: at 21.95% examples, 4784 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:35:09,226 : INFO : PROGRESS: at 22.24% examples, 4780 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:36:09,463 : INFO : PROGRESS: at 22.52% examples, 4779 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:37:09,529 : INFO : PROGRESS: at 22.81% examples, 4776 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:38:10,189 : INFO : PROGRESS: at 23.08% examples, 4772 words/s, in_qsize 18, out_qsize 0\n",
      "2016-11-27 20:39:10,764 : INFO : PROGRESS: at 23.33% examples, 4771 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:40:11,025 : INFO : PROGRESS: at 23.64% examples, 4770 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:41:11,805 : INFO : PROGRESS: at 23.90% examples, 4767 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:42:11,900 : INFO : PROGRESS: at 24.17% examples, 4761 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:43:11,985 : INFO : PROGRESS: at 24.46% examples, 4758 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:44:12,403 : INFO : PROGRESS: at 24.77% examples, 4757 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:45:12,859 : INFO : PROGRESS: at 25.06% examples, 4752 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:46:14,003 : INFO : PROGRESS: at 25.34% examples, 4750 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 20:47:14,018 : INFO : PROGRESS: at 25.63% examples, 4751 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 20:48:14,653 : INFO : PROGRESS: at 25.95% examples, 4749 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:49:14,977 : INFO : PROGRESS: at 26.22% examples, 4750 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:50:14,993 : INFO : PROGRESS: at 26.50% examples, 4749 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:51:15,068 : INFO : PROGRESS: at 26.78% examples, 4747 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:52:16,106 : INFO : PROGRESS: at 27.06% examples, 4743 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:53:16,485 : INFO : PROGRESS: at 27.35% examples, 4743 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:54:17,102 : INFO : PROGRESS: at 27.65% examples, 4741 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:55:17,623 : INFO : PROGRESS: at 27.92% examples, 4740 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:56:17,686 : INFO : PROGRESS: at 28.18% examples, 4739 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 20:57:18,534 : INFO : PROGRESS: at 28.47% examples, 4736 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 20:58:18,743 : INFO : PROGRESS: at 28.74% examples, 4735 words/s, in_qsize 22, out_qsize 0\n",
      "2016-11-27 20:59:19,727 : INFO : PROGRESS: at 29.02% examples, 4733 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:00:20,022 : INFO : PROGRESS: at 29.28% examples, 4731 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:01:20,325 : INFO : PROGRESS: at 29.59% examples, 4730 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:02:20,680 : INFO : PROGRESS: at 29.91% examples, 4729 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:03:21,121 : INFO : PROGRESS: at 30.17% examples, 4728 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:04:21,494 : INFO : PROGRESS: at 30.47% examples, 4727 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:05:21,973 : INFO : PROGRESS: at 30.73% examples, 4727 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:06:22,037 : INFO : PROGRESS: at 31.00% examples, 4727 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 21:07:22,133 : INFO : PROGRESS: at 31.29% examples, 4725 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:08:22,518 : INFO : PROGRESS: at 31.60% examples, 4727 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:09:22,534 : INFO : PROGRESS: at 31.88% examples, 4726 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:10:23,429 : INFO : PROGRESS: at 32.13% examples, 4726 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:11:23,597 : INFO : PROGRESS: at 32.38% examples, 4727 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 21:12:23,773 : INFO : PROGRESS: at 32.66% examples, 4725 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:13:23,783 : INFO : PROGRESS: at 32.96% examples, 4725 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:14:23,802 : INFO : PROGRESS: at 33.24% examples, 4724 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 21:15:24,876 : INFO : PROGRESS: at 33.52% examples, 4724 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:16:25,391 : INFO : PROGRESS: at 33.83% examples, 4724 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:17:25,601 : INFO : PROGRESS: at 34.10% examples, 4724 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:18:25,870 : INFO : PROGRESS: at 34.38% examples, 4725 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:19:26,112 : INFO : PROGRESS: at 34.70% examples, 4724 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:20:27,197 : INFO : PROGRESS: at 35.01% examples, 4725 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:21:27,908 : INFO : PROGRESS: at 35.31% examples, 4725 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 21:22:28,027 : INFO : PROGRESS: at 35.60% examples, 4723 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:23:29,967 : INFO : PROGRESS: at 35.90% examples, 4723 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:24:30,506 : INFO : PROGRESS: at 36.18% examples, 4724 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:25:30,658 : INFO : PROGRESS: at 36.47% examples, 4724 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:26:30,912 : INFO : PROGRESS: at 36.77% examples, 4724 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:27:31,019 : INFO : PROGRESS: at 37.06% examples, 4723 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:28:31,104 : INFO : PROGRESS: at 37.34% examples, 4722 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:29:31,580 : INFO : PROGRESS: at 37.62% examples, 4722 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:30:32,006 : INFO : PROGRESS: at 37.91% examples, 4721 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:31:33,500 : INFO : PROGRESS: at 38.24% examples, 4721 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:32:34,504 : INFO : PROGRESS: at 38.50% examples, 4719 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:33:34,538 : INFO : PROGRESS: at 38.80% examples, 4720 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:34:34,665 : INFO : PROGRESS: at 39.10% examples, 4720 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:35:34,718 : INFO : PROGRESS: at 39.38% examples, 4719 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 21:36:34,961 : INFO : PROGRESS: at 39.66% examples, 4719 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 21:37:35,048 : INFO : PROGRESS: at 39.93% examples, 4720 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 21:38:35,167 : INFO : PROGRESS: at 40.22% examples, 4719 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:39:35,211 : INFO : PROGRESS: at 40.52% examples, 4720 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:40:35,360 : INFO : PROGRESS: at 40.77% examples, 4718 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:41:35,499 : INFO : PROGRESS: at 41.04% examples, 4718 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:42:35,690 : INFO : PROGRESS: at 41.31% examples, 4719 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:43:35,826 : INFO : PROGRESS: at 41.61% examples, 4717 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:44:35,968 : INFO : PROGRESS: at 41.89% examples, 4720 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:45:36,533 : INFO : PROGRESS: at 42.22% examples, 4719 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:46:37,165 : INFO : PROGRESS: at 42.53% examples, 4719 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:47:37,643 : INFO : PROGRESS: at 42.79% examples, 4719 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:48:38,053 : INFO : PROGRESS: at 43.09% examples, 4719 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:49:38,221 : INFO : PROGRESS: at 43.40% examples, 4718 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:50:39,593 : INFO : PROGRESS: at 43.70% examples, 4718 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:51:39,788 : INFO : PROGRESS: at 43.99% examples, 4718 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:52:40,092 : INFO : PROGRESS: at 44.28% examples, 4717 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:53:40,167 : INFO : PROGRESS: at 44.56% examples, 4717 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:54:40,971 : INFO : PROGRESS: at 44.83% examples, 4715 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:55:41,156 : INFO : PROGRESS: at 45.12% examples, 4715 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:56:41,412 : INFO : PROGRESS: at 45.42% examples, 4713 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 21:57:42,417 : INFO : PROGRESS: at 45.75% examples, 4716 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:58:43,073 : INFO : PROGRESS: at 46.03% examples, 4714 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 21:59:43,356 : INFO : PROGRESS: at 46.32% examples, 4714 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 22:00:43,663 : INFO : PROGRESS: at 46.57% examples, 4714 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:01:43,853 : INFO : PROGRESS: at 46.88% examples, 4714 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:02:44,194 : INFO : PROGRESS: at 47.14% examples, 4714 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:03:44,299 : INFO : PROGRESS: at 47.45% examples, 4714 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:04:44,875 : INFO : PROGRESS: at 47.72% examples, 4713 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:05:44,961 : INFO : PROGRESS: at 48.01% examples, 4714 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 22:06:45,093 : INFO : PROGRESS: at 48.30% examples, 4713 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:07:45,348 : INFO : PROGRESS: at 48.59% examples, 4713 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:08:45,438 : INFO : PROGRESS: at 48.83% examples, 4713 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:09:47,994 : INFO : PROGRESS: at 49.17% examples, 4713 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:10:48,784 : INFO : PROGRESS: at 49.49% examples, 4713 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:11:49,496 : INFO : PROGRESS: at 49.80% examples, 4713 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:12:50,118 : INFO : PROGRESS: at 50.11% examples, 4712 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:13:50,173 : INFO : PROGRESS: at 50.39% examples, 4711 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:14:51,105 : INFO : PROGRESS: at 50.66% examples, 4711 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:15:51,136 : INFO : PROGRESS: at 50.93% examples, 4710 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:16:51,475 : INFO : PROGRESS: at 51.22% examples, 4710 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:17:51,722 : INFO : PROGRESS: at 51.49% examples, 4709 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 22:18:52,008 : INFO : PROGRESS: at 51.77% examples, 4710 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 22:19:52,071 : INFO : PROGRESS: at 52.02% examples, 4709 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 22:20:52,342 : INFO : PROGRESS: at 52.30% examples, 4709 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:21:52,478 : INFO : PROGRESS: at 52.57% examples, 4708 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:22:52,970 : INFO : PROGRESS: at 52.83% examples, 4708 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:23:53,625 : INFO : PROGRESS: at 53.11% examples, 4709 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:24:53,883 : INFO : PROGRESS: at 53.37% examples, 4709 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:25:54,299 : INFO : PROGRESS: at 53.65% examples, 4709 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:26:55,831 : INFO : PROGRESS: at 53.96% examples, 4708 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:27:56,645 : INFO : PROGRESS: at 54.27% examples, 4707 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:28:58,077 : INFO : PROGRESS: at 54.57% examples, 4706 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 22:29:58,372 : INFO : PROGRESS: at 54.86% examples, 4706 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:31:00,043 : INFO : PROGRESS: at 55.14% examples, 4704 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:32:00,755 : INFO : PROGRESS: at 55.38% examples, 4706 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:33:01,966 : INFO : PROGRESS: at 55.64% examples, 4704 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:34:02,174 : INFO : PROGRESS: at 55.95% examples, 4705 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 22:35:02,625 : INFO : PROGRESS: at 56.21% examples, 4705 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:36:02,636 : INFO : PROGRESS: at 56.51% examples, 4704 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 22:37:03,274 : INFO : PROGRESS: at 56.78% examples, 4703 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 22:38:03,533 : INFO : PROGRESS: at 57.06% examples, 4703 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:39:04,418 : INFO : PROGRESS: at 57.35% examples, 4703 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:40:05,753 : INFO : PROGRESS: at 57.64% examples, 4702 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:41:05,795 : INFO : PROGRESS: at 57.91% examples, 4702 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:42:05,875 : INFO : PROGRESS: at 58.20% examples, 4701 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:43:06,336 : INFO : PROGRESS: at 58.49% examples, 4701 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 22:44:07,305 : INFO : PROGRESS: at 58.82% examples, 4701 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:45:07,710 : INFO : PROGRESS: at 59.09% examples, 4700 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:46:08,059 : INFO : PROGRESS: at 59.37% examples, 4700 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:47:08,806 : INFO : PROGRESS: at 59.66% examples, 4700 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 22:48:09,041 : INFO : PROGRESS: at 59.96% examples, 4699 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:49:09,517 : INFO : PROGRESS: at 60.23% examples, 4701 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:50:09,651 : INFO : PROGRESS: at 60.48% examples, 4700 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:51:09,847 : INFO : PROGRESS: at 60.78% examples, 4699 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:52:10,024 : INFO : PROGRESS: at 61.07% examples, 4699 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 22:53:10,180 : INFO : PROGRESS: at 61.32% examples, 4698 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:54:10,673 : INFO : PROGRESS: at 61.56% examples, 4697 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:55:11,157 : INFO : PROGRESS: at 61.86% examples, 4698 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 22:56:11,199 : INFO : PROGRESS: at 62.15% examples, 4697 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:57:11,654 : INFO : PROGRESS: at 62.43% examples, 4697 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:58:12,037 : INFO : PROGRESS: at 62.67% examples, 4697 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 22:59:13,390 : INFO : PROGRESS: at 62.93% examples, 4696 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 23:00:14,699 : INFO : PROGRESS: at 63.23% examples, 4697 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:01:15,084 : INFO : PROGRESS: at 63.49% examples, 4696 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 23:02:16,581 : INFO : PROGRESS: at 63.74% examples, 4696 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:03:19,138 : INFO : PROGRESS: at 64.03% examples, 4695 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:04:19,434 : INFO : PROGRESS: at 64.33% examples, 4695 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:05:19,548 : INFO : PROGRESS: at 64.61% examples, 4694 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:06:20,611 : INFO : PROGRESS: at 64.92% examples, 4693 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:07:20,979 : INFO : PROGRESS: at 65.20% examples, 4692 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:08:21,523 : INFO : PROGRESS: at 65.45% examples, 4691 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:09:22,349 : INFO : PROGRESS: at 65.75% examples, 4691 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 23:10:22,645 : INFO : PROGRESS: at 66.01% examples, 4690 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 23:11:23,033 : INFO : PROGRESS: at 66.29% examples, 4689 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 23:12:23,629 : INFO : PROGRESS: at 66.56% examples, 4689 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:13:23,654 : INFO : PROGRESS: at 66.82% examples, 4688 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:14:23,911 : INFO : PROGRESS: at 67.08% examples, 4687 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:15:24,602 : INFO : PROGRESS: at 67.36% examples, 4686 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:16:25,216 : INFO : PROGRESS: at 67.64% examples, 4685 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:17:26,234 : INFO : PROGRESS: at 67.89% examples, 4685 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:18:26,255 : INFO : PROGRESS: at 68.15% examples, 4684 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:19:26,691 : INFO : PROGRESS: at 68.44% examples, 4682 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:20:26,768 : INFO : PROGRESS: at 68.71% examples, 4682 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:21:27,115 : INFO : PROGRESS: at 68.99% examples, 4682 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 23:22:28,086 : INFO : PROGRESS: at 69.26% examples, 4680 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:23:28,183 : INFO : PROGRESS: at 69.50% examples, 4680 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:24:28,480 : INFO : PROGRESS: at 69.75% examples, 4679 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:25:28,664 : INFO : PROGRESS: at 70.02% examples, 4679 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:26:28,958 : INFO : PROGRESS: at 70.30% examples, 4678 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 23:27:29,611 : INFO : PROGRESS: at 70.57% examples, 4677 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:28:31,935 : INFO : PROGRESS: at 70.84% examples, 4675 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:29:34,809 : INFO : PROGRESS: at 71.09% examples, 4674 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:30:35,029 : INFO : PROGRESS: at 71.39% examples, 4674 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:31:35,325 : INFO : PROGRESS: at 71.66% examples, 4673 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:32:36,165 : INFO : PROGRESS: at 71.93% examples, 4672 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 23:33:38,164 : INFO : PROGRESS: at 72.22% examples, 4672 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:34:39,251 : INFO : PROGRESS: at 72.51% examples, 4671 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:35:39,274 : INFO : PROGRESS: at 72.75% examples, 4671 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:36:39,917 : INFO : PROGRESS: at 73.02% examples, 4669 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:37:40,285 : INFO : PROGRESS: at 73.26% examples, 4669 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:38:40,463 : INFO : PROGRESS: at 73.55% examples, 4668 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 23:39:40,811 : INFO : PROGRESS: at 73.82% examples, 4667 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:40:41,118 : INFO : PROGRESS: at 74.09% examples, 4665 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-27 23:41:41,332 : INFO : PROGRESS: at 74.35% examples, 4665 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:42:41,451 : INFO : PROGRESS: at 74.61% examples, 4664 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:43:41,680 : INFO : PROGRESS: at 74.88% examples, 4662 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:44:42,720 : INFO : PROGRESS: at 75.18% examples, 4660 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:45:43,101 : INFO : PROGRESS: at 75.44% examples, 4660 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:46:43,541 : INFO : PROGRESS: at 75.69% examples, 4658 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:47:44,124 : INFO : PROGRESS: at 75.93% examples, 4657 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:48:44,422 : INFO : PROGRESS: at 76.18% examples, 4656 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:49:44,538 : INFO : PROGRESS: at 76.44% examples, 4654 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:50:44,570 : INFO : PROGRESS: at 76.75% examples, 4653 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:51:44,733 : INFO : PROGRESS: at 77.04% examples, 4651 words/s, in_qsize 12, out_qsize 0\n",
      "2016-11-27 23:52:45,977 : INFO : PROGRESS: at 77.25% examples, 4649 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:53:46,560 : INFO : PROGRESS: at 77.47% examples, 4648 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:54:47,410 : INFO : PROGRESS: at 77.73% examples, 4647 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:55:47,740 : INFO : PROGRESS: at 77.98% examples, 4645 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:56:48,972 : INFO : PROGRESS: at 78.24% examples, 4644 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:57:49,252 : INFO : PROGRESS: at 78.49% examples, 4642 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:58:49,286 : INFO : PROGRESS: at 78.76% examples, 4641 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-27 23:59:49,727 : INFO : PROGRESS: at 79.02% examples, 4640 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:00:49,764 : INFO : PROGRESS: at 79.28% examples, 4639 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:01:50,123 : INFO : PROGRESS: at 79.54% examples, 4637 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:02:50,442 : INFO : PROGRESS: at 79.77% examples, 4636 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:03:50,471 : INFO : PROGRESS: at 80.04% examples, 4635 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:04:50,724 : INFO : PROGRESS: at 80.28% examples, 4634 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:05:51,023 : INFO : PROGRESS: at 80.57% examples, 4633 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-28 00:06:51,695 : INFO : PROGRESS: at 80.85% examples, 4632 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:07:53,317 : INFO : PROGRESS: at 81.11% examples, 4630 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:08:53,494 : INFO : PROGRESS: at 81.33% examples, 4628 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:09:54,678 : INFO : PROGRESS: at 81.58% examples, 4627 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:10:54,852 : INFO : PROGRESS: at 81.82% examples, 4627 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:11:55,022 : INFO : PROGRESS: at 82.10% examples, 4626 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:12:55,274 : INFO : PROGRESS: at 82.35% examples, 4624 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:13:57,220 : INFO : PROGRESS: at 82.64% examples, 4622 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:14:57,887 : INFO : PROGRESS: at 82.92% examples, 4621 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:15:58,365 : INFO : PROGRESS: at 83.17% examples, 4619 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:16:59,359 : INFO : PROGRESS: at 83.43% examples, 4618 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:18:00,202 : INFO : PROGRESS: at 83.69% examples, 4616 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:19:00,897 : INFO : PROGRESS: at 83.96% examples, 4616 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:20:01,400 : INFO : PROGRESS: at 84.20% examples, 4614 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:21:01,697 : INFO : PROGRESS: at 84.46% examples, 4613 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:22:01,858 : INFO : PROGRESS: at 84.74% examples, 4612 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:23:02,181 : INFO : PROGRESS: at 84.98% examples, 4610 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:24:03,004 : INFO : PROGRESS: at 85.23% examples, 4608 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:25:03,645 : INFO : PROGRESS: at 85.52% examples, 4607 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:26:04,022 : INFO : PROGRESS: at 85.78% examples, 4606 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:27:04,109 : INFO : PROGRESS: at 86.05% examples, 4604 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:28:04,494 : INFO : PROGRESS: at 86.28% examples, 4603 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:29:04,600 : INFO : PROGRESS: at 86.51% examples, 4602 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:30:04,630 : INFO : PROGRESS: at 86.78% examples, 4600 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:31:04,808 : INFO : PROGRESS: at 87.03% examples, 4598 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:32:04,857 : INFO : PROGRESS: at 87.28% examples, 4595 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:33:04,979 : INFO : PROGRESS: at 87.51% examples, 4595 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:34:05,662 : INFO : PROGRESS: at 87.77% examples, 4593 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:35:05,721 : INFO : PROGRESS: at 88.04% examples, 4591 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-28 00:36:06,006 : INFO : PROGRESS: at 88.27% examples, 4589 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:37:06,056 : INFO : PROGRESS: at 88.55% examples, 4587 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:38:06,189 : INFO : PROGRESS: at 88.78% examples, 4586 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-28 00:39:06,876 : INFO : PROGRESS: at 89.04% examples, 4584 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:40:07,140 : INFO : PROGRESS: at 89.28% examples, 4582 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:41:08,709 : INFO : PROGRESS: at 89.53% examples, 4581 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:42:08,871 : INFO : PROGRESS: at 89.81% examples, 4580 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:43:08,912 : INFO : PROGRESS: at 90.04% examples, 4578 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:44:09,131 : INFO : PROGRESS: at 90.28% examples, 4577 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-28 00:45:10,040 : INFO : PROGRESS: at 90.52% examples, 4576 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:46:10,043 : INFO : PROGRESS: at 90.76% examples, 4575 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:47:10,374 : INFO : PROGRESS: at 91.02% examples, 4574 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:48:10,832 : INFO : PROGRESS: at 91.29% examples, 4572 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:49:11,616 : INFO : PROGRESS: at 91.56% examples, 4571 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:50:11,860 : INFO : PROGRESS: at 91.83% examples, 4569 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:51:12,301 : INFO : PROGRESS: at 92.09% examples, 4567 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:52:12,671 : INFO : PROGRESS: at 92.33% examples, 4566 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:53:13,345 : INFO : PROGRESS: at 92.59% examples, 4565 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:54:15,373 : INFO : PROGRESS: at 92.87% examples, 4563 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:55:17,078 : INFO : PROGRESS: at 93.12% examples, 4561 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:56:17,317 : INFO : PROGRESS: at 93.37% examples, 4559 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:57:17,437 : INFO : PROGRESS: at 93.65% examples, 4558 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:58:18,082 : INFO : PROGRESS: at 93.90% examples, 4556 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 00:59:18,770 : INFO : PROGRESS: at 94.20% examples, 4555 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 01:00:18,773 : INFO : PROGRESS: at 94.44% examples, 4553 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 01:01:19,927 : INFO : PROGRESS: at 94.68% examples, 4551 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 01:02:20,352 : INFO : PROGRESS: at 94.93% examples, 4550 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 01:03:20,692 : INFO : PROGRESS: at 95.18% examples, 4549 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 01:04:20,693 : INFO : PROGRESS: at 95.43% examples, 4547 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-28 01:05:21,223 : INFO : PROGRESS: at 95.71% examples, 4546 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 01:06:22,718 : INFO : PROGRESS: at 95.96% examples, 4544 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 01:07:22,827 : INFO : PROGRESS: at 96.20% examples, 4543 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 01:08:23,342 : INFO : PROGRESS: at 96.41% examples, 4541 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 01:09:23,551 : INFO : PROGRESS: at 96.68% examples, 4540 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 01:10:24,202 : INFO : PROGRESS: at 96.93% examples, 4538 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 01:11:24,292 : INFO : PROGRESS: at 97.19% examples, 4537 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 01:12:24,502 : INFO : PROGRESS: at 97.42% examples, 4536 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 01:13:24,973 : INFO : PROGRESS: at 97.66% examples, 4534 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-28 01:14:25,121 : INFO : PROGRESS: at 97.89% examples, 4533 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 01:15:26,315 : INFO : PROGRESS: at 98.16% examples, 4531 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 01:16:26,937 : INFO : PROGRESS: at 98.40% examples, 4530 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 01:17:28,062 : INFO : PROGRESS: at 98.64% examples, 4529 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 01:18:28,187 : INFO : PROGRESS: at 98.89% examples, 4528 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 01:19:28,267 : INFO : PROGRESS: at 99.15% examples, 4527 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 01:20:28,419 : INFO : PROGRESS: at 99.41% examples, 4525 words/s, in_qsize 23, out_qsize 0\n",
      "2016-11-28 01:21:28,625 : INFO : PROGRESS: at 99.71% examples, 4524 words/s, in_qsize 24, out_qsize 0\n",
      "2016-11-28 01:22:28,777 : INFO : PROGRESS: at 99.96% examples, 4523 words/s, in_qsize 17, out_qsize 0\n",
      "2016-11-28 01:22:31,614 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2016-11-28 01:22:32,235 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2016-11-28 01:22:32,657 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2016-11-28 01:22:32,724 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2016-11-28 01:22:33,007 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2016-11-28 01:22:35,805 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2016-11-28 01:22:35,809 : INFO : worker thread finished; awaiting finish of 5 more threads\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%matplotlib notebook\n",
    "graph = MetricsGraph()\n",
    "graph.init_graph()\n",
    "# when resuming, resume from an epoch with a previously created doc2vec model to get the learning rate right\n",
    "start_from = 1\n",
    "for epoch in range(start_from,DOC2VEC_MAX_EPOCHS+1):\n",
    "    GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "    info(\"****************** Epoch {} --- Working on {} *******************\".format(epoch, GLOBAL_VARS.MODEL_NAME))\n",
    "    \n",
    "    # if we have the model, just load it, otherwise train the previous model\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)):\n",
    "        doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "        GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "    else:\n",
    "        # train the doc2vec model\n",
    "        doc2vec_model.train(sentences=StochasticDocumentGenerator(training_file, training_docs_list, line_positions), \n",
    "                            report_delay=REPORT_DELAY)\n",
    "        #doc2vec_model.alpha -= 0.001  # decrease the learning rate\n",
    "        #doc2vec_model.min_alpha = doc2vec_model.alpha  # fix the learning rate, no decay\n",
    "        ensure_disk_location_exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME))\n",
    "        doc2vec_model.save(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "        GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "\n",
    "#     # Training and validation of SVMs using those docvecs\n",
    "#     train_classifications(sections)\n",
    "#     validation_vectors_matrix = get_validation_docs_with_inference(doc2vec_model, doc_classification_map)\n",
    "#     metrics = do_validation(validation_vectors_matrix, doc_classification_map, sections, \"sections\")\n",
    "#     ensure_disk_location_exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "#                                              GLOBAL_VARS.SVM_MODEL_NAME))\n",
    "#     pickle.dump(metrics, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, GLOBAL_VARS.SVM_MODEL_NAME, METRICS), 'w'))\n",
    "#     print \"Coverage Error: {}, Average No of Labels: {}, Top 1: {}, Top 3: {}, Top 5: {}, F1 Micro: {}, Total Positive: {}\".format(\n",
    "#         metrics['coverage_error'], metrics['average_num_of_labels'], metrics['top_1'], metrics['top_3'], metrics['top_5'], \n",
    "#         metrics['f1_micro'], metrics['total_positive'])\n",
    "                                                                                     \n",
    "#     epoch_metrics.append(metrics)\n",
    "#     graph.add_metrics_to_graph(metrics, epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loaded metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/local/shalaby/parameter_search_doc2vec_models/sample_0.0001'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_model_save_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        this.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width);\n",
       "        canvas.attr('height', height);\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'];\n",
       "    var y0 = fig.canvas.height - msg['y0'];\n",
       "    var x1 = msg['x1'];\n",
       "    var y1 = fig.canvas.height - msg['y1'];\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x;\n",
       "    var y = canvas_pos.y;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overriden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc2vec_size_3000_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None_curriter_{}_iter_1000_reg_0.001\n",
      "Epoch 01: Coverage Error -> 6.56\n",
      "Epoch 02: Coverage Error -> 6.51\n",
      "Epoch 03: Coverage Error -> 6.56\n",
      "Epoch 04: Coverage Error -> 6.45\n",
      "Epoch 05: Coverage Error -> 6.49\n",
      "Epoch 06: Coverage Error -> 6.39\n",
      "Epoch 07: Coverage Error -> 6.48\n",
      "Epoch 08: Coverage Error -> 6.55\n",
      "Epoch 09: Coverage Error -> 6.51\n",
      "Epoch 10: Coverage Error -> 6.54\n",
      "Epoch 11: Coverage Error -> 6.59\n",
      "Epoch 12: Coverage Error -> 6.50\n",
      "Epoch 13: Coverage Error -> 4.30\n",
      "Epoch 14: Coverage Error -> 4.44\n",
      "Epoch 15: Coverage Error -> 4.58\n",
      "Epoch 16: Coverage Error -> 4.75\n",
      "Epoch 17: Coverage Error -> 4.79\n",
      "Epoch 18: Coverage Error -> 4.75\n",
      "Epoch 19: Coverage Error -> 4.68\n",
      "Epoch 20: Coverage Error -> 4.69\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "graph = MetricsGraph()\n",
    "graph.init_graph()\n",
    "print placeholder_model_name + \"_\" + GLOBAL_VARS.SVM_MODEL_NAME\n",
    "\n",
    "for epoch in range(1,DOC2VEC_MAX_EPOCHS+1):\n",
    "    try:\n",
    "        model_name = placeholder_model_name.format(epoch)\n",
    "        metrics = pickle.load(open(os.path.join(doc2vec_model_save_location, model_name, GLOBAL_VARS.SVM_MODEL_NAME, METRICS)))\n",
    "        print \"Epoch {:02d}: Coverage Error -> {:.2f}\".format(epoch, metrics['coverage_error'])\n",
    "        graph.add_metrics_to_graph(metrics, epoch)\n",
    "    except IOError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.0518029456576938,\n",
       " 4.3138649060436771,\n",
       " 3.7851701371254443,\n",
       " 1.4565769426104622,\n",
       " 1.3819197562214323]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[metric['coverage_error'] for metric in epoch_metrics]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Thesis Venv",
   "language": "python",
   "name": "python-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
