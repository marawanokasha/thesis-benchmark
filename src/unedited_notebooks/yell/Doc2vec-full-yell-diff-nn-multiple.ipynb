{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K40m (CNMeM is disabled, cuDNN 5105)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "from functools import partial\n",
    "\n",
    "from thesis.utils.metrics import *\n",
    "\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IS_SAMPLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SVM_SEED = 1234\n",
    "DOC2VEC_SEED = 1234\n",
    "WORD2VEC_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUMBER_INDICATOR = \"number_inidicator\"\n",
    "CURRENCY_INDICATOR = \"currency_inidicator\"\n",
    "CHEMICAL_INDICATOR = \"chemical_inidicator\"\n",
    "MIN_WORD_COUNT = 100\n",
    "MIN_SIZE = 0\n",
    "NUM_CORES = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GLOBAL_VARS = namedtuple('GLOBAL_VARS', ['MODEL_NAME', 'DOC2VEC_MODEL_NAME', 'DOC2VEC_MODEL', \n",
    "                                         'SVM_MODEL_NAME', 'NN_MODEL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VOCAB_MODEL = \"vocab_model\"\n",
    "MODEL_PREFIX = \"model\"\n",
    "VALIDATION_MATRIX = \"validation_matrix.pkl\"\n",
    "TEST_MATRIX = \"test_matrix.pkl\"\n",
    "METRICS = \"metrics.pkl\"\n",
    "CLASSIFIER = \"classifier.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_PARAMETER_SEARCH_PREFIX = \"{}_batch_{}_nn_parameter_searches.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training_file = \"/home/local/shalaby/docs_output_sample_100.json\"\n",
    "\n",
    "root_location = \"/mnt/data2/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "doc2vec_model_save_location = os.path.join(root_location, \"parameter_search_doc2vec_models_new\", \"full\")\n",
    "nn_parameter_search_location = os.path.join(root_location, \"nn_parameter_search\")\n",
    "if not os.path.exists(doc2vec_model_save_location):\n",
    "    os.makedirs(doc2vec_model_save_location)\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL)):\n",
    "    os.makedirs(os.path.join(doc2vec_model_save_location, VOCAB_MODEL))\n",
    "\n",
    "training_file = root_location + \"docs_output.json\"\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "classification_index_file = exports_location + \"classification_index.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "valid_classes_file = exports_location + \"valid_classes.pkl\"\n",
    "valid_subclasses_file = exports_location + \"valid_subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "training_docs_list_file = exports_location + \"training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"validation_docs_list.pkl\"\n",
    "test_docs_list_file = exports_location + \"test_docs_list.pkl\"\n",
    "\n",
    "preprocessed_location = root_location + \"preprocessed_data/\"\n",
    "\n",
    "training_preprocessed_files_prefix = preprocessed_location + \"training_docs_merged_data_preprocessed-\"\n",
    "training_preprocessed_docids_files_prefix = preprocessed_location + \"training_docs_merged_docids_preprocessed-\"\n",
    "validation_preprocessed_files_prefix = preprocessed_location + \"validation_docs_merged_data_preprocessed-\"\n",
    "validation_preprocessed_docids_files_prefix = preprocessed_location + \"validation_docs_merged_docids_preprocessed-\"\n",
    "test_preprocessed_files_prefix = preprocessed_location + \"test_docs_merged_data_preprocessed-\"\n",
    "test_preprocessed_docids_files_prefix = preprocessed_location + \"test_docs_merged_docids_preprocessed-\"\n",
    "\n",
    "word2vec_questions_file = result = root_location + 'tensorflow/word2vec/questions-words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.4 s, sys: 2.42 s, total: 23.8 s\n",
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "valid_classes = pickle.load(open(valid_classes_file))\n",
    "valid_subclasses = pickle.load(open(valid_subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "# classifications_index = pickle.load(open(classification_index_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286325"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321473"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ensure_disk_location_exists(location):\n",
    "    if not os.path.exists(location):\n",
    "        os.makedirs(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_docs_with_inference(doc2vec_model, doc_classification_map, classifications,\n",
    "                                           docs_list, file_to_write, preprocessed_files_prefix,\n",
    "                                           preprocessed_docids_files_prefix):\n",
    "    \"\"\"\n",
    "    Use the trained doc2vec model to get the paragraph vector representations of the validation or test documents\n",
    "    \"\"\"\n",
    "\n",
    "    def infer_one_doc(doc_tuple):\n",
    "        # doc2vec_model.random = np.random.RandomState(DOC2VEC_SEED)\n",
    "        doc_id, doc_tokens = doc_tuple\n",
    "        rep = doc2vec_model.infer_vector(doc_tokens)\n",
    "        return (doc_id, rep)\n",
    "\n",
    "\n",
    "    one_hot_encoder = OneHotEncoder(classifications)\n",
    "    classifications_set = set(classifications)\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write)):\n",
    "        info(\"===== Loading inference vectors\")\n",
    "        inference_labels = []\n",
    "        inference_vectors_matrix = pickle.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write)))\n",
    "        info(\"Loaded inference vectors matrix\")\n",
    "        for i, doc_id in enumerate(docs_list):\n",
    "            curr_doc_labels = set(doc_classification_map[doc_id]) & classifications_set\n",
    "            inference_labels.append(one_hot_encoder.get_label_vector(curr_doc_labels))\n",
    "            if i % 100000 == 0:\n",
    "                info(\"Finished {} in validation loading\".format(i))\n",
    "        inference_labels = np.array(inference_labels, dtype=np.int8)\n",
    "    else:\n",
    "        inference_documents_reps = {}\n",
    "        inference_vectors = []\n",
    "        inference_labels = []\n",
    "        info(\"===== Getting vectors with inference\")\n",
    "\n",
    "\n",
    "        # Multi-threaded inference\n",
    "        inference_docs_iterator = DocumentBatchGenerator(preprocessed_files_prefix,\n",
    "                                                          preprocessed_docids_files_prefix, batch_size=None)\n",
    "        generator_func = inference_docs_iterator.__iter__()\n",
    "        pool = ThreadPool(NUM_CORES)\n",
    "        # map consumes the whole iterator on the spot, so we have to use itertools.islice to fake mini-batching\n",
    "        mini_batch_size = 1000\n",
    "        while True:\n",
    "            threaded_reps_partial = pool.map(infer_one_doc, itertools.islice(generator_func, mini_batch_size))\n",
    "            info(\"Finished: {}\".format(str(inference_docs_iterator.curr_index)))\n",
    "            if threaded_reps_partial:\n",
    "                # threaded_reps.extend(threaded_reps_partial)\n",
    "                inference_documents_reps.update(threaded_reps_partial)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # create matrix for the inferred vectors\n",
    "        for doc_id in docs_list:\n",
    "            inference_vectors.append(inference_documents_reps[doc_id])\n",
    "            curr_doc_labels = set(doc_classification_map[doc_id]) & classifications_set\n",
    "            inference_labels.append(one_hot_encoder.get_label_vector(curr_doc_labels))\n",
    "        inference_vectors_matrix = np.array(inference_vectors)\n",
    "        inference_labels = np.array(inference_labels, dtype=np.int8)\n",
    "        pickle.dump(inference_vectors_matrix,\n",
    "                    open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write), 'w'))\n",
    "\n",
    "    return inference_vectors_matrix, inference_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class OneHotEncoder():\n",
    "    \n",
    "    def __init__(self, classifications):\n",
    "        self.classifications = classifications\n",
    "        self.one_hot_indices = {}\n",
    "\n",
    "        # convert character classifications to bit vectors\n",
    "        for i, clssf in enumerate(classifications):\n",
    "            bits = [0] * len(classifications)\n",
    "            bits[i] = 1\n",
    "            self.one_hot_indices[clssf] = i\n",
    "    \n",
    "    def get_label_vector(self, labels):\n",
    "        \"\"\"\n",
    "        classes: array of string with the classes assigned to the instance\n",
    "        \"\"\"\n",
    "        output_vector = [0] * len(self.classifications)\n",
    "        for label in labels:\n",
    "            index = self.one_hot_indices[label]\n",
    "            output_vector[index] = 1\n",
    "            \n",
    "        return output_vector\n",
    "\n",
    "def get_training_data(doc2vec_model, classifications):\n",
    "    one_hot_encoder = OneHotEncoder(classifications)\n",
    "    classifications_set = set(classifications)\n",
    "    training_data = []\n",
    "    training_labels_mat = np.zeros((len(training_docs_list), len(classifications)), dtype=np.int8)\n",
    "#     training_data_mat = np.zeros((len(training_docs_list), DOC2VEC_SIZE), dtype=np.float32)\n",
    "    for i,doc_id in enumerate(training_docs_list):\n",
    "        # converting from memmap to a normal array\n",
    "#         normal_array = []\n",
    "#         normal_array[:] = doc2vec_model.docvecs[doc_id][:]\n",
    "        normal_array = doc2vec_model.docvecs[doc_id]\n",
    "        training_data.append(normal_array)\n",
    "#         eligible_classifications = [clssf for clssf in doc_classification_map[doc_id] if clssf in classifications]\n",
    "        eligible_classifications = set(doc_classification_map[doc_id]) & classifications_set\n",
    "        training_labels_mat[i][:] = one_hot_encoder.get_label_vector(eligible_classifications)\n",
    "        if i % 100000 == 0:\n",
    "            info(\"Finished {} in training\".format(i))\n",
    "    info(\"doing matrix creation\")\n",
    "    training_data_mat = np.array(training_data)\n",
    "#     training_labels_mat = np.array(training_labels, dtype=np.int8)\n",
    "    del training_data\n",
    "    return training_data_mat, training_labels_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DocumentBatchGenerator(object):\n",
    "    def __init__(self, filename_prefix, filename_docids_prefix, batch_size=10000 ):\n",
    "        \"\"\"\n",
    "        batch_size cant be > 10,000 due to a limitation in doc2vec training, \n",
    "        None means no batching (only use for inference)\n",
    "        \"\"\"\n",
    "        assert batch_size <= 10000 or batch_size is None\n",
    "        self.filename_prefix = filename_prefix\n",
    "        self.filename_docids_prefix = filename_docids_prefix\n",
    "        self.curr_lines = []\n",
    "        self.curr_docids = []\n",
    "        self.batch_size = batch_size\n",
    "        self.curr_index = 0\n",
    "        self.batch_end = -1\n",
    "    def load_new_batch_in_memory(self):\n",
    "        del self.curr_lines, self.curr_docids\n",
    "        self.curr_lines, self.docids = [], []\n",
    "        info(\"Loading new batch for index: {}\".format(self.curr_index) )\n",
    "        try:\n",
    "            with open(self.filename_prefix + str(self.curr_index)) as preproc_file:\n",
    "                for line in preproc_file:\n",
    "                    self.curr_lines.append(line.split(\" \"))\n",
    "#                     if i % 1000 == 0:\n",
    "#                         print i\n",
    "            self.curr_docids = pickle.load(open(self.filename_docids_prefix + str(self.curr_index), \"r\"))\n",
    "            self.batch_end = self.curr_index + len(self.curr_lines) -1 \n",
    "            info(\"Finished loading new batch\")\n",
    "        except IOError:\n",
    "            info(\"No more batches to load, exiting at index: {}\".format(self.curr_index))\n",
    "            raise StopIteration()\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            if self.curr_index > self.batch_end:\n",
    "                self.load_new_batch_in_memory()\n",
    "            for (doc_id, tokens) in zip(self.curr_docids, self.curr_lines):\n",
    "                if self.batch_size is not None:\n",
    "                    curr_batch_iter = 0\n",
    "                    # divide the document to batches according to the batch size\n",
    "                    while curr_batch_iter < len(tokens):\n",
    "                        yield LabeledSentence(words=tokens[curr_batch_iter: curr_batch_iter + self.batch_size], tags=[doc_id])\n",
    "                        curr_batch_iter += self.batch_size\n",
    "                else:\n",
    "                    yield doc_id, tokens\n",
    "                self.curr_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Specific Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_keras_nn_model(input_size, output_size, \n",
    "                          first_hidden_layer_size, first_hidden_layer_activation, \n",
    "                          second_hidden_layer_size, second_hidden_layer_activation, \n",
    "                          input_dropout_do, hidden_dropout_do, second_hidden_dropout_do=False):\n",
    "    \n",
    "    doc_input = Input(shape=(input_size,), name='doc_input')\n",
    "    if input_dropout_do:\n",
    "        hidden = Dropout(0.7)(doc_input)\n",
    "    hidden = Dense(first_hidden_layer_size, activation=first_hidden_layer_activation, \n",
    "                   name='hidden_layer_{}'.format(first_hidden_layer_activation))(doc_input if not input_dropout_do else hidden)\n",
    "    if hidden_dropout_do:\n",
    "        hidden = Dropout(0.5)(hidden)\n",
    "    if second_hidden_layer_size is not None:\n",
    "        hidden = Dense(second_hidden_layer_size, activation=second_hidden_layer_activation, \n",
    "                       name='hidden_layer2_{}'.format(second_hidden_layer_activation))(hidden)\n",
    "    if second_hidden_dropout_do:\n",
    "        hidden = Dropout(0.5)(hidden)\n",
    "    softmax_output = Dense(output_size, activation='sigmoid', name='softmax_output')(hidden)\n",
    "\n",
    "    model = Model(input=doc_input, output=softmax_output)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_binary_0_5 = lambda x: 1 if x > 0.5 else 0\n",
    "get_binary_0_5 = np.vectorize(get_binary_0_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stopper_deltas = {\n",
    "    'sections': 0.0001,\n",
    "    'classes': 0.00001,\n",
    "    'subclasses': 0.00001\n",
    "}\n",
    "early_stopper_patience = {\n",
    "    'sections': 5,\n",
    "    'classes': 10,\n",
    "    'subclasses': 10\n",
    "}\n",
    "epochs_before_validation = {\n",
    "    'sections': 10,\n",
    "    'classes': 50,\n",
    "    'subclasses': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2vec_methods_dict = {\n",
    "#     'doc2vec_size_50_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None' : 7,\n",
    "#     'doc2vec_size_50_w_8_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None' : 8,\n",
    "#     'doc2vec_size_50_w_8_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 8,\n",
    "#     'doc2vec_size_100_w_2_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 9,\n",
    "#     'doc2vec_size_100_w_5_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 18,\n",
    "#     'doc2vec_size_100_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 10,\n",
    "#     'doc2vec_size_100_w_8_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None': 4,\n",
    "#     'doc2vec_size_100_w_8_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 7,\n",
    "#     'doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None': 8,\n",
    "#     'doc2vec_size_200_w_2_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 7,\n",
    "#     'doc2vec_size_200_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None': 6,\n",
    "#     'doc2vec_size_200_w_4_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 8,\n",
    "#     'doc2vec_size_200_w_8_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None': 14,\n",
    "#     'doc2vec_size_200_w_8_type_dm_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 6,\n",
    "#     'doc2vec_size_200_w_8_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 8,\n",
    "    'doc2vec_size_300_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None': 9,\n",
    "#     'doc2vec_size_500_w_8_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None': 8,\n",
    "#     'doc2vec_size_500_w_8_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 8,\n",
    "#     'doc2vec_size_1000_w_8_type_pv-dbow_concat_1_mean_0_trainwords_0_hs_0_neg_10_vocabsize_None': 6,\n",
    "#     'doc2vec_size_1000_w_8_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifications = sections\n",
    "classifications_type = 'sections'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MetricsCallback(keras.callbacks.Callback):\n",
    "    \n",
    "    EPOCHS_BEFORE_VALIDATION = epochs_before_validation[classifications_type]\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.epoch_index = 0\n",
    "        self.val_loss_reductions = 0\n",
    "        self.metrics_dict = {}\n",
    "        self.best_val_loss = np.iinfo(np.int32).max\n",
    "        self.best_weights = None\n",
    "        self.best_validation_metrics = None\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epoch_index += 1\n",
    "        if logs['val_loss'] < self.best_val_loss:\n",
    "            self.val_loss_reductions += 1\n",
    "            self.best_val_loss = logs['val_loss']\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            print '\\r    \\r' # to remove the previous line of verbose output of model fit\n",
    "            time.sleep(0.2)\n",
    "            info('Found lower val loss for epoch {} => {}'.format(self.epoch_index, round(logs['val_loss'], 5)))\n",
    "            if self.val_loss_reductions % MetricsCallback.EPOCHS_BEFORE_VALIDATION == 0:\n",
    "                \n",
    "                info('Validation Loss Reduced {} times'.format(self.val_loss_reductions))\n",
    "                info('Evaluating on Validation Data')\n",
    "                yvp = self.model.predict(Xv)\n",
    "                yvp_binary = get_binary_0_5(yvp)\n",
    "                info('Generating Validation Metrics')\n",
    "                validation_metrics = get_metrics(yv, yvp, yvp_binary)\n",
    "                print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "                    validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "                    validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "                self.metrics_dict[self.epoch_index] = validation_metrics\n",
    "#                 self.best_validation_metrics = validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NN_OUTPUT_NEURONS = len(classifications)\n",
    "\n",
    "EARLY_STOPPER_MIN_DELTA = early_stopper_deltas[classifications_type]\n",
    "EARLY_STOPPER_PATIENCE = early_stopper_patience[classifications_type]\n",
    "\n",
    "NN_MAX_EPOCHS = 100\n",
    "NN_RANDOM_SEARCH_BUDGET = 20\n",
    "NN_PARAM_SAMPLE_SEED = 1234\n",
    "\n",
    "NN_BATCH_SIZE = 1024\n",
    "\n",
    "MODEL_VERBOSITY = 1\n",
    "\n",
    "to_skip = []\n",
    "\n",
    "load_existing_results = True\n",
    "save_results = True\n",
    "\n",
    "\n",
    "first_hidden_layer_sizes = [100,200,500]\n",
    "# first_hidden_layer_sizes = [1000,2000]\n",
    "# second_hidden_layer_sizes = [1000,2000,3000,4000]\n",
    "second_hidden_layer_sizes = [None,500,1000,2000]\n",
    "first_hidden_layer_activations = ['relu','sigmoid', 'tanh']\n",
    "second_hidden_layer_activations = ['relu','sigmoid', 'tanh']\n",
    "# first_hidden_layer_activations = ['relu']\n",
    "# second_hidden_layer_activations = ['relu']\n",
    "# input_dropout_options = [False, True]\n",
    "# hidden_dropout_options = [False, True]\n",
    "input_dropout_options = [False]\n",
    "hidden_dropout_options = [False, True]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Uncomment for Specific Configuration\n",
    "NN_RANDOM_SEARCH_BUDGET = 1\n",
    "first_hidden_layer_sizes = [500]\n",
    "second_hidden_layer_sizes = [2000]\n",
    "first_hidden_layer_activations = ['tanh']\n",
    "second_hidden_layer_activations = ['sigmoid']\n",
    "input_dropout_options = [False]\n",
    "hidden_dropout_options = [True]\n",
    "second_hidden_dropout_options = [False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:28:43,536 : INFO : loading Doc2Vec object from /mnt/data2/shalaby/parameter_search_doc2vec_models_new/full/doc2vec_size_300_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_9/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* doc2vec_size_300_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None->9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:28:48,871 : INFO : loading docvecs recursively from /mnt/data2/shalaby/parameter_search_doc2vec_models_new/full/doc2vec_size_300_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_9/model.docvecs.* with mmap=None\n",
      "2017-03-09 13:28:48,873 : INFO : loading doctag_syn0 from /mnt/data2/shalaby/parameter_search_doc2vec_models_new/full/doc2vec_size_300_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_9/model.docvecs.doctag_syn0.npy with mmap=None\n",
      "2017-03-09 13:28:49,479 : INFO : loading syn1neg from /mnt/data2/shalaby/parameter_search_doc2vec_models_new/full/doc2vec_size_300_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_9/model.syn1neg.npy with mmap=None\n",
      "2017-03-09 13:28:49,732 : INFO : loading syn0 from /mnt/data2/shalaby/parameter_search_doc2vec_models_new/full/doc2vec_size_300_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_9/model.syn0.npy with mmap=None\n",
      "2017-03-09 13:28:49,988 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-03-09 13:28:49,989 : INFO : setting ignored attribute cum_table to None\n",
      "2017-03-09 13:28:50,701 : INFO : Getting training Data\n",
      "2017-03-09 13:28:50,703 : INFO : Finished 0 in training\n",
      "2017-03-09 13:28:51,413 : INFO : Finished 100000 in training\n",
      "2017-03-09 13:28:52,192 : INFO : Finished 200000 in training\n",
      "2017-03-09 13:28:52,913 : INFO : Finished 300000 in training\n",
      "2017-03-09 13:28:53,605 : INFO : Finished 400000 in training\n",
      "2017-03-09 13:28:54,278 : INFO : Finished 500000 in training\n",
      "2017-03-09 13:28:54,887 : INFO : Finished 600000 in training\n",
      "2017-03-09 13:28:55,569 : INFO : Finished 700000 in training\n",
      "2017-03-09 13:28:56,191 : INFO : Finished 800000 in training\n",
      "2017-03-09 13:28:56,779 : INFO : Finished 900000 in training\n",
      "2017-03-09 13:28:57,379 : INFO : Finished 1000000 in training\n",
      "2017-03-09 13:28:57,898 : INFO : Finished 1100000 in training\n",
      "2017-03-09 13:28:58,480 : INFO : Finished 1200000 in training\n",
      "2017-03-09 13:28:58,999 : INFO : doing matrix creation\n",
      "2017-03-09 13:29:00,239 : INFO : Getting Validation Embeddings\n",
      "2017-03-09 13:29:00,240 : INFO : ===== Loading inference vectors\n",
      "2017-03-09 13:29:40,047 : INFO : Loaded inference vectors matrix\n",
      "2017-03-09 13:29:40,049 : INFO : Finished 0 in validation loading\n",
      "2017-03-09 13:29:40,426 : INFO : Finished 100000 in validation loading\n",
      "2017-03-09 13:29:40,710 : INFO : Finished 200000 in validation loading\n",
      "2017-03-09 13:29:41,014 : INFO : Finished 300000 in validation loading\n",
      "2017-03-09 13:29:42,736 : INFO : No Previous results exist in /mnt/data2/shalaby/nn_parameter_search/doc2vec_size_300_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_9/sections_batch_1024_nn_parameter_searches.pkl\n",
      "2017-03-09 13:29:42,738 : INFO : ***************************************************************************************\n",
      "2017-03-09 13:29:42,739 : INFO : nn_1st-size_500_1st-act_tanh_2nd-size_2000_2nd-act_sigmoid_in-drop_False_hid-drop_True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "doc_input (InputLayer)           (None, 300)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer_tanh (Dense)        (None, 500)           150500      doc_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 500)           0           hidden_layer_tanh[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "hidden_layer2_sigmoid (Dense)    (None, 2000)          1002000     dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "softmax_output (Dense)           (None, 8)             16008       hidden_layer2_sigmoid[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 1168508\n",
      "____________________________________________________________________________________________________\n",
      "Train on 1286325 samples, validate on 321473 samples\n",
      "Epoch 1/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:30:28,316 : INFO : Found lower val loss for epoch 1 => 0.18529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 41s - loss: 0.2076 - val_loss: 0.1853\n",
      "Epoch 2/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:31:10,687 : INFO : Found lower val loss for epoch 2 => 0.16751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 42s - loss: 0.1739 - val_loss: 0.1675\n",
      "Epoch 3/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:31:52,425 : INFO : Found lower val loss for epoch 3 => 0.15652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 41s - loss: 0.1616 - val_loss: 0.1565\n",
      "Epoch 4/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:32:33,480 : INFO : Found lower val loss for epoch 4 => 0.14938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 41s - loss: 0.1546 - val_loss: 0.1494\n",
      "Epoch 5/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:33:15,074 : INFO : Found lower val loss for epoch 5 => 0.14687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 41s - loss: 0.1504 - val_loss: 0.1469\n",
      "Epoch 6/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:33:57,050 : INFO : Found lower val loss for epoch 6 => 0.14425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 41s - loss: 0.1476 - val_loss: 0.1442\n",
      "Epoch 7/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:34:38,099 : INFO : Found lower val loss for epoch 7 => 0.14221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 41s - loss: 0.1455 - val_loss: 0.1422\n",
      "Epoch 8/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:35:19,681 : INFO : Found lower val loss for epoch 8 => 0.14033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 41s - loss: 0.1439 - val_loss: 0.1403\n",
      "Epoch 9/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:36:01,021 : INFO : Found lower val loss for epoch 9 => 0.13991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 41s - loss: 0.1426 - val_loss: 0.1399\n",
      "Epoch 10/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:36:43,044 : INFO : Found lower val loss for epoch 10 => 0.13861\n",
      "2017-03-09 13:36:43,045 : INFO : Validation Loss Reduced 10 times\n",
      "2017-03-09 13:36:43,046 : INFO : Evaluating on Validation Data\n",
      "2017-03-09 13:37:19,370 : INFO : Generating Validation Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Validation Metrics: Cov Err: 1.419 | Top 3: 0.975 | Top 5: 0.996 | F1 Micro: 0.791 | F1 Macro: 0.737\n",
      "1286325/1286325 [==============================] - 86s - loss: 0.1416 - val_loss: 0.1386\n",
      "Epoch 11/100\n",
      "1286325/1286325 [==============================] - 41s - loss: 0.1407 - val_loss: 0.1390\n",
      "Epoch 12/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:38:50,035 : INFO : Found lower val loss for epoch 12 => 0.13788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 41s - loss: 0.1399 - val_loss: 0.1379\n",
      "Epoch 13/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:39:30,930 : INFO : Found lower val loss for epoch 13 => 0.13752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 40s - loss: 0.1393 - val_loss: 0.1375\n",
      "Epoch 14/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:40:12,649 : INFO : Found lower val loss for epoch 14 => 0.13749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 41s - loss: 0.1388 - val_loss: 0.1375\n",
      "Epoch 15/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:40:54,084 : INFO : Found lower val loss for epoch 15 => 0.13688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 41s - loss: 0.1384 - val_loss: 0.1369\n",
      "Epoch 16/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:41:32,161 : INFO : Found lower val loss for epoch 16 => 0.13679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 38s - loss: 0.1379 - val_loss: 0.1368\n",
      "Epoch 17/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:42:09,165 : INFO : Found lower val loss for epoch 17 => 0.13582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 37s - loss: 0.1378 - val_loss: 0.1358\n",
      "Epoch 18/100\n",
      "1286325/1286325 [==============================] - 35s - loss: 0.1374 - val_loss: 0.1361\n",
      "Epoch 19/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:43:25,926 : INFO : Found lower val loss for epoch 19 => 0.13552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 41s - loss: 0.1371 - val_loss: 0.1355\n",
      "Epoch 20/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:44:07,236 : INFO : Found lower val loss for epoch 20 => 0.13538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 41s - loss: 0.1370 - val_loss: 0.1354\n",
      "Epoch 21/100\n",
      "1286325/1286325 [==============================] - 40s - loss: 0.1368 - val_loss: 0.1359\n",
      "Epoch 22/100\n",
      "1286325/1286325 [==============================] - 40s - loss: 0.1368 - val_loss: 0.1354\n",
      "Epoch 23/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:46:10,019 : INFO : Found lower val loss for epoch 23 => 0.13528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 41s - loss: 0.1367 - val_loss: 0.1353\n",
      "Epoch 24/100\n",
      "1286325/1286325 [==============================] - 41s - loss: 0.1366 - val_loss: 0.1359\n",
      "Epoch 25/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:47:33,180 : INFO : Found lower val loss for epoch 25 => 0.1351\n",
      "2017-03-09 13:47:33,182 : INFO : Validation Loss Reduced 20 times\n",
      "2017-03-09 13:47:33,184 : INFO : Evaluating on Validation Data\n",
      "2017-03-09 13:48:08,773 : INFO : Generating Validation Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Validation Metrics: Cov Err: 1.404 | Top 3: 0.976 | Top 5: 0.996 | F1 Micro: 0.801 | F1 Macro: 0.749\n",
      "1286325/1286325 [==============================] - 85s - loss: 0.1365 - val_loss: 0.1351\n",
      "Epoch 26/100\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:48:58,149 : INFO : Found lower val loss for epoch 26 => 0.13475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286325/1286325 [==============================] - 41s - loss: 0.1365 - val_loss: 0.1348\n",
      "Epoch 27/100\n",
      "1286325/1286325 [==============================] - 41s - loss: 0.1366 - val_loss: 0.1369\n",
      "Epoch 28/100\n",
      "1286325/1286325 [==============================] - 41s - loss: 0.1365 - val_loss: 0.1359\n",
      "Epoch 29/100\n",
      "1286325/1286325 [==============================] - 41s - loss: 0.1366 - val_loss: 0.1351\n",
      "Epoch 30/100\n",
      "1286325/1286325 [==============================] - 41s - loss: 0.1366 - val_loss: 0.1348\n",
      "Epoch 31/100\n",
      "1286325/1286325 [==============================] - 41s - loss: 0.1368 - val_loss: 0.1351\n",
      "Epoch 32/100\n",
      "1286325/1286325 [==============================] - 41s - loss: 0.1367 - val_loss: 0.1354"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:53:06,981 : INFO : Evaluating on Validation Data using saved best weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00031: early stopping\n",
      "CPU times: user 8min 46s, sys: 14min 30s, total: 23min 16s\n",
      "Wall time: 23min 23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 13:53:43,083 : INFO : Generating Validation Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Validation Metrics: Cov Err: 1.404 | Top 3: 0.976 | Top 5: 0.996 | F1 Micro: 0.801 | F1 Macro: 0.744\n"
     ]
    }
   ],
   "source": [
    "for (doc2vec_method_name, epoch) in sorted(doc2vec_methods_dict.items(), key=lambda x: x[0]):\n",
    "    print '********* {}->{}'.format(doc2vec_method_name, epoch)\n",
    "\n",
    "    TRAINING_METRICS_FILENAME = '{}_training_metrics.pkl'.format(classifications_type)\n",
    "    VALIDATION_METRICS_FILENAME= '{}_validation_metrics.pkl'.format(classifications_type)\n",
    "    TEST_METRICS_FILENAME = '{}_test_metrics.pkl'.format(classifications_type)\n",
    "\n",
    "    placeholder_model_name = doc2vec_method_name\n",
    "    placeholder_model_name = os.path.join(placeholder_model_name, \"epoch_{}\")\n",
    "    GLOBAL_VARS.DOC2VEC_MODEL_NAME = doc2vec_method_name\n",
    "    \n",
    "    GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "    \n",
    "    # if we have the model, just load it, otherwise train the previous model\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)):\n",
    "        doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "        GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "    else:\n",
    "        info(\"Couldnt find the doc2vec model with epoch {}\".format(epoch))\n",
    "        raise Exception()\n",
    "\n",
    "\n",
    "    info('Getting training Data')\n",
    "    X, y = get_training_data(doc2vec_model, classifications)\n",
    "\n",
    "    info('Getting Validation Embeddings')\n",
    "    Xv, yv = get_docs_with_inference(doc2vec_model, doc_classification_map, classifications, \n",
    "                                     validation_docs_list, VALIDATION_MATRIX, \n",
    "                                     validation_preprocessed_files_prefix, \n",
    "                                     validation_preprocessed_docids_files_prefix)\n",
    "    # create nn parameter search directory\n",
    "    if not os.path.exists(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME)):\n",
    "        os.makedirs(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME))\n",
    "        \n",
    "    DOC2VEC_SIZE = X.shape[1]\n",
    "        \n",
    "    param_sampler = ParameterSampler({\n",
    "        'first_hidden_layer_size':first_hidden_layer_sizes,\n",
    "        'first_hidden_layer_activation':first_hidden_layer_activations,\n",
    "        'second_hidden_layer_size':second_hidden_layer_sizes,\n",
    "        'second_hidden_layer_activation':second_hidden_layer_activations,\n",
    "        'input_dropout':input_dropout_options,\n",
    "        'hidden_dropout':hidden_dropout_options,\n",
    "        'second_hidden_dropout':second_hidden_dropout_options\n",
    "    }, n_iter=NN_RANDOM_SEARCH_BUDGET, random_state=NN_PARAM_SAMPLE_SEED)\n",
    "    \n",
    "    param_results_dict = {}\n",
    "    if load_existing_results:\n",
    "        param_results_path = os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                           NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE)))\n",
    "        if os.path.exists(param_results_path):\n",
    "            param_results_dict = pickle.load(open(param_results_path))\n",
    "        else:\n",
    "            info('No Previous results exist in {}'.format(param_results_path))\n",
    "    \n",
    "    for parameters in param_sampler:\n",
    "        start_time = time.time()\n",
    "        first_hidden_layer_size = parameters['first_hidden_layer_size']\n",
    "        first_hidden_layer_activation = parameters['first_hidden_layer_activation']\n",
    "        second_hidden_layer_size = parameters['second_hidden_layer_size']\n",
    "        second_hidden_layer_activation = parameters['second_hidden_layer_activation']\n",
    "        input_dropout_do = parameters['input_dropout']\n",
    "        hidden_dropout_do = parameters['hidden_dropout']\n",
    "        second_hidden_dropout_do = parameters['second_hidden_dropout']\n",
    "\n",
    "        GLOBAL_VARS.NN_MODEL_NAME = 'nn_1st-size_{}_1st-act_{}_2nd-size_{}_2nd-act_{}_in-drop_{}_hid-drop_{}'.format(\n",
    "            first_hidden_layer_size, first_hidden_layer_activation, second_hidden_layer_size, \n",
    "            second_hidden_layer_activation, input_dropout_do, hidden_dropout_do\n",
    "        )\n",
    "        if second_hidden_dropout_do:\n",
    "            GLOBAL_VARS.NN_MODEL_NAME = GLOBAL_VARS.NN_MODEL_NAME + '_2nd-hid-drop_{}'.format(str(second_hidden_dropout_do))\n",
    "            \n",
    "        if GLOBAL_VARS.NN_MODEL_NAME in param_results_dict.keys() or GLOBAL_VARS.NN_MODEL_NAME in to_skip:\n",
    "            print \"skipping: {}\".format(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "            continue\n",
    "#         if first_hidden_layer_size < DOC2VEC_SIZE or second_hidden_layer_size < NN_OUTPUT_NEURONS:\n",
    "#             print \"skipping: {} due to 1st layer size {} < {} or 2nd layer size {} < {}\".format(GLOBAL_VARS.NN_MODEL_NAME,\n",
    "#                                                                                                 first_hidden_layer_size, DOC2VEC_SIZE, \n",
    "#                                                                                                 second_hidden_layer_size, NN_OUTPUT_NEURONS)\n",
    "#             continue\n",
    "            \n",
    "\n",
    "        info('***************************************************************************************')\n",
    "        info(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "\n",
    "        model = create_keras_nn_model(DOC2VEC_SIZE, NN_OUTPUT_NEURONS, \n",
    "                                      first_hidden_layer_size, first_hidden_layer_activation, \n",
    "                                      second_hidden_layer_size, second_hidden_layer_activation, \n",
    "                                      input_dropout_do, hidden_dropout_do, second_hidden_dropout_do)\n",
    "        model.summary()\n",
    "\n",
    "        early_stopper = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=EARLY_STOPPER_MIN_DELTA, \\\n",
    "                                                      patience=EARLY_STOPPER_PATIENCE, verbose=1, mode='auto')\n",
    "        metrics_callback = MetricsCallback()\n",
    "\n",
    "        # Model Fitting\n",
    "        %time history = model.fit(x=X, y=y, validation_data=(Xv,yv), batch_size=NN_BATCH_SIZE, \\\n",
    "                                  nb_epoch=NN_MAX_EPOCHS, verbose=MODEL_VERBOSITY, callbacks=[early_stopper, metrics_callback])\n",
    "\n",
    "        \n",
    "        # using the recorded weights of the best recorded validation loss\n",
    "        last_model_weights = model.get_weights()\n",
    "        info('Evaluating on Validation Data using saved best weights')\n",
    "        model.set_weights(metrics_callback.best_weights)\n",
    "        yvp = model.predict(Xv)\n",
    "        yvp_binary = get_binary_0_5(yvp)\n",
    "        #print yvp\n",
    "        info('Generating Validation Metrics')\n",
    "        validation_metrics = get_metrics(yv, yvp, yvp_binary)\n",
    "        print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "            validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "            validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "        best_validation_metrics = validation_metrics\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        param_results_dict[GLOBAL_VARS.NN_MODEL_NAME] = dict()\n",
    "        param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['best_validation_metrics'] = best_validation_metrics\n",
    "        param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['epochs'] = len(history.history['val_loss'])\n",
    "        param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['best_weights'] = metrics_callback.best_weights\n",
    "#         param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['last_weights'] = last_model_weights\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['duration'] =  duration\n",
    "\n",
    "        del history, last_model_weights, metrics_callback, model\n",
    "\n",
    "    del doc2vec_model\n",
    "    \n",
    "    if save_results:\n",
    "        pickle.dump(param_results_dict, open(os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                                       NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE))), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'average_num_of_labels': 1.24,\n",
       " 'coverage_error': 3.0568010377232304,\n",
       " 'f1_macro': 0.12989896435822171,\n",
       " 'f1_micro': 0.61596229250132206,\n",
       " 'precision_macro': 0.31123006867587472,\n",
       " 'precision_micro': 0.78345783632282651,\n",
       " 'recall_macro': 0.098871691319240992,\n",
       " 'recall_micro': 0.50747018204645322,\n",
       " 'top_1': 0.6752693032015066,\n",
       " 'top_3': 0.8362887633396108,\n",
       " 'top_5': 0.903008160703076,\n",
       " 'total_positive': 257959}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(second_hidden_dropout_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.34 s, sys: 44 ms, total: 1.39 s\n",
      "Wall time: 1.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pickle.dump(param_results_dict, open(os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                           NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE))), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== NN: nn_1st-size_500_1st-act_sigmoid_2nd-size_None_2nd-act_sigmoid_in-drop_False_hid-drop_True\n",
      "Epochs => 26\n",
      "Best Val: Coverage Error => 3.1476 | F1 Micro => 0.5901 | F1 Macro => 0.1052 | Top 3 => 0.8330\n",
      "========== NN: nn_1st-size_500_1st-act_sigmoid_2nd-size_2000_2nd-act_relu_in-drop_False_hid-drop_True\n",
      "Epochs => 44\n",
      "Best Val: Coverage Error => 2.9189 | F1 Micro => 0.6550 | F1 Macro => 0.1744 | Top 3 => 0.8492\n",
      "========== NN: nn_1st-size_500_1st-act_sigmoid_2nd-size_1000_2nd-act_relu_in-drop_False_hid-drop_True\n",
      "Epochs => 45\n",
      "Best Val: Coverage Error => 2.9413 | F1 Micro => 0.6516 | F1 Macro => 0.1737 | Top 3 => 0.8484\n",
      "========== NN: nn_1st-size_500_1st-act_relu_2nd-size_2000_2nd-act_relu_in-drop_False_hid-drop_True\n",
      "Epochs => 41\n",
      "Best Val: Coverage Error => 2.9182 | F1 Micro => 0.6500 | F1 Macro => 0.1717 | Top 3 => 0.8481\n",
      "========== NN: nn_1st-size_100_1st-act_sigmoid_2nd-size_1000_2nd-act_sigmoid_in-drop_False_hid-drop_True\n",
      "Epochs => 25\n",
      "Best Val: Coverage Error => 3.5353 | F1 Micro => 0.5666 | F1 Macro => 0.0768 | Top 3 => 0.8014\n",
      "========== NN: nn_1st-size_500_1st-act_tanh_2nd-size_1000_2nd-act_sigmoid_in-drop_False_hid-drop_True\n",
      "Epochs => 39\n",
      "Best Val: Coverage Error => 2.8958 | F1 Micro => 0.6604 | F1 Macro => 0.1869 | Top 3 => 0.8547\n",
      "========== NN: nn_1st-size_200_1st-act_sigmoid_2nd-size_500_2nd-act_tanh_in-drop_False_hid-drop_True\n",
      "Epochs => 46\n",
      "Best Val: Coverage Error => 3.1170 | F1 Micro => 0.6351 | F1 Macro => 0.1335 | Top 3 => 0.8329\n",
      "========== NN: nn_1st-size_500_1st-act_tanh_2nd-size_None_2nd-act_tanh_in-drop_False_hid-drop_True\n",
      "Epochs => 100\n",
      "Best Val: Coverage Error => 3.4469 | F1 Micro => 0.5866 | F1 Macro => 0.1396 | Top 3 => 0.8203\n",
      "========== NN: nn_1st-size_500_1st-act_tanh_2nd-size_None_2nd-act_relu_in-drop_False_hid-drop_True\n",
      "Epochs => 100\n",
      "Best Val: Coverage Error => 3.4553 | F1 Micro => 0.5853 | F1 Macro => 0.1370 | Top 3 => 0.8195\n",
      "========== NN: nn_1st-size_100_1st-act_tanh_2nd-size_500_2nd-act_tanh_in-drop_False_hid-drop_True\n",
      "Epochs => 57\n",
      "Best Val: Coverage Error => 3.1239 | F1 Micro => 0.6270 | F1 Macro => 0.1342 | Top 3 => 0.8312\n",
      "========== NN: nn_1st-size_200_1st-act_sigmoid_2nd-size_2000_2nd-act_tanh_in-drop_False_hid-drop_True\n",
      "Epochs => 62\n",
      "Best Val: Coverage Error => 3.1238 | F1 Micro => 0.6292 | F1 Macro => 0.1283 | Top 3 => 0.8316\n",
      "========== NN: nn_1st-size_200_1st-act_sigmoid_2nd-size_1000_2nd-act_sigmoid_in-drop_False_hid-drop_True\n",
      "Epochs => 25\n",
      "Best Val: Coverage Error => 3.1244 | F1 Micro => 0.5942 | F1 Macro => 0.1161 | Top 3 => 0.8308\n",
      "========== NN: nn_1st-size_500_1st-act_relu_2nd-size_500_2nd-act_tanh_in-drop_False_hid-drop_True\n",
      "Epochs => 59\n",
      "Best Val: Coverage Error => 2.7937 | F1 Micro => 0.6639 | F1 Macro => 0.1817 | Top 3 => 0.8574\n",
      "========== NN: nn_1st-size_500_1st-act_relu_2nd-size_500_2nd-act_sigmoid_in-drop_False_hid-drop_True\n",
      "Epochs => 28\n",
      "Best Val: Coverage Error => 2.8626 | F1 Micro => 0.6573 | F1 Macro => 0.1986 | Top 3 => 0.8520\n",
      "========== NN: nn_1st-size_200_1st-act_sigmoid_2nd-size_None_2nd-act_sigmoid_in-drop_False_hid-drop_True\n",
      "Epochs => 34\n",
      "Best Val: Coverage Error => 3.4741 | F1 Micro => 0.5511 | F1 Macro => 0.0667 | Top 3 => 0.8127\n",
      "========== NN: nn_1st-size_200_1st-act_sigmoid_2nd-size_2000_2nd-act_sigmoid_in-drop_False_hid-drop_True\n",
      "Epochs => 27\n",
      "Best Val: Coverage Error => 3.0726 | F1 Micro => 0.5982 | F1 Macro => 0.1229 | Top 3 => 0.8346\n",
      "========== NN: nn_1st-size_200_1st-act_tanh_2nd-size_2000_2nd-act_relu_in-drop_False_hid-drop_True\n",
      "Epochs => 23\n",
      "Best Val: Coverage Error => 2.9561 | F1 Micro => 0.6482 | F1 Macro => 0.1660 | Top 3 => 0.8486\n",
      "========== NN: nn_1st-size_500_1st-act_sigmoid_2nd-size_None_2nd-act_tanh_in-drop_False_hid-drop_True\n",
      "Epochs => 26\n",
      "Best Val: Coverage Error => 3.1507 | F1 Micro => 0.5906 | F1 Macro => 0.1046 | Top 3 => 0.8329\n",
      "========== NN: nn_1st-size_100_1st-act_tanh_2nd-size_1000_2nd-act_tanh_in-drop_False_hid-drop_True\n",
      "Epochs => 54\n",
      "Best Val: Coverage Error => 3.1339 | F1 Micro => 0.6268 | F1 Macro => 0.1379 | Top 3 => 0.8320\n",
      "========== NN: nn_1st-size_100_1st-act_sigmoid_2nd-size_None_2nd-act_sigmoid_in-drop_False_hid-drop_True\n",
      "Epochs => 31\n",
      "Best Val: Coverage Error => 3.9217 | F1 Micro => 0.5065 | F1 Macro => 0.0443 | Top 3 => 0.7862\n"
     ]
    }
   ],
   "source": [
    "for key in param_results_dict.keys():\n",
    "    print('========== NN: {}'.format(key))\n",
    "    val = param_results_dict[key]\n",
    "#     val_metrics = val['last_validation_metrics']\n",
    "    val_metrics2 =  val['best_validation_metrics']\n",
    "    \n",
    "    print('Epochs => {}'.format(val['epochs']))\n",
    "    print('Best Val: Coverage Error => {:.4f} | F1 Micro => {:.4f} | F1 Macro => {:.4f} | Top 3 => {:.4f}'.format(val_metrics2['coverage_error'], \n",
    "                                                                                        val_metrics2['f1_micro'], val_metrics2['f1_macro'],\n",
    "                                                                                        val_metrics2['top_3']))\n",
    "#     print('Best Val Loss => {}'.format(val[\"metrics_callback\"].best_val_loss))\n",
    "#     print('Last Val: Coverage Error => {:.4f} | F1 Micro => {:.4f} | F1 Macro => {:.4f} | Top 3 => {:.4f}'.format(val_metrics['coverage_error'], \n",
    "#                                                                                         val_metrics['f1_micro'], val_metrics['f1_macro'],\n",
    "#                                                                                         val_metrics['top_3']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run network for specific configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_BATCH_SIZE = 4096\n",
    "NN_OUTPUT_NEURONS = len(classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "first_hidden_layer_size = 500\n",
    "first_hidden_layer_activation = 'tanh'\n",
    "second_hidden_layer_size = 2000\n",
    "second_hidden_layer_activation = 'sigmoid'\n",
    "input_dropout_do = False\n",
    "hidden_dropout_do = True\n",
    "\n",
    "#     print \"===================================================================================\\n\" + \\\n",
    "#           \"========== 1st Layer Size: {}, 1st Layer Activation: {}, \\n 2nd Layer Size: {}, 2nd Layer Activation: {}, \\n\" + \\\n",
    "#           \"Input Dropout: {}, Hidden Dropout: {} \\n\" + \\\n",
    "#           \"==========================\".format(first_hidden_layer_size, first_hidden_layer_activation, \n",
    "#                                                 second_hidden_layer_size, second_hidden_layer_activation, \n",
    "#                                                 input_dropout_do, hidden_dropout_do)\n",
    "\n",
    "GLOBAL_VARS.NN_MODEL_NAME = 'nn_1st-size_{}_1st-act_{}_2nd-size_{}_2nd-act_{}_in-drop_{}_hid-drop_{}'.format(\n",
    "    first_hidden_layer_size, first_hidden_layer_activation, second_hidden_layer_size, \n",
    "    second_hidden_layer_activation, input_dropout_do, hidden_dropout_do\n",
    ")\n",
    "if GLOBAL_VARS.NN_MODEL_NAME in param_results_dict.keys():\n",
    "    print \"Should be skipping: {}\".format(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "\n",
    "info('***************************************************************************************')\n",
    "info(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "\n",
    "model = create_keras_nn_model(DOC2VEC_SIZE, NN_OUTPUT_NEURONS, \n",
    "                              first_hidden_layer_size, first_hidden_layer_activation, \n",
    "                              second_hidden_layer_size, second_hidden_layer_activation, \n",
    "                              input_dropout_do, hidden_dropout_do)\n",
    "model.summary()\n",
    "\n",
    "early_stopper = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=EARLY_STOPPER_MIN_DELTA, \\\n",
    "                                              patience=EARLY_STOPPER_PATIENCE, verbose=1, mode='auto')\n",
    "metrics_callback = MetricsCallback()\n",
    "\n",
    "# Model Fitting\n",
    "%time history = model.fit(x=X, y=y, validation_data=(Xv,yv), batch_size=NN_BATCH_SIZE, \\\n",
    "                          nb_epoch=NN_MAX_EPOCHS, verbose=0, callbacks=[early_stopper, metrics_callback])\n",
    "\n",
    "#     info('Evaluating on Training Data')\n",
    "#     yp = model.predict(X, batch_size=NN_BATCH_SIZE)\n",
    "#     yp_binary = get_binary_0_5(yp)\n",
    "#     #print yp\n",
    "#     info('Generating Training Metrics')\n",
    "#     training_metrics = get_metrics(y, yp, yp_binary)\n",
    "#     print \"** Training Metrics: Cov Err: {:.3f}, Avg Labels: {:.3f}, \\n\\t\\t Top 1: {:.3f}, Top 3: {:.3f}, Top 5: {:.3f}, \\n\\t\\t F1 Micro: {:.3f}, F1 Macro: {:.3f}, Total Pos: {:,d}\".format(\n",
    "#         training_metrics['coverage_error'], training_metrics['average_num_of_labels'], \n",
    "#         training_metrics['top_1'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "#         training_metrics['f1_micro'],training_metrics['f1_macro'],  training_metrics['total_positive'])\n",
    "\n",
    "info('Evaluating on Validation Data using last weights')\n",
    "yvp = model.predict(Xv)\n",
    "yvp_binary = get_binary_0_5(yvp)\n",
    "#print yvp\n",
    "info('Generating Validation Metrics')\n",
    "validation_metrics = get_metrics(yv, yvp, yvp_binary)\n",
    "print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "    validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "last_validation_metrics = validation_metrics\n",
    "\n",
    "# using the recorded weights of the best recorded validation loss\n",
    "last_model_weights = model.get_weights()\n",
    "info('Evaluating on Validation Data using saved best weights')\n",
    "model.set_weights(metrics_callback.best_weights)\n",
    "yvp = model.predict(Xv)\n",
    "yvp_binary = get_binary_0_5(yvp)\n",
    "#print yvp\n",
    "info('Generating Validation Metrics')\n",
    "validation_metrics = get_metrics_detailed(yv, yvp, yvp_binary)\n",
    "print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "    validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "best_validation_metrics = validation_metrics\n",
    "\n",
    "\n",
    "param_results_dict[GLOBAL_VARS.NN_MODEL_NAME] = dict()\n",
    "#     param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['training_metrics'] = training_metrics\n",
    "param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['last_validation_metrics'] = last_validation_metrics\n",
    "param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['best_validation_metrics'] = best_validation_metrics\n",
    "param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['history'] = history\n",
    "param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['metrics_callback'] = metrics_callback\n",
    "param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['last_weights'] = last_model_weights\n",
    "\n",
    "duration = time.time() - start_time\n",
    "param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['duration'] =  duration\n",
    "\n",
    "pickle.dump(param_results_dict, open(os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                       NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE))), 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifications = sections\n",
    "classifications_type = 'sections'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_results_dict = pickle.load(open(os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                           NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 14:02:32,369 : INFO : Getting Test Vectors Embeddings\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'doc2vec_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-10ed300abb60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Test Metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Getting Test Vectors Embeddings'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m Xt, yt = get_docs_with_inference(doc2vec_model, doc_classification_map, classifications, \n\u001b[0m\u001b[0;32m      4\u001b[0m                                                \u001b[0mtest_docs_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTEST_MATRIX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                                \u001b[0mtest_preprocessed_files_prefix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'doc2vec_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Test Metrics\n",
    "info('Getting Test Vectors Embeddings')\n",
    "Xt, yt = get_docs_with_inference(doc2vec_model, doc_classification_map, classifications, \n",
    "                                               test_docs_list, TEST_MATRIX, \n",
    "                                               test_preprocessed_files_prefix, \n",
    "                                               test_preprocessed_docids_files_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_OUTPUT_NEURONS = len(classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_results_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_hidden_layer_size = 500\n",
    "first_hidden_layer_activation = 'tanh'\n",
    "second_hidden_layer_size = 2000\n",
    "second_hidden_layer_activation = 'sigmoid'\n",
    "input_dropout_do = False\n",
    "hidden_dropout_do = True\n",
    "\n",
    "#     print \"===================================================================================\\n\" + \\\n",
    "#           \"========== 1st Layer Size: {}, 1st Layer Activation: {}, \\n 2nd Layer Size: {}, 2nd Layer Activation: {}, \\n\" + \\\n",
    "#           \"Input Dropout: {}, Hidden Dropout: {} \\n\" + \\\n",
    "#           \"==========================\".format(first_hidden_layer_size, first_hidden_layer_activation, \n",
    "#                                                 second_hidden_layer_size, second_hidden_layer_activation, \n",
    "#                                                 input_dropout_do, hidden_dropout_do)\n",
    "\n",
    "GLOBAL_VARS.NN_MODEL_NAME = 'nn_1st-size_{}_1st-act_{}_2nd-size_{}_2nd-act_{}_in-drop_{}_hid-drop_{}'.format(\n",
    "    first_hidden_layer_size, first_hidden_layer_activation, second_hidden_layer_size, \n",
    "    second_hidden_layer_activation, input_dropout_do, hidden_dropout_do\n",
    ")\n",
    "if GLOBAL_VARS.NN_MODEL_NAME not in param_results_dict.keys():\n",
    "    print \"Can't find model: {}\".format(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "    raise Exception()\n",
    "\n",
    "info('***************************************************************************************')\n",
    "info(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "\n",
    "model = create_keras_nn_model(DOC2VEC_SIZE, NN_OUTPUT_NEURONS, \n",
    "                              first_hidden_layer_size, first_hidden_layer_activation, \n",
    "                              second_hidden_layer_size, second_hidden_layer_activation, \n",
    "                              input_dropout_do, hidden_dropout_do)\n",
    "model.summary()\n",
    "\n",
    "# get model best weights\n",
    "# weights = param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['metrics_callback'].best_weights\n",
    "weights = param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['best_weights']\n",
    "model.set_weights(weights)\n",
    "\n",
    "info('Evaluating on Test Data using best weights')\n",
    "ytp = model.predict(Xt)\n",
    "ytp_binary = get_binary_0_5(ytp)\n",
    "#print yvp\n",
    "info('Generating Test Metrics')\n",
    "test_metrics = get_metrics(yt, ytp, ytp_binary)\n",
    "print \"** Test Metrics: Cov Err: {:.3f}, Avg Labels: {:.3f}, \\n\\t\\t Top 1: {:.3f}, Top 3: {:.3f}, Top 5: {:.3f}, \\n\\t\\t F1 Micro: {:.3f}, F1 Macro: {:.3f}, Total Pos: {:,d}\".format(\n",
    "    test_metrics['coverage_error'], test_metrics['average_num_of_labels'], \n",
    "    test_metrics['top_1'], test_metrics['top_3'], test_metrics['top_5'], \n",
    "    test_metrics['f1_micro'], test_metrics['f1_macro'], test_metrics['total_positive'])\n",
    "\n",
    "    \n",
    "ensure_disk_location_exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                             GLOBAL_VARS.NN_MODEL_NAME))\n",
    "\n",
    "pickle.dump(test_metrics, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                      GLOBAL_VARS.NN_MODEL_NAME, TEST_METRICS_FILENAME), 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== NN: nn_1st-size_500_1st-act_softmax_2nd-size_None_2nd-act_softmax_in-drop_True_hid-drop_False\n",
      "Epochs => 100\n",
      "Best Val Loss => 0.166829405505\n",
      "Last Val: Coverage Error => 1.4960 | F1 Micro => 0.7434 | F1 Macro => 0.6293 | Top 3 => 0.9637\n",
      "Best Val: Coverage Error => 1.4960 | F1 Micro => 0.7434 | F1 Macro => 0.6293 | Top 3 => 0.9637\n",
      "========== NN: nn_1st-size_200_1st-act_tanh_2nd-size_500_2nd-act_relu_in-drop_False_hid-drop_True\n",
      "Epochs => 47\n",
      "Best Val Loss => 0.147426413828\n",
      "Last Val: Coverage Error => 1.4576 | F1 Micro => 0.7738 | F1 Macro => 0.7021 | Top 3 => 0.9697\n",
      "Best Val: Coverage Error => 1.4576 | F1 Micro => 0.7738 | F1 Macro => 0.7021 | Top 3 => 0.9697\n",
      "========== NN: nn_1st-size_200_1st-act_tanh_2nd-size_500_2nd-act_tanh_in-drop_True_hid-drop_True\n",
      "Epochs => 28\n",
      "Best Val Loss => 0.192695072037\n",
      "Last Val: Coverage Error => 1.6299 | F1 Micro => 0.6631 | F1 Macro => 0.4731 | Top 3 => 0.9455\n",
      "Best Val: Coverage Error => 1.6299 | F1 Micro => 0.6631 | F1 Macro => 0.4731 | Top 3 => 0.9455\n"
     ]
    }
   ],
   "source": [
    "for key in param_results_dict.keys():\n",
    "    print('========== NN: {}'.format(key))\n",
    "    val = param_results_dict[key]\n",
    "    val_metrics = val['last_validation_metrics']\n",
    "    val_metrics2 =  val['best_validation_metrics']\n",
    "    \n",
    "    print('Epochs => {}'.format(len(val['history'].history['val_loss'])))\n",
    "    print('Best Val Loss => {}'.format(val[\"metrics_callback\"].best_val_loss))\n",
    "    print('Last Val: Coverage Error => {:.4f} | F1 Micro => {:.4f} | F1 Macro => {:.4f} | Top 3 => {:.4f}'.format(val_metrics['coverage_error'], \n",
    "                                                                                        val_metrics['f1_micro'], val_metrics['f1_macro'],\n",
    "                                                                                        val_metrics['top_3']))\n",
    "    print('Best Val: Coverage Error => {:.4f} | F1 Micro => {:.4f} | F1 Macro => {:.4f} | Top 3 => {:.4f}'.format(val_metrics2['coverage_error'], \n",
    "                                                                                        val_metrics2['f1_micro'], val_metrics2['f1_macro'],\n",
    "                                                                                        val_metrics2['top_3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(param_results_dict, open(os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                           NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE))), 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
