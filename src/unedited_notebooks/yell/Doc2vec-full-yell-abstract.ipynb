{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K40m (CNMeM is disabled, cuDNN 5105)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "from functools import partial\n",
    "\n",
    "from thesis.utils.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IS_SAMPLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_SEED = 1234\n",
    "DOC2VEC_SEED = 1234\n",
    "WORD2VEC_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUMBER_INDICATOR = \"number_inidicator\"\n",
    "CURRENCY_INDICATOR = \"currency_inidicator\"\n",
    "CHEMICAL_INDICATOR = \"chemical_inidicator\"\n",
    "MIN_WORD_COUNT = 3\n",
    "MIN_SIZE = 0\n",
    "NUM_CORES = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GLOBAL_VARS = namedtuple('GLOBAL_VARS', ['MODEL_NAME', 'DOC2VEC_MODEL_NAME', 'DOC2VEC_MODEL', \n",
    "                                         'SVM_MODEL_NAME', 'NN_MODEL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_MODEL = \"vocab_model\"\n",
    "MODEL_PREFIX = \"model\"\n",
    "VALIDATION_MATRIX = \"validation_matrix.pkl\"\n",
    "TEST_MATRIX = \"test_matrix.pkl\"\n",
    "METRICS = \"metrics.pkl\"\n",
    "CLASSIFIER = \"classifier.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training_file = \"/home/local/shalaby/docs_output_sample_100.json\"\n",
    "\n",
    "root_location = \"/mnt/data2/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "doc2vec_model_save_location = os.path.join(root_location, \"parameter_search_doc2vec_models_new_abstract\", \"full\")\n",
    "if not os.path.exists(doc2vec_model_save_location):\n",
    "    os.makedirs(doc2vec_model_save_location)\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL)):\n",
    "    os.makedirs(os.path.join(doc2vec_model_save_location, VOCAB_MODEL))\n",
    "\n",
    "training_file = root_location + \"docs_output.json\"\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "training_docs_list_file = exports_location + \"training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"validation_docs_list.pkl\"\n",
    "test_docs_list_file = exports_location + \"test_docs_list.pkl\"\n",
    "\n",
    "abstracts_map_file = exports_location + \"abstracts_tokenized_map.pkl\"\n",
    "\n",
    "preprocessed_location = root_location + \"preprocessed_data/\"\n",
    "\n",
    "training_preprocessed_files_prefix = preprocessed_location + \"training_docs_merged_data_preprocessed-\"\n",
    "training_preprocessed_docids_files_prefix = preprocessed_location + \"training_docs_merged_docids_preprocessed-\"\n",
    "validation_preprocessed_files_prefix = preprocessed_location + \"validation_docs_merged_data_preprocessed-\"\n",
    "validation_preprocessed_docids_files_prefix = preprocessed_location + \"validation_docs_merged_docids_preprocessed-\"\n",
    "\n",
    "word2vec_questions_file = result = root_location + 'tensorflow/word2vec/questions-words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.4 s, sys: 4.44 s, total: 32.9 s\n",
      "Wall time: 32.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 43s, sys: 48.2 s, total: 5min 31s\n",
      "Wall time: 5min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "abstracts_map = pickle.load(open(abstracts_map_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286325"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321473"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only use the docs that have an abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_docs_list = list(set(training_docs_list) - (set(training_docs_list) - set(abstracts_map.keys())))\n",
    "validation_docs_list = list(set(validation_docs_list) - (set(validation_docs_list) - set(abstracts_map.keys())))\n",
    "test_docs_list = list(set(test_docs_list) - (set(test_docs_list) - set(abstracts_map.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1279894"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319877"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemtokenizer(text):\n",
    "    \"\"\" MAIN FUNCTION to get clean stems out of a text. A list of clean stems are returned \"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\s+', gaps=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stems = []  # result\n",
    "    for token in tokens:\n",
    "        stem = token.lower()\n",
    "        stem = stem.strip(string.punctuation)\n",
    "        if stem:\n",
    "            if is_number(stem):\n",
    "                stem = NUMBER_INDICATOR\n",
    "            elif is_currency(stem):\n",
    "                stem = CURRENCY_INDICATOR\n",
    "            elif is_chemical(stem):\n",
    "                stem = CHEMICAL_INDICATOR\n",
    "            else:\n",
    "                stem = stem.strip(string.punctuation)\n",
    "            if stem and len(stem) >= MIN_SIZE:\n",
    "                # extract uni-grams\n",
    "                stems.append(stem)\n",
    "    del tokens\n",
    "    return stems\n",
    "\n",
    "def is_number(str):\n",
    "    \"\"\" Returns true if given string is a number (float or int)\"\"\"\n",
    "    try:\n",
    "        float(str.replace(\",\", \"\"))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_currency(str):\n",
    "    return str[0] == \"$\"\n",
    "\n",
    "def is_chemical(str):\n",
    "    return str.count(\"-\") > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ensure_disk_location_exists(location):\n",
    "    if not os.path.exists(location):\n",
    "        os.makedirs(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_docs_with_inference(doc2vec_model, doc_classification_map, classifications,\n",
    "                                           docs_list, file_to_write, abstracts_map):\n",
    "    \"\"\"\n",
    "    Use the trained doc2vec model to get the paragraph vector representations of the validation or test documents\n",
    "    \"\"\"\n",
    "\n",
    "    def infer_one_doc(doc_tuple):\n",
    "        # doc2vec_model.random = np.random.RandomState(DOC2VEC_SEED)\n",
    "        doc_id, doc_tokens = doc_tuple\n",
    "        rep = doc2vec_model.infer_vector(doc_tokens)\n",
    "        return (doc_id, rep)\n",
    "\n",
    "\n",
    "    one_hot_encoder = OneHotEncoder(classifications)\n",
    "    classifications_set = set(classifications)\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write)):\n",
    "        info(\"===== Loading inference vectors\")\n",
    "        inference_labels = []\n",
    "        inference_vectors_matrix = pickle.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write)))\n",
    "        info(\"Loaded inference vectors matrix\")\n",
    "        for i, doc_id in enumerate(docs_list):\n",
    "            curr_doc_labels = set(doc_classification_map[doc_id]) & classifications_set\n",
    "            inference_labels.append(one_hot_encoder.get_label_vector(curr_doc_labels))\n",
    "            if i % 100000 == 0:\n",
    "                info(\"Finished {} in inference loading\".format(i))\n",
    "        inference_labels = np.array(inference_labels, dtype=np.int8)\n",
    "    else:\n",
    "        inference_documents_reps = {}\n",
    "        inference_vectors = []\n",
    "        inference_labels = []\n",
    "        info(\"===== Getting vectors with inference\")\n",
    "\n",
    "\n",
    "        # Multi-threaded inference\n",
    "        inference_docs_iterator = AbstractDocumentGenerator(abstracts_map, docs_list, False)\n",
    "        generator_func = inference_docs_iterator.__iter__()\n",
    "        pool = ThreadPool(NUM_CORES)\n",
    "        # map consumes the whole iterator on the spot, so we have to use itertools.islice to fake mini-batching\n",
    "#         mini_batch_size = 1000\n",
    "#         while True:\n",
    "#             threaded_reps_partial = pool.map(infer_one_doc, itertools.islice(generator_func, mini_batch_size))\n",
    "#             info(\"Finished: {}\".format(str(inference_docs_iterator.curr_index)))\n",
    "#             if threaded_reps_partial:\n",
    "#                 # threaded_reps.extend(threaded_reps_partial)\n",
    "#                 inference_documents_reps.update(threaded_reps_partial)\n",
    "#             else:\n",
    "#                 break\n",
    "\n",
    "        threaded_reps_partial = pool.map(infer_one_doc, generator_func)\n",
    "        info(\"Finished: {}\".format(str(inference_docs_iterator.curr_index)))\n",
    "        inference_documents_reps.update(threaded_reps_partial)\n",
    "\n",
    "        # create matrix for the inferred vectors\n",
    "        for doc_id in docs_list:\n",
    "            inference_vectors.append(inference_documents_reps[doc_id])\n",
    "            curr_doc_labels = set(doc_classification_map[doc_id]) & classifications_set\n",
    "            inference_labels.append(one_hot_encoder.get_label_vector(curr_doc_labels))\n",
    "        inference_vectors_matrix = np.array(inference_vectors)\n",
    "        inference_labels = np.array(inference_labels, dtype=np.int8)\n",
    "        pickle.dump(inference_vectors_matrix,\n",
    "                    open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, file_to_write), 'w'))\n",
    "\n",
    "    return inference_vectors_matrix, inference_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OneHotEncoder():\n",
    "    \n",
    "    def __init__(self, classifications):\n",
    "        self.classifications = classifications\n",
    "        self.one_hot_indices = {}\n",
    "\n",
    "        # convert character classifications to bit vectors\n",
    "        for i, clssf in enumerate(classifications):\n",
    "            bits = [0] * len(classifications)\n",
    "            bits[i] = 1\n",
    "            self.one_hot_indices[clssf] = i\n",
    "    \n",
    "    def get_label_vector(self, labels):\n",
    "        \"\"\"\n",
    "        classes: array of string with the classes assigned to the instance\n",
    "        \"\"\"\n",
    "        output_vector = [0] * len(self.classifications)\n",
    "        for label in labels:\n",
    "            index = self.one_hot_indices[label]\n",
    "            output_vector[index] = 1\n",
    "            \n",
    "        return output_vector\n",
    "\n",
    "def get_training_data(doc2vec_model, classifications):\n",
    "    one_hot_encoder = OneHotEncoder(classifications)\n",
    "    classifications_set = set(classifications)\n",
    "    training_data = []\n",
    "    training_labels_mat = np.zeros((len(training_docs_list), len(classifications)), dtype=np.int8)\n",
    "    for i,doc_id in enumerate(training_docs_list):\n",
    "        normal_array = doc2vec_model.docvecs[doc_id]\n",
    "        training_data.append(normal_array)\n",
    "        eligible_classifications = set(doc_classification_map[doc_id]) & classifications_set\n",
    "        training_labels_mat[i][:] = one_hot_encoder.get_label_vector(eligible_classifications)\n",
    "        if i % 100000 == 0:\n",
    "            info(\"Finished {} in training\".format(i))\n",
    "    info(\"doing matrix creation\")\n",
    "    training_data_mat = np.array(training_data)\n",
    "    del training_data\n",
    "    return training_data_mat, training_labels_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AbstractDocumentGenerator(object):\n",
    "    def __init__(self, abstracts_map, document_list, return_labeled_sentence=True):\n",
    "        self.abstracts_map = abstracts_map\n",
    "        self.docs_list = document_list\n",
    "        self.docs_set = set(self.docs_list)\n",
    "        self.return_labeled_sentence = return_labeled_sentence\n",
    "        self.curr_index = 0\n",
    "    def __iter__(self):\n",
    "        for i, doc_id in enumerate(self.abstracts_map):\n",
    "            if doc_id in self.docs_set and self.abstracts_map.get(doc_id) is not None:\n",
    "                text = self.abstracts_map[doc_id]\n",
    "                if self.return_labeled_sentence:\n",
    "                    yield LabeledSentence(words=text, tags=[doc_id])\n",
    "                else:\n",
    "                    yield doc_id, text\n",
    "            self.curr_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2vec and SVM Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOC2VEC_SIZE = 500\n",
    "DOC2VEC_WINDOW = 4\n",
    "DOC2VEC_MAX_VOCAB_SIZE = None\n",
    "DOC2VEC_SAMPLE = 1e-3\n",
    "DOC2VEC_TYPE = 1\n",
    "DOC2VEC_HIERARCHICAL_SAMPLE = 0\n",
    "DOC2VEC_NEGATIVE_SAMPLE_SIZE = 10\n",
    "DOC2VEC_CONCAT = 0\n",
    "DOC2VEC_MEAN = 1\n",
    "DOC2VEC_TRAIN_WORDS = 0\n",
    "DOC2VEC_EPOCHS = 1 # we do our training manually one epoch at a time\n",
    "DOC2VEC_MAX_EPOCHS = 20\n",
    "REPORT_DELAY = 20 # report the progress every x seconds\n",
    "REPORT_VOCAB_PROGRESS = 100000 # report vocab progress every x documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SVM_ITERATIONS = 10\n",
    "SVM_CONVERGENCE = 0.001\n",
    "SVM_REG = 0.001\n",
    "SVM_CLASS_WEIGHTS = None\n",
    "GLOBAL_VARS.SVM_MODEL_NAME = 'svm_iter_{}_reg_{}_classweights_{}'.format(SVM_ITERATIONS, SVM_REG, str(SVM_CLASS_WEIGHTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_svm_model_path(method, classification, reg=SVM_REG, iterations=SVM_ITERATIONS):\n",
    "    location = os.path.join(save_parent_location, \"models\", method, \n",
    "                            \"iter_\" + str(iterations) + \"_reg_\" + str(reg),\n",
    "                            classification + \"_model.svm\")\n",
    "    ensure_hdfs_location_exists(location)\n",
    "    return location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_{}'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}'.format(DOC2VEC_SIZE, \n",
    "                                                                DOC2VEC_WINDOW, \n",
    "                                                                'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                                DOC2VEC_TRAIN_WORDS,\n",
    "                                                                DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                                str(DOC2VEC_MAX_VOCAB_SIZE))\n",
    "GLOBAL_VARS.DOC2VEC_MODEL_NAME = placeholder_model_name\n",
    "placeholder_model_name = os.path.join(placeholder_model_name, \"epoch_{}\")\n",
    "placeholder_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc2vec_model = Doc2Vec(size=DOC2VEC_SIZE , window=DOC2VEC_WINDOW, min_count=MIN_WORD_COUNT, \n",
    "                max_vocab_size= DOC2VEC_MAX_VOCAB_SIZE,\n",
    "                sample=DOC2VEC_SAMPLE, seed=DOC2VEC_SEED, workers=NUM_CORES,\n",
    "                # doc2vec algorithm dm=1 => PV-DM, dm=2 => PV-DBOW, PV-DM dictates CBOW for words\n",
    "                dm=DOC2VEC_TYPE,\n",
    "                # hs=0 => negative sampling, hs=1 => hierarchical softmax\n",
    "                hs=DOC2VEC_HIERARCHICAL_SAMPLE, negative=DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                dm_concat=DOC2VEC_CONCAT,\n",
    "                # 0 for sum, 1 for mean\n",
    "                dm_mean=0 if DOC2VEC_MEAN != 2 else 1,\n",
    "                # would train words with skip-gram on top of cbow, we don't need that for now\n",
    "                dbow_words=DOC2VEC_TRAIN_WORDS,\n",
    "                iter=DOC2VEC_EPOCHS)\n",
    "\n",
    "GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-26 19:51:28,579 : INFO : collecting all words and their counts\n",
      "2017-02-26 19:51:28,583 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-02-26 19:51:31,605 : INFO : PROGRESS: at example #100000, processed 10713666 words (3547687/s), 101619 word types, 100000 tags\n",
      "2017-02-26 19:51:33,965 : INFO : PROGRESS: at example #200000, processed 21384107 words (4524544/s), 153654 word types, 200000 tags\n",
      "2017-02-26 19:51:36,340 : INFO : PROGRESS: at example #300000, processed 32064844 words (4498821/s), 196489 word types, 300000 tags\n",
      "2017-02-26 19:51:38,780 : INFO : PROGRESS: at example #400000, processed 42767189 words (4390312/s), 233764 word types, 400000 tags\n",
      "2017-02-26 19:51:41,167 : INFO : PROGRESS: at example #500000, processed 53456551 words (4480994/s), 267398 word types, 500000 tags\n",
      "2017-02-26 19:51:43,564 : INFO : PROGRESS: at example #600000, processed 64123059 words (4453212/s), 298880 word types, 600000 tags\n",
      "2017-02-26 19:51:46,032 : INFO : PROGRESS: at example #700000, processed 74812069 words (4332680/s), 327688 word types, 700000 tags\n",
      "2017-02-26 19:51:48,437 : INFO : PROGRESS: at example #800000, processed 85507502 words (4450981/s), 355103 word types, 800000 tags\n",
      "2017-02-26 19:51:50,800 : INFO : PROGRESS: at example #900000, processed 96177822 words (4518836/s), 381514 word types, 900000 tags\n",
      "2017-02-26 19:51:53,208 : INFO : PROGRESS: at example #1000000, processed 106907956 words (4458703/s), 406201 word types, 1000000 tags\n",
      "2017-02-26 19:51:55,688 : INFO : PROGRESS: at example #1100000, processed 117601501 words (4313309/s), 430054 word types, 1100000 tags\n",
      "2017-02-26 19:51:58,113 : INFO : PROGRESS: at example #1200000, processed 128314380 words (4419635/s), 452621 word types, 1200000 tags\n",
      "2017-02-26 19:52:00,094 : INFO : collected 470291 word types and 1279894 unique tags from a corpus of 1279894 examples and 136858217 words\n",
      "2017-02-26 19:52:00,657 : INFO : min_count=3 retains 181046 unique words (drops 289245)\n",
      "2017-02-26 19:52:00,659 : INFO : min_count leaves 136490138 word corpus (99% of original 136858217)\n",
      "2017-02-26 19:52:01,110 : INFO : deleting the raw counts dictionary of 470291 items\n",
      "2017-02-26 19:52:01,125 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-02-26 19:52:01,126 : INFO : downsampling leaves estimated 97217682 word corpus (71.2% of prior 136490138)\n",
      "2017-02-26 19:52:01,127 : INFO : estimated required memory for 181046 words and 500 dimensions: 3630473800 bytes\n",
      "2017-02-26 19:52:01,784 : INFO : resetting layer weights\n",
      "2017-02-26 19:52:26,006 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/vocab_model/model, separately None\n",
      "2017-02-26 19:52:26,008 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/vocab_model/model.docvecs.doctag_syn0.npy\n",
      "2017-02-26 19:52:32,039 : INFO : storing numpy array 'syn1neg' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/vocab_model/model.syn1neg.npy\n",
      "2017-02-26 19:52:32,720 : INFO : not storing attribute syn0norm\n",
      "2017-02-26 19:52:32,722 : INFO : storing numpy array 'syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/vocab_model/model.syn0.npy\n",
      "2017-02-26 19:52:33,474 : INFO : not storing attribute cum_table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 10.9 s, total: 1min 31s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_docs_iterator = AbstractDocumentGenerator(abstracts_map, training_docs_list)\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX)):\n",
    "    doc2vec_model.build_vocab(sentences=training_docs_iterator, progress_per=REPORT_VOCAB_PROGRESS)\n",
    "    doc2vec_model.save(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "else:\n",
    "    doc2vec_model_vocab_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, VOCAB_MODEL, MODEL_PREFIX))\n",
    "    doc2vec_model.reset_from(doc2vec_model_vocab_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocab_counts = {k:doc2vec_model.vocab[k].count for k in doc2vec_model.vocab.keys()}\n",
    "# dd = sorted(vocab_counts, key=vocab_counts.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Training, validation and Metrics Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2vec_model.alpha = 0.025\n",
    "doc2vec_model.min_alpha = 0.025\n",
    "DOC2VEC_ALPHA_DECREASE = 0.001\n",
    "epoch_validation_metrics = []\n",
    "epoch_training_metrics = []\n",
    "epoch_word2vec_metrics = []\n",
    "classifications = sections\n",
    "classifications_type = 'sections'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VALIDATION_METRICS_FILENAME= '{}_validation_metrics.pkl'.format(classifications_type)\n",
    "TRAINING_METRICS_FILENAME = '{}_training_metrics.pkl'.format(classifications_type)\n",
    "METRICS_FIG_PNG_FILENAME = '{}_validation_metrics.png'.format(classifications_type)\n",
    "METRICS_FIG_PDF_FILENAME = '{}_validation_metrics.pdf'.format(classifications_type)\n",
    "WORD2VEC_METRICS_FILENAME = 'word2vec_metrics.pkl'\n",
    "\n",
    "# for epoch in range(DOC2VEC_MAX_EPOCHS):\n",
    "#     GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "#     ensure_disk_location_exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "#                                              GLOBAL_VARS.SVM_MODEL_NAME))\n",
    "#     pickle.dump(metrics, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, GLOBAL_VARS.SVM_MODEL_NAME, METRICS), 'w'))\n",
    "# fig_save_location = placeholder_model_name.format('run')\n",
    "# plt.savefig(os.path.join(fig_save_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        this.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width);\n",
       "        canvas.attr('height', height);\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'];\n",
       "    var y0 = fig.canvas.height - msg['y0'];\n",
       "    var x1 = msg['x1'];\n",
       "    var y1 = fig.canvas.height - msg['y1'];\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x;\n",
       "    var y = canvas_pos.y;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overriden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-27 00:36:21,341 : INFO : ****************** Epoch 1 --- Working on doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_1 *******************\n",
      "2017-02-27 00:36:21,343 : INFO : loading Doc2Vec object from /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_1/model\n",
      "2017-02-27 00:36:27,168 : INFO : loading docvecs recursively from /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_1/model.docvecs.* with mmap=None\n",
      "2017-02-27 00:36:27,172 : INFO : loading doctag_syn0 from /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_1/model.docvecs.doctag_syn0.npy with mmap=None\n",
      "2017-02-27 00:36:37,536 : INFO : loading syn1neg from /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_1/model.syn1neg.npy with mmap=None\n",
      "2017-02-27 00:36:38,312 : INFO : loading syn0 from /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_1/model.syn0.npy with mmap=None\n",
      "2017-02-27 00:36:39,909 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-02-27 00:36:39,911 : INFO : setting ignored attribute cum_table to None\n",
      "2017-02-27 00:36:40,494 : INFO : Loading Classifier\n",
      "2017-02-27 00:36:40,503 : INFO : Getting Validation Embeddings\n",
      "2017-02-27 00:36:40,504 : INFO : ===== Loading inference vectors\n",
      "2017-02-27 00:38:07,269 : INFO : Loaded inference vectors matrix\n",
      "2017-02-27 00:38:07,444 : INFO : Finished 0 in validation loading\n",
      "2017-02-27 00:38:53,282 : INFO : Finished 100000 in validation loading\n",
      "2017-02-27 00:39:25,975 : INFO : Finished 200000 in validation loading\n",
      "2017-02-27 00:39:51,864 : INFO : Finished 300000 in validation loading\n",
      "2017-02-27 00:39:57,043 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "** Validation Metrics: Cov Err: 3.125, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.491, Top 3: 0.722, Top 5: 0.787, \n",
      "\t\t F1 Micro: 0.001, F1 Macro: 0.000, Total Pos: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-27 00:40:24,179 : INFO : ****************** Epoch 2 --- Working on doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_2 *******************\n",
      "2017-02-27 00:40:24,209 : INFO : loading Doc2Vec object from /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_2/model\n",
      "2017-02-27 00:46:54,442 : INFO : loading docvecs recursively from /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_2/model.docvecs.* with mmap=None\n",
      "2017-02-27 00:46:54,447 : INFO : loading doctag_syn0 from /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_2/model.docvecs.doctag_syn0.npy with mmap=None\n",
      "2017-02-27 00:47:13,193 : INFO : loading syn1neg from /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_2/model.syn1neg.npy with mmap=None\n",
      "2017-02-27 00:47:16,216 : INFO : loading syn0 from /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_2/model.syn0.npy with mmap=None\n",
      "2017-02-27 00:47:19,929 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-02-27 00:47:20,027 : INFO : setting ignored attribute cum_table to None\n",
      "2017-02-27 00:48:31,920 : INFO : Loading Classifier\n",
      "2017-02-27 00:48:32,194 : INFO : Getting Validation Embeddings\n",
      "2017-02-27 00:48:32,196 : INFO : ===== Loading inference vectors\n",
      "2017-02-27 00:49:58,197 : INFO : Loaded inference vectors matrix\n",
      "2017-02-27 00:49:58,266 : INFO : Finished 0 in validation loading\n",
      "2017-02-27 00:53:02,136 : INFO : Finished 100000 in validation loading\n",
      "2017-02-27 00:53:58,297 : INFO : Finished 200000 in validation loading\n",
      "2017-02-27 00:54:21,038 : INFO : Finished 300000 in validation loading\n",
      "2017-02-27 00:54:25,891 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "** Validation Metrics: Cov Err: 3.117, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.486, Top 3: 0.722, Top 5: 0.787, \n",
      "\t\t F1 Micro: 0.172, F1 Macro: 0.058, Total Pos: 46,632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-27 00:54:52,101 : INFO : ****************** Epoch 3 --- Working on doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_3 *******************\n",
      "2017-02-27 00:54:52,139 : INFO : loading Doc2Vec object from /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_3/model\n",
      "2017-02-27 00:55:04,783 : INFO : loading docvecs recursively from /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_3/model.docvecs.* with mmap=None\n",
      "2017-02-27 00:55:04,788 : INFO : loading doctag_syn0 from /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_3/model.docvecs.doctag_syn0.npy with mmap=None\n",
      "2017-02-27 00:55:12,493 : INFO : loading syn1neg from /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_3/model.syn1neg.npy with mmap=None\n",
      "2017-02-27 00:55:13,792 : INFO : loading syn0 from /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_3/model.syn0.npy with mmap=None\n",
      "2017-02-27 00:55:15,198 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-02-27 00:55:15,200 : INFO : setting ignored attribute cum_table to None\n",
      "2017-02-27 00:55:28,507 : INFO : Loading Classifier\n",
      "2017-02-27 00:55:28,630 : INFO : Getting Validation Embeddings\n",
      "2017-02-27 00:55:28,631 : INFO : ===== Loading inference vectors\n",
      "2017-02-27 00:57:05,659 : INFO : Loaded inference vectors matrix\n",
      "2017-02-27 00:57:05,668 : INFO : Finished 0 in validation loading\n",
      "2017-02-27 00:57:06,844 : INFO : Finished 100000 in validation loading\n",
      "2017-02-27 00:57:07,576 : INFO : Finished 200000 in validation loading\n",
      "2017-02-27 00:57:08,309 : INFO : Finished 300000 in validation loading\n",
      "2017-02-27 00:57:09,006 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "** Validation Metrics: Cov Err: 3.255, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.472, Top 3: 0.695, Top 5: 0.763, \n",
      "\t\t F1 Micro: 0.268, F1 Macro: 0.090, Total Pos: 90,127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-27 00:57:44,879 : INFO : ****************** Epoch 4 --- Working on doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_4 *******************\n",
      "2017-02-27 00:57:51,197 : INFO : training model with 22 workers on 181046 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=10\n",
      "2017-02-27 00:57:51,203 : INFO : expecting 1279894 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-27 00:57:58,684 : INFO : PROGRESS: at 0.01% examples, 1253 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 00:58:18,702 : INFO : PROGRESS: at 1.76% examples, 67891 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 00:58:38,749 : INFO : PROGRESS: at 4.13% examples, 89296 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 00:58:58,759 : INFO : PROGRESS: at 6.83% examples, 102480 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 00:59:18,793 : INFO : PROGRESS: at 9.48% examples, 108981 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 00:59:38,908 : INFO : PROGRESS: at 12.28% examples, 114260 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 00:59:58,898 : INFO : PROGRESS: at 15.28% examples, 119552 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:00:18,913 : INFO : PROGRESS: at 18.37% examples, 124045 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:00:38,922 : INFO : PROGRESS: at 21.44% examples, 127328 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:00:58,948 : INFO : PROGRESS: at 24.84% examples, 131584 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:01:18,962 : INFO : PROGRESS: at 27.94% examples, 133668 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:01:38,985 : INFO : PROGRESS: at 31.24% examples, 136186 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:01:58,994 : INFO : PROGRESS: at 34.62% examples, 138632 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:02:19,001 : INFO : PROGRESS: at 37.96% examples, 140569 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:02:39,040 : INFO : PROGRESS: at 41.53% examples, 143004 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:02:59,065 : INFO : PROGRESS: at 44.69% examples, 143792 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:03:19,120 : INFO : PROGRESS: at 48.05% examples, 145104 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:03:39,133 : INFO : PROGRESS: at 51.53% examples, 146588 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:03:59,156 : INFO : PROGRESS: at 54.88% examples, 147577 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:04:19,226 : INFO : PROGRESS: at 56.15% examples, 143128 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:04:39,264 : INFO : PROGRESS: at 58.68% examples, 142228 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:04:59,310 : INFO : PROGRESS: at 62.80% examples, 145054 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:05:19,333 : INFO : PROGRESS: at 67.02% examples, 147844 words/s, in_qsize 43, out_qsize 1\n",
      "2017-02-27 01:05:39,342 : INFO : PROGRESS: at 71.11% examples, 150122 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:05:59,386 : INFO : PROGRESS: at 75.02% examples, 151892 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:06:19,413 : INFO : PROGRESS: at 79.15% examples, 153935 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:06:39,665 : INFO : PROGRESS: at 81.55% examples, 152494 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:06:59,726 : INFO : PROGRESS: at 84.94% examples, 153008 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:07:19,779 : INFO : PROGRESS: at 88.58% examples, 153920 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:07:39,824 : INFO : PROGRESS: at 92.61% examples, 155438 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:07:59,824 : INFO : PROGRESS: at 96.85% examples, 157212 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:08:11,613 : INFO : worker thread finished; awaiting finish of 21 more threads\n",
      "2017-02-27 01:08:11,616 : INFO : worker thread finished; awaiting finish of 20 more threads\n",
      "2017-02-27 01:08:11,629 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2017-02-27 01:08:11,636 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2017-02-27 01:08:11,640 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2017-02-27 01:08:11,648 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2017-02-27 01:08:11,672 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2017-02-27 01:08:11,722 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2017-02-27 01:08:11,725 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2017-02-27 01:08:11,764 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2017-02-27 01:08:11,795 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2017-02-27 01:08:11,804 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2017-02-27 01:08:11,822 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2017-02-27 01:08:11,829 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2017-02-27 01:08:11,833 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-02-27 01:08:11,838 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-02-27 01:08:11,841 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-02-27 01:08:11,855 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-02-27 01:08:11,862 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-02-27 01:08:11,864 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-27 01:08:11,865 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-27 01:08:11,870 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-27 01:08:11,870 : INFO : training on 136858217 raw words (98502805 effective words) took 618.9s, 159169 effective words/s\n",
      "2017-02-27 01:08:11,890 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_4/model, separately None\n",
      "2017-02-27 01:08:11,902 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_4/model.docvecs.doctag_syn0.npy\n",
      "2017-02-27 01:08:13,787 : INFO : storing numpy array 'syn1neg' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_4/model.syn1neg.npy\n",
      "2017-02-27 01:08:13,993 : INFO : not storing attribute syn0norm\n",
      "2017-02-27 01:08:13,994 : INFO : storing numpy array 'syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_4/model.syn0.npy\n",
      "2017-02-27 01:08:14,219 : INFO : not storing attribute cum_table\n",
      "2017-02-27 01:08:40,357 : INFO : Getting training Data\n",
      "2017-02-27 01:08:40,361 : INFO : Finished 0 in training\n",
      "2017-02-27 01:08:48,597 : INFO : Finished 100000 in training\n",
      "2017-02-27 01:08:53,821 : INFO : Finished 200000 in training\n",
      "2017-02-27 01:08:58,159 : INFO : Finished 300000 in training\n",
      "2017-02-27 01:09:02,209 : INFO : Finished 400000 in training\n",
      "2017-02-27 01:09:05,661 : INFO : Finished 500000 in training\n",
      "2017-02-27 01:09:09,159 : INFO : Finished 600000 in training\n",
      "2017-02-27 01:09:11,955 : INFO : Finished 700000 in training\n",
      "2017-02-27 01:09:14,759 : INFO : Finished 800000 in training\n",
      "2017-02-27 01:09:17,248 : INFO : Finished 900000 in training\n",
      "2017-02-27 01:09:19,103 : INFO : Finished 1000000 in training\n",
      "2017-02-27 01:09:20,933 : INFO : Finished 1100000 in training\n",
      "2017-02-27 01:09:22,507 : INFO : Finished 1200000 in training\n",
      "2017-02-27 01:09:23,887 : INFO : doing matrix creation\n",
      "2017-02-27 01:09:30,792 : INFO : Training Classifier\n",
      "2017-02-27 01:13:28,027 : INFO : Getting Validation Embeddings\n",
      "2017-02-27 01:13:28,057 : INFO : ===== Getting vectors with inference\n",
      "2017-02-27 01:32:21,767 : INFO : Finished: 1999720\n",
      "2017-02-27 01:34:06,492 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]]\n",
      "** Validation Metrics: Cov Err: 3.188, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.478, Top 3: 0.711, Top 5: 0.775, \n",
      "\t\t F1 Micro: 0.304, F1 Macro: 0.108, Total Pos: 107,372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-27 01:34:28,104 : INFO : ****************** Epoch 5 --- Working on doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_5 *******************\n",
      "2017-02-27 01:34:35,828 : INFO : training model with 22 workers on 181046 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=10\n",
      "2017-02-27 01:34:35,834 : INFO : expecting 1279894 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-27 01:34:44,139 : INFO : PROGRESS: at 0.01% examples, 1109 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:35:04,290 : INFO : PROGRESS: at 0.98% examples, 36249 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:35:24,368 : INFO : PROGRESS: at 3.34% examples, 70764 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:35:44,378 : INFO : PROGRESS: at 6.57% examples, 97139 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:36:04,394 : INFO : PROGRESS: at 10.00% examples, 113704 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:36:24,422 : INFO : PROGRESS: at 13.46% examples, 124186 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:36:44,447 : INFO : PROGRESS: at 17.18% examples, 133393 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:37:04,453 : INFO : PROGRESS: at 20.97% examples, 140705 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:37:24,570 : INFO : PROGRESS: at 24.77% examples, 146159 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:37:44,578 : INFO : PROGRESS: at 28.28% examples, 149002 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:38:04,589 : INFO : PROGRESS: at 32.18% examples, 153185 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:38:24,674 : INFO : PROGRESS: at 35.94% examples, 155958 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:38:44,677 : INFO : PROGRESS: at 40.07% examples, 159741 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:39:04,684 : INFO : PROGRESS: at 43.53% examples, 160554 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:39:24,697 : INFO : PROGRESS: at 47.44% examples, 162715 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:39:44,750 : INFO : PROGRESS: at 51.13% examples, 163917 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:40:04,763 : INFO : PROGRESS: at 54.91% examples, 165256 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:40:24,825 : INFO : PROGRESS: at 58.93% examples, 167132 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:40:44,835 : INFO : PROGRESS: at 62.74% examples, 168215 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:41:04,855 : INFO : PROGRESS: at 66.55% examples, 169179 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:41:24,863 : INFO : PROGRESS: at 70.02% examples, 169267 words/s, in_qsize 44, out_qsize 1\n",
      "2017-02-27 01:41:44,887 : INFO : PROGRESS: at 73.57% examples, 169571 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:42:04,916 : INFO : PROGRESS: at 77.00% examples, 169543 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:42:24,944 : INFO : PROGRESS: at 80.48% examples, 169617 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:42:44,979 : INFO : PROGRESS: at 84.28% examples, 170303 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:43:05,056 : INFO : PROGRESS: at 87.94% examples, 170672 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:43:25,162 : INFO : PROGRESS: at 91.47% examples, 170761 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 01:43:45,163 : INFO : PROGRESS: at 95.06% examples, 171002 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:44:05,207 : INFO : PROGRESS: at 98.77% examples, 171417 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 01:44:10,379 : INFO : worker thread finished; awaiting finish of 21 more threads\n",
      "2017-02-27 01:44:10,395 : INFO : worker thread finished; awaiting finish of 20 more threads\n",
      "2017-02-27 01:44:10,408 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2017-02-27 01:44:10,413 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2017-02-27 01:44:10,436 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2017-02-27 01:44:10,463 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2017-02-27 01:44:10,495 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2017-02-27 01:44:10,499 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2017-02-27 01:44:10,605 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2017-02-27 01:44:10,643 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2017-02-27 01:44:10,660 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2017-02-27 01:44:10,669 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2017-02-27 01:44:10,675 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2017-02-27 01:44:10,690 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2017-02-27 01:44:10,695 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-02-27 01:44:10,698 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-02-27 01:44:10,702 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-02-27 01:44:10,713 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-02-27 01:44:10,721 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-02-27 01:44:10,724 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-27 01:44:10,736 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-27 01:44:10,745 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-27 01:44:10,746 : INFO : training on 136858217 raw words (98494362 effective words) took 573.1s, 171869 effective words/s\n",
      "2017-02-27 01:44:10,753 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_5/model, separately None\n",
      "2017-02-27 01:44:10,763 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_5/model.docvecs.doctag_syn0.npy\n",
      "2017-02-27 01:44:12,371 : INFO : storing numpy array 'syn1neg' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_5/model.syn1neg.npy\n",
      "2017-02-27 01:44:12,571 : INFO : not storing attribute syn0norm\n",
      "2017-02-27 01:44:12,573 : INFO : storing numpy array 'syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_5/model.syn0.npy\n",
      "2017-02-27 01:44:12,851 : INFO : not storing attribute cum_table\n",
      "2017-02-27 01:48:26,630 : INFO : Getting training Data\n",
      "2017-02-27 01:48:26,637 : INFO : Finished 0 in training\n",
      "2017-02-27 01:48:27,548 : INFO : Finished 100000 in training\n",
      "2017-02-27 01:48:28,377 : INFO : Finished 200000 in training\n",
      "2017-02-27 01:48:29,239 : INFO : Finished 300000 in training\n",
      "2017-02-27 01:48:29,983 : INFO : Finished 400000 in training\n",
      "2017-02-27 01:48:30,606 : INFO : Finished 500000 in training\n",
      "2017-02-27 01:48:31,240 : INFO : Finished 600000 in training\n",
      "2017-02-27 01:48:31,895 : INFO : Finished 700000 in training\n",
      "2017-02-27 01:48:32,667 : INFO : Finished 800000 in training\n",
      "2017-02-27 01:48:33,535 : INFO : Finished 900000 in training\n",
      "2017-02-27 01:48:34,296 : INFO : Finished 1000000 in training\n",
      "2017-02-27 01:48:34,992 : INFO : Finished 1100000 in training\n",
      "2017-02-27 01:48:35,669 : INFO : Finished 1200000 in training\n",
      "2017-02-27 01:48:36,198 : INFO : doing matrix creation\n",
      "2017-02-27 01:48:42,552 : INFO : Training Classifier\n",
      "2017-02-27 01:53:15,743 : INFO : Getting Validation Embeddings\n",
      "2017-02-27 01:53:15,795 : INFO : ===== Getting vectors with inference\n",
      "2017-02-27 02:08:26,730 : INFO : Finished: 1999720\n",
      "2017-02-27 02:09:32,067 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "** Validation Metrics: Cov Err: 3.222, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.482, Top 3: 0.701, Top 5: 0.768, \n",
      "\t\t F1 Micro: 0.299, F1 Macro: 0.112, Total Pos: 102,263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-27 02:09:57,109 : INFO : ****************** Epoch 6 --- Working on doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_6 *******************\n",
      "2017-02-27 02:10:00,898 : INFO : training model with 22 workers on 181046 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=10\n",
      "2017-02-27 02:10:00,902 : INFO : expecting 1279894 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-27 02:10:08,629 : INFO : PROGRESS: at 0.01% examples, 2400 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:10:28,645 : INFO : PROGRESS: at 2.52% examples, 108152 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 02:10:48,665 : INFO : PROGRESS: at 5.78% examples, 132632 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:11:08,726 : INFO : PROGRESS: at 9.48% examples, 148136 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:11:28,730 : INFO : PROGRESS: at 13.44% examples, 159293 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:11:48,748 : INFO : PROGRESS: at 17.89% examples, 170892 words/s, in_qsize 44, out_qsize 1\n",
      "2017-02-27 02:12:08,756 : INFO : PROGRESS: at 22.70% examples, 181596 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:12:28,843 : INFO : PROGRESS: at 27.28% examples, 187637 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:12:48,864 : INFO : PROGRESS: at 32.35% examples, 195193 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:13:08,873 : INFO : PROGRESS: at 37.76% examples, 202915 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:13:28,956 : INFO : PROGRESS: at 42.81% examples, 207330 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:13:48,970 : INFO : PROGRESS: at 48.46% examples, 213627 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:14:08,991 : INFO : PROGRESS: at 53.92% examples, 218120 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:14:29,002 : INFO : PROGRESS: at 59.47% examples, 222333 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 02:14:49,004 : INFO : PROGRESS: at 64.30% examples, 223375 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:15:09,029 : INFO : PROGRESS: at 69.50% examples, 225471 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 02:15:29,064 : INFO : PROGRESS: at 74.58% examples, 227057 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:15:49,085 : INFO : PROGRESS: at 79.99% examples, 229350 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:16:09,087 : INFO : PROGRESS: at 85.13% examples, 230641 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 02:16:29,088 : INFO : PROGRESS: at 90.47% examples, 232330 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:16:49,126 : INFO : PROGRESS: at 95.94% examples, 234160 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:17:04,140 : INFO : worker thread finished; awaiting finish of 21 more threads\n",
      "2017-02-27 02:17:04,164 : INFO : worker thread finished; awaiting finish of 20 more threads\n",
      "2017-02-27 02:17:04,202 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2017-02-27 02:17:04,207 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2017-02-27 02:17:04,213 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2017-02-27 02:17:04,252 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2017-02-27 02:17:04,259 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2017-02-27 02:17:04,267 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2017-02-27 02:17:04,273 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2017-02-27 02:17:04,278 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2017-02-27 02:17:04,292 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2017-02-27 02:17:04,302 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2017-02-27 02:17:04,336 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2017-02-27 02:17:04,358 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2017-02-27 02:17:04,363 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-02-27 02:17:04,369 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-02-27 02:17:04,373 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-02-27 02:17:04,379 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-02-27 02:17:04,382 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-02-27 02:17:04,384 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-27 02:17:04,390 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-27 02:17:04,391 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-27 02:17:04,392 : INFO : training on 136858217 raw words (98494055 effective words) took 418.8s, 235206 effective words/s\n",
      "2017-02-27 02:17:04,395 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_6/model, separately None\n",
      "2017-02-27 02:17:04,403 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_6/model.docvecs.doctag_syn0.npy\n",
      "2017-02-27 02:17:06,031 : INFO : storing numpy array 'syn1neg' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_6/model.syn1neg.npy\n",
      "2017-02-27 02:17:06,233 : INFO : not storing attribute syn0norm\n",
      "2017-02-27 02:17:06,235 : INFO : storing numpy array 'syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_6/model.syn0.npy\n",
      "2017-02-27 02:17:06,475 : INFO : not storing attribute cum_table\n",
      "2017-02-27 02:17:22,767 : INFO : Getting training Data\n",
      "2017-02-27 02:17:22,772 : INFO : Finished 0 in training\n",
      "2017-02-27 02:17:23,678 : INFO : Finished 100000 in training\n",
      "2017-02-27 02:17:24,819 : INFO : Finished 200000 in training\n",
      "2017-02-27 02:17:26,261 : INFO : Finished 300000 in training\n",
      "2017-02-27 02:17:27,817 : INFO : Finished 400000 in training\n",
      "2017-02-27 02:17:29,096 : INFO : Finished 500000 in training\n",
      "2017-02-27 02:17:30,346 : INFO : Finished 600000 in training\n",
      "2017-02-27 02:17:31,683 : INFO : Finished 700000 in training\n",
      "2017-02-27 02:17:32,784 : INFO : Finished 800000 in training\n",
      "2017-02-27 02:17:34,001 : INFO : Finished 900000 in training\n",
      "2017-02-27 02:17:35,262 : INFO : Finished 1000000 in training\n",
      "2017-02-27 02:17:36,643 : INFO : Finished 1100000 in training\n",
      "2017-02-27 02:17:38,149 : INFO : Finished 1200000 in training\n",
      "2017-02-27 02:17:39,379 : INFO : doing matrix creation\n",
      "2017-02-27 02:17:45,717 : INFO : Training Classifier\n",
      "2017-02-27 02:21:53,616 : INFO : Getting Validation Embeddings\n",
      "2017-02-27 02:21:53,626 : INFO : ===== Getting vectors with inference\n",
      "2017-02-27 02:31:42,674 : INFO : Finished: 1999720\n",
      "2017-02-27 02:32:16,018 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "** Validation Metrics: Cov Err: 3.290, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.470, Top 3: 0.690, Top 5: 0.758, \n",
      "\t\t F1 Micro: 0.301, F1 Macro: 0.108, Total Pos: 112,426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-27 02:32:36,327 : INFO : ****************** Epoch 7 --- Working on doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_7 *******************\n",
      "2017-02-27 02:32:36,716 : INFO : training model with 22 workers on 181046 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=10\n",
      "2017-02-27 02:32:36,720 : INFO : expecting 1279894 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-27 02:32:38,568 : INFO : PROGRESS: at 0.17% examples, 142882 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 02:32:58,602 : INFO : PROGRESS: at 7.01% examples, 326048 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 02:33:18,617 : INFO : PROGRESS: at 13.78% examples, 329480 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:33:38,670 : INFO : PROGRESS: at 20.62% examples, 331622 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 02:33:58,695 : INFO : PROGRESS: at 27.41% examples, 332203 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 02:34:18,749 : INFO : PROGRESS: at 34.22% examples, 332535 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:34:38,769 : INFO : PROGRESS: at 41.07% examples, 333246 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:34:58,808 : INFO : PROGRESS: at 47.87% examples, 333367 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 02:35:18,825 : INFO : PROGRESS: at 54.80% examples, 334252 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:35:38,828 : INFO : PROGRESS: at 61.61% examples, 334367 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:35:58,840 : INFO : PROGRESS: at 68.27% examples, 333614 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:36:18,853 : INFO : PROGRESS: at 74.80% examples, 332631 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:36:38,861 : INFO : PROGRESS: at 81.59% examples, 332782 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 02:36:58,874 : INFO : PROGRESS: at 88.36% examples, 332782 words/s, in_qsize 44, out_qsize 1\n",
      "2017-02-27 02:37:18,892 : INFO : PROGRESS: at 94.99% examples, 332371 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 02:37:33,450 : INFO : worker thread finished; awaiting finish of 21 more threads\n",
      "2017-02-27 02:37:33,457 : INFO : worker thread finished; awaiting finish of 20 more threads\n",
      "2017-02-27 02:37:33,463 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2017-02-27 02:37:33,470 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2017-02-27 02:37:33,477 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2017-02-27 02:37:33,482 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2017-02-27 02:37:33,488 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2017-02-27 02:37:33,535 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2017-02-27 02:37:33,557 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2017-02-27 02:37:33,582 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2017-02-27 02:37:33,585 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2017-02-27 02:37:33,590 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2017-02-27 02:37:33,599 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2017-02-27 02:37:33,609 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2017-02-27 02:37:33,613 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-02-27 02:37:33,618 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-02-27 02:37:33,624 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-02-27 02:37:33,637 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-02-27 02:37:33,641 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-02-27 02:37:33,650 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-27 02:37:33,652 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-27 02:37:33,653 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-27 02:37:33,654 : INFO : training on 136858217 raw words (98498023 effective words) took 296.2s, 332496 effective words/s\n",
      "2017-02-27 02:37:33,655 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_7/model, separately None\n",
      "2017-02-27 02:37:33,656 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_7/model.docvecs.doctag_syn0.npy\n",
      "2017-02-27 02:37:35,283 : INFO : storing numpy array 'syn1neg' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_7/model.syn1neg.npy\n",
      "2017-02-27 02:37:35,481 : INFO : not storing attribute syn0norm\n",
      "2017-02-27 02:37:35,482 : INFO : storing numpy array 'syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_7/model.syn0.npy\n",
      "2017-02-27 02:37:35,705 : INFO : not storing attribute cum_table\n",
      "2017-02-27 02:41:56,466 : INFO : Getting training Data\n",
      "2017-02-27 02:41:56,468 : INFO : Finished 0 in training\n",
      "2017-02-27 02:41:57,605 : INFO : Finished 100000 in training\n",
      "2017-02-27 02:41:58,834 : INFO : Finished 200000 in training\n",
      "2017-02-27 02:41:59,921 : INFO : Finished 300000 in training\n",
      "2017-02-27 02:42:00,879 : INFO : Finished 400000 in training\n",
      "2017-02-27 02:42:01,756 : INFO : Finished 500000 in training\n",
      "2017-02-27 02:42:02,962 : INFO : Finished 600000 in training\n",
      "2017-02-27 02:42:04,203 : INFO : Finished 700000 in training\n",
      "2017-02-27 02:42:05,356 : INFO : Finished 800000 in training\n",
      "2017-02-27 02:42:06,490 : INFO : Finished 900000 in training\n",
      "2017-02-27 02:42:07,618 : INFO : Finished 1000000 in training\n",
      "2017-02-27 02:42:08,796 : INFO : Finished 1100000 in training\n",
      "2017-02-27 02:42:10,046 : INFO : Finished 1200000 in training\n",
      "2017-02-27 02:42:11,134 : INFO : doing matrix creation\n",
      "2017-02-27 02:42:19,963 : INFO : Training Classifier\n",
      "2017-02-27 02:47:11,822 : INFO : Getting Validation Embeddings\n",
      "2017-02-27 02:47:11,922 : INFO : ===== Getting vectors with inference\n",
      "2017-02-27 03:06:11,313 : INFO : Finished: 1999720\n",
      "2017-02-27 03:07:35,719 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]]\n",
      "** Validation Metrics: Cov Err: 3.252, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.473, Top 3: 0.695, Top 5: 0.766, \n",
      "\t\t F1 Micro: 0.309, F1 Macro: 0.115, Total Pos: 113,022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-27 03:08:02,362 : INFO : ****************** Epoch 8 --- Working on doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8 *******************\n",
      "2017-02-27 03:08:07,929 : INFO : training model with 22 workers on 181046 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=10\n",
      "2017-02-27 03:08:07,939 : INFO : expecting 1279894 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-27 03:08:12,092 : INFO : PROGRESS: at 0.01% examples, 2038 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 03:08:32,148 : INFO : PROGRESS: at 2.22% examples, 92793 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:08:52,153 : INFO : PROGRESS: at 5.26% examples, 119108 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 03:09:12,206 : INFO : PROGRESS: at 8.83% examples, 136859 words/s, in_qsize 44, out_qsize 1\n",
      "2017-02-27 03:09:32,256 : INFO : PROGRESS: at 12.47% examples, 146830 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 03:09:52,291 : INFO : PROGRESS: at 16.65% examples, 158114 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:10:12,318 : INFO : PROGRESS: at 21.28% examples, 169415 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 03:10:32,422 : INFO : PROGRESS: at 25.70% examples, 175980 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:10:52,491 : INFO : PROGRESS: at 30.26% examples, 181781 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:11:12,558 : INFO : PROGRESS: at 34.13% examples, 182680 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:11:32,581 : INFO : PROGRESS: at 37.49% examples, 180978 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:11:52,580 : INFO : PROGRESS: at 41.65% examples, 183086 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:12:12,587 : INFO : PROGRESS: at 45.37% examples, 183089 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:12:32,653 : INFO : PROGRESS: at 48.10% examples, 179355 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:12:52,685 : INFO : PROGRESS: at 51.48% examples, 178409 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:13:12,688 : INFO : PROGRESS: at 55.86% examples, 180831 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:13:32,719 : INFO : PROGRESS: at 60.30% examples, 183177 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 03:13:52,763 : INFO : PROGRESS: at 64.60% examples, 184770 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:14:12,805 : INFO : PROGRESS: at 68.72% examples, 185725 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 03:14:32,895 : INFO : PROGRESS: at 73.13% examples, 187350 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 03:14:52,922 : INFO : PROGRESS: at 77.39% examples, 188477 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 03:15:12,944 : INFO : PROGRESS: at 82.11% examples, 190516 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:15:32,950 : INFO : PROGRESS: at 86.91% examples, 192609 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:15:52,990 : INFO : PROGRESS: at 91.63% examples, 194315 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 03:16:12,995 : INFO : PROGRESS: at 96.45% examples, 196095 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:16:27,428 : INFO : worker thread finished; awaiting finish of 21 more threads\n",
      "2017-02-27 03:16:27,444 : INFO : worker thread finished; awaiting finish of 20 more threads\n",
      "2017-02-27 03:16:27,459 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2017-02-27 03:16:27,465 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2017-02-27 03:16:27,476 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2017-02-27 03:16:27,512 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2017-02-27 03:16:27,514 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2017-02-27 03:16:27,529 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2017-02-27 03:16:27,539 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2017-02-27 03:16:27,572 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2017-02-27 03:16:27,583 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2017-02-27 03:16:27,586 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2017-02-27 03:16:27,624 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2017-02-27 03:16:27,628 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2017-02-27 03:16:27,635 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-02-27 03:16:27,638 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-02-27 03:16:27,640 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-02-27 03:16:27,660 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-02-27 03:16:27,662 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-02-27 03:16:27,664 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-27 03:16:27,672 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-27 03:16:27,678 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-27 03:16:27,679 : INFO : training on 136858217 raw words (98496510 effective words) took 499.1s, 197348 effective words/s\n",
      "2017-02-27 03:16:27,680 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model, separately None\n",
      "2017-02-27 03:16:27,683 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.docvecs.doctag_syn0.npy\n",
      "2017-02-27 03:16:29,339 : INFO : storing numpy array 'syn1neg' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.syn1neg.npy\n",
      "2017-02-27 03:16:29,535 : INFO : not storing attribute syn0norm\n",
      "2017-02-27 03:16:29,536 : INFO : storing numpy array 'syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.syn0.npy\n",
      "2017-02-27 03:16:29,847 : INFO : not storing attribute cum_table\n",
      "2017-02-27 03:16:45,244 : INFO : Getting training Data\n",
      "2017-02-27 03:16:45,262 : INFO : Finished 0 in training\n",
      "2017-02-27 03:16:47,038 : INFO : Finished 100000 in training\n",
      "2017-02-27 03:16:48,083 : INFO : Finished 200000 in training\n",
      "2017-02-27 03:16:48,927 : INFO : Finished 300000 in training\n",
      "2017-02-27 03:16:49,760 : INFO : Finished 400000 in training\n",
      "2017-02-27 03:16:50,508 : INFO : Finished 500000 in training\n",
      "2017-02-27 03:16:51,297 : INFO : Finished 600000 in training\n",
      "2017-02-27 03:16:52,013 : INFO : Finished 700000 in training\n",
      "2017-02-27 03:16:52,763 : INFO : Finished 800000 in training\n",
      "2017-02-27 03:16:53,450 : INFO : Finished 900000 in training\n",
      "2017-02-27 03:16:54,093 : INFO : Finished 1000000 in training\n",
      "2017-02-27 03:16:54,778 : INFO : Finished 1100000 in training\n",
      "2017-02-27 03:16:55,511 : INFO : Finished 1200000 in training\n",
      "2017-02-27 03:16:56,130 : INFO : doing matrix creation\n",
      "2017-02-27 03:16:58,622 : INFO : Training Classifier\n",
      "2017-02-27 03:21:00,204 : INFO : Getting Validation Embeddings\n",
      "2017-02-27 03:21:00,251 : INFO : ===== Getting vectors with inference\n",
      "2017-02-27 03:35:02,112 : INFO : Finished: 1999720\n",
      "2017-02-27 03:36:10,052 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "** Validation Metrics: Cov Err: 3.229, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.475, Top 3: 0.701, Top 5: 0.772, \n",
      "\t\t F1 Micro: 0.302, F1 Macro: 0.113, Total Pos: 107,665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-27 03:36:31,631 : INFO : ****************** Epoch 9 --- Working on doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_9 *******************\n",
      "2017-02-27 03:36:35,233 : INFO : training model with 22 workers on 181046 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=10\n",
      "2017-02-27 03:36:35,253 : INFO : expecting 1279894 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-27 03:36:38,595 : INFO : PROGRESS: at 0.01% examples, 3116 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:36:58,654 : INFO : PROGRESS: at 2.41% examples, 106203 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 03:37:18,669 : INFO : PROGRESS: at 5.71% examples, 132995 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:37:38,715 : INFO : PROGRESS: at 9.66% examples, 152606 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 03:37:58,745 : INFO : PROGRESS: at 13.86% examples, 165522 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:38:18,794 : INFO : PROGRESS: at 18.36% examples, 176414 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 03:38:38,808 : INFO : PROGRESS: at 22.57% examples, 181409 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 03:38:58,823 : INFO : PROGRESS: at 26.62% examples, 183987 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:39:18,863 : INFO : PROGRESS: at 31.34% examples, 189857 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:39:38,903 : INFO : PROGRESS: at 35.97% examples, 194059 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 03:39:58,911 : INFO : PROGRESS: at 40.69% examples, 197769 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 03:40:18,914 : INFO : PROGRESS: at 45.35% examples, 200602 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:40:38,952 : INFO : PROGRESS: at 50.20% examples, 203755 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:40:58,968 : INFO : PROGRESS: at 54.81% examples, 205429 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:41:18,984 : INFO : PROGRESS: at 59.89% examples, 208611 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:41:38,993 : INFO : PROGRESS: at 64.77% examples, 210651 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:41:59,024 : INFO : PROGRESS: at 69.69% examples, 212569 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:42:19,088 : INFO : PROGRESS: at 74.38% examples, 213667 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 03:42:39,120 : INFO : PROGRESS: at 79.23% examples, 215057 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:42:59,137 : INFO : PROGRESS: at 83.88% examples, 215765 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 03:43:19,140 : INFO : PROGRESS: at 88.34% examples, 215959 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:43:39,153 : INFO : PROGRESS: at 93.22% examples, 217141 words/s, in_qsize 44, out_qsize 0\n",
      "2017-02-27 03:43:59,212 : INFO : PROGRESS: at 97.85% examples, 217630 words/s, in_qsize 43, out_qsize 0\n",
      "2017-02-27 03:44:07,704 : INFO : worker thread finished; awaiting finish of 21 more threads\n",
      "2017-02-27 03:44:07,724 : INFO : worker thread finished; awaiting finish of 20 more threads\n",
      "2017-02-27 03:44:07,733 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2017-02-27 03:44:07,737 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2017-02-27 03:44:07,766 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2017-02-27 03:44:07,770 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2017-02-27 03:44:07,830 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2017-02-27 03:44:07,945 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2017-02-27 03:44:07,955 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2017-02-27 03:44:07,989 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2017-02-27 03:44:08,033 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2017-02-27 03:44:08,041 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2017-02-27 03:44:08,049 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2017-02-27 03:44:08,052 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2017-02-27 03:44:08,055 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-02-27 03:44:08,078 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-02-27 03:44:08,081 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-02-27 03:44:08,092 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-02-27 03:44:08,096 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-02-27 03:44:08,100 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-27 03:44:08,109 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-27 03:44:08,112 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-27 03:44:08,113 : INFO : training on 136858217 raw words (98501225 effective words) took 451.8s, 218014 effective words/s\n",
      "2017-02-27 03:44:08,117 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_9/model, separately None\n",
      "2017-02-27 03:44:08,118 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_9/model.docvecs.doctag_syn0.npy\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "639947000 requested and 66490348 written",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-115-b1c422d35d90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'%matplotlib notebook\\ngraph = MetricsGraph()\\ngraph.init_graph(len(classifications) +2)\\n# when resuming, resume from an epoch with a previously created doc2vec model to get the learning rate right\\nstart_from = 1\\nfor epoch in range(start_from, DOC2VEC_MAX_EPOCHS+1):\\n    GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\\n    info(\"****************** Epoch {} --- Working on {} *******************\".format(epoch, GLOBAL_VARS.MODEL_NAME))\\n    \\n    # if we have the model, just load it, otherwise train the previous model\\n    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)):\\n        doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\\n        GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\\n    else:\\n        # train the doc2vec model\\n        training_docs_iterator = AbstractDocumentGenerator(abstracts_map, training_docs_list)\\n        doc2vec_model.train(sentences=training_docs_iterator, report_delay=REPORT_DELAY)\\n        doc2vec_model.alpha -= DOC2VEC_ALPHA_DECREASE  # decrease the learning rate\\n        doc2vec_model.min_alpha = doc2vec_model.alpha  # fix the learning rate, no decay\\n        ensure_disk_location_exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME))\\n        doc2vec_model.save(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\\n        GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\\n        \\n        # get the word2vec analogy accuracy score\\n#         word2vec_result = doc2vec_model.accuracy(word2vec_questions_file, restrict_vocab=None)\\n#         epoch_word2vec_metrics.append(word2vec_result)\\n#         pickle.dump(word2vec_result, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME,\\n#                                                        WORD2VEC_METRICS_FILENAME), \\'w\\'))\\n\\n\\n    ensure_disk_location_exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \\n                                             GLOBAL_VARS.SVM_MODEL_NAME))\\n    \\n    if not os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \\n                                                          GLOBAL_VARS.SVM_MODEL_NAME, CLASSIFIER)):\\n\\n        info(\\'Getting training Data\\')\\n        X, y = get_training_data(doc2vec_model, classifications)\\n        \\n        info(\\'Training Classifier\\')\\n        clf = OneVsRestClassifier(linear_model.SGDClassifier(loss=\\'hinge\\', penalty=\\'l2\\', \\n                                                             #alpha is the 1/C parameter\\n                                                             alpha=SVM_REG, fit_intercept=True, n_iter=SVM_ITERATIONS,\\n                                                             #n_jobs=-1 means use all cpus\\n                                                             shuffle=True, verbose=0, n_jobs=1,\\n                                                             #eta0 is the learning rate when we use constant configuration\\n                                                             random_state=SVM_SEED, learning_rate=\\'optimal\\', eta0=0.0, \\n                                                             class_weight=SVM_CLASS_WEIGHTS, warm_start=False), n_jobs=1)\\n\\n\\n        # Training of a classifier\\n        clf.fit(X,y)\\n        pickle.dump(clf, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \\n                                                              GLOBAL_VARS.SVM_MODEL_NAME, CLASSIFIER), \\'w\\'))\\n        \\n    #     # Training Metrics\\n    #     info(\\'Evaluating on Training Data\\')\\n    #     yp = clf.predict(X)\\n    #     yp_score = clf.decision_function(X)\\n    #     print yp\\n    #     training_metrics = get_metrics(y, yp_score, yp)\\n    #     print \"** Training Metrics: Cov Err: {:.3f}, Avg Labels: {:.3f}, \\\\n\\\\t\\\\t Top 1: {:.3f}, Top 3: {:.3f}, Top 5: {:.3f}, \\\\n\\\\t\\\\t F1 Micro: {:.3f}, F1 Macro: {:.3f}, Total Pos: {:,d}\".format(\\n    #         training_metrics[\\'coverage_error\\'], training_metrics[\\'average_num_of_labels\\'], \\n    #         training_metrics[\\'top_1\\'], training_metrics[\\'top_3\\'], training_metrics[\\'top_5\\'], \\n    #         training_metrics[\\'f1_micro\\'], training_metrics[\\'f1_macro\\'], training_metrics[\\'total_positive\\'])\\n\\n    #     epoch_training_metrics.append(training_metrics)\\n          # Saving the metrics\\n    #     pickle.dump(training_metrics, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \\n    #                                                           GLOBAL_VARS.SVM_MODEL_NAME, TRAINING_METRICS_FILENAME), \\'w\\'))\\n        del X, y\\n\\n    else:\\n        info(\\'Loading Classifier\\')\\n        clf = pickle.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \\n                                                          GLOBAL_VARS.SVM_MODEL_NAME, CLASSIFIER), \\'r\\'))\\n\\n    # Validation Metrics\\n    info(\\'Getting Validation Embeddings\\')\\n    Xv, yv = get_docs_with_inference(doc2vec_model, doc_classification_map, classifications, \\n                                                    validation_docs_list, VALIDATION_MATRIX, abstracts_map)\\n    info(\\'Evaluating on Validation Data\\')\\n    yvp = clf.predict(Xv)\\n    yvp_score = clf.decision_function(Xv)\\n    print yvp\\n    validation_metrics = get_metrics(yv, yvp_score, yvp)\\n    print \"** Validation Metrics: Cov Err: {:.3f}, Avg Labels: {:.3f}, \\\\n\\\\t\\\\t Top 1: {:.3f}, Top 3: {:.3f}, Top 5: {:.3f}, \\\\n\\\\t\\\\t F1 Micro: {:.3f}, F1 Macro: {:.3f}, Total Pos: {:,d}\".format(\\n        validation_metrics[\\'coverage_error\\'], validation_metrics[\\'average_num_of_labels\\'], \\n        validation_metrics[\\'top_1\\'], validation_metrics[\\'top_3\\'], validation_metrics[\\'top_5\\'], \\n        validation_metrics[\\'f1_micro\\'], validation_metrics[\\'f1_macro\\'], validation_metrics[\\'total_positive\\'])\\n    \\n    graph.add_metrics_to_graph(validation_metrics, epoch)\\n    \\n    epoch_validation_metrics.append(validation_metrics)\\n    \\n    \\n    # Saving the metrics\\n#     pickle.dump(training_metrics, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \\n#                                                           GLOBAL_VARS.SVM_MODEL_NAME, TRAINING_METRICS_FILENAME), \\'w\\'))\\n    pickle.dump(validation_metrics, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \\n                                                          GLOBAL_VARS.SVM_MODEL_NAME, VALIDATION_METRICS_FILENAME), \\'w\\'))'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1641\u001b[0m         \u001b[1;31m# don't bother storing the cached normalized vectors, recalculable table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1642\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'syn0norm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'table'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cum_table'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1643\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1645\u001b[0m     \u001b[0msave\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaveLoad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/gensim/utils.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# `fname_or_handle` does not have write attribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             self._smart_save(fname_or_handle, separately, sep_limit, ignore,\n\u001b[1;32m--> 478\u001b[1;33m                              pickle_protocol=pickle_protocol)\n\u001b[0m\u001b[0;32m    479\u001b[0m \u001b[1;31m#endclass SaveLoad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/gensim/utils.pyc\u001b[0m in \u001b[0;36m_smart_save\u001b[1;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         restores = self._save_specials(fname, separately, sep_limit, ignore, pickle_protocol,\n\u001b[1;32m--> 349\u001b[1;33m                                        compress, subname)\n\u001b[0m\u001b[0;32m    350\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/gensim/utils.pyc\u001b[0m in \u001b[0;36m_save_specials\u001b[1;34m(self, fname, separately, sep_limit, ignore, pickle_protocol, compress, subname)\u001b[0m\n\u001b[0;32m    390\u001b[0m                 \u001b[0mcfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mattrib\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m                 restores.extend(val._save_specials(cfname, None, sep_limit, ignore,\n\u001b[1;32m--> 392\u001b[1;33m                                                    pickle_protocol, compress, subname))\n\u001b[0m\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/gensim/utils.pyc\u001b[0m in \u001b[0;36m_save_specials\u001b[1;34m(self, fname, separately, sep_limit, ignore, pickle_protocol, compress, subname)\u001b[0m\n\u001b[0;32m    403\u001b[0m                         \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavez_compressed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m                         \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrib\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mattrib\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mignore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[1;32m--> 491\u001b[1;33m                            pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[0;32m    492\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/numpy/lib/format.pyc\u001b[0m in \u001b[0;36mwrite_array\u001b[1;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m             \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m             for chunk in numpy.nditer(\n",
      "\u001b[1;31mIOError\u001b[0m: 639947000 requested and 66490348 written"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%matplotlib notebook\n",
    "graph = MetricsGraph()\n",
    "graph.init_graph(len(classifications) +2)\n",
    "# when resuming, resume from an epoch with a previously created doc2vec model to get the learning rate right\n",
    "start_from = 1\n",
    "for epoch in range(start_from, DOC2VEC_MAX_EPOCHS+1):\n",
    "    GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "    info(\"****************** Epoch {} --- Working on {} *******************\".format(epoch, GLOBAL_VARS.MODEL_NAME))\n",
    "    \n",
    "    # if we have the model, just load it, otherwise train the previous model\n",
    "    if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)):\n",
    "        doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "        GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "    else:\n",
    "        # train the doc2vec model\n",
    "        training_docs_iterator = AbstractDocumentGenerator(abstracts_map, training_docs_list)\n",
    "        doc2vec_model.train(sentences=training_docs_iterator, report_delay=REPORT_DELAY)\n",
    "        doc2vec_model.alpha -= DOC2VEC_ALPHA_DECREASE  # decrease the learning rate\n",
    "        doc2vec_model.min_alpha = doc2vec_model.alpha  # fix the learning rate, no decay\n",
    "        ensure_disk_location_exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME))\n",
    "        doc2vec_model.save(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "        GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "        \n",
    "        # get the word2vec analogy accuracy score\n",
    "#         word2vec_result = doc2vec_model.accuracy(word2vec_questions_file, restrict_vocab=None)\n",
    "#         epoch_word2vec_metrics.append(word2vec_result)\n",
    "#         pickle.dump(word2vec_result, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME,\n",
    "#                                                        WORD2VEC_METRICS_FILENAME), 'w'))\n",
    "\n",
    "\n",
    "    ensure_disk_location_exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                             GLOBAL_VARS.SVM_MODEL_NAME))\n",
    "    \n",
    "    if not os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                          GLOBAL_VARS.SVM_MODEL_NAME, CLASSIFIER)):\n",
    "\n",
    "        info('Getting training Data')\n",
    "        X, y = get_training_data(doc2vec_model, classifications)\n",
    "        \n",
    "        info('Training Classifier')\n",
    "        clf = OneVsRestClassifier(linear_model.SGDClassifier(loss='hinge', penalty='l2', \n",
    "                                                             #alpha is the 1/C parameter\n",
    "                                                             alpha=SVM_REG, fit_intercept=True, n_iter=SVM_ITERATIONS,\n",
    "                                                             #n_jobs=-1 means use all cpus\n",
    "                                                             shuffle=True, verbose=0, n_jobs=1,\n",
    "                                                             #eta0 is the learning rate when we use constant configuration\n",
    "                                                             random_state=SVM_SEED, learning_rate='optimal', eta0=0.0, \n",
    "                                                             class_weight=SVM_CLASS_WEIGHTS, warm_start=False), n_jobs=1)\n",
    "\n",
    "\n",
    "        # Training of a classifier\n",
    "        clf.fit(X,y)\n",
    "        pickle.dump(clf, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                              GLOBAL_VARS.SVM_MODEL_NAME, CLASSIFIER), 'w'))\n",
    "        \n",
    "    #     # Training Metrics\n",
    "    #     info('Evaluating on Training Data')\n",
    "    #     yp = clf.predict(X)\n",
    "    #     yp_score = clf.decision_function(X)\n",
    "    #     print yp\n",
    "    #     training_metrics = get_metrics(y, yp_score, yp)\n",
    "    #     print \"** Training Metrics: Cov Err: {:.3f}, Avg Labels: {:.3f}, \\n\\t\\t Top 1: {:.3f}, Top 3: {:.3f}, Top 5: {:.3f}, \\n\\t\\t F1 Micro: {:.3f}, F1 Macro: {:.3f}, Total Pos: {:,d}\".format(\n",
    "    #         training_metrics['coverage_error'], training_metrics['average_num_of_labels'], \n",
    "    #         training_metrics['top_1'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "    #         training_metrics['f1_micro'], training_metrics['f1_macro'], training_metrics['total_positive'])\n",
    "\n",
    "    #     epoch_training_metrics.append(training_metrics)\n",
    "          # Saving the metrics\n",
    "    #     pickle.dump(training_metrics, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "    #                                                           GLOBAL_VARS.SVM_MODEL_NAME, TRAINING_METRICS_FILENAME), 'w'))\n",
    "        del X, y\n",
    "\n",
    "    else:\n",
    "        info('Loading Classifier')\n",
    "        clf = pickle.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                          GLOBAL_VARS.SVM_MODEL_NAME, CLASSIFIER), 'r'))\n",
    "\n",
    "    # Validation Metrics\n",
    "    info('Getting Validation Embeddings')\n",
    "    Xv, yv = get_docs_with_inference(doc2vec_model, doc_classification_map, classifications, \n",
    "                                                    validation_docs_list, VALIDATION_MATRIX, abstracts_map)\n",
    "    info('Evaluating on Validation Data')\n",
    "    yvp = clf.predict(Xv)\n",
    "    yvp_score = clf.decision_function(Xv)\n",
    "    print yvp\n",
    "    validation_metrics = get_metrics(yv, yvp_score, yvp)\n",
    "    print \"** Validation Metrics: Cov Err: {:.3f}, Avg Labels: {:.3f}, \\n\\t\\t Top 1: {:.3f}, Top 3: {:.3f}, Top 5: {:.3f}, \\n\\t\\t F1 Micro: {:.3f}, F1 Macro: {:.3f}, Total Pos: {:,d}\".format(\n",
    "        validation_metrics['coverage_error'], validation_metrics['average_num_of_labels'], \n",
    "        validation_metrics['top_1'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "        validation_metrics['f1_micro'], validation_metrics['f1_macro'], validation_metrics['total_positive'])\n",
    "    \n",
    "    graph.add_metrics_to_graph(validation_metrics, epoch)\n",
    "    \n",
    "    epoch_validation_metrics.append(validation_metrics)\n",
    "    \n",
    "    \n",
    "    # Saving the metrics\n",
    "#     pickle.dump(training_metrics, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "#                                                           GLOBAL_VARS.SVM_MODEL_NAME, TRAINING_METRICS_FILENAME), 'w'))\n",
    "    pickle.dump(validation_metrics, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                          GLOBAL_VARS.SVM_MODEL_NAME, VALIDATION_METRICS_FILENAME), 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-26 23:26:11,267 : INFO : saving Doc2Vec object under /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_4/model, separately None\n",
      "2017-02-26 23:26:11,273 : INFO : storing numpy array 'doctag_syn0' to /mnt/data2/shalaby/parameter_search_doc2vec_models_new_abstract/full/doc2vec_size_500_w_4_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_4/model.docvecs.doctag_syn0.npy\n"
     ]
    }
   ],
   "source": [
    "doc2vec_model.save(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-07 08:32:48,888 : INFO : ****************** Epoch 6 --- Working on doc2vec_size_500_w_8_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_6 *******************\n",
      "2017-01-07 08:32:48,891 : INFO : loading Doc2Vec object from /big/s/shalaby/parameter_search_doc2vec_models_new/full/doc2vec_size_500_w_8_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_6/model\n",
      "2017-01-07 08:33:14,046 : INFO : loading docvecs recursively from /big/s/shalaby/parameter_search_doc2vec_models_new/full/doc2vec_size_500_w_8_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_6/model.docvecs.* with mmap=None\n",
      "2017-01-07 08:33:14,048 : INFO : loading doctag_syn0 from /big/s/shalaby/parameter_search_doc2vec_models_new/full/doc2vec_size_500_w_8_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_6/model.docvecs.doctag_syn0.npy with mmap=None\n",
      "2017-01-07 08:33:21,936 : INFO : loading syn1neg from /big/s/shalaby/parameter_search_doc2vec_models_new/full/doc2vec_size_500_w_8_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_6/model.syn1neg.npy with mmap=None\n",
      "2017-01-07 08:33:24,446 : INFO : loading syn0 from /big/s/shalaby/parameter_search_doc2vec_models_new/full/doc2vec_size_500_w_8_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_6/model.syn0.npy with mmap=None\n",
      "2017-01-07 08:33:26,751 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-01-07 08:33:26,768 : INFO : setting ignored attribute cum_table to None\n"
     ]
    }
   ],
   "source": [
    "GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(6)\n",
    "info(\"****************** Epoch {} --- Working on {} *******************\".format(epoch, GLOBAL_VARS.MODEL_NAME))\n",
    "\n",
    "# if we have the model, just load it, otherwise train the previous model\n",
    "if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)):\n",
    "    doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-07 08:05:32,383 : INFO : saving Doc2Vec object under /big/s/shalaby/parameter_search_doc2vec_models_new/full/doc2vec_size_500_w_8_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_6/model, separately None\n",
      "2017-01-07 08:05:32,384 : INFO : storing numpy array 'doctag_syn0' to /big/s/shalaby/parameter_search_doc2vec_models_new/full/doc2vec_size_500_w_8_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_6/model.docvecs.doctag_syn0.npy\n",
      "2017-01-07 08:06:07,546 : INFO : storing numpy array 'syn1neg' to /big/s/shalaby/parameter_search_doc2vec_models_new/full/doc2vec_size_500_w_8_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_6/model.syn1neg.npy\n",
      "2017-01-07 08:06:22,413 : INFO : not storing attribute syn0norm\n",
      "2017-01-07 08:06:22,414 : INFO : storing numpy array 'syn0' to /big/s/shalaby/parameter_search_doc2vec_models_new/full/doc2vec_size_500_w_8_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_6/model.syn0.npy\n",
      "2017-01-07 08:06:37,511 : INFO : not storing attribute cum_table\n",
      "2017-01-07 08:07:37,639 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-01-07 08:08:36,552 : INFO : capital-common-countries: 9.2% (28/306)\n",
      "2017-01-07 08:09:21,442 : INFO : capital-world: 6.8% (31/453)\n",
      "2017-01-07 08:09:36,517 : INFO : currency: 0.0% (0/152)\n",
      "2017-01-07 08:13:17,488 : INFO : city-in-state: 3.3% (75/2252)\n",
      "2017-01-07 08:13:44,345 : INFO : family: 14.3% (39/272)\n",
      "2017-01-07 08:15:09,837 : INFO : gram1-adjective-to-adverb: 6.8% (59/870)\n",
      "2017-01-07 08:16:14,734 : INFO : gram2-opposite: 17.5% (114/650)\n",
      "2017-01-07 08:18:26,319 : INFO : gram3-comparative: 72.7% (969/1332)\n",
      "2017-01-07 08:19:40,242 : INFO : gram4-superlative: 34.1% (258/756)\n",
      "2017-01-07 08:21:23,693 : INFO : gram5-present-participle: 23.0% (243/1056)\n",
      "2017-01-07 08:23:05,897 : INFO : gram6-nationality-adjective: 11.7% (120/1030)\n",
      "2017-01-07 08:25:25,383 : INFO : gram7-past-tense: 12.2% (172/1406)\n",
      "2017-01-07 08:27:28,904 : INFO : gram8-plural: 43.9% (553/1260)\n",
      "2017-01-07 08:28:50,549 : INFO : gram9-plural-verbs: 47.9% (389/812)\n",
      "2017-01-07 08:28:50,553 : INFO : total: 24.2% (3050/12607)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3h 47min 44s, sys: 4h 31min 2s, total: 8h 18min 46s\n",
      "Wall time: 21min 14s\n",
      "CPU times: user 3h 48min 38s, sys: 4h 31min 12s, total: 8h 19min 51s\n",
      "Wall time: 23min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ensure_disk_location_exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME))\n",
    "doc2vec_model.save(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))\n",
    "GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "\n",
    "# get the word2vec analogy accuracy score\n",
    "%time word2vec_result = doc2vec_model.accuracy(word2vec_questions_file, restrict_vocab=None)\n",
    "epoch_word2vec_metrics.append(word2vec_result)\n",
    "pickle.dump(word2vec_result, open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME,\n",
    "                                               WORD2VEC_METRICS_FILENAME), 'w'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
