{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import cPickle as pickle\n",
    "import string\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STOP_WORDS = nltk.corpus.stopwords.words('english')\n",
    "NUMBER_INDICATOR = \"number_inidicator\"\n",
    "CURRENCY_INDICATOR = \"currency_inidicator\"\n",
    "CHEMICAL_INDICATOR = \"chemical_inidicator\"\n",
    "MIN_WORD_COUNT = 5\n",
    "MIN_SIZE = 0\n",
    "NUM_CORES = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemtokenizer(text):\n",
    "    \"\"\" MAIN FUNCTION to get clean stems out of a text. A list of clean stems are returned \"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\s+', gaps=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stems = []  # result\n",
    "    for token in tokens:\n",
    "        stem = token.lower()\n",
    "        stem = stem.strip(string.punctuation)\n",
    "        if stem:\n",
    "            if is_number(stem):\n",
    "                stem = NUMBER_INDICATOR\n",
    "            elif is_currency(stem):\n",
    "                stem = CURRENCY_INDICATOR\n",
    "            elif is_chemical(stem):\n",
    "                stem = CHEMICAL_INDICATOR\n",
    "            elif is_stopword(stem):\n",
    "                stem = None\n",
    "            else:\n",
    "                stem = stem.strip(string.punctuation)\n",
    "            if stem and len(stem) >= MIN_SIZE:\n",
    "                # extract uni-grams\n",
    "                stems.append(stem)\n",
    "    del tokens\n",
    "    return stems\n",
    "\n",
    "def is_stopword(word):\n",
    "  return word in STOP_WORDS\n",
    "\n",
    "def is_number(str):\n",
    "    \"\"\" Returns true if given string is a number (float or int)\"\"\"\n",
    "    try:\n",
    "        float(str.replace(\",\", \"\"))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_currency(str):\n",
    "    return str[0] == \"$\"\n",
    "\n",
    "def is_chemical(str):\n",
    "    return str.count(\"-\") > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOC2VEC_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training_file = \"/home/local/shalaby/docs_output_sample_100.json\"\n",
    "training_file = \"/home/local/shalaby/docs_output.json\"\n",
    "doc_classifications_map_file = \"/home/local/shalaby/doc_classification_map.pkl\"\n",
    "sections_file = \"/home/local/shalaby/sections.pkl\"\n",
    "training_docs_list_file = \"/home/local/shalaby/training_docs_list.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_classifications_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "#open(sections_file).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286325"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename):\n",
    "            (doc_id, text) = eval(line)\n",
    "            if doc_id in training_docs_list:\n",
    "                yield LabeledSentence(words=stemtokenizer(text), tags=[doc_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOC2VEC_SIZE = 400\n",
    "DOC2VEC_WINDOW = 8\n",
    "DOC2VEC_MAX_VOCAB_SIZE = 300000\n",
    "DOC2VEC_SAMPLE = 1e-5\n",
    "DOC2VEC_TYPE = 1\n",
    "DOC2VEC_HIERARCHICAL_SAMPLE = 0\n",
    "DOC2VEC_NEGATIVE_SAMPLE_SIZE = 10\n",
    "DOC2VEC_CONCAT = 1\n",
    "DOC2VEC_MEAN = 0\n",
    "DOC2VEC_EPOCHS = 1\n",
    "REPORT_DELAY = 30 # report the progress every x seconds\n",
    "REPORT_VOCAB_PROGRESS = 10000 # report the progress every x terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model with the Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-16 15:45:13,435 : INFO : loading Doc2Vec object from /home/local/shalaby/models/doc2vec_size_400_w_8_type_dm_hs_0_iter_10\n",
      "loading Doc2Vec object from /home/local/shalaby/models/doc2vec_size_400_w_8_type_dm_hs_0_iter_10\n",
      "2016-08-16 15:45:59,375 : INFO : loading docvecs recursively from /home/local/shalaby/models/doc2vec_size_400_w_8_type_dm_hs_0_iter_10.docvecs.* with mmap=None\n",
      "loading docvecs recursively from /home/local/shalaby/models/doc2vec_size_400_w_8_type_dm_hs_0_iter_10.docvecs.* with mmap=None\n",
      "2016-08-16 15:45:59,378 : INFO : loading doctag_syn0 from /home/local/shalaby/models/doc2vec_size_400_w_8_type_dm_hs_0_iter_10.docvecs.doctag_syn0.npy with mmap=None\n",
      "loading doctag_syn0 from /home/local/shalaby/models/doc2vec_size_400_w_8_type_dm_hs_0_iter_10.docvecs.doctag_syn0.npy with mmap=None\n"
     ]
    }
   ],
   "source": [
    "file_name = 'doc2vec_size_{}_w_{}_type_{}_hs_{}_iter_{}'.format(DOC2VEC_SIZE, DOC2VEC_WINDOW, \n",
    "                                                                'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                DOC2VEC_HIERARCHICAL_SAMPLE,10)\n",
    "loaded_model = Doc2Vec.load('/home/local/shalaby/models/{}'.format(file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(size=DOC2VEC_SIZE , window=DOC2VEC_WINDOW, min_count=MIN_WORD_COUNT, \n",
    "                max_vocab_size= DOC2VEC_MAX_VOCAB_SIZE,\n",
    "                sample=DOC2VEC_SAMPLE, seed=DOC2VEC_SEED, workers=NUM_CORES,\n",
    "                # doc2vec algorithm dm=1 => PV-DM, dm=2 => PV-DBOW, PV-DM dictates CBOW for words\n",
    "                dm=DOC2VEC_TYPE,\n",
    "                # hs=0 => negative sampling, hs=1 => hierarchical softmax\n",
    "                hs=DOC2VEC_HIERARCHICAL_SAMPLE, negative=DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                dm_concat=DOC2VEC_CONCAT,\n",
    "                # would train words with skip-gram on top of cbow, we don't need that\n",
    "                dbow_words=0,\n",
    "                iter=DOC2VEC_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d400,n10,w8,mc5,s1e-05,t7)\n"
     ]
    }
   ],
   "source": [
    "print model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the model using the vocab from the previously trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-16 15:49:28,921 : INFO : using concatenative 6800-dimensional layer1\n",
      "using concatenative 6800-dimensional layer1\n",
      "2016-08-16 15:49:28,925 : INFO : resetting layer weights\n",
      "resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model.reset_from(loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now for the actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-21 05:36:43,143 : INFO : collecting all words and their counts\n",
      "2016-08-21 05:36:43,576 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2016-08-21 05:48:48,018 : INFO : pruned out 0 tokens with count <=1 (before 302522, after 302522)\n",
      "2016-08-21 05:48:48,645 : INFO : pruned out 159875 tokens with count <=2 (before 302570, after 142695)\n",
      "2016-08-21 05:57:51,459 : INFO : pruned out 161713 tokens with count <=3 (before 300024, after 138311)\n",
      "2016-08-21 06:05:46,060 : INFO : pruned out 164238 tokens with count <=4 (before 300203, after 135965)\n",
      "2016-08-21 06:10:39,585 : INFO : PROGRESS: at example #10000, processed 47284481 words (23224/s), 213774 word types, 10000 tags\n",
      "2016-08-21 06:15:42,454 : INFO : pruned out 166772 tokens with count <=5 (before 301439, after 134667)\n",
      "2016-08-21 06:25:33,615 : INFO : pruned out 166994 tokens with count <=6 (before 300411, after 133417)\n",
      "2016-08-21 06:34:05,011 : INFO : pruned out 169278 tokens with count <=7 (before 300076, after 130798)\n",
      "2016-08-21 06:43:46,436 : INFO : pruned out 170240 tokens with count <=8 (before 300173, after 129933)\n",
      "2016-08-21 06:44:14,571 : INFO : PROGRESS: at example #20000, processed 93868144 words (23118/s), 136955 word types, 20000 tags\n",
      "2016-08-21 06:52:42,747 : INFO : pruned out 171800 tokens with count <=9 (before 300156, after 128356)\n",
      "2016-08-21 07:03:05,381 : INFO : pruned out 173161 tokens with count <=10 (before 300497, after 127336)\n",
      "2016-08-21 07:12:59,795 : INFO : pruned out 174010 tokens with count <=11 (before 300348, after 126338)\n",
      "2016-08-21 07:17:08,614 : INFO : PROGRESS: at example #30000, processed 139867020 words (23301/s), 202785 word types, 30000 tags\n",
      "2016-08-21 07:23:10,434 : INFO : pruned out 174535 tokens with count <=12 (before 300494, after 125959)\n",
      "2016-08-21 07:32:50,199 : INFO : pruned out 174850 tokens with count <=13 (before 300001, after 125151)\n",
      "2016-08-21 07:41:44,237 : INFO : pruned out 175846 tokens with count <=14 (before 300021, after 124175)\n",
      "2016-08-21 07:50:36,482 : INFO : PROGRESS: at example #40000, processed 186029595 words (22990/s), 287654 word types, 40000 tags\n",
      "2016-08-21 07:51:21,197 : INFO : pruned out 176562 tokens with count <=15 (before 300018, after 123456)\n",
      "2016-08-21 08:02:20,335 : INFO : pruned out 176886 tokens with count <=16 (before 300023, after 123137)\n",
      "2016-08-21 08:13:08,973 : INFO : pruned out 177467 tokens with count <=17 (before 300135, after 122668)\n",
      "2016-08-21 08:22:26,515 : INFO : pruned out 178154 tokens with count <=18 (before 300220, after 122066)\n",
      "2016-08-21 08:23:54,759 : INFO : PROGRESS: at example #50000, processed 230910143 words (22459/s), 148762 word types, 50000 tags\n",
      "2016-08-21 08:31:07,763 : INFO : pruned out 178662 tokens with count <=19 (before 300281, after 121619)\n",
      "2016-08-21 08:40:55,594 : INFO : pruned out 178559 tokens with count <=20 (before 300016, after 121457)\n",
      "2016-08-21 08:51:20,741 : INFO : pruned out 179109 tokens with count <=21 (before 300006, after 120897)\n",
      "2016-08-21 08:58:01,248 : INFO : PROGRESS: at example #60000, processed 279864551 words (23921/s), 248214 word types, 60000 tags\n",
      "2016-08-21 09:01:23,701 : INFO : pruned out 179460 tokens with count <=22 (before 300007, after 120547)\n",
      "2016-08-21 09:10:54,660 : INFO : pruned out 179946 tokens with count <=23 (before 300013, after 120067)\n",
      "2016-08-21 09:21:48,481 : INFO : pruned out 180431 tokens with count <=24 (before 300063, after 119632)\n",
      "2016-08-21 09:31:14,705 : INFO : pruned out 181972 tokens with count <=25 (before 300989, after 119017)\n",
      "2016-08-21 09:31:54,865 : INFO : PROGRESS: at example #70000, processed 327091189 words (23223/s), 132653 word types, 70000 tags\n",
      "2016-08-21 09:40:18,069 : INFO : pruned out 181996 tokens with count <=26 (before 300744, after 118748)\n",
      "2016-08-21 09:51:13,428 : INFO : pruned out 181767 tokens with count <=27 (before 300027, after 118260)\n",
      "2016-08-21 10:00:14,368 : INFO : pruned out 182422 tokens with count <=28 (before 300277, after 117855)\n",
      "2016-08-21 10:06:02,028 : INFO : PROGRESS: at example #80000, processed 372971752 words (22411/s), 228197 word types, 80000 tags\n",
      "2016-08-21 10:10:35,998 : INFO : pruned out 182997 tokens with count <=29 (before 300427, after 117430)\n",
      "2016-08-21 10:22:25,315 : INFO : pruned out 183111 tokens with count <=30 (before 300235, after 117124)\n",
      "2016-08-21 10:32:19,483 : INFO : pruned out 183557 tokens with count <=31 (before 300282, after 116725)\n",
      "2016-08-21 10:39:11,178 : INFO : PROGRESS: at example #90000, processed 417301042 words (22285/s), 239769 word types, 90000 tags\n",
      "2016-08-21 10:43:14,053 : INFO : pruned out 183504 tokens with count <=32 (before 300051, after 116547)\n",
      "2016-08-21 10:52:45,500 : INFO : pruned out 183700 tokens with count <=33 (before 300032, after 116332)\n",
      "2016-08-21 11:02:32,071 : INFO : pruned out 183920 tokens with count <=34 (before 300013, after 116093)\n",
      "2016-08-21 11:12:47,278 : INFO : PROGRESS: at example #100000, processed 464279502 words (23301/s), 291465 word types, 100000 tags\n",
      "2016-08-21 11:13:31,871 : INFO : pruned out 184192 tokens with count <=35 (before 300001, after 115809)\n",
      "2016-08-21 11:24:36,928 : INFO : pruned out 184410 tokens with count <=36 (before 300039, after 115629)\n",
      "2016-08-21 11:34:06,079 : INFO : pruned out 187592 tokens with count <=37 (before 302769, after 115177)\n",
      "2016-08-21 11:43:55,293 : INFO : pruned out 185079 tokens with count <=38 (before 300153, after 115074)\n",
      "2016-08-21 11:46:02,022 : INFO : PROGRESS: at example #110000, processed 509187798 words (22513/s), 153296 word types, 110000 tags\n",
      "2016-08-21 11:55:19,878 : INFO : pruned out 185274 tokens with count <=39 (before 300086, after 114812)\n",
      "2016-08-21 12:06:32,334 : INFO : pruned out 185339 tokens with count <=40 (before 300004, after 114665)\n",
      "2016-08-21 12:15:45,066 : INFO : pruned out 185838 tokens with count <=41 (before 300092, after 114254)\n",
      "2016-08-21 12:19:10,999 : INFO : PROGRESS: at example #120000, processed 553437262 words (22247/s), 177588 word types, 120000 tags\n",
      "2016-08-21 12:26:40,594 : INFO : pruned out 186015 tokens with count <=42 (before 300023, after 114008)\n",
      "2016-08-21 12:37:35,221 : INFO : pruned out 188695 tokens with count <=43 (before 302456, after 113761)\n",
      "2016-08-21 12:48:36,922 : INFO : pruned out 186267 tokens with count <=44 (before 300072, after 113805)\n",
      "2016-08-21 12:53:24,456 : INFO : PROGRESS: at example #130000, processed 600435436 words (22887/s), 213820 word types, 130000 tags\n",
      "2016-08-21 12:57:51,618 : INFO : pruned out 187500 tokens with count <=45 (before 301105, after 113605)\n",
      "2016-08-21 13:08:49,738 : INFO : pruned out 186808 tokens with count <=46 (before 300148, after 113340)\n",
      "2016-08-21 13:19:57,189 : INFO : pruned out 187596 tokens with count <=47 (before 300676, after 113080)\n",
      "2016-08-21 13:27:18,448 : INFO : PROGRESS: at example #140000, processed 647527652 words (23152/s), 249927 word types, 140000 tags\n",
      "2016-08-21 13:30:11,264 : INFO : pruned out 187199 tokens with count <=48 (before 300129, after 112930)\n",
      "2016-08-21 13:40:46,100 : INFO : pruned out 187297 tokens with count <=49 (before 300004, after 112707)\n",
      "2016-08-21 13:51:13,053 : INFO : pruned out 187948 tokens with count <=50 (before 300416, after 112468)\n",
      "2016-08-21 14:00:25,086 : INFO : PROGRESS: at example #150000, processed 691706395 words (22237/s), 275165 word types, 150000 tags\n",
      "2016-08-21 14:02:11,903 : INFO : pruned out 187897 tokens with count <=51 (before 300120, after 112223)\n",
      "2016-08-21 14:12:45,636 : INFO : pruned out 190982 tokens with count <=52 (before 303069, after 112087)\n",
      "2016-08-21 14:22:19,457 : INFO : pruned out 188192 tokens with count <=53 (before 300019, after 111827)\n",
      "2016-08-21 14:33:30,196 : INFO : pruned out 188372 tokens with count <=54 (before 300048, after 111676)\n",
      "2016-08-21 14:33:51,436 : INFO : PROGRESS: at example #160000, processed 738405955 words (23275/s), 117848 word types, 160000 tags\n",
      "2016-08-21 14:43:54,459 : INFO : pruned out 188590 tokens with count <=55 (before 300071, after 111481)\n",
      "2016-08-21 14:54:43,841 : INFO : pruned out 188716 tokens with count <=56 (before 300064, after 111348)\n",
      "2016-08-21 15:06:58,227 : INFO : pruned out 188855 tokens with count <=57 (before 300052, after 111197)\n",
      "2016-08-21 15:07:13,831 : INFO : PROGRESS: at example #170000, processed 783158241 words (22349/s), 114576 word types, 170000 tags\n",
      "2016-08-21 15:18:48,588 : INFO : pruned out 190010 tokens with count <=58 (before 301092, after 111082)\n",
      "2016-08-21 15:29:37,199 : INFO : pruned out 189253 tokens with count <=59 (before 300180, after 110927)\n",
      "2016-08-21 15:40:48,754 : INFO : PROGRESS: at example #180000, processed 829383720 words (22941/s), 291649 word types, 180000 tags\n",
      "2016-08-21 15:41:19,438 : INFO : pruned out 189267 tokens with count <=60 (before 300080, after 110813)\n",
      "2016-08-21 15:51:41,407 : INFO : pruned out 189363 tokens with count <=61 (before 300037, after 110674)\n",
      "2016-08-21 16:03:01,403 : INFO : pruned out 189573 tokens with count <=62 (before 300104, after 110531)\n",
      "2016-08-21 16:13:46,335 : INFO : pruned out 191317 tokens with count <=63 (before 301704, after 110387)\n",
      "2016-08-21 16:15:08,643 : INFO : PROGRESS: at example #190000, processed 877613452 words (23413/s), 134890 word types, 190000 tags\n",
      "2016-08-21 16:23:19,648 : INFO : pruned out 189819 tokens with count <=64 (before 300054, after 110235)\n",
      "2016-08-21 16:33:41,063 : INFO : pruned out 189901 tokens with count <=65 (before 300011, after 110110)\n",
      "2016-08-21 16:44:04,919 : INFO : pruned out 190198 tokens with count <=66 (before 300136, after 109938)\n",
      "2016-08-21 16:48:39,385 : INFO : PROGRESS: at example #200000, processed 924424627 words (23280/s), 190349 word types, 200000 tags\n",
      "2016-08-21 16:55:36,083 : INFO : pruned out 190225 tokens with count <=67 (before 300010, after 109785)\n",
      "2016-08-21 17:01:29,356 : INFO : pruned out 190600 tokens with count <=68 (before 300147, after 109547)\n",
      "2016-08-21 17:09:11,252 : INFO : pruned out 239220 tokens with count <=69 (before 348547, after 109327)\n",
      "2016-08-21 17:18:04,295 : INFO : pruned out 190437 tokens with count <=70 (before 300063, after 109626)\n",
      "2016-08-21 17:22:00,163 : INFO : PROGRESS: at example #210000, processed 969808812 words (22683/s), 177846 word types, 210000 tags\n",
      "2016-08-21 17:30:08,013 : INFO : pruned out 190530 tokens with count <=71 (before 300005, after 109475)\n",
      "2016-08-21 17:41:31,956 : INFO : pruned out 190685 tokens with count <=72 (before 300073, after 109388)\n",
      "2016-08-21 17:52:39,877 : INFO : pruned out 190689 tokens with count <=73 (before 300036, after 109347)\n",
      "2016-08-21 17:55:30,245 : INFO : PROGRESS: at example #220000, processed 1015108899 words (22536/s), 170919 word types, 220000 tags\n",
      "2016-08-21 18:01:41,070 : INFO : pruned out 191047 tokens with count <=74 (before 300210, after 109163)\n",
      "2016-08-21 18:12:23,083 : INFO : pruned out 191065 tokens with count <=75 (before 300088, after 109023)\n",
      "2016-08-21 18:24:11,223 : INFO : pruned out 191196 tokens with count <=76 (before 300129, after 108933)\n",
      "2016-08-21 18:28:45,921 : INFO : PROGRESS: at example #230000, processed 1060031915 words (22510/s), 203449 word types, 230000 tags\n",
      "2016-08-21 18:34:24,192 : INFO : pruned out 191219 tokens with count <=77 (before 300044, after 108825)\n",
      "2016-08-21 18:44:21,296 : INFO : pruned out 191352 tokens with count <=78 (before 300014, after 108662)\n",
      "2016-08-21 18:54:57,621 : INFO : pruned out 191493 tokens with count <=79 (before 300078, after 108585)\n",
      "2016-08-21 19:03:02,481 : INFO : PROGRESS: at example #240000, processed 1108111213 words (23378/s), 254306 word types, 240000 tags\n",
      "2016-08-21 19:05:53,172 : INFO : pruned out 191519 tokens with count <=80 (before 300002, after 108483)\n",
      "2016-08-21 19:16:17,661 : INFO : pruned out 191728 tokens with count <=81 (before 300094, after 108366)\n",
      "2016-08-21 19:26:44,861 : INFO : pruned out 191878 tokens with count <=82 (before 300102, after 108224)\n",
      "2016-08-21 19:36:26,691 : INFO : PROGRESS: at example #250000, processed 1153431560 words (22612/s), 277271 word types, 250000 tags\n",
      "2016-08-21 19:37:56,900 : INFO : pruned out 192305 tokens with count <=83 (before 300406, after 108101)\n",
      "2016-08-21 19:49:48,348 : INFO : pruned out 191991 tokens with count <=84 (before 300008, after 108017)\n",
      "2016-08-21 20:01:19,691 : INFO : pruned out 192133 tokens with count <=85 (before 300033, after 107900)\n",
      "2016-08-21 20:09:59,963 : INFO : PROGRESS: at example #260000, processed 1199424615 words (22844/s), 294011 word types, 260000 tags\n",
      "2016-08-21 20:10:24,884 : INFO : pruned out 192436 tokens with count <=86 (before 300232, after 107796)\n",
      "2016-08-21 20:22:27,117 : INFO : pruned out 192362 tokens with count <=87 (before 300034, after 107672)\n",
      "2016-08-21 20:33:12,071 : INFO : pruned out 192543 tokens with count <=88 (before 300142, after 107599)\n",
      "2016-08-21 20:43:32,479 : INFO : pruned out 192514 tokens with count <=89 (before 300006, after 107492)\n",
      "2016-08-21 20:44:02,764 : INFO : PROGRESS: at example #270000, processed 1247388152 words (23479/s), 115625 word types, 270000 tags\n",
      "2016-08-21 20:54:54,342 : INFO : pruned out 192638 tokens with count <=90 (before 300049, after 107411)\n",
      "2016-08-21 21:04:14,958 : INFO : pruned out 193311 tokens with count <=91 (before 300591, after 107280)\n",
      "2016-08-21 21:16:32,289 : INFO : pruned out 192850 tokens with count <=92 (before 300056, after 107206)\n",
      "2016-08-21 21:17:33,521 : INFO : PROGRESS: at example #280000, processed 1293770823 words (23067/s), 126852 word types, 280000 tags\n",
      "2016-08-21 21:27:18,403 : INFO : pruned out 192883 tokens with count <=93 (before 300004, after 107121)\n",
      "2016-08-21 21:38:06,652 : INFO : pruned out 193908 tokens with count <=94 (before 300945, after 107037)\n",
      "2016-08-21 21:50:16,101 : INFO : pruned out 193056 tokens with count <=95 (before 300014, after 106958)\n",
      "2016-08-21 21:51:08,233 : INFO : PROGRESS: at example #290000, processed 1339487922 words (22691/s), 120808 word types, 290000 tags\n",
      "2016-08-21 22:00:05,612 : INFO : pruned out 193486 tokens with count <=96 (before 300325, after 106839)\n",
      "2016-08-21 22:11:20,051 : INFO : pruned out 193267 tokens with count <=97 (before 300006, after 106739)\n",
      "2016-08-21 22:19:59,179 : INFO : pruned out 193346 tokens with count <=98 (before 300009, after 106663)\n",
      "2016-08-21 22:24:05,609 : INFO : PROGRESS: at example #300000, processed 1383802276 words (22410/s), 170252 word types, 300000 tags\n",
      "2016-08-21 22:31:37,826 : INFO : pruned out 193427 tokens with count <=99 (before 300003, after 106576)\n",
      "2016-08-21 22:43:36,435 : INFO : pruned out 193472 tokens with count <=100 (before 300002, after 106530)\n",
      "2016-08-21 22:56:04,841 : INFO : pruned out 193577 tokens with count <=101 (before 300028, after 106451)\n",
      "2016-08-21 22:57:48,118 : INFO : PROGRESS: at example #310000, processed 1429867859 words (22776/s), 135046 word types, 310000 tags\n",
      "2016-08-21 23:07:42,966 : INFO : pruned out 193727 tokens with count <=102 (before 300123, after 106396)\n",
      "2016-08-21 23:19:33,905 : INFO : pruned out 213128 tokens with count <=103 (before 319489, after 106361)\n",
      "2016-08-21 23:29:14,736 : INFO : pruned out 194285 tokens with count <=104 (before 300565, after 106280)\n",
      "2016-08-21 23:31:42,946 : INFO : PROGRESS: at example #320000, processed 1476378035 words (22857/s), 151819 word types, 320000 tags\n",
      "2016-08-21 23:39:55,943 : INFO : pruned out 194047 tokens with count <=105 (before 300234, after 106187)\n",
      "2016-08-21 23:51:14,902 : INFO : pruned out 193970 tokens with count <=106 (before 300114, after 106144)\n",
      "2016-08-22 00:03:17,789 : INFO : pruned out 193928 tokens with count <=107 (before 300001, after 106073)\n",
      "2016-08-22 00:05:54,495 : INFO : PROGRESS: at example #330000, processed 1523570868 words (23003/s), 152748 word types, 330000 tags\n",
      "2016-08-22 00:14:43,644 : INFO : pruned out 194006 tokens with count <=108 (before 300006, after 106000)\n",
      "2016-08-22 00:26:45,304 : INFO : pruned out 194147 tokens with count <=109 (before 300083, after 105936)\n",
      "2016-08-22 00:38:19,492 : INFO : pruned out 195255 tokens with count <=110 (before 301130, after 105875)\n",
      "2016-08-22 00:39:09,344 : INFO : PROGRESS: at example #340000, processed 1567165248 words (21853/s), 118306 word types, 340000 tags\n",
      "2016-08-22 00:50:32,598 : INFO : pruned out 194256 tokens with count <=111 (before 300064, after 105808)\n",
      "2016-08-22 01:02:31,439 : INFO : pruned out 194991 tokens with count <=112 (before 300734, after 105743)\n",
      "2016-08-22 01:11:55,267 : INFO : PROGRESS: at example #350000, processed 1611529768 words (22566/s), 253524 word types, 350000 tags\n",
      "2016-08-22 01:14:51,257 : INFO : pruned out 194358 tokens with count <=113 (before 300043, after 105685)\n",
      "2016-08-22 01:25:12,379 : INFO : pruned out 194445 tokens with count <=114 (before 300047, after 105602)\n",
      "2016-08-22 01:37:30,042 : INFO : pruned out 194491 tokens with count <=115 (before 300037, after 105546)\n",
      "2016-08-22 01:46:18,795 : INFO : PROGRESS: at example #360000, processed 1659084298 words (23045/s), 284126 word types, 360000 tags\n",
      "2016-08-22 01:47:23,090 : INFO : pruned out 194573 tokens with count <=116 (before 300061, after 105488)\n",
      "2016-08-22 01:59:42,256 : INFO : pruned out 194650 tokens with count <=117 (before 300067, after 105417)\n",
      "2016-08-22 02:10:15,911 : INFO : pruned out 195296 tokens with count <=118 (before 300663, after 105367)\n",
      "2016-08-22 02:19:49,195 : INFO : PROGRESS: at example #370000, processed 1704784479 words (22731/s), 274158 word types, 370000 tags\n",
      "2016-08-22 02:21:33,090 : INFO : pruned out 194819 tokens with count <=119 (before 300125, after 105306)\n",
      "2016-08-22 02:33:12,328 : INFO : pruned out 194764 tokens with count <=120 (before 300013, after 105249)\n",
      "2016-08-22 02:43:33,699 : INFO : pruned out 194799 tokens with count <=121 (before 300030, after 105231)\n",
      "2016-08-22 02:53:10,400 : INFO : PROGRESS: at example #380000, processed 1751950982 words (23569/s), 286856 word types, 380000 tags\n",
      "2016-08-22 02:54:05,216 : INFO : pruned out 195158 tokens with count <=122 (before 300116, after 104958)\n",
      "2016-08-22 03:05:54,967 : INFO : pruned out 205620 tokens with count <=123 (before 310430, after 104810)\n",
      "2016-08-22 03:17:33,281 : INFO : pruned out 195274 tokens with count <=124 (before 300005, after 104731)\n",
      "2016-08-22 03:26:16,481 : INFO : PROGRESS: at example #390000, processed 1797705994 words (23037/s), 261093 word types, 390000 tags\n",
      "2016-08-22 03:28:15,117 : INFO : pruned out 195394 tokens with count <=125 (before 300064, after 104670)\n",
      "2016-08-22 03:39:51,478 : INFO : pruned out 195668 tokens with count <=126 (before 300304, after 104636)\n",
      "2016-08-22 03:50:24,370 : INFO : pruned out 195617 tokens with count <=127 (before 300206, after 104589)\n",
      "2016-08-22 03:59:46,159 : INFO : PROGRESS: at example #400000, processed 1844263517 words (23166/s), 275043 word types, 400000 tags\n",
      "2016-08-22 04:01:08,899 : INFO : pruned out 195548 tokens with count <=128 (before 300119, after 104571)\n",
      "2016-08-22 04:13:01,390 : INFO : pruned out 195681 tokens with count <=129 (before 300163, after 104482)\n",
      "2016-08-22 04:23:09,430 : INFO : pruned out 196022 tokens with count <=130 (before 300452, after 104430)\n",
      "2016-08-22 04:33:08,326 : INFO : PROGRESS: at example #410000, processed 1891090207 words (23388/s), 298899 word types, 410000 tags\n",
      "2016-08-22 04:33:12,804 : INFO : pruned out 195754 tokens with count <=131 (before 300103, after 104349)\n",
      "2016-08-22 04:44:56,765 : INFO : pruned out 195724 tokens with count <=132 (before 300014, after 104290)\n",
      "2016-08-22 04:57:43,489 : INFO : pruned out 195840 tokens with count <=133 (before 300076, after 104236)\n",
      "2016-08-22 05:06:00,200 : INFO : PROGRESS: at example #420000, processed 1935564556 words (22554/s), 283969 word types, 420000 tags\n",
      "2016-08-22 05:07:06,680 : INFO : pruned out 195836 tokens with count <=134 (before 300003, after 104167)\n",
      "2016-08-22 05:17:19,108 : INFO : pruned out 195921 tokens with count <=135 (before 300029, after 104108)\n",
      "2016-08-22 05:26:13,647 : INFO : pruned out 229116 tokens with count <=136 (before 333165, after 104049)\n",
      "2016-08-22 05:36:31,536 : INFO : pruned out 196212 tokens with count <=137 (before 300195, after 103983)\n",
      "2016-08-22 05:38:59,482 : INFO : PROGRESS: at example #430000, processed 1982157371 words (23540/s), 148187 word types, 430000 tags\n",
      "2016-08-22 05:47:02,408 : INFO : pruned out 196098 tokens with count <=138 (before 300047, after 103949)\n",
      "2016-08-22 05:56:17,865 : INFO : pruned out 196235 tokens with count <=139 (before 300141, after 103906)\n",
      "2016-08-22 06:07:59,545 : INFO : pruned out 200977 tokens with count <=140 (before 304835, after 103858)\n",
      "2016-08-22 06:12:58,438 : INFO : PROGRESS: at example #440000, processed 2030245238 words (23584/s), 194400 word types, 440000 tags\n",
      "2016-08-22 06:17:59,816 : INFO : pruned out 196387 tokens with count <=141 (before 300183, after 103796)\n",
      "2016-08-22 06:30:48,494 : INFO : pruned out 196276 tokens with count <=142 (before 300019, after 103743)\n",
      "2016-08-22 06:42:04,292 : INFO : pruned out 196313 tokens with count <=143 (before 300005, after 103692)\n",
      "2016-08-22 06:45:50,826 : INFO : PROGRESS: at example #450000, processed 2075337318 words (22861/s), 177463 word types, 450000 tags\n",
      "2016-08-22 06:47:44,355 : INFO : pruned out 197556 tokens with count <=144 (before 301195, after 103639)\n",
      "2016-08-22 06:59:33,303 : INFO : pruned out 196435 tokens with count <=145 (before 300026, after 103591)\n",
      "2016-08-22 07:11:33,471 : INFO : pruned out 196490 tokens with count <=146 (before 300027, after 103537)\n",
      "2016-08-22 07:17:59,882 : INFO : PROGRESS: at example #460000, processed 2119843889 words (23071/s), 219728 word types, 460000 tags\n",
      "2016-08-22 07:21:57,858 : INFO : pruned out 196520 tokens with count <=147 (before 300022, after 103502)\n",
      "2016-08-22 07:33:32,463 : INFO : pruned out 197618 tokens with count <=148 (before 301089, after 103471)\n",
      "2016-08-22 07:43:45,271 : INFO : pruned out 196612 tokens with count <=149 (before 300047, after 103435)\n",
      "2016-08-22 07:51:25,845 : INFO : PROGRESS: at example #470000, processed 2167531840 words (23773/s), 253808 word types, 470000 tags\n",
      "2016-08-22 07:54:25,136 : INFO : pruned out 196794 tokens with count <=150 (before 300180, after 103386)\n",
      "2016-08-22 08:04:31,705 : INFO : pruned out 197072 tokens with count <=151 (before 300430, after 103358)\n",
      "2016-08-22 08:15:45,068 : INFO : pruned out 196738 tokens with count <=152 (before 300061, after 103323)\n",
      "2016-08-22 08:24:15,224 : INFO : PROGRESS: at example #480000, processed 2212838177 words (23005/s), 267497 word types, 480000 tags\n",
      "2016-08-22 08:26:23,003 : INFO : pruned out 196837 tokens with count <=153 (before 300111, after 103274)\n",
      "2016-08-22 08:36:34,085 : INFO : pruned out 197067 tokens with count <=154 (before 300290, after 103223)\n",
      "2016-08-22 08:47:17,779 : INFO : pruned out 196826 tokens with count <=155 (before 300003, after 103177)\n",
      "2016-08-22 08:57:52,983 : INFO : PROGRESS: at example #490000, processed 2261233464 words (23984/s), 293357 word types, 490000 tags\n",
      "2016-08-22 08:58:05,793 : INFO : pruned out 197104 tokens with count <=156 (before 300244, after 103140)\n",
      "2016-08-22 09:09:33,300 : INFO : pruned out 196917 tokens with count <=157 (before 300022, after 103105)\n",
      "2016-08-22 09:20:43,650 : INFO : pruned out 197376 tokens with count <=158 (before 300435, after 103059)\n",
      "2016-08-22 09:30:55,097 : INFO : PROGRESS: at example #500000, processed 2307393239 words (23288/s), 281686 word types, 500000 tags\n",
      "2016-08-22 09:31:58,906 : INFO : pruned out 197147 tokens with count <=159 (before 300179, after 103032)\n",
      "2016-08-22 09:43:29,346 : INFO : pruned out 197743 tokens with count <=160 (before 300739, after 102996)\n",
      "2016-08-22 09:51:19,507 : INFO : pruned out 197165 tokens with count <=161 (before 300104, after 102939)\n",
      "2016-08-22 10:03:19,290 : INFO : pruned out 197089 tokens with count <=162 (before 300003, after 102914)\n",
      "2016-08-22 10:03:56,424 : INFO : PROGRESS: at example #510000, processed 2353487594 words (23264/s), 121230 word types, 510000 tags\n",
      "2016-08-22 10:12:48,911 : INFO : pruned out 197173 tokens with count <=163 (before 300036, after 102863)\n",
      "2016-08-22 10:25:20,334 : INFO : pruned out 197189 tokens with count <=164 (before 300010, after 102821)\n",
      "2016-08-22 10:36:12,906 : INFO : pruned out 197526 tokens with count <=165 (before 300319, after 102793)\n",
      "2016-08-22 10:36:48,991 : INFO : PROGRESS: at example #520000, processed 2399411378 words (23281/s), 113821 word types, 520000 tags\n",
      "2016-08-22 10:44:29,208 : INFO : pruned out 199946 tokens with count <=166 (before 302705, after 102759)\n",
      "2016-08-22 10:55:26,161 : INFO : pruned out 197270 tokens with count <=167 (before 300003, after 102733)\n",
      "2016-08-22 11:06:38,116 : INFO : pruned out 197380 tokens with count <=168 (before 300081, after 102701)\n",
      "2016-08-22 11:09:43,734 : INFO : PROGRESS: at example #530000, processed 2447366696 words (24284/s), 169676 word types, 530000 tags\n",
      "2016-08-22 11:16:17,941 : INFO : pruned out 197424 tokens with count <=169 (before 300107, after 102683)\n",
      "2016-08-22 11:27:27,011 : INFO : pruned out 200643 tokens with count <=170 (before 303323, after 102680)\n",
      "2016-08-22 11:38:49,900 : INFO : pruned out 197383 tokens with count <=171 (before 300047, after 102664)\n",
      "2016-08-22 11:43:02,739 : INFO : PROGRESS: at example #540000, processed 2496082705 words (24370/s), 196796 word types, 540000 tags\n",
      "2016-08-22 11:49:02,716 : INFO : pruned out 197381 tokens with count <=172 (before 300002, after 102621)\n",
      "2016-08-22 11:59:44,778 : INFO : pruned out 197440 tokens with count <=173 (before 300029, after 102589)\n",
      "2016-08-22 12:09:32,692 : INFO : pruned out 197512 tokens with count <=174 (before 300057, after 102545)\n",
      "2016-08-22 12:15:12,433 : INFO : PROGRESS: at example #550000, processed 2540074970 words (22797/s), 218677 word types, 550000 tags\n",
      "2016-08-22 12:20:16,392 : INFO : pruned out 198338 tokens with count <=175 (before 300844, after 102506)\n",
      "2016-08-22 12:31:59,401 : INFO : pruned out 197544 tokens with count <=176 (before 300041, after 102497)\n",
      "2016-08-22 12:43:26,379 : INFO : pruned out 197648 tokens with count <=177 (before 300125, after 102477)\n",
      "2016-08-22 12:47:38,494 : INFO : PROGRESS: at example #560000, processed 2585913531 words (23554/s), 176410 word types, 560000 tags\n",
      "2016-08-22 12:54:25,645 : INFO : pruned out 197964 tokens with count <=178 (before 300402, after 102438)\n",
      "2016-08-22 13:06:55,025 : INFO : pruned out 197622 tokens with count <=179 (before 300038, after 102416)\n",
      "2016-08-22 13:17:05,822 : INFO : pruned out 197666 tokens with count <=180 (before 300063, after 102397)\n",
      "2016-08-22 13:20:49,070 : INFO : PROGRESS: at example #570000, processed 2634107582 words (24211/s), 179930 word types, 570000 tags\n",
      "2016-08-22 13:28:42,785 : INFO : pruned out 198754 tokens with count <=181 (before 301117, after 102363)\n",
      "2016-08-22 13:37:45,365 : INFO : pruned out 197860 tokens with count <=182 (before 300182, after 102322)\n",
      "2016-08-22 13:47:48,007 : INFO : pruned out 198016 tokens with count <=183 (before 300317, after 102301)\n",
      "2016-08-22 13:53:35,866 : INFO : PROGRESS: at example #580000, processed 2680876293 words (23779/s), 206943 word types, 580000 tags\n",
      "2016-08-22 13:59:10,446 : INFO : pruned out 197736 tokens with count <=184 (before 300015, after 102279)\n",
      "2016-08-22 14:10:36,084 : INFO : pruned out 202178 tokens with count <=185 (before 304423, after 102245)\n",
      "2016-08-22 14:22:22,221 : INFO : pruned out 197885 tokens with count <=186 (before 300077, after 102192)\n",
      "2016-08-22 14:25:21,113 : INFO : PROGRESS: at example #590000, processed 2724944493 words (23129/s), 170973 word types, 590000 tags\n",
      "2016-08-22 14:32:22,039 : INFO : pruned out 197928 tokens with count <=187 (before 300098, after 102170)\n",
      "2016-08-22 14:43:09,800 : INFO : pruned out 197930 tokens with count <=188 (before 300062, after 102132)\n",
      "2016-08-22 14:54:42,491 : INFO : pruned out 199846 tokens with count <=189 (before 301981, after 102135)\n",
      "2016-08-22 14:57:51,908 : INFO : PROGRESS: at example #600000, processed 2771132400 words (23676/s), 162897 word types, 600000 tags\n",
      "2016-08-22 15:04:03,764 : INFO : pruned out 197952 tokens with count <=190 (before 300067, after 102115)\n",
      "2016-08-22 15:14:49,003 : INFO : pruned out 198161 tokens with count <=191 (before 300235, after 102074)\n",
      "2016-08-22 15:25:52,351 : INFO : pruned out 197966 tokens with count <=192 (before 300002, after 102036)\n",
      "2016-08-22 15:29:55,134 : INFO : PROGRESS: at example #610000, processed 2815860731 words (23256/s), 202278 word types, 610000 tags\n",
      "2016-08-22 15:35:28,332 : INFO : pruned out 198013 tokens with count <=193 (before 300016, after 102003)\n",
      "2016-08-22 15:46:38,889 : INFO : pruned out 198282 tokens with count <=194 (before 300264, after 101982)\n",
      "2016-08-22 15:58:14,692 : INFO : pruned out 198293 tokens with count <=195 (before 300239, after 101946)\n",
      "2016-08-22 16:02:12,643 : INFO : PROGRESS: at example #620000, processed 2862240026 words (23937/s), 172050 word types, 620000 tags\n",
      "2016-08-22 16:10:05,536 : INFO : pruned out 198215 tokens with count <=196 (before 300120, after 101905)\n",
      "2016-08-22 16:21:37,811 : INFO : pruned out 198273 tokens with count <=197 (before 300142, after 101869)\n",
      "2016-08-22 16:32:21,544 : INFO : pruned out 198186 tokens with count <=198 (before 300042, after 101856)\n",
      "2016-08-22 16:34:40,988 : INFO : PROGRESS: at example #630000, processed 2907994014 words (23483/s), 139232 word types, 630000 tags\n",
      "2016-08-22 16:38:42,510 : INFO : pruned out 201341 tokens with count <=199 (before 303151, after 101810)\n",
      "2016-08-22 16:48:03,731 : INFO : pruned out 198256 tokens with count <=200 (before 300036, after 101780)\n",
      "2016-08-22 16:58:00,792 : INFO : pruned out 198238 tokens with count <=201 (before 300011, after 101773)\n",
      "2016-08-22 17:07:17,818 : INFO : PROGRESS: at example #640000, processed 2954468622 words (23749/s), 251209 word types, 640000 tags\n",
      "2016-08-22 17:10:21,208 : INFO : pruned out 198347 tokens with count <=202 (before 300083, after 101736)\n",
      "2016-08-22 17:20:39,154 : INFO : pruned out 198285 tokens with count <=203 (before 300007, after 101722)\n",
      "2016-08-22 17:29:57,649 : INFO : pruned out 198650 tokens with count <=204 (before 300338, after 101688)\n",
      "2016-08-22 17:40:04,327 : INFO : PROGRESS: at example #650000, processed 3001325007 words (23827/s), 269229 word types, 650000 tags\n",
      "2016-08-22 17:41:28,062 : INFO : pruned out 198871 tokens with count <=205 (before 300538, after 101667)\n",
      "2016-08-22 17:53:25,304 : INFO : pruned out 198655 tokens with count <=206 (before 300294, after 101639)\n",
      "2016-08-22 18:05:11,721 : INFO : pruned out 198398 tokens with count <=207 (before 300008, after 101610)\n",
      "2016-08-22 18:12:33,223 : INFO : PROGRESS: at example #660000, processed 3046501415 words (23180/s), 237525 word types, 660000 tags\n",
      "2016-08-22 18:15:39,768 : INFO : pruned out 198928 tokens with count <=208 (before 300519, after 101591)\n",
      "2016-08-22 18:27:50,712 : INFO : pruned out 198477 tokens with count <=209 (before 300044, after 101567)\n",
      "2016-08-22 18:36:41,258 : INFO : pruned out 198458 tokens with count <=210 (before 300003, after 101545)\n",
      "2016-08-22 18:46:02,372 : INFO : PROGRESS: at example #670000, processed 3094144540 words (23713/s), 268558 word types, 670000 tags\n",
      "2016-08-22 18:48:09,060 : INFO : pruned out 198546 tokens with count <=211 (before 300069, after 101523)\n",
      "2016-08-22 18:59:21,989 : INFO : pruned out 198897 tokens with count <=212 (before 300405, after 101508)\n",
      "2016-08-22 19:10:02,207 : INFO : pruned out 198943 tokens with count <=213 (before 300420, after 101477)\n",
      "2016-08-22 19:18:59,515 : INFO : PROGRESS: at example #680000, processed 3141490864 words (23946/s), 263649 word types, 680000 tags\n",
      "2016-08-22 19:20:58,769 : INFO : pruned out 198601 tokens with count <=214 (before 300065, after 101464)\n",
      "2016-08-22 19:32:42,506 : INFO : pruned out 198891 tokens with count <=215 (before 300339, after 101448)\n",
      "2016-08-22 19:44:16,654 : INFO : pruned out 198592 tokens with count <=216 (before 300010, after 101418)\n",
      "2016-08-22 19:51:32,885 : INFO : PROGRESS: at example #690000, processed 3186841818 words (23216/s), 236732 word types, 690000 tags\n",
      "2016-08-22 19:55:43,910 : INFO : pruned out 198691 tokens with count <=217 (before 300097, after 101406)\n",
      "2016-08-22 20:06:30,124 : INFO : pruned out 198740 tokens with count <=218 (before 300124, after 101384)\n",
      "2016-08-22 20:16:25,822 : INFO : pruned out 198718 tokens with count <=219 (before 300082, after 101364)\n",
      "2016-08-22 20:24:23,080 : INFO : PROGRESS: at example #700000, processed 3234270567 words (24073/s), 247464 word types, 700000 tags\n",
      "2016-08-22 20:28:00,977 : INFO : pruned out 198683 tokens with count <=220 (before 300025, after 101342)\n",
      "2016-08-22 20:39:52,050 : INFO : pruned out 198709 tokens with count <=221 (before 300025, after 101316)\n",
      "2016-08-22 20:50:48,932 : INFO : pruned out 200526 tokens with count <=222 (before 301829, after 101303)\n",
      "2016-08-22 20:57:28,376 : INFO : PROGRESS: at example #710000, processed 3282836041 words (24462/s), 235575 word types, 710000 tags\n",
      "2016-08-22 21:01:43,113 : INFO : pruned out 199706 tokens with count <=223 (before 300979, after 101273)\n",
      "2016-08-22 21:10:47,504 : INFO : pruned out 231723 tokens with count <=224 (before 332983, after 101260)\n",
      "2016-08-22 21:22:27,943 : INFO : pruned out 199940 tokens with count <=225 (before 301170, after 101230)\n",
      "2016-08-22 21:29:54,992 : INFO : PROGRESS: at example #720000, processed 3327993400 words (23197/s), 238655 word types, 720000 tags\n",
      "2016-08-22 21:33:03,522 : INFO : pruned out 203914 tokens with count <=226 (before 305110, after 101196)\n",
      "2016-08-22 21:43:43,822 : INFO : pruned out 198878 tokens with count <=227 (before 300063, after 101185)\n",
      "2016-08-22 21:55:33,327 : INFO : pruned out 198831 tokens with count <=228 (before 300003, after 101172)\n",
      "2016-08-22 22:01:50,252 : INFO : PROGRESS: at example #730000, processed 3372255856 words (23110/s), 206703 word types, 730000 tags\n",
      "2016-08-22 22:07:01,221 : INFO : pruned out 198897 tokens with count <=229 (before 300048, after 101151)\n",
      "2016-08-22 22:18:52,244 : INFO : pruned out 199287 tokens with count <=230 (before 300418, after 101131)\n",
      "2016-08-22 22:29:04,848 : INFO : pruned out 198927 tokens with count <=231 (before 300032, after 101105)\n",
      "2016-08-22 22:34:27,258 : INFO : PROGRESS: at example #740000, processed 3419019349 words (23895/s), 203370 word types, 740000 tags\n",
      "2016-08-22 22:39:50,450 : INFO : pruned out 198925 tokens with count <=232 (before 300009, after 101084)\n",
      "2016-08-22 22:50:41,128 : INFO : pruned out 198933 tokens with count <=233 (before 300001, after 101068)\n",
      "2016-08-22 23:00:24,812 : INFO : pruned out 199008 tokens with count <=234 (before 300042, after 101034)\n",
      "2016-08-22 23:06:48,661 : INFO : PROGRESS: at example #750000, processed 3464566111 words (23460/s), 225252 word types, 750000 tags\n",
      "2016-08-22 23:11:08,114 : INFO : pruned out 199387 tokens with count <=235 (before 300414, after 101027)\n",
      "2016-08-22 23:21:16,165 : INFO : pruned out 199466 tokens with count <=236 (before 300476, after 101010)\n",
      "2016-08-22 23:32:17,318 : INFO : pruned out 199173 tokens with count <=237 (before 300170, after 100997)\n",
      "2016-08-22 23:39:27,393 : INFO : PROGRESS: at example #760000, processed 3510821109 words (23614/s), 222076 word types, 760000 tags\n",
      "2016-08-22 23:44:26,272 : INFO : pruned out 199041 tokens with count <=238 (before 300007, after 100966)\n",
      "2016-08-22 23:55:38,622 : INFO : pruned out 199058 tokens with count <=239 (before 300003, after 100945)\n",
      "2016-08-23 00:07:53,655 : INFO : pruned out 199067 tokens with count <=240 (before 300001, after 100934)\n",
      "2016-08-23 00:11:32,673 : INFO : PROGRESS: at example #770000, processed 3555259773 words (23081/s), 184199 word types, 770000 tags\n",
      "2016-08-23 00:19:06,680 : INFO : pruned out 199474 tokens with count <=241 (before 300385, after 100911)\n",
      "2016-08-24 04:31:32,696 : INFO : pruned out 205268 tokens with count <=389 (before 304540, after 99272)\n",
      "2016-08-24 04:39:49,173 : INFO : pruned out 202829 tokens with count <=390 (before 302089, after 99260)\n",
      "2016-08-24 04:45:45,070 : INFO : PROGRESS: at example #1270000, processed 5866723325 words (24179/s), 213219 word types, 1270000 tags\n",
      "2016-08-24 04:50:05,637 : INFO : pruned out 201129 tokens with count <=391 (before 300385, after 99256)\n",
      "2016-08-24 05:01:58,160 : INFO : pruned out 201158 tokens with count <=392 (before 300402, after 99244)\n",
      "2016-08-24 05:12:20,218 : INFO : pruned out 200799 tokens with count <=393 (before 300034, after 99235)\n",
      "2016-08-24 05:18:12,422 : INFO : PROGRESS: at example #1280000, processed 5912995886 words (23761/s), 203954 word types, 1280000 tags\n",
      "2016-08-24 05:24:08,869 : INFO : pruned out 200782 tokens with count <=394 (before 300005, after 99223)\n",
      "2016-08-24 05:35:45,902 : INFO : pruned out 202258 tokens with count <=395 (before 301474, after 99216)\n",
      "2016-08-24 05:38:05,832 : INFO : collected 144476 word types and 1286325 unique tags from a corpus of 1286325 examples and 5940110000 words\n",
      "2016-08-24 05:38:06,188 : INFO : min_count=5 retains 102128 unique words (drops 42348)\n",
      "2016-08-24 05:38:06,189 : INFO : min_count leaves 5770508682 word corpus (99% of original 5770565838)\n",
      "2016-08-24 05:38:06,703 : INFO : deleting the raw counts dictionary of 144476 items\n",
      "2016-08-24 05:38:06,719 : INFO : sample=1e-05 downsamples 4352 most-common words\n",
      "2016-08-24 05:38:06,720 : INFO : downsampling leaves estimated 1939803701 word corpus (33.6% of prior 5770508682)\n",
      "2016-08-24 05:38:06,721 : INFO : estimated required memory for 102128 words and 400 dimensions: 5307735400 bytes\n",
      "2016-08-24 05:38:07,270 : INFO : using concatenative 6800-dimensional layer1\n",
      "2016-08-24 05:38:07,271 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2d 23h 43min 39s, sys: 31min 49s, total: 3d 15min 28s\n",
      "Wall time: 3d 11min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.build_vocab(sentences=LabeledLineSentence(training_file), progress_per=REPORT_VOCAB_PROGRESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-24 23:42:55,721 : INFO : saving Doc2Vec object under /home/local/shalaby/models/doc2vec_size_400_w_8_type_dm_concat_1_mean_0_hs_0_neg_10_vocabsize_300000_iter_1_vocab_only, separately None\n",
      "2016-08-24 23:42:55,723 : INFO : storing numpy array 'doctag_syn0' to /home/local/shalaby/models/doc2vec_size_400_w_8_type_dm_concat_1_mean_0_hs_0_neg_10_vocabsize_300000_iter_1_vocab_only.docvecs.doctag_syn0.npy\n",
      "2016-08-24 23:42:58,412 : INFO : storing numpy array 'syn1neg' to /home/local/shalaby/models/doc2vec_size_400_w_8_type_dm_concat_1_mean_0_hs_0_neg_10_vocabsize_300000_iter_1_vocab_only.syn1neg.npy\n",
      "2016-08-24 23:43:01,510 : INFO : not storing attribute syn0norm\n",
      "2016-08-24 23:43:01,511 : INFO : storing numpy array 'syn0' to /home/local/shalaby/models/doc2vec_size_400_w_8_type_dm_concat_1_mean_0_hs_0_neg_10_vocabsize_300000_iter_1_vocab_only.syn0.npy\n",
      "2016-08-24 23:43:01,730 : INFO : not storing attribute cum_table\n"
     ]
    }
   ],
   "source": [
    "# file_name = 'doc2vec_size_{}_w_{}_type_{}_hs_{}_iter_{}'.format(DOC2VEC_SIZE, DOC2VEC_WINDOW, \n",
    "#                                                                 'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "#                                                                 DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE)\n",
    "file_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_hs_{}_neg_{}_vocabsize_{}_iter_{}_vocab_only'.format(DOC2VEC_SIZE, DOC2VEC_WINDOW, \n",
    "                                                                'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                                DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                                str(DOC2VEC_MAX_VOCAB_SIZE), DOC2VEC_EPOCHS)\n",
    "model.save('/home/local/shalaby/models/{}'.format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-24 23:45:02,265 : INFO : training model with 7 workers on 102129 vocabulary and 6800 features, using sg=0 hs=0 sample=1e-05 negative=10\n",
      "2016-08-24 23:45:02,266 : INFO : expecting 1286325 sentences, matching count from corpus used for vocabulary survey\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(sentences=LabeledLineSentence(training_file), report_delay=REPORT_DELAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# file_name = 'doc2vec_size_{}_w_{}_type_{}_hs_{}_iter_{}'.format(DOC2VEC_SIZE, DOC2VEC_WINDOW, \n",
    "#                                                                 'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "#                                                                 DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE)\n",
    "file_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_hs_{}_neg_{}_vocabsize_{}_iter_{}'.format(DOC2VEC_SIZE, DOC2VEC_WINDOW, \n",
    "                                                                'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                                DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                                str(DOC2VEC_MAX_VOCAB_SIZE), DOC2VEC_EPOCHS)\n",
    "model.save('/home/local/shalaby/models/{}'.format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-16 14:34:05,764 : INFO : precomputing L2-norms of word weight vectors\n",
      "precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Doc2Vec' object has no attribute 'syn0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-aa93d828dd27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'08887671'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab)\u001b[0m\n\u001b[0;32m   1206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1207\u001b[0m         \"\"\"\n\u001b[1;32m-> 1208\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36minit_sims\u001b[1;34m(self, replace)\u001b[0m\n\u001b[0;32m   1536\u001b[0m                     \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1537\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1538\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1540\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mestimate_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Doc2Vec' object has no attribute 'syn0'"
     ]
    }
   ],
   "source": [
    "model.most_similar('08887671')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Doc2Vec' object has no attribute 'syn0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-717e96f4d1c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'08887671'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/stud/shalaby/.virtualenv/thesis-env/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m   1474\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1475\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1476\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1478\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Doc2Vec' object has no attribute 'syn0'"
     ]
    }
   ],
   "source": [
    "model['08887671']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6147817"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "model.raw_vocab = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.raw_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__hash__',\n",
       " '__ignoreds',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__numpys',\n",
       " '__recursive_saveloads',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__scipys',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slotnames__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_adapt_by_suffix',\n",
       " '_do_train_job',\n",
       " '_load_specials',\n",
       " '_raw_word_count',\n",
       " '_save_specials',\n",
       " '_smart_save',\n",
       " 'accuracy',\n",
       " 'alpha',\n",
       " 'batch_words',\n",
       " 'build_vocab',\n",
       " 'cbow_mean',\n",
       " 'clear_sims',\n",
       " 'comment',\n",
       " 'corpus_count',\n",
       " 'create_binary_tree',\n",
       " 'dbow',\n",
       " 'dbow_words',\n",
       " 'dm',\n",
       " 'dm_concat',\n",
       " 'dm_tag_count',\n",
       " 'docvecs',\n",
       " 'doesnt_match',\n",
       " 'estimate_memory',\n",
       " 'finalize_vocab',\n",
       " 'hashfxn',\n",
       " 'hs',\n",
       " 'index2word',\n",
       " 'infer_vector',\n",
       " 'init_sims',\n",
       " 'intersect_word2vec_format',\n",
       " 'iter',\n",
       " 'layer1_size',\n",
       " 'load',\n",
       " 'load_word2vec_format',\n",
       " 'log_accuracy',\n",
       " 'make_cum_table',\n",
       " 'max_vocab_size',\n",
       " 'min_alpha',\n",
       " 'min_alpha_yet_reached',\n",
       " 'min_count',\n",
       " 'most_similar',\n",
       " 'most_similar_cosmul',\n",
       " 'n_similarity',\n",
       " 'negative',\n",
       " 'null_word',\n",
       " 'random',\n",
       " 'raw_vocab',\n",
       " 'reset_from',\n",
       " 'reset_weights',\n",
       " 'sample',\n",
       " 'save',\n",
       " 'save_word2vec_format',\n",
       " 'scale_vocab',\n",
       " 'scan_vocab',\n",
       " 'score',\n",
       " 'seed',\n",
       " 'seeded_vector',\n",
       " 'sg',\n",
       " 'similar_by_vector',\n",
       " 'similar_by_word',\n",
       " 'similarity',\n",
       " 'sort_vocab',\n",
       " 'sorted_vocab',\n",
       " 'syn0_lockf',\n",
       " 'total_train_time',\n",
       " 'train',\n",
       " 'train_count',\n",
       " 'vector_size',\n",
       " 'vocab',\n",
       " 'window',\n",
       " 'wmdistance',\n",
       " 'workers']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6147817"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Doc2Vec.wmdistance of <gensim.models.doc2vec.Doc2Vec object at 0x7fd85f134210>>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wmdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
