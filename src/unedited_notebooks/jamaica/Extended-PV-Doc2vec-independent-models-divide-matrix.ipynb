{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of training and validation matrices for classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple, defaultdict\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "import gzip\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "from functools import partial\n",
    "\n",
    "import seaborn\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "from thesis.utils.metrics import *\n",
    "from thesis.utils.classification import *\n",
    "from thesis.utils.file import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables used throughout the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CORES = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GLOBAL_VARS = namedtuple('GLOBAL_VARS', ['MODEL_NAME', 'DOC2VEC_MODEL_NAME', 'DOC2VEC_MODEL', 'DOC2VEC_RAW_MODEL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_MODEL = \"vocab_model\"\n",
    "MODEL_PREFIX = \"model\"\n",
    "VALIDATION_MATRIX = \"validation_matrix.pkl\"\n",
    "VALIDATION_DICT = \"validation_dict.pkl\"\n",
    "TEST_MATRIX = \"test_matrix.pkl\"\n",
    "TEST_DICT = \"test_dict.pkl\"\n",
    "METRICS = \"metrics.pkl\"\n",
    "CLASSIFIER = \"classifier.pkl\"\n",
    "TYPE_CLASSIFIER= \"{}_classifier.pkl\"\n",
    "\n",
    "TRAINING_DATA_MATRIX = \"X_level_{}.npy\"\n",
    "TRAINING_LABELS_MATRIX = \"y_{}.npy\"\n",
    "VALIDATION_DATA_MATRIX = \"Xv_level_{}.npy\"\n",
    "VALIDATION_LABELS_MATRIX = \"yv_{}.npy\"\n",
    "TEST_DATA_MATRIX = \"Xt_level_{}.npy\"\n",
    "TEST_LABELS_MATRIX = \"yt_{}.npy\"\n",
    "\n",
    "TRAINING_DATA_MATRIX_PART = \"X_level_{}-{}.npy\"\n",
    "TRAINING_LABELS_MATRIX_PART = \"y_{}-{}.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GZIP_EXTENSION = \".gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_location = \"/mnt/virtual-machines/data/\"\n",
    "big_data_location = \"/mnt/virtual-machines/data/\"\n",
    "\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "matrices_save_location = big_data_location + \"extended_pv_matrices\"\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "valid_classes_file = exports_location + \"valid_classes.pkl\"\n",
    "valid_subclasses_file = exports_location + \"valid_subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "training_docs_list_file = exports_location + \"training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"validation_docs_list.pkl\"\n",
    "test_docs_list_file = exports_location + \"test_docs_list.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load general data required for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.3 s, sys: 1.42 s, total: 18.8 s\n",
      "Wall time: 18.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "valid_classes = pickle.load(open(valid_classes_file))\n",
    "valid_subclasses = pickle.load(open(valid_subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286325"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321473"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401877"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables for generation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_ABSTRACT_CHUNKS = 3\n",
    "NUM_DESC_CHUNKS = 23\n",
    "NUM_CLAIMS_CHUNKS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEVEL_1_ID = \"{}\"\n",
    "LEVEL_2_ID = \"{}_{}\"\n",
    "LEVEL_3_ID = \"{}_{}_part-{}\"\n",
    "\n",
    "PART_LEVEL_NAME = \"{}_{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOCUMENT_ORDER = [\n",
    "    (1, \"document\"), \n",
    "    (2, \"abstract\"), (3, \"abstract\"), \n",
    "    (2, \"description\"), (3, \"description\"), \n",
    "    (2, \"claims\"), (3, \"claims\")\n",
    "]\n",
    "DOCUMENT_PART_SIZES = {\n",
    "    \n",
    "    \"1_document\": 1,\n",
    "    \"2_abstract\": 1,\n",
    "    \"2_description\": 1,\n",
    "    \"2_claims\": 1,\n",
    "    \"3_abstract\": NUM_ABSTRACT_CHUNKS,\n",
    "    \"3_description\": NUM_DESC_CHUNKS,\n",
    "    \"3_claims\": NUM_CLAIMS_CHUNKS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOC2VEC_SIZE = 200\n",
    "DOC2VEC_WINDOW = 2\n",
    "DOC2VEC_MAX_VOCAB_SIZE = None\n",
    "DOC2VEC_SAMPLE = 1e-3\n",
    "DOC2VEC_TYPE = 1\n",
    "DOC2VEC_HIERARCHICAL_SAMPLE = 0\n",
    "DOC2VEC_NEGATIVE_SAMPLE_SIZE = 10\n",
    "DOC2VEC_CONCAT = 0\n",
    "DOC2VEC_MEAN = 1\n",
    "DOC2VEC_TRAIN_WORDS = 0\n",
    "DOC2VEC_EPOCHS = 1 # we do our training manually one epoch at a time\n",
    "DOC2VEC_MAX_EPOCHS = 8\n",
    "REPORT_DELAY = 20 # report the progress every x seconds\n",
    "REPORT_VOCAB_PROGRESS = 100000 # report vocab progress every x documents\n",
    "\n",
    "DOC2VEC_MMAP = 'r'\n",
    "\n",
    "DOC2VEC_EPOCH = 8\n",
    "\n",
    "\n",
    "raw_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}'.format(DOC2VEC_SIZE,\n",
    "                        DOC2VEC_WINDOW,\n",
    "                        'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                        DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                        DOC2VEC_TRAIN_WORDS,\n",
    "                        DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                        str(DOC2VEC_MAX_VOCAB_SIZE)\n",
    "                        )\n",
    "raw_model_name = os.path.join(raw_model_name, \"epoch_{}\")\n",
    "GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME = raw_model_name.format(DOC2VEC_EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities for data matrix filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_training_data(classifications_type, level):\n",
    "    info(\"Loading Training Data from file\")\n",
    "    training_data = np.load(open(os.path.join(matrices_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                              TRAINING_DATA_MATRIX.format(level))))\n",
    "    training_labels = np.load(open(os.path.join(matrices_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                TRAINING_LABELS_MATRIX.format(classifications_type))))\n",
    "    return training_data, training_labels\n",
    "\n",
    "def get_validation_data(classifications_type, level):\n",
    "    info(\"Loading Validation Data from file\")\n",
    "    validation_data = np.load(open(os.path.join(matrices_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                VALIDATION_DATA_MATRIX.format(level))))\n",
    "    validation_labels = np.load(open(os.path.join(matrices_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                  VALIDATION_LABELS_MATRIX.format(classifications_type))))\n",
    "    return validation_data, validation_labels\n",
    "\n",
    "def get_test_data(classifications_type, level):\n",
    "    info(\"Loading Test Data from file\")\n",
    "    test_data = np.load(open(os.path.join(matrices_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                TEST_DATA_MATRIX.format(level))))\n",
    "    test_labels = np.load(open(os.path.join(matrices_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                  TEST_LABELS_MATRIX.format(classifications_type))))\n",
    "    return test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_part_ids(doc_id, part_level, part_name):\n",
    "    \"\"\"\n",
    "    Returns the ids to look for, for a given document id, part level and part name\n",
    "    ex get_part_ids(x, 3, \"abstract) => [\"x_abstract_part-1\", \"x_abstract_part-2\", \"x_abstract_part-3\", ...]\n",
    "    \"\"\"\n",
    "    if part_name == \"document\": \n",
    "        return [LEVEL_1_ID.format(doc_id)]\n",
    "    elif part_level == 2:\n",
    "        return [LEVEL_2_ID.format(doc_id, part_name)]\n",
    "    elif part_level == 3:\n",
    "        ids = []\n",
    "        for i in range(DOCUMENT_PART_SIZES[PART_LEVEL_NAME.format(part_level, part_name)]):\n",
    "            ids.append(LEVEL_3_ID.format(doc_id, part_name, i+1))\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sequence_insert_location(my_part_level, my_part_name, max_level):\n",
    "    \"\"\"\n",
    "    for a given level and name, determines where its position in the sequence begins\n",
    "    \"\"\"\n",
    "    assert DOCUMENT_PART_SIZES.get(PART_LEVEL_NAME.format(my_part_level, my_part_name)) is not None\n",
    "    loc = 0\n",
    "    for part_level, part_name in DOCUMENT_ORDER:\n",
    "        if part_level <= max_level:\n",
    "            if part_level == my_part_level and part_name == my_part_name:\n",
    "                break\n",
    "            else:\n",
    "                loc += DOCUMENT_PART_SIZES[PART_LEVEL_NAME.format(part_level, part_name)]\n",
    "    return loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Training and Validation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8\n"
     ]
    }
   ],
   "source": [
    "placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}'.format(DOC2VEC_SIZE, \n",
    "                                                            DOC2VEC_WINDOW, \n",
    "                                                            'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                            DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                            DOC2VEC_TRAIN_WORDS,\n",
    "                                                            DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                            str(DOC2VEC_MAX_VOCAB_SIZE))\n",
    "GLOBAL_VARS.DOC2VEC_MODEL_NAME = placeholder_model_name\n",
    "placeholder_model_name = os.path.join(placeholder_model_name, \"epoch_{}\")\n",
    "\n",
    "epoch = 8\n",
    "\n",
    "GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "print GLOBAL_VARS.MODEL_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEVEL_TO_GENERATE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifications_type = \"subclasses\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-13 20:52:06,917 : INFO : Loading Training Data from file\n"
     ]
    }
   ],
   "source": [
    "X,y = get_training_data(classifications_type, LEVEL_TO_GENERATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1286325, 34, 200)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (100000, 34, 200)\n",
      "100000 (100000, 34, 200)\n",
      "200000 (100000, 34, 200)\n",
      "300000 (100000, 34, 200)\n",
      "400000 (100000, 34, 200)\n",
      "500000 (100000, 34, 200)\n",
      "600000 (100000, 34, 200)\n",
      "700000 (100000, 34, 200)\n",
      "800000 (100000, 34, 200)\n",
      "900000 (100000, 34, 200)\n",
      "1000000 (100000, 34, 200)\n",
      "1100000 (100000, 34, 200)\n",
      "1200000 (86325, 34, 200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(int(math.ceil(float(X.shape[0])/batch_size))):\n",
    "    index = i*batch_size\n",
    "    X_part = X[index: (i+1)* batch_size,:]\n",
    "    print index, X_part.shape\n",
    "    np.save(open(os.path.join(matrices_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                          TRAINING_DATA_MATRIX_PART.format(LEVEL_TO_GENERATE, index)), \"w\"), X_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For y labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 940)\n",
      "(100000, 940)\n",
      "(100000, 940)\n",
      "(100000, 940)\n",
      "(100000, 940)\n",
      "(100000, 940)\n",
      "(100000, 940)\n",
      "(100000, 940)\n",
      "(100000, 940)\n",
      "(100000, 940)\n",
      "(100000, 940)\n",
      "(100000, 940)\n",
      "(86325, 940)\n"
     ]
    }
   ],
   "source": [
    "for i in range(int(math.ceil(float(y.shape[0])/batch_size))):\n",
    "    index = i * batch_size\n",
    "    y_part = y[index: (i+1)* batch_size,:]\n",
    "    print y_part.shape\n",
    "    np.save(open(os.path.join(matrices_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                          TRAINING_LABELS_MATRIX_PART.format(classifications_type, index)), \"w\"), y_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEVEL_TO_GENERATE = 2\n",
    "EMBEDDING_SIZE = DOC2VEC_SIZE\n",
    "ZERO_VECTOR = [0] * DOC2VEC_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "sequence_size = sum([DOCUMENT_PART_SIZES[\"{}_{}\".format(part_level, part_name)] for part_level, part_name in DOCUMENT_ORDER if part_level <= LEVEL_TO_GENERATE])\n",
    "print sequence_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-12 14:11:50,944 : INFO : ********** Generating Matrices for LEVEL:3 ************\n",
      "2017-04-12 14:11:50,946 : INFO : ======== Working on Level: 1 => document\n",
      "2017-04-12 14:11:50,947 : INFO : Loading Doc2vec model: doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_1_document/epoch_8\n",
      "2017-04-12 14:11:50,948 : INFO : loading Doc2Vec object from /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_1_document/epoch_8/model\n",
      "2017-04-12 14:11:58,955 : INFO : loading docvecs recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_1_document/epoch_8/model.docvecs.* with mmap=r\n",
      "2017-04-12 14:11:58,956 : INFO : loading doctag_syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_1_document/epoch_8/model.docvecs.doctag_syn0.npy with mmap=r\n",
      "2017-04-12 14:11:58,958 : INFO : loading wv recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_1_document/epoch_8/model.wv.* with mmap=r\n",
      "2017-04-12 14:11:58,958 : INFO : loading syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_1_document/epoch_8/model.wv.syn0.npy with mmap=r\n",
      "2017-04-12 14:11:58,960 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-12 14:11:58,960 : INFO : loading syn1neg from /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_1_document/epoch_8/model.syn1neg.npy with mmap=r\n",
      "2017-04-12 14:11:58,962 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-12 14:11:58,963 : INFO : loaded /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_1_document/epoch_8/model\n",
      "2017-04-12 14:12:00,336 : INFO : Loading Validation Dict\n",
      "2017-04-12 14:12:29,515 : INFO : Filling training matrix\n",
      "2017-04-12 14:12:40,345 : INFO : Filling validation matrix\n",
      "2017-04-12 14:12:41,957 : INFO : ======== Working on Level: 2 => abstract\n",
      "2017-04-12 14:12:41,971 : INFO : Loading Doc2vec model: doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_abstract/epoch_8\n",
      "2017-04-12 14:12:41,987 : INFO : loading Doc2Vec object from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_abstract/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_abstract/epoch_8/model\n",
      "2017-04-12 14:12:49,494 : INFO : loading docvecs recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_abstract/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_abstract/epoch_8/model.docvecs.* with mmap=r\n",
      "2017-04-12 14:12:49,496 : INFO : loading doctag_syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_abstract/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_abstract/epoch_8/model.docvecs.doctag_syn0.npy with mmap=r\n",
      "2017-04-12 14:12:49,498 : INFO : loading wv recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_abstract/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_abstract/epoch_8/model.wv.* with mmap=r\n",
      "2017-04-12 14:12:49,499 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-12 14:12:49,499 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-12 14:12:49,500 : INFO : loaded /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_abstract/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_abstract/epoch_8/model\n",
      "2017-04-12 14:12:50,215 : INFO : Loading Validation Dict\n",
      "2017-04-12 14:13:26,162 : INFO : Filling training matrix\n",
      "2017-04-12 14:13:35,939 : INFO : Filling validation matrix\n",
      "2017-04-12 14:13:37,285 : INFO : ======== Working on Level: 3 => abstract\n",
      "2017-04-12 14:13:37,296 : INFO : Loading Doc2vec model: doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_abstract/epoch_8\n",
      "2017-04-12 14:13:37,296 : INFO : loading Doc2Vec object from /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_abstract/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_abstract/epoch_8/model\n",
      "2017-04-12 14:13:55,523 : INFO : loading docvecs recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_abstract/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_abstract/epoch_8/model.docvecs.* with mmap=r\n",
      "2017-04-12 14:13:55,524 : INFO : loading doctag_syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_abstract/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_abstract/epoch_8/model.docvecs.doctag_syn0.npy with mmap=r\n",
      "2017-04-12 14:13:55,526 : INFO : loading wv recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_abstract/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_abstract/epoch_8/model.wv.* with mmap=r\n",
      "2017-04-12 14:13:55,527 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-12 14:13:55,528 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-12 14:13:55,530 : INFO : loaded /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_abstract/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_abstract/epoch_8/model\n",
      "2017-04-12 14:13:55,995 : INFO : Loading Validation Dict\n",
      "2017-04-12 14:15:42,964 : INFO : Filling training matrix\n",
      "2017-04-12 14:16:09,298 : INFO : Filling validation matrix\n",
      "2017-04-12 14:16:12,754 : INFO : ======== Working on Level: 2 => description\n",
      "2017-04-12 14:16:12,769 : INFO : Loading Doc2vec model: doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_description/epoch_8\n",
      "2017-04-12 14:16:12,772 : INFO : loading Doc2Vec object from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_description/epoch_8/model\n",
      "2017-04-12 14:16:17,524 : INFO : loading docvecs recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_description/epoch_8/model.docvecs.* with mmap=r\n",
      "2017-04-12 14:16:17,525 : INFO : loading doctag_syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_description/epoch_8/model.docvecs.doctag_syn0.npy with mmap=r\n",
      "2017-04-12 14:16:17,527 : INFO : loading wv recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_description/epoch_8/model.wv.* with mmap=r\n",
      "2017-04-12 14:16:17,527 : INFO : loading syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_description/epoch_8/model.wv.syn0.npy with mmap=r\n",
      "2017-04-12 14:16:17,529 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-12 14:16:17,529 : INFO : loading syn1neg from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_description/epoch_8/model.syn1neg.npy with mmap=r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-12 14:16:17,531 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-12 14:16:17,531 : INFO : loaded /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_description/epoch_8/model\n",
      "2017-04-12 14:16:19,931 : INFO : Loading Validation Dict\n",
      "2017-04-12 14:16:56,411 : INFO : Filling training matrix\n",
      "2017-04-12 14:17:05,961 : INFO : Filling validation matrix\n",
      "2017-04-12 14:17:07,046 : INFO : ======== Working on Level: 3 => description\n",
      "2017-04-12 14:17:07,062 : INFO : Loading Doc2vec model: doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_description/epoch_8\n",
      "2017-04-12 14:17:07,070 : INFO : loading Doc2Vec object from /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_description/epoch_8/model\n",
      "2017-04-12 14:19:30,723 : INFO : loading docvecs recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_description/epoch_8/model.docvecs.* with mmap=r\n",
      "2017-04-12 14:19:30,725 : INFO : loading doctag_syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_description/epoch_8/model.docvecs.doctag_syn0.npy with mmap=r\n",
      "2017-04-12 14:19:30,726 : INFO : loading doctag_syn0_lockf from /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_description/epoch_8/model.docvecs.doctag_syn0_lockf.npy with mmap=r\n",
      "2017-04-12 14:19:30,727 : INFO : loading wv recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_description/epoch_8/model.wv.* with mmap=r\n",
      "2017-04-12 14:19:30,728 : INFO : loading syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_description/epoch_8/model.wv.syn0.npy with mmap=r\n",
      "2017-04-12 14:19:30,729 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-12 14:19:30,730 : INFO : loading syn1neg from /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_description/epoch_8/model.syn1neg.npy with mmap=r\n",
      "2017-04-12 14:19:30,731 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-12 14:19:30,731 : INFO : loaded /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_description/epoch_8/model\n",
      "2017-04-12 14:19:32,451 : INFO : Loading Validation Dict\n",
      "2017-04-12 14:33:03,482 : INFO : Filling training matrix\n",
      "2017-04-12 14:36:05,600 : INFO : Filling validation matrix\n",
      "2017-04-12 14:36:26,681 : INFO : ======== Working on Level: 2 => claims\n",
      "2017-04-12 14:36:26,683 : INFO : Loading Doc2vec model: doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_claims/epoch_8\n",
      "2017-04-12 14:36:26,684 : INFO : loading Doc2Vec object from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_claims/epoch_8/model\n",
      "2017-04-12 14:36:30,151 : INFO : loading docvecs recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_claims/epoch_8/model.docvecs.* with mmap=r\n",
      "2017-04-12 14:36:30,152 : INFO : loading doctag_syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_claims/epoch_8/model.docvecs.doctag_syn0.npy with mmap=r\n",
      "2017-04-12 14:36:30,153 : INFO : loading wv recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_claims/epoch_8/model.wv.* with mmap=r\n",
      "2017-04-12 14:36:30,154 : INFO : loading syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_claims/epoch_8/model.wv.syn0.npy with mmap=r\n",
      "2017-04-12 14:36:30,155 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-12 14:36:30,155 : INFO : loading syn1neg from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_claims/epoch_8/model.syn1neg.npy with mmap=r\n",
      "2017-04-12 14:36:30,156 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-12 14:36:30,157 : INFO : loaded /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_claims/epoch_8/model\n",
      "2017-04-12 14:36:49,744 : INFO : Loading Validation Dict\n",
      "2017-04-12 14:37:30,782 : INFO : Filling training matrix\n",
      "2017-04-12 14:37:39,895 : INFO : Filling validation matrix\n",
      "2017-04-12 14:37:41,157 : INFO : ======== Working on Level: 3 => claims\n",
      "2017-04-12 14:37:41,160 : INFO : Loading Doc2vec model: doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_claims/epoch_8\n",
      "2017-04-12 14:37:41,166 : INFO : loading Doc2Vec object from /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_claims/epoch_8/model\n",
      "2017-04-12 14:37:52,698 : INFO : loading docvecs recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_claims/epoch_8/model.docvecs.* with mmap=r\n",
      "2017-04-12 14:37:52,699 : INFO : loading doctag_syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_claims/epoch_8/model.docvecs.doctag_syn0.npy with mmap=r\n",
      "2017-04-12 14:37:52,719 : INFO : loading wv recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_claims/epoch_8/model.wv.* with mmap=r\n",
      "2017-04-12 14:37:52,720 : INFO : loading syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_claims/epoch_8/model.wv.syn0.npy with mmap=r\n",
      "2017-04-12 14:37:52,725 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-12 14:37:52,726 : INFO : loading syn1neg from /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_claims/epoch_8/model.syn1neg.npy with mmap=r\n",
      "2017-04-12 14:37:52,737 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-12 14:37:52,738 : INFO : loaded /mnt/virtual-machines/data/parameter_search_doc2vec_models_3_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_3_claims/epoch_8/model\n",
      "2017-04-12 14:37:53,383 : INFO : Loading Validation Dict\n",
      "2017-04-12 14:39:33,535 : INFO : Filling training matrix\n",
      "2017-04-12 14:39:34,280 : INFO : ZERO_VECTOR for 07576200_claims_part-2\n",
      "2017-04-12 14:39:34,281 : INFO : ZERO_VECTOR for 07576200_claims_part-3\n",
      "2017-04-12 14:39:34,284 : INFO : ZERO_VECTOR for 07576200_claims_part-4\n",
      "2017-04-12 14:39:37,998 : INFO : ZERO_VECTOR for 07820861_claims_part-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-12 14:39:37,999 : INFO : ZERO_VECTOR for 07820861_claims_part-3\n",
      "2017-04-12 14:39:38,000 : INFO : ZERO_VECTOR for 07820861_claims_part-4\n",
      "2017-04-12 14:39:38,173 : INFO : ZERO_VECTOR for 07163959_claims_part-2\n",
      "2017-04-12 14:39:38,174 : INFO : ZERO_VECTOR for 07163959_claims_part-3\n",
      "2017-04-12 14:39:38,174 : INFO : ZERO_VECTOR for 07163959_claims_part-4\n",
      "2017-04-12 14:39:39,033 : INFO : ZERO_VECTOR for 08461390_claims_part-2\n",
      "2017-04-12 14:39:39,034 : INFO : ZERO_VECTOR for 08461390_claims_part-3\n",
      "2017-04-12 14:39:39,036 : INFO : ZERO_VECTOR for 08461390_claims_part-4\n",
      "2017-04-12 14:39:41,141 : INFO : ZERO_VECTOR for 08222448_claims_part-2\n",
      "2017-04-12 14:39:41,144 : INFO : ZERO_VECTOR for 08222448_claims_part-3\n",
      "2017-04-12 14:39:41,145 : INFO : ZERO_VECTOR for 08222448_claims_part-4\n",
      "2017-04-12 14:39:43,064 : INFO : ZERO_VECTOR for 07531654_claims_part-2\n",
      "2017-04-12 14:39:43,066 : INFO : ZERO_VECTOR for 07531654_claims_part-3\n",
      "2017-04-12 14:39:43,066 : INFO : ZERO_VECTOR for 07531654_claims_part-4\n",
      "2017-04-12 14:39:43,882 : INFO : ZERO_VECTOR for 07301035_claims_part-2\n",
      "2017-04-12 14:39:43,885 : INFO : ZERO_VECTOR for 07301035_claims_part-3\n",
      "2017-04-12 14:39:43,886 : INFO : ZERO_VECTOR for 07301035_claims_part-4\n",
      "2017-04-12 14:39:44,100 : INFO : ZERO_VECTOR for 07488829_claims_part-2\n",
      "2017-04-12 14:39:44,101 : INFO : ZERO_VECTOR for 07488829_claims_part-3\n",
      "2017-04-12 14:39:44,102 : INFO : ZERO_VECTOR for 07488829_claims_part-4\n",
      "2017-04-12 14:39:44,138 : INFO : ZERO_VECTOR for 07482464_claims_part-2\n",
      "2017-04-12 14:39:44,140 : INFO : ZERO_VECTOR for 07482464_claims_part-3\n",
      "2017-04-12 14:39:44,141 : INFO : ZERO_VECTOR for 07482464_claims_part-4\n",
      "2017-04-12 14:39:45,410 : INFO : ZERO_VECTOR for 07371768_claims_part-2\n",
      "2017-04-12 14:39:45,411 : INFO : ZERO_VECTOR for 07371768_claims_part-3\n",
      "2017-04-12 14:39:45,411 : INFO : ZERO_VECTOR for 07371768_claims_part-4\n",
      "2017-04-12 14:39:48,116 : INFO : ZERO_VECTOR for 07553976_claims_part-2\n",
      "2017-04-12 14:39:48,117 : INFO : ZERO_VECTOR for 07553976_claims_part-3\n",
      "2017-04-12 14:39:48,118 : INFO : ZERO_VECTOR for 07553976_claims_part-4\n",
      "2017-04-12 14:39:49,243 : INFO : ZERO_VECTOR for 07517994_claims_part-2\n",
      "2017-04-12 14:39:49,244 : INFO : ZERO_VECTOR for 07517994_claims_part-3\n",
      "2017-04-12 14:39:49,245 : INFO : ZERO_VECTOR for 07517994_claims_part-4\n",
      "2017-04-12 14:39:53,320 : INFO : ZERO_VECTOR for 06998485_claims_part-2\n",
      "2017-04-12 14:39:53,321 : INFO : ZERO_VECTOR for 06998485_claims_part-3\n",
      "2017-04-12 14:39:53,322 : INFO : ZERO_VECTOR for 06998485_claims_part-4\n",
      "2017-04-12 14:40:03,837 : INFO : ZERO_VECTOR for 08614340_claims_part-2\n",
      "2017-04-12 14:40:03,838 : INFO : ZERO_VECTOR for 08614340_claims_part-3\n",
      "2017-04-12 14:40:03,839 : INFO : ZERO_VECTOR for 08614340_claims_part-4\n",
      "2017-04-12 14:40:04,920 : INFO : ZERO_VECTOR for 08748597_claims_part-2\n",
      "2017-04-12 14:40:04,921 : INFO : ZERO_VECTOR for 08748597_claims_part-3\n",
      "2017-04-12 14:40:04,922 : INFO : ZERO_VECTOR for 08748597_claims_part-4\n",
      "2017-04-12 14:40:05,370 : INFO : Filling validation matrix\n",
      "2017-04-12 14:40:07,302 : INFO : ZERO_VECTOR for 08378142_claims_part-2\n",
      "2017-04-12 14:40:07,303 : INFO : ZERO_VECTOR for 08378142_claims_part-3\n",
      "2017-04-12 14:40:07,303 : INFO : ZERO_VECTOR for 08378142_claims_part-4\n",
      "2017-04-12 14:40:08,069 : INFO : ZERO_VECTOR for 08552218_claims_part-2\n",
      "2017-04-12 14:40:08,070 : INFO : ZERO_VECTOR for 08552218_claims_part-3\n",
      "2017-04-12 14:40:08,070 : INFO : ZERO_VECTOR for 08552218_claims_part-4\n",
      "2017-04-12 14:40:09,197 : INFO : Saving training matrix\n",
      "2017-04-12 14:40:35,670 : INFO : Saving validation matrix\n"
     ]
    }
   ],
   "source": [
    "X_data = np.ndarray((len(training_docs_list), sequence_size, EMBEDDING_SIZE), dtype=np.float32)\n",
    "Xv_data = np.ndarray((len(validation_docs_list), sequence_size, EMBEDDING_SIZE), dtype=np.float32)\n",
    "\n",
    "info(\"********** Generating Matrices for LEVEL:{} ************\".format(LEVEL_TO_GENERATE))\n",
    "\n",
    "for part_level, part_name in DOCUMENT_ORDER:\n",
    "    if part_level <= LEVEL_TO_GENERATE:\n",
    "        \n",
    "        info(\"======== Working on Level: {} => {}\".format(part_level, part_name))\n",
    "        \n",
    "        sequence_insert_location = get_sequence_insert_location(part_level, part_name, LEVEL_TO_GENERATE)\n",
    "        \n",
    "        \n",
    "        doc2vec_model_save_location = os.path.join(root_location,\n",
    "                                                   \"parameter_search_doc2vec_models_\" + str(part_level) + '_' + part_name,\n",
    "                                                   \"full\")\n",
    "        \n",
    "        placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}_model_{}'.format(DOC2VEC_SIZE,\n",
    "                                                                DOC2VEC_WINDOW,\n",
    "                                                                'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                                DOC2VEC_TRAIN_WORDS,\n",
    "                                                                DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                                str(DOC2VEC_MAX_VOCAB_SIZE),\n",
    "                                                                str(part_level) + '_' + part_name\n",
    "                                                                )\n",
    "        GLOBAL_VARS.DOC2VEC_MODEL_NAME = placeholder_model_name\n",
    "        placeholder_model_name = os.path.join(placeholder_model_name, \"epoch_{}\")\n",
    "        epoch = DOC2VEC_EPOCH\n",
    "        GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "        \n",
    "        \n",
    "        info(\"Loading Doc2vec model: {}\".format(GLOBAL_VARS.MODEL_NAME))\n",
    "        doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX), mmap=DOC2VEC_MMAP)\n",
    "        \n",
    "        \n",
    "        info(\"Loading Validation Dict\")\n",
    "        validation_dict = dict(pickle.load(gzip.open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_DICT + GZIP_EXTENSION))))\n",
    "        part_level_name = PART_LEVEL_NAME.format(part_level, part_name)\n",
    "        \n",
    "        \n",
    "        def fill_matrix(data_matrix, source_dict, docs_list, start_location, use_get=False):\n",
    "            \"\"\"\n",
    "            use_get is for doc2vec_model.docvecs since it doesnt support .get(), so we catch the exception and\n",
    "            fill with zeros in that case. This should really happen very rarely (if ever) so this exception handling\n",
    "            should not be a drain on performance\n",
    "            \"\"\"\n",
    "            for i, doc_id in enumerate(docs_list):\n",
    "                child_ids = get_part_ids(doc_id, part_level, part_name)\n",
    "\n",
    "                j = start_location\n",
    "                for child_id in child_ids:\n",
    "                    try:\n",
    "                        if not use_get or source_dict.get(child_id) is not None:\n",
    "                            data_matrix[i][j] = source_dict[child_id]\n",
    "                        else:\n",
    "                            info(\"ZERO_VECTOR for {}\".format(child_id))\n",
    "                            data_matrix[i][j] = ZERO_VECTOR\n",
    "                    except:\n",
    "                        info(\"ZERO_VECTOR for {}\".format(child_id))\n",
    "                        data_matrix[i][j] = ZERO_VECTOR\n",
    "                    j+= 1\n",
    "        \n",
    "        info(\"Filling training matrix\")\n",
    "        fill_matrix(X_data, doc2vec_model.docvecs, training_docs_list, sequence_insert_location, use_get=False)\n",
    "        info(\"Filling validation matrix\")\n",
    "        fill_matrix(Xv_data, validation_dict, validation_docs_list, sequence_insert_location, use_get=True)\n",
    "        \n",
    "        \n",
    "ensure_disk_location_exists(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME))\n",
    "info(\"Saving training matrix\")\n",
    "np.save(open(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME, \n",
    "                          TRAINING_DATA_MATRIX.format(LEVEL_TO_GENERATE)), \"w\"), X_data)\n",
    "info(\"Saving validation matrix\")\n",
    "np.save(open(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME, \n",
    "                          VALIDATION_DATA_MATRIX.format(LEVEL_TO_GENERATE)), \"w\"), Xv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate test matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEVEL_TO_GENERATE = 3\n",
    "EMBEDDING_SIZE = DOC2VEC_SIZE\n",
    "ZERO_VECTOR = [0] * DOC2VEC_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "sequence_size = sum([DOCUMENT_PART_SIZES[\"{}_{}\".format(part_level, part_name)] for part_level, part_name in DOCUMENT_ORDER if part_level <= LEVEL_TO_GENERATE])\n",
    "print sequence_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-13 01:00:31,855 : INFO : ********** Generating Matrices for LEVEL:3 ************\n",
      "2017-04-13 01:00:31,856 : INFO : ======== Working on Level: 1 => document\n",
      "2017-04-13 01:00:31,857 : INFO : Loading Test Dict\n",
      "2017-04-13 01:01:08,159 : INFO : Filling test matrix\n",
      "2017-04-13 01:01:10,003 : INFO : ======== Working on Level: 2 => abstract\n",
      "2017-04-13 01:01:10,006 : INFO : Loading Test Dict\n",
      "2017-04-13 01:01:45,889 : INFO : Filling test matrix\n",
      "2017-04-13 01:01:47,582 : INFO : ======== Working on Level: 3 => abstract\n",
      "2017-04-13 01:01:47,583 : INFO : Loading Test Dict\n",
      "2017-04-13 01:03:38,426 : INFO : Filling test matrix\n",
      "2017-04-13 01:03:42,300 : INFO : ======== Working on Level: 2 => description\n",
      "2017-04-13 01:03:42,309 : INFO : Loading Test Dict\n",
      "2017-04-13 01:04:19,430 : INFO : Filling test matrix\n",
      "2017-04-13 01:04:21,060 : INFO : ======== Working on Level: 3 => description\n",
      "2017-04-13 01:04:21,065 : INFO : Loading Test Dict\n",
      "2017-04-13 01:18:28,382 : INFO : Filling test matrix\n",
      "2017-04-13 01:18:53,298 : INFO : ======== Working on Level: 2 => claims\n",
      "2017-04-13 01:18:53,300 : INFO : Loading Test Dict\n",
      "2017-04-13 01:19:43,208 : INFO : Filling test matrix\n",
      "2017-04-13 01:19:44,827 : INFO : ======== Working on Level: 3 => claims\n",
      "2017-04-13 01:19:44,831 : INFO : Loading Test Dict\n",
      "2017-04-13 01:22:10,799 : INFO : Filling test matrix\n",
      "2017-04-13 01:22:12,089 : INFO : ZERO_VECTOR for 07371868_claims_part-2\n",
      "2017-04-13 01:22:12,094 : INFO : ZERO_VECTOR for 07371868_claims_part-3\n",
      "2017-04-13 01:22:12,098 : INFO : ZERO_VECTOR for 07371868_claims_part-4\n",
      "2017-04-13 01:22:13,649 : INFO : ZERO_VECTOR for 08822687_claims_part-2\n",
      "2017-04-13 01:22:13,652 : INFO : ZERO_VECTOR for 08822687_claims_part-3\n",
      "2017-04-13 01:22:13,655 : INFO : ZERO_VECTOR for 08822687_claims_part-4\n",
      "2017-04-13 01:22:15,461 : INFO : Saving test matrix\n"
     ]
    }
   ],
   "source": [
    "Xt_data = np.ndarray((len(test_docs_list), sequence_size, EMBEDDING_SIZE), dtype=np.float32)\n",
    "\n",
    "info(\"********** Generating Matrices for LEVEL:{} ************\".format(LEVEL_TO_GENERATE))\n",
    "\n",
    "for part_level, part_name in DOCUMENT_ORDER:\n",
    "    if part_level <= LEVEL_TO_GENERATE:\n",
    "        \n",
    "        info(\"======== Working on Level: {} => {}\".format(part_level, part_name))\n",
    "        \n",
    "        sequence_insert_location = get_sequence_insert_location(part_level, part_name, LEVEL_TO_GENERATE)\n",
    "        \n",
    "        \n",
    "        doc2vec_model_save_location = os.path.join(root_location,\n",
    "                                                   \"parameter_search_doc2vec_models_\" + str(part_level) + '_' + part_name,\n",
    "                                                   \"full\")\n",
    "        \n",
    "        placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}_model_{}'.format(DOC2VEC_SIZE,\n",
    "                                                                DOC2VEC_WINDOW,\n",
    "                                                                'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                                DOC2VEC_TRAIN_WORDS,\n",
    "                                                                DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                                str(DOC2VEC_MAX_VOCAB_SIZE),\n",
    "                                                                str(part_level) + '_' + part_name\n",
    "                                                                )\n",
    "        GLOBAL_VARS.DOC2VEC_MODEL_NAME = placeholder_model_name\n",
    "        placeholder_model_name = os.path.join(placeholder_model_name, \"epoch_{}\")\n",
    "        epoch = DOC2VEC_EPOCH\n",
    "        GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "        \n",
    "        \n",
    "        info(\"Loading Test Dict\")\n",
    "        test_dict = dict(pickle.load(gzip.open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, TEST_DICT))))\n",
    "        #test_dict = dict(pickle.load(gzip.open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, TEST_DICT + GZIP_EXTENSION))))\n",
    "        part_level_name = PART_LEVEL_NAME.format(part_level, part_name)\n",
    "        \n",
    "        \n",
    "        def fill_matrix(data_matrix, source_dict, docs_list, start_location, use_get=False):\n",
    "            \"\"\"\n",
    "            use_get is for doc2vec_model.docvecs since it doesnt support .get(), so we catch the exception and\n",
    "            fill with zeros in that case. This should really happen very rarely (if ever) so this exception handling\n",
    "            should not be a drain on performance\n",
    "            \"\"\"\n",
    "            for i, doc_id in enumerate(docs_list):\n",
    "                child_ids = get_part_ids(doc_id, part_level, part_name)\n",
    "\n",
    "                j = start_location\n",
    "                for child_id in child_ids:\n",
    "                    try:\n",
    "                        if not use_get or source_dict.get(child_id) is not None:\n",
    "                            data_matrix[i][j] = source_dict[child_id]\n",
    "                        else:\n",
    "                            info(\"ZERO_VECTOR for {}\".format(child_id))\n",
    "                            data_matrix[i][j] = ZERO_VECTOR\n",
    "                    except:\n",
    "                        info(\"ZERO_VECTOR for {}\".format(child_id))\n",
    "                        data_matrix[i][j] = ZERO_VECTOR\n",
    "                    j+= 1\n",
    "        \n",
    "        info(\"Filling test matrix\")\n",
    "        fill_matrix(Xt_data, test_dict, test_docs_list, sequence_insert_location, use_get=True)\n",
    "        \n",
    "        \n",
    "ensure_disk_location_exists(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME))\n",
    "info(\"Saving test matrix\")\n",
    "np.save(open(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME, \n",
    "                          TEST_DATA_MATRIX.format(LEVEL_TO_GENERATE)), \"w\"), Xt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training and validation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_labels(classifications, docs_list):\n",
    "    one_hot_encoder = OneHotEncoder(classifications)\n",
    "    classifications_set = set(classifications)\n",
    "    labels_mat = np.zeros((len(docs_list), len(classifications)), dtype=np.int8)\n",
    "    for i, doc_id in enumerate(docs_list):\n",
    "        eligible_classifications = set(doc_classification_map[doc_id]) & classifications_set\n",
    "        labels_mat[i][:] = one_hot_encoder.get_label_vector(eligible_classifications)\n",
    "    return labels_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-13 01:46:03,620 : INFO : Creating Test Labels for sections\n",
      "2017-04-13 01:46:05,054 : INFO : Creating Test Labels for classes\n",
      "2017-04-13 01:46:10,331 : INFO : Creating Test Labels for subclasses\n"
     ]
    }
   ],
   "source": [
    "classifications_to_create = [\n",
    "    (\"sections\", sections),\n",
    "    (\"classes\", valid_classes),\n",
    "    (\"subclasses\", valid_subclasses)\n",
    "]\n",
    "\n",
    "for classifications_type, classifications in classifications_to_create:\n",
    "    info(\"Creating Training Labels for {}\".format(classifications_type))\n",
    "    y = create_labels(classifications, training_docs_list)\n",
    "    info(\"Creating Validation Labels for {}\".format(classifications_type))\n",
    "    yv = create_labels(classifications, validation_docs_list)\n",
    "    info(\"Creating Test Labels for {}\".format(classifications_type))\n",
    "    yt = create_labels(classifications, test_docs_list)\n",
    "    \n",
    "    ensure_disk_location_exists(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME))\n",
    "    np.save(open(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME, \n",
    "                                  TRAINING_LABELS_MATRIX.format(classifications_type)), \"w\"), y)\n",
    "    np.save(open(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME, \n",
    "                                  VALIDATION_LABELS_MATRIX.format(classifications_type)), \"w\"), yv)\n",
    "    np.save(open(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME, \n",
    "                                  TEST_LABELS_MATRIX.format(classifications_type)), \"w\"), yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
