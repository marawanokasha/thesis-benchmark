{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import cPickle as pickle\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import urllib2\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "\n",
    "from multiprocessing import Pool as ThreadPool\n",
    "import itertools\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import nltk\n",
    "\n",
    "from thesis.utils.text import get_sentences, sentence_wordtokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# root_location = \"/mnt/data2/shalaby/\"\n",
    "root_location = \"/home/local/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "# training_file = root_location + 'docs_output_training_validation_documents_' + str(SAMPLE_RATIO)\n",
    "training_file = root_location + 'docs_output.json'\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "# training_docs_list_file = exports_location + \"training_documents_\" + str(SAMPLE_RATIO) + \"_sample.pkl\"\n",
    "# validation_docs_list_file = exports_location + \"validation_documents_\" + str(SAMPLE_RATIO) + \"_sample.pkl\"\n",
    "training_docs_list_file = exports_location + \"extended_pv_training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"extended_pv_validation_docs_list.pkl\"\n",
    "test_docs_list_file = exports_location + \"extended_pv_test_docs_list.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25 s, sys: 3.84 s, total: 28.8 s\n",
      "Wall time: 29.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120156"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Extraction Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#ES_URL = 'http://localhost:9200/patents/patent/{}'\n",
    "ES_URL = 'http://yell.dbs.ifi.lmu.de:9200/patents/patent/{}'\n",
    "HEADING_TAG = 'heading'\n",
    "PARAGRAPH_TAG = 'p'\n",
    "UL_TAG = 'ul'\n",
    "LI_TAG = 'li'\n",
    "OL_TAG = 'ol'\n",
    "DESC_OF_DRAWINGS_TAG = 'description-of-drawings'\n",
    "MIN_PARAGRAPH_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def merge_with_previous(curr_node_tag, previous_node_tag, previous_node_text):\n",
    "    if curr_node_tag == PARAGRAPH_TAG and previous_node_tag == HEADING_TAG:\n",
    "        return True\n",
    "    if previous_node_text and len(previous_node_text) < MIN_PARAGRAPH_LENGTH:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def get_paragraphs(root):\n",
    "    paragraphs = []\n",
    "    previous_node_text = None\n",
    "    previous_tag = None\n",
    "    for child in root:\n",
    "        node_text = None\n",
    "        if child.tag != DESC_OF_DRAWINGS_TAG:\n",
    "            node_text = get_node_text(child)\n",
    "            if node_text.strip():\n",
    "                if merge_with_previous(child.tag, previous_tag, previous_node_text) and len(paragraphs) > 0:\n",
    "                    paragraphs[-1] += ' ' + node_text\n",
    "                else:\n",
    "                    paragraphs.append(node_text)\n",
    "        else:\n",
    "            node_text = extract_desc_of_drawings_paragraph(child)\n",
    "            paragraphs.append(node_text)\n",
    "            \n",
    "        previous_tag = child.tag\n",
    "        previous_node_text = node_text\n",
    "    return paragraphs\n",
    "    \n",
    "def extract_desc_of_drawings_paragraph(node):\n",
    "    previous_tag = None\n",
    "    sentences = []\n",
    "    for child in node:\n",
    "        node_text = get_node_text(child)\n",
    "        if child.tag == PARAGRAPH_TAG and previous_tag == HEADING_TAG:\n",
    "            sentences[-1] += ' ' + node_text\n",
    "        else:\n",
    "            # a paragraph in drawings descriptions is treated as a sentence\n",
    "            if child.tag == PARAGRAPH_TAG:\n",
    "                node_text = apply_sentence_end(node_text)\n",
    "            sentences.append(node_text)\n",
    "        previous_tag = child.tag\n",
    "    \n",
    "    return ' '.join(sentences)\n",
    "\n",
    "def apply_sentence_end(text):\n",
    "    if text and text.strip():\n",
    "        text = text.strip().strip(';.')\n",
    "        text += '. '\n",
    "    return text\n",
    "\n",
    "def itertext_custom(self):\n",
    "    tag = self.tag\n",
    "    if not isinstance(tag, basestring) and tag is not None:\n",
    "        return\n",
    "    if self.text:\n",
    "        if tag == LI_TAG:\n",
    "            yield apply_sentence_end(self.text)\n",
    "        else:\n",
    "            yield self.text.replace('\\n',' ')\n",
    "    for e in self:\n",
    "        for s in e.itertext_custom():\n",
    "            yield s\n",
    "        if e.tail:\n",
    "            yield e.tail\n",
    "\n",
    "ET.Element.itertext_custom = itertext_custom\n",
    "# def get_node_text(node):\n",
    "#     node_text = ''\n",
    "#     for child in node:\n",
    "#         # for ul tags, get li tags as sentences\n",
    "#         if child.tag == UL_TAG:\n",
    "#             li_sentences = [apply_sentence_end(get_node_text_iterative(c)) for c in child]\n",
    "#             child_text = ' '.join(li_sentences)\n",
    "#         else:\n",
    "#             child_text = get_node_text_iterative(child)\n",
    "#         node_text += child_text\n",
    "#     return node_text\n",
    "        \n",
    "get_node_text = lambda node: ''.join(node.itertext_custom()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conc_paragraphs(parag1, parag2):\n",
    "    return parag1.strip('.') + '.' + ' ' + parag2\n",
    "\n",
    "def concatenate_sentences_to_paragraphs(paragraphs):\n",
    "    \"\"\"\n",
    "    for 1 sentence paragraphs, concatenate them to the next or previous paragraph depending on context\n",
    "    \"\"\"\n",
    "    for i in range(len(paragraphs)):\n",
    "        if i >= len((paragraphs)): break\n",
    "        parag = paragraphs[i]\n",
    "        sentences = get_sentences(parag)\n",
    "        \n",
    "        if len(sentences) == 1:\n",
    "            prev_paragraph = paragraphs[i-1] if i-1 >= 0 else None\n",
    "            next_paragraph = paragraphs[i+1] if i+1 < len(paragraphs) else None\n",
    "\n",
    "            if (next_paragraph and len(get_sentences(next_paragraph)) == 1):\n",
    "                # If a series of 1 sentence length paragraphs exist, conc all of them in one paragraph\n",
    "                while True:\n",
    "                    if next_paragraph and len(get_sentences(next_paragraph)) == 1:\n",
    "                        parag = conc_paragraphs(parag, next_paragraph)\n",
    "                        paragraphs[i] = parag\n",
    "                        del paragraphs[i+1]\n",
    "\n",
    "                        # reinitialize for loop\n",
    "                        next_paragraph = paragraphs[i+1] if i+1 < len(paragraphs) else None\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "            # otherwise, just concatenate the 1 sentence paragraph to the previous paragraph\n",
    "            elif prev_paragraph:\n",
    "#                 print '============== Found prev eligible paragraph'\n",
    "                prev_paragraph = conc_paragraphs(prev_paragraph, parag)\n",
    "                paragraphs[i-1] = prev_paragraph\n",
    "                del paragraphs[i]\n",
    "\n",
    "            # if this is the first paragraph, then just concatenate it with the next one\n",
    "            elif next_paragraph:\n",
    "                parag = conc_paragraphs(parag, next_paragraph)\n",
    "                paragraphs[i] = parag\n",
    "                del paragraphs[i+1]\n",
    "\n",
    "def get_adjusted_paragraphs(root, conc_sentences=True):\n",
    "    paragraphs = get_paragraphs(root)\n",
    "    if conc_sentences:\n",
    "        concatenate_sentences_to_paragraphs(paragraphs)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_patent(doc_id):\n",
    "    url_to_fetch = ES_URL.format(doc_id)\n",
    "\n",
    "    response = urllib2.urlopen(url_to_fetch)\n",
    "    patent_content = response.read()\n",
    "\n",
    "    patent_object = json.loads(patent_content)['_source']\n",
    "    return patent_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Actual Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ABSTRACT_ID = \"{}_abstract\"\n",
    "DESC_ID = \"{}_description\"\n",
    "CLAIMS_ID = \"{}_claims\"\n",
    "\n",
    "ABSTRACT_PART_ID = \"{}_abstract_part-{}\"\n",
    "DESC_PART_ID = \"{}_description_part-{}\"\n",
    "CLAIMS_PART_ID = \"{}_claims_part-{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000\n",
    "\n",
    "preprocessed_location = \"/home/local/shalaby/\" + \"preprocessed_data/extended_pv_abs_desc_claims_all_levels_3/\"\n",
    "TRAINING_PREPROCESSED_FILES_PREFIX = preprocessed_location + \"extended_pv_training_docs_data_preprocessed-\"\n",
    "VALIDATION_PREPROCESSED_FILES_PREFIX = preprocessed_location + \"extended_pv_validation_docs_data_preprocessed-\"\n",
    "TEST_PREPROCESSED_FILES_PREFIX = preprocessed_location + \"extended_pv_test_docs_data_preprocessed-\"\n",
    "\n",
    "if not os.path.exists(preprocessed_location):\n",
    "    os.makedirs(preprocessed_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_ABSTRACT_PARTS = 3\n",
    "NUM_DESC_PARTS = 23\n",
    "NUM_CLAIMS_PARTS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def multithreaded_extended_batch_creation(start_index):\n",
    "\n",
    "    if os.path.exists(FILE_PREFIX + str(start_index)):\n",
    "        info(\"Batch {} already exists, skipping..\".format(start_index))\n",
    "        return\n",
    "    \n",
    "    info(\"Batch creation working on {}\\n\".format(start_index))\n",
    "    token_lines = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    len_abs_sentences = 0\n",
    "    len_desc_sentences = 0\n",
    "    len_desc_paragraphs = 0\n",
    "    len_claims_sentences = 0\n",
    "    \n",
    "    len_abs_tokens = []\n",
    "    len_desc_tokens = []\n",
    "    len_claims_tokens = []\n",
    "\n",
    "    for doc_index, doc_id in enumerate(DOCS_LIST[start_index:]):\n",
    "        patent_doc = get_patent(doc_id)\n",
    "        \n",
    "        # Abstract\n",
    "        abstract = patent_doc['abstract'][0]\n",
    "        root = ET.fromstring(abstract.encode('utf-8'))\n",
    "        abs_paragraphs = get_adjusted_paragraphs(root)\n",
    "        \n",
    "        # Description\n",
    "        desc = patent_doc['description'][0]\n",
    "        root = ET.fromstring(desc.encode('utf-8'))\n",
    "        desc_paragraphs = get_adjusted_paragraphs(root)\n",
    "        \n",
    "        # Claims\n",
    "        claims = patent_doc['claims'][0]\n",
    "        root = ET.fromstring(claims.encode('utf-8'))\n",
    "        claims_paragraphs = get_adjusted_paragraphs(root, conc_sentences=False)\n",
    "#         claims_paragraphs = []\n",
    "#         for claim in patent_doc['claims']:\n",
    "#             claims_paragraphs.append(claim.strip())\n",
    "\n",
    "#         abstract_sentences = sum([get_sentences(abs_parag) for abs_parag in abs_paragraphs], [])\n",
    "#         desc_sentences = sum([get_sentences(desc_parag) for desc_parag in desc_paragraphs], [])\n",
    "#         claims_sentences = sum([get_sentences(claim_parag) for claim_parag in claims_paragraphs], [])\n",
    "        \n",
    "\n",
    "        abstract_tokens = sum([sentence_wordtokenizer(parag) for parag in abs_paragraphs], [])\n",
    "        desc_tokens = sum([sentence_wordtokenizer(parag) for parag in desc_paragraphs], [])\n",
    "        claims_tokens = sum([sentence_wordtokenizer(parag) for parag in claims_paragraphs], [])\n",
    "        \n",
    "        \n",
    "        \n",
    "        # lists of list of tokens\n",
    "        doc_tokens_list = [doc_id]  + abstract_tokens + desc_tokens + claims_tokens\n",
    "        abstract_tokens_list = [ABSTRACT_ID.format(doc_id)] + abstract_tokens\n",
    "        description_tokens_list = [DESC_ID.format(doc_id)] + desc_tokens\n",
    "        claims_tokens_list = [CLAIMS_ID.format(doc_id)] + claims_tokens\n",
    "        \n",
    "        # now add the tokens lists that will be written to the file\n",
    "        token_lines.append(doc_tokens_list)\n",
    "        token_lines.append(abstract_tokens_list)\n",
    "        token_lines.append(description_tokens_list)\n",
    "        token_lines.append(claims_tokens_list)\n",
    "        \n",
    "        for i in range(NUM_ABSTRACT_PARTS):\n",
    "            start, end = get_doc_range(i, len(abstract_tokens), NUM_ABSTRACT_PARTS)\n",
    "            token_lines.append([ABSTRACT_PART_ID.format(doc_id, i+1)] + abstract_tokens[start: end])\n",
    "        \n",
    "        for i in range(NUM_DESC_PARTS):\n",
    "            start, end = get_doc_range(i, len(desc_tokens), NUM_DESC_PARTS)\n",
    "            token_lines.append([DESC_PART_ID.format(doc_id, i+1)] + desc_tokens[start: end])    \n",
    "        \n",
    "        for i in range(NUM_CLAIMS_PARTS):\n",
    "            start, end = get_doc_range(i, len(claims_tokens), NUM_CLAIMS_PARTS)\n",
    "            token_lines.append([CLAIMS_PART_ID.format(doc_id, i+1)] + claims_tokens[start: end])\n",
    "            \n",
    "        if doc_index % 1000 == 0: info(\"Doc: {:6} -> Total Lines to write: {:8}\".format(start_index + doc_index, len(token_lines)))\n",
    "        if doc_index >= BATCH_SIZE - 1:\n",
    "            break\n",
    "    duration = time.time() - start_time\n",
    "    info(\"Finished batch {} of size {:d} in {:.0f}m {:.0f}s\".format(start_index, BATCH_SIZE, * divmod(duration, 60)))\n",
    "    info(\"For index {}, the actual number of lines written is: {}\".format(start_index, len(token_lines)))\n",
    "    \n",
    "    write_batch(FILE_PREFIX, token_lines, start_index)\n",
    "    del token_lines\n",
    "    \n",
    "def get_doc_range(i, number_of_tokens, number_of_parts):\n",
    "    start, end = 0,0\n",
    "    if number_of_tokens < number_of_parts:\n",
    "        if i==0:\n",
    "            return 0, None\n",
    "        else:\n",
    "            return number_of_tokens,None\n",
    "    if i == 0:\n",
    "        start = 0\n",
    "    else:\n",
    "        start = (number_of_tokens / number_of_parts) * i\n",
    "    if i+1 == number_of_parts:\n",
    "        end = None\n",
    "    else:\n",
    "        end = (number_of_tokens / number_of_parts) * (i+1)\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def write_batch(file_prefix, batch_lines, batch_start):\n",
    "    if len(batch_lines):\n",
    "        print \"writing batch %d\" % batch_start\n",
    "        with open(file_prefix + str(batch_start), 'w') as batch_file:\n",
    "            for line in batch_lines:\n",
    "                batch_file.write((u\" \".join(line) + \"\\n\").encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DOCS_LIST = training_docs_list\n",
    "FILE_PREFIX = TRAINING_PREPROCESSED_FILES_PREFIX\n",
    "SAMPLE_SIZE = len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batches = range(0, (divmod(SAMPLE_SIZE, BATCH_SIZE)[0]+1) * BATCH_SIZE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 10000,\n",
       " 20000,\n",
       " 30000,\n",
       " 40000,\n",
       " 50000,\n",
       " 60000,\n",
       " 70000,\n",
       " 80000,\n",
       " 90000,\n",
       " 100000,\n",
       " 110000,\n",
       " 120000]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 11:09:54,684 : INFO : Batch creation working on 30000\n",
      "\n",
      "2017-03-20 11:09:54,684 : INFO : Batch creation working on 40000\n",
      "\n",
      "2017-03-20 11:09:54,689 : INFO : Batch creation working on 20000\n",
      "\n",
      "2017-03-20 11:09:54,689 : INFO : Batch creation working on 10000\n",
      "\n",
      "2017-03-20 11:09:54,685 : INFO : Batch creation working on 50000\n",
      "\n",
      "2017-03-20 11:09:54,690 : INFO : Batch creation working on 100000\n",
      "\n",
      "2017-03-20 11:09:54,690 : INFO : Batch creation working on 90000\n",
      "\n",
      "2017-03-20 11:09:54,688 : INFO : Batch creation working on 70000\n",
      "\n",
      "2017-03-20 11:09:54,684 : INFO : Batch creation working on 0\n",
      "\n",
      "2017-03-20 11:09:54,694 : INFO : Batch creation working on 80000\n",
      "\n",
      "2017-03-20 11:09:54,695 : INFO : Batch creation working on 60000\n",
      "\n",
      "2017-03-20 11:09:54,691 : INFO : Batch creation working on 110000\n",
      "\n",
      "2017-03-20 11:09:54,891 : INFO : Doc:  60000 -> Total Lines to write:       34\n",
      "2017-03-20 11:09:54,934 : INFO : Doc:  50000 -> Total Lines to write:       34\n",
      "2017-03-20 11:09:54,967 : INFO : Doc:  10000 -> Total Lines to write:       34\n",
      "2017-03-20 11:09:55,016 : INFO : Doc:  70000 -> Total Lines to write:       34\n",
      "2017-03-20 11:09:55,036 : INFO : Doc: 100000 -> Total Lines to write:       34\n",
      "2017-03-20 11:09:55,038 : INFO : Doc:      0 -> Total Lines to write:       34\n",
      "2017-03-20 11:09:55,052 : INFO : Doc:  30000 -> Total Lines to write:       34\n",
      "2017-03-20 11:09:55,098 : INFO : Doc:  90000 -> Total Lines to write:       34\n",
      "2017-03-20 11:09:55,171 : INFO : Doc: 110000 -> Total Lines to write:       34\n",
      "2017-03-20 11:09:55,183 : INFO : Doc:  20000 -> Total Lines to write:       34\n",
      "2017-03-20 11:09:55,210 : INFO : Doc:  40000 -> Total Lines to write:       34\n",
      "2017-03-20 11:09:56,121 : INFO : Doc:  80000 -> Total Lines to write:       34\n",
      "2017-03-20 11:14:44,795 : INFO : Doc:   1000 -> Total Lines to write:    34034\n",
      "2017-03-20 11:14:58,477 : INFO : Doc:  81000 -> Total Lines to write:    34034\n",
      "2017-03-20 11:15:03,754 : INFO : Doc:  21000 -> Total Lines to write:    34034\n",
      "2017-03-20 11:15:06,370 : INFO : Doc:  91000 -> Total Lines to write:    34034\n",
      "2017-03-20 11:15:15,245 : INFO : Doc:  41000 -> Total Lines to write:    34034\n",
      "2017-03-20 11:15:17,203 : INFO : Doc:  51000 -> Total Lines to write:    34034\n",
      "2017-03-20 11:15:17,679 : INFO : Doc:  11000 -> Total Lines to write:    34034\n",
      "2017-03-20 11:15:17,888 : INFO : Doc:  71000 -> Total Lines to write:    34034\n",
      "2017-03-20 11:15:22,223 : INFO : Doc:  31000 -> Total Lines to write:    34034\n",
      "2017-03-20 11:15:30,145 : INFO : Doc: 111000 -> Total Lines to write:    34034\n",
      "2017-03-20 11:15:31,044 : INFO : Doc: 101000 -> Total Lines to write:    34034\n",
      "2017-03-20 11:15:35,441 : INFO : Doc:  61000 -> Total Lines to write:    34034\n",
      "2017-03-20 11:18:46,932 : INFO : Doc:   2000 -> Total Lines to write:    68034\n",
      "2017-03-20 11:19:42,019 : INFO : Doc:  22000 -> Total Lines to write:    68034\n",
      "2017-03-20 11:19:54,834 : INFO : Doc:  52000 -> Total Lines to write:    68034\n",
      "2017-03-20 11:19:57,458 : INFO : Doc:  72000 -> Total Lines to write:    68034\n",
      "2017-03-20 11:20:04,666 : INFO : Doc:  12000 -> Total Lines to write:    68034\n",
      "2017-03-20 11:20:08,204 : INFO : Doc:  82000 -> Total Lines to write:    68034\n",
      "2017-03-20 11:20:08,693 : INFO : Doc:  62000 -> Total Lines to write:    68034\n",
      "2017-03-20 11:20:12,674 : INFO : Doc:  32000 -> Total Lines to write:    68034\n",
      "2017-03-20 11:20:23,575 : INFO : Doc:  92000 -> Total Lines to write:    68034\n",
      "2017-03-20 11:20:39,783 : INFO : Doc: 102000 -> Total Lines to write:    68034\n",
      "2017-03-20 11:20:47,593 : INFO : Doc:  42000 -> Total Lines to write:    68034\n",
      "2017-03-20 11:20:54,383 : INFO : Doc: 112000 -> Total Lines to write:    68034\n",
      "2017-03-20 11:23:26,020 : INFO : Doc:   3000 -> Total Lines to write:   102034\n",
      "2017-03-20 11:24:31,899 : INFO : Doc:  23000 -> Total Lines to write:   102034\n",
      "2017-03-20 11:24:46,317 : INFO : Doc:  53000 -> Total Lines to write:   102034\n",
      "2017-03-20 11:24:57,590 : INFO : Doc:  83000 -> Total Lines to write:   102034\n",
      "2017-03-20 11:25:04,806 : INFO : Doc:  73000 -> Total Lines to write:   102034\n",
      "2017-03-20 11:25:09,035 : INFO : Doc:  13000 -> Total Lines to write:   102034\n",
      "2017-03-20 11:25:17,572 : INFO : Doc:  63000 -> Total Lines to write:   102034\n",
      "2017-03-20 11:25:26,155 : INFO : Doc:  93000 -> Total Lines to write:   102034\n",
      "2017-03-20 11:25:29,694 : INFO : Doc:  33000 -> Total Lines to write:   102034\n",
      "2017-03-20 11:25:49,211 : INFO : Doc:  43000 -> Total Lines to write:   102034\n",
      "2017-03-20 11:26:16,535 : INFO : Doc: 103000 -> Total Lines to write:   102034\n",
      "2017-03-20 11:26:35,879 : INFO : Doc: 113000 -> Total Lines to write:   102034\n",
      "2017-03-20 11:28:10,085 : INFO : Doc:   4000 -> Total Lines to write:   136034\n",
      "2017-03-20 11:29:44,256 : INFO : Doc:  24000 -> Total Lines to write:   136034\n",
      "2017-03-20 11:30:05,436 : INFO : Doc:  14000 -> Total Lines to write:   136034\n",
      "2017-03-20 11:30:07,887 : INFO : Doc:  74000 -> Total Lines to write:   136034\n",
      "2017-03-20 11:30:33,299 : INFO : Doc:  54000 -> Total Lines to write:   136034\n",
      "2017-03-20 11:30:36,720 : INFO : Doc:  84000 -> Total Lines to write:   136034\n",
      "2017-03-20 11:30:40,302 : INFO : Doc:  64000 -> Total Lines to write:   136034\n",
      "2017-03-20 11:30:58,525 : INFO : Doc:  34000 -> Total Lines to write:   136034\n",
      "2017-03-20 11:31:10,193 : INFO : Doc:  94000 -> Total Lines to write:   136034\n",
      "2017-03-20 11:31:14,840 : INFO : Doc:  44000 -> Total Lines to write:   136034\n",
      "2017-03-20 11:31:58,339 : INFO : Doc: 104000 -> Total Lines to write:   136034\n",
      "2017-03-20 11:32:15,402 : INFO : Doc: 114000 -> Total Lines to write:   136034\n",
      "2017-03-20 11:33:08,900 : INFO : Doc:   5000 -> Total Lines to write:   170034\n",
      "2017-03-20 11:34:34,561 : INFO : Doc:  25000 -> Total Lines to write:   170034\n",
      "2017-03-20 11:34:43,031 : INFO : Doc:  15000 -> Total Lines to write:   170034\n",
      "2017-03-20 11:35:41,513 : INFO : Doc:  75000 -> Total Lines to write:   170034\n",
      "2017-03-20 11:35:48,584 : INFO : Doc:  55000 -> Total Lines to write:   170034\n",
      "2017-03-20 11:35:50,486 : INFO : Doc:  85000 -> Total Lines to write:   170034\n",
      "2017-03-20 11:36:15,794 : INFO : Doc:  65000 -> Total Lines to write:   170034\n",
      "2017-03-20 11:36:19,252 : INFO : Doc:  35000 -> Total Lines to write:   170034\n",
      "2017-03-20 11:36:50,987 : INFO : Doc:  45000 -> Total Lines to write:   170034\n",
      "2017-03-20 11:36:50,991 : INFO : Doc:  95000 -> Total Lines to write:   170034\n",
      "2017-03-20 11:37:38,702 : INFO : Doc: 105000 -> Total Lines to write:   170034\n",
      "2017-03-20 11:37:46,136 : INFO : Doc: 115000 -> Total Lines to write:   170034\n",
      "2017-03-20 11:39:09,171 : INFO : Doc:   6000 -> Total Lines to write:   204034\n",
      "2017-03-20 11:39:46,707 : INFO : Doc:  26000 -> Total Lines to write:   204034\n",
      "2017-03-20 11:40:10,626 : INFO : Doc:  16000 -> Total Lines to write:   204034\n",
      "2017-03-20 11:40:58,518 : INFO : Doc:  76000 -> Total Lines to write:   204034\n",
      "2017-03-20 11:41:12,249 : INFO : Doc:  86000 -> Total Lines to write:   204034\n",
      "2017-03-20 11:41:15,676 : INFO : Doc:  56000 -> Total Lines to write:   204034\n",
      "2017-03-20 11:41:28,442 : INFO : Doc:  66000 -> Total Lines to write:   204034\n",
      "2017-03-20 11:42:28,642 : INFO : Doc:  36000 -> Total Lines to write:   204034\n",
      "2017-03-20 11:42:30,271 : INFO : Doc:  96000 -> Total Lines to write:   204034\n",
      "2017-03-20 11:42:30,955 : INFO : Doc:  46000 -> Total Lines to write:   204034\n",
      "2017-03-20 11:42:59,629 : INFO : Doc: 106000 -> Total Lines to write:   204034\n",
      "2017-03-20 11:43:44,968 : INFO : Doc: 116000 -> Total Lines to write:   204034\n",
      "2017-03-20 11:44:28,848 : INFO : Doc:   7000 -> Total Lines to write:   238034\n",
      "2017-03-20 11:44:54,008 : INFO : Doc:  27000 -> Total Lines to write:   238034\n",
      "2017-03-20 11:45:33,700 : INFO : Doc:  17000 -> Total Lines to write:   238034\n",
      "2017-03-20 11:46:08,135 : INFO : Doc:  77000 -> Total Lines to write:   238034\n",
      "2017-03-20 11:46:42,089 : INFO : Doc:  67000 -> Total Lines to write:   238034\n",
      "2017-03-20 11:46:44,122 : INFO : Doc:  87000 -> Total Lines to write:   238034\n",
      "2017-03-20 11:46:52,604 : INFO : Doc:  57000 -> Total Lines to write:   238034\n",
      "2017-03-20 11:48:08,108 : INFO : Doc:  97000 -> Total Lines to write:   238034\n",
      "2017-03-20 11:48:08,885 : INFO : Doc:  37000 -> Total Lines to write:   238034\n",
      "2017-03-20 11:48:27,310 : INFO : Doc:  47000 -> Total Lines to write:   238034\n",
      "2017-03-20 11:48:41,676 : INFO : Doc: 107000 -> Total Lines to write:   238034\n",
      "2017-03-20 11:49:28,119 : INFO : Doc:   8000 -> Total Lines to write:   272034\n",
      "2017-03-20 11:49:57,049 : INFO : Doc:  28000 -> Total Lines to write:   272034\n",
      "2017-03-20 11:50:35,792 : INFO : Doc: 117000 -> Total Lines to write:   238034\n",
      "2017-03-20 11:51:04,216 : INFO : Doc:  18000 -> Total Lines to write:   272034\n",
      "2017-03-20 11:51:55,539 : INFO : Doc:  78000 -> Total Lines to write:   272034\n",
      "2017-03-20 11:52:08,098 : INFO : Doc:  88000 -> Total Lines to write:   272034\n",
      "2017-03-20 11:52:19,220 : INFO : Doc:  58000 -> Total Lines to write:   272034\n",
      "2017-03-20 11:52:47,823 : INFO : Doc:  68000 -> Total Lines to write:   272034\n",
      "2017-03-20 11:53:35,223 : INFO : Doc:  38000 -> Total Lines to write:   272034\n",
      "2017-03-20 11:53:58,119 : INFO : Doc:  48000 -> Total Lines to write:   272034\n",
      "2017-03-20 11:53:58,540 : INFO : Doc:  98000 -> Total Lines to write:   272034\n",
      "2017-03-20 11:54:28,880 : INFO : Doc: 108000 -> Total Lines to write:   272034\n",
      "2017-03-20 11:54:54,073 : INFO : Doc:  29000 -> Total Lines to write:   306034\n",
      "2017-03-20 11:55:02,649 : INFO : Doc:   9000 -> Total Lines to write:   306034\n",
      "2017-03-20 11:56:06,187 : INFO : Doc:  19000 -> Total Lines to write:   306034\n",
      "2017-03-20 11:56:43,352 : INFO : Doc: 118000 -> Total Lines to write:   272034\n",
      "2017-03-20 11:57:32,131 : INFO : Doc:  79000 -> Total Lines to write:   306034\n",
      "2017-03-20 11:57:52,705 : INFO : Doc:  69000 -> Total Lines to write:   306034\n",
      "2017-03-20 11:57:54,142 : INFO : Doc:  59000 -> Total Lines to write:   306034\n",
      "2017-03-20 11:57:56,469 : INFO : Doc:  89000 -> Total Lines to write:   306034\n",
      "2017-03-20 11:58:56,205 : INFO : Doc:  39000 -> Total Lines to write:   306034\n",
      "2017-03-20 11:59:28,459 : INFO : Doc:  99000 -> Total Lines to write:   306034\n",
      "2017-03-20 11:59:49,473 : INFO : Doc:  49000 -> Total Lines to write:   306034\n",
      "2017-03-20 11:59:58,510 : INFO : Doc: 109000 -> Total Lines to write:   306034\n",
      "2017-03-20 12:00:25,496 : INFO : Finished batch 0 of size 10000 in 50m 31s\n",
      "2017-03-20 12:00:25,499 : INFO : For index 0, the actual number of lines written is: 340000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 12:00:33,894 : INFO : Finished batch 20000 of size 10000 in 50m 39s\n",
      "2017-03-20 12:00:33,914 : INFO : For index 20000, the actual number of lines written is: 340000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 12:01:04,141 : INFO : Batch creation working on 120000\n",
      "\n",
      "2017-03-20 12:01:04,594 : INFO : Doc: 120000 -> Total Lines to write:       34\n",
      "2017-03-20 12:01:15,751 : INFO : Finished batch 10000 of size 10000 in 51m 21s\n",
      "2017-03-20 12:01:15,777 : INFO : For index 10000, the actual number of lines written is: 340000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 12:01:43,376 : INFO : Doc: 119000 -> Total Lines to write:   306034\n",
      "2017-03-20 12:01:57,076 : INFO : Finished batch 120000 of size 10000 in 0m 53s\n",
      "2017-03-20 12:01:57,078 : INFO : For index 120000, the actual number of lines written is: 5304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 120000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 12:02:32,720 : INFO : Finished batch 70000 of size 10000 in 52m 38s\n",
      "2017-03-20 12:02:32,734 : INFO : For index 70000, the actual number of lines written is: 340000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 12:03:10,067 : INFO : Finished batch 60000 of size 10000 in 53m 15s\n",
      "2017-03-20 12:03:10,073 : INFO : For index 60000, the actual number of lines written is: 340000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 12:03:16,333 : INFO : Finished batch 50000 of size 10000 in 53m 22s\n",
      "2017-03-20 12:03:16,337 : INFO : For index 50000, the actual number of lines written is: 340000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 12:03:43,296 : INFO : Finished batch 80000 of size 10000 in 53m 49s\n",
      "2017-03-20 12:03:43,299 : INFO : For index 80000, the actual number of lines written is: 340000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 80000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 12:03:52,604 : INFO : Finished batch 30000 of size 10000 in 53m 58s\n",
      "2017-03-20 12:03:52,608 : INFO : For index 30000, the actual number of lines written is: 340000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 12:04:13,342 : INFO : Finished batch 40000 of size 10000 in 54m 19s\n",
      "2017-03-20 12:04:13,345 : INFO : For index 40000, the actual number of lines written is: 340000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 12:04:25,185 : INFO : Finished batch 90000 of size 10000 in 54m 30s\n",
      "2017-03-20 12:04:25,188 : INFO : For index 90000, the actual number of lines written is: 340000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 12:04:37,227 : INFO : Finished batch 100000 of size 10000 in 54m 43s\n",
      "2017-03-20 12:04:37,230 : INFO : For index 100000, the actual number of lines written is: 340000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 12:04:55,472 : INFO : Finished batch 110000 of size 10000 in 55m 1s\n",
      "2017-03-20 12:04:55,475 : INFO : For index 110000, the actual number of lines written is: 340000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 110000\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pool = ThreadPool(12)\n",
    "    # +1 since range is end-exclusive\n",
    "    batches = range(0, (divmod(SAMPLE_SIZE, BATCH_SIZE)[0]+1) * BATCH_SIZE, BATCH_SIZE )\n",
    "    indices = pool.map(multithreaded_extended_batch_creation, batches)\n",
    "    pool.close()\n",
    "    pool.terminate()\n",
    "finally:\n",
    "    pool.close()\n",
    "    pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-16 10:44:02,083 : INFO : Batch creation working on 0\n",
      "\n",
      "2017-03-16 10:44:02,175 : INFO : Doc:      0 -> Total Lines to write:       78\n",
      "2017-03-16 10:44:18,155 : INFO : Finished batch 0 of size 100 in 0m 16s\n",
      "2017-03-16 10:44:18,160 : INFO : For index 0, the actual number of lines written is: 7800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 0\n"
     ]
    }
   ],
   "source": [
    "multithreaded_extended_batch_creation(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DOCS_LIST = validation_docs_list\n",
    "FILE_PREFIX = VALIDATION_PREPROCESSED_FILES_PREFIX\n",
    "SAMPLE_SIZE = len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 12:05:47,774 : INFO : Batch creation working on 0\n",
      "\n",
      "2017-03-20 12:05:47,775 : INFO : Batch creation working on 20000\n",
      "\n",
      "2017-03-20 12:05:47,775 : INFO : Batch creation working on 10000\n",
      "\n",
      "2017-03-20 12:05:47,893 : INFO : Doc:      0 -> Total Lines to write:       34\n",
      "2017-03-20 12:05:47,929 : INFO : Doc:  20000 -> Total Lines to write:       34\n",
      "2017-03-20 12:05:48,042 : INFO : Doc:  10000 -> Total Lines to write:       34\n",
      "2017-03-20 12:09:09,861 : INFO : Doc:   1000 -> Total Lines to write:    34034\n",
      "2017-03-20 12:09:19,943 : INFO : Doc:  21000 -> Total Lines to write:    34034\n",
      "2017-03-20 12:09:20,645 : INFO : Doc:  11000 -> Total Lines to write:    34034\n",
      "2017-03-20 12:12:16,121 : INFO : Doc:   2000 -> Total Lines to write:    68034\n",
      "2017-03-20 12:12:48,471 : INFO : Doc:  22000 -> Total Lines to write:    68034\n",
      "2017-03-20 12:13:07,367 : INFO : Doc:  12000 -> Total Lines to write:    68034\n",
      "2017-03-20 12:15:29,959 : INFO : Doc:   3000 -> Total Lines to write:   102034\n",
      "2017-03-20 12:16:20,220 : INFO : Doc:  23000 -> Total Lines to write:   102034\n",
      "2017-03-20 12:16:24,201 : INFO : Doc:  13000 -> Total Lines to write:   102034\n",
      "2017-03-20 12:18:36,254 : INFO : Doc:   4000 -> Total Lines to write:   136034\n",
      "2017-03-20 12:19:43,872 : INFO : Doc:  14000 -> Total Lines to write:   136034\n",
      "2017-03-20 12:20:01,085 : INFO : Doc:  24000 -> Total Lines to write:   136034\n",
      "2017-03-20 12:21:52,904 : INFO : Doc:   5000 -> Total Lines to write:   170034\n",
      "2017-03-20 12:23:09,294 : INFO : Doc:  15000 -> Total Lines to write:   170034\n",
      "2017-03-20 12:23:41,072 : INFO : Doc:  25000 -> Total Lines to write:   170034\n",
      "2017-03-20 12:24:57,967 : INFO : Doc:   6000 -> Total Lines to write:   204034\n",
      "2017-03-20 12:26:30,184 : INFO : Doc:  16000 -> Total Lines to write:   204034\n",
      "2017-03-20 12:27:13,686 : INFO : Doc:  26000 -> Total Lines to write:   204034\n",
      "2017-03-20 12:28:13,793 : INFO : Doc:   7000 -> Total Lines to write:   238034\n",
      "2017-03-20 12:29:54,016 : INFO : Doc:  17000 -> Total Lines to write:   238034\n",
      "2017-03-20 12:31:09,550 : INFO : Doc:  27000 -> Total Lines to write:   238034\n",
      "2017-03-20 12:31:40,466 : INFO : Doc:   8000 -> Total Lines to write:   272034\n",
      "2017-03-20 12:33:45,410 : INFO : Doc:  18000 -> Total Lines to write:   272034\n",
      "2017-03-20 12:34:52,911 : INFO : Doc:  28000 -> Total Lines to write:   272034\n",
      "2017-03-20 12:35:02,205 : INFO : Doc:   9000 -> Total Lines to write:   306034\n",
      "2017-03-20 12:37:12,911 : INFO : Doc:  19000 -> Total Lines to write:   306034\n",
      "2017-03-20 12:38:35,236 : INFO : Finished batch 0 of size 10000 in 32m 47s\n",
      "2017-03-20 12:38:35,238 : INFO : For index 0, the actual number of lines written is: 340000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 12:38:35,937 : INFO : Doc:  29000 -> Total Lines to write:   306034\n",
      "2017-03-20 12:40:52,729 : INFO : Finished batch 10000 of size 10000 in 35m 5s\n",
      "2017-03-20 12:40:52,734 : INFO : For index 10000, the actual number of lines written is: 340000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 12:40:56,764 : INFO : Finished batch 20000 of size 10000 in 35m 9s\n",
      "2017-03-20 12:40:56,766 : INFO : For index 20000, the actual number of lines written is: 328950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 20000\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pool = ThreadPool(11)\n",
    "    # +1 since range is end-exclusive\n",
    "    batches = range(0, (divmod(SAMPLE_SIZE, BATCH_SIZE)[0]+1) * BATCH_SIZE, BATCH_SIZE )\n",
    "    indices = pool.map(multithreaded_extended_batch_creation, batches)\n",
    "    pool.close()\n",
    "    pool.terminate()\n",
    "finally:\n",
    "    pool.close()\n",
    "    pool.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DOCS_LIST = test_docs_list\n",
    "FILE_PREFIX = TEST_PREPROCESSED_FILES_PREFIX\n",
    "SAMPLE_SIZE = len(test_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-08 04:30:26,423 : INFO : Batch creation working on 0\n",
      "\n",
      "2017-03-08 04:30:26,423 : INFO : Batch creation working on 20000\n",
      "\n",
      "2017-03-08 04:30:26,423 : INFO : Batch creation working on 10000\n",
      "\n",
      "2017-03-08 04:30:26,423 : INFO : Batch creation working on 30000\n",
      "\n",
      "2017-03-08 04:30:48,394 : INFO : Doc:  10000 -> Total Lines to write:      105\n",
      "2017-03-08 04:30:48,491 : INFO : Doc:      0 -> Total Lines to write:       86\n",
      "2017-03-08 04:30:48,639 : INFO : Doc:  20000 -> Total Lines to write:      188\n",
      "2017-03-08 04:30:53,858 : INFO : Doc:  30000 -> Total Lines to write:      514\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pool = ThreadPool(16)\n",
    "    # +1 since range is end-exclusive\n",
    "    batches = range(0, (divmod(SAMPLE_SIZE, BATCH_SIZE)[0]+1) * BATCH_SIZE, BATCH_SIZE )\n",
    "    indices = pool.map(multithreaded_extended_batch_creation, batches)\n",
    "    pool.close()\n",
    "    pool.terminate()\n",
    "finally:\n",
    "    pool.close()\n",
    "    pool.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multithreaded_extended_batch_creation(start_index):\n",
    "\n",
    "#     if os.path.exists(FILE_PREFIX + str(start_index)):\n",
    "#         info(\"Batch {} already exists, skipping..\".format(start_index))\n",
    "#         return\n",
    "    \n",
    "    info(\"Batch creation working on {}\\n\".format(start_index))\n",
    "    token_lines = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    len_abs_sentences = 0\n",
    "    len_desc_sentences = 0\n",
    "    len_desc_paragraphs = 0\n",
    "    len_claims_sentences = 0\n",
    "    \n",
    "    len_abs_tokens = []\n",
    "    len_desc_tokens = []\n",
    "    len_claims_tokens = []\n",
    "    \n",
    "    \n",
    "    len_desc_parag_tokens = []\n",
    "\n",
    "    len_claims_paragraphs = []\n",
    "    \n",
    "    for doc_index, doc_id in enumerate(DOCS_LIST[start_index:]):\n",
    "        patent_doc = get_patent(doc_id)\n",
    "        \n",
    "        # Abstract\n",
    "        abstract = patent_doc['abstract'][0]\n",
    "        root = ET.fromstring(abstract.encode('utf-8'))\n",
    "        abs_paragraphs = get_adjusted_paragraphs(root)\n",
    "        \n",
    "        # Description\n",
    "        desc = patent_doc['description'][0]\n",
    "        root = ET.fromstring(desc.encode('utf-8'))\n",
    "        desc_paragraphs = get_adjusted_paragraphs(root)\n",
    "        \n",
    "        # Claims\n",
    "        claims = patent_doc['claims'][0]\n",
    "        root = ET.fromstring(claims.encode('utf-8'))\n",
    "        claims_paragraphs = get_adjusted_paragraphs(root, conc_sentences=False)\n",
    "#         claims_paragraphs = []\n",
    "#         for claim in patent_doc['claims']:\n",
    "#             claims_paragraphs.append(claim.strip())\n",
    "\n",
    "        len_claims_paragraphs.append(len(claims_paragraphs))\n",
    "    \n",
    "        abstract_sentences = sum([get_sentences(abs_parag) for abs_parag in abs_paragraphs], [])\n",
    "        desc_sentences = sum([get_sentences(desc_parag) for desc_parag in desc_paragraphs], [])\n",
    "        claims_sentences = sum([get_sentences(claim_parag) for claim_parag in claims_paragraphs], [])\n",
    "        \n",
    "        len_abs_sentences += len(abstract_sentences)\n",
    "        len_desc_sentences += len(desc_sentences)\n",
    "        len_claims_sentences += len(claims_sentences)\n",
    "        \n",
    "        len_desc_paragraphs += len(desc_paragraphs)\n",
    "        \n",
    "\n",
    "        abstract_tokens = sum([sentence_wordtokenizer(parag) for parag in abs_paragraphs], [])\n",
    "        \n",
    "        len_desc_parag_tokens.extend([len(sentence_wordtokenizer(parag)) for parag in desc_paragraphs])\n",
    "        desc_tokens = sum([sentence_wordtokenizer(parag) for parag in desc_paragraphs], [])\n",
    "        \n",
    "        claims_tokens = sum([sentence_wordtokenizer(parag) for parag in claims_paragraphs], [])\n",
    "        \n",
    "        \n",
    "        len_abs_tokens.append(len(abstract_tokens))\n",
    "        len_desc_tokens.append(len(desc_tokens))\n",
    "        len_claims_tokens.append(len(claims_tokens))\n",
    "        \n",
    "        # lists of list of tokens\n",
    "        doc_tokens_list = [doc_id]  + abstract_tokens + desc_tokens + claims_tokens\n",
    "        abstract_tokens_list = [ABSTRACT_ID.format(doc_id)] + abstract_tokens\n",
    "        description_tokens_list = [DESC_ID.format(doc_id)] + desc_tokens\n",
    "        claims_tokens_list = [CLAIMS_ID.format(doc_id)] + claims_tokens\n",
    "        \n",
    "        # now add the tokens lists that will be written to the file\n",
    "        token_lines.append(doc_tokens_list)\n",
    "        token_lines.append(abstract_tokens_list)\n",
    "        token_lines.append(description_tokens_list)\n",
    "        token_lines.append(claims_tokens_list)\n",
    "        \n",
    "        if doc_index % 1000 == 0: info(\"Doc: {:6} -> Total Lines to write: {:8}\".format(start_index + doc_index, len(token_lines)))\n",
    "        if doc_index >= BATCH_SIZE - 1:\n",
    "            break\n",
    "    duration = time.time() - start_time\n",
    "    info(\"Finished batch {} of size {:d} in {:.0f}m {:.0f}s\".format(start_index, BATCH_SIZE, * divmod(duration, 60)))\n",
    "    info(\"For index {}, the actual number of lines written is: {}\".format(start_index, len(token_lines)))\n",
    "    \n",
    "    print \"Average Abstract Sentences: {}\".format(len_abs_sentences/doc_index)\n",
    "    print \"Average Desc Sentences: {}\".format(len_desc_sentences/doc_index)\n",
    "    print \"Average Desc Paragraphs: {}\".format(len_desc_paragraphs/doc_index)\n",
    "    print \"Average Claims Sentences: {}\".format(len_claims_sentences/doc_index)\n",
    "    \n",
    "    \n",
    "    print \"Abstract Tokens: Mean: {} - Median: {}\".format(np.mean(len_abs_tokens), np.median(len_abs_tokens))\n",
    "    print \"Description Tokens: Mean: {} - Median: {}\".format(np.mean(len_desc_tokens), np.median(len_desc_tokens))\n",
    "    print \"Claims Tokens: Mean: {} - Median: {}\".format(np.mean(len_claims_tokens), np.median(len_claims_tokens))\n",
    "    print \"Description Paragraphs Tokens: Mean: {} - Median: {}\".format(np.mean(len_desc_parag_tokens), \n",
    "                                                                        np.median(len_desc_parag_tokens))\n",
    "    \n",
    "    print \"Claims Paragraphs: Mean: {} - Median: {}\".format(np.mean(len_claims_paragraphs), \n",
    "                                                                        np.median(len_claims_paragraphs))\n",
    "#     write_batch(FILE_PREFIX, token_lines, start_index)\n",
    "    del token_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOCS_LIST = training_docs_list\n",
    "FILE_PREFIX = TRAINING_PREPROCESSED_FILES_PREFIX\n",
    "SAMPLE_SIZE = len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-16 01:00:43,555 : INFO : Batch creation working on 0\n",
      "\n",
      "2017-03-16 01:00:43,555 : INFO : Batch creation working on 10000\n",
      "\n",
      "2017-03-16 01:00:43,558 : INFO : Batch creation working on 40000\n",
      "\n",
      "2017-03-16 01:00:43,555 : INFO : Batch creation working on 20000\n",
      "\n",
      "2017-03-16 01:00:43,558 : INFO : Batch creation working on 50000\n",
      "\n",
      "2017-03-16 01:00:43,559 : INFO : Batch creation working on 30000\n",
      "\n",
      "2017-03-16 01:00:43,591 : INFO : Batch creation working on 60000\n",
      "\n",
      "2017-03-16 01:00:43,601 : INFO : Batch creation working on 70000\n",
      "\n",
      "2017-03-16 01:00:43,690 : INFO : Doc:  50000 -> Total Lines to write:        4\n",
      "2017-03-16 01:00:43,742 : INFO : Doc:  60000 -> Total Lines to write:        4\n",
      "2017-03-16 01:00:43,767 : INFO : Doc:      0 -> Total Lines to write:        4\n",
      "2017-03-16 01:00:43,806 : INFO : Doc:  40000 -> Total Lines to write:        4\n",
      "2017-03-16 01:00:43,846 : INFO : Doc:  20000 -> Total Lines to write:        4\n",
      "2017-03-16 01:00:43,904 : INFO : Doc:  10000 -> Total Lines to write:        4\n",
      "2017-03-16 01:00:43,900 : INFO : Doc:  30000 -> Total Lines to write:        4\n",
      "2017-03-16 01:00:43,927 : INFO : Doc:  70000 -> Total Lines to write:        4\n",
      "2017-03-16 01:05:30,660 : INFO : Finished batch 0 of size 1000 in 4m 47s\n",
      "2017-03-16 01:05:30,663 : INFO : For index 0, the actual number of lines written is: 4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Abstract Sentences: 3\n",
      "Average Desc Sentences: 236\n",
      "Average Desc Paragraphs: 56\n",
      "Average Claims Sentences: 19\n",
      "Abstract Tokens: Mean: 118.576 - Median: 114.0\n",
      "Description Tokens: Mean: 7694.354 - Median: 5256.5\n",
      "Claims Tokens: Mean: 1101.352 - Median: 828.0\n",
      "Description Paragraphs Tokens: Mean: 137.372194747 - Median: 113.0\n",
      "Claims Paragraphs: Mean: 19.411 - Median: 16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-16 01:05:31,749 : INFO : Batch creation working on 80000\n",
      "\n",
      "2017-03-16 01:05:33,147 : INFO : Doc:  80000 -> Total Lines to write:        4\n",
      "2017-03-16 01:05:49,041 : INFO : Finished batch 40000 of size 1000 in 5m 5s\n",
      "2017-03-16 01:05:49,060 : INFO : For index 40000, the actual number of lines written is: 4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Abstract Sentences: 3\n",
      "Average Desc Sentences: 261\n",
      "Average Desc Paragraphs: 61\n",
      "Average Claims Sentences: 17\n",
      "Abstract Tokens: Mean: 119.666 - Median: 119.0\n",
      "Description Tokens: Mean: 8411.443 - Median: 6107.0\n",
      "Claims Tokens: Mean: 1111.596 - Median: 944.0\n",
      "Description Paragraphs Tokens: Mean: 137.076788944 - Median: 113.0\n",
      "Claims Paragraphs: Mean: 17.226 - Median: 16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-16 01:05:49,886 : INFO : Batch creation working on 90000\n",
      "\n",
      "2017-03-16 01:05:50,033 : INFO : Doc:  90000 -> Total Lines to write:        4\n",
      "2017-03-16 01:05:56,244 : INFO : Finished batch 50000 of size 1000 in 5m 13s\n",
      "2017-03-16 01:05:56,248 : INFO : For index 50000, the actual number of lines written is: 4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Abstract Sentences: 3\n",
      "Average Desc Sentences: 265\n",
      "Average Desc Paragraphs: 62\n",
      "Average Claims Sentences: 17\n",
      "Abstract Tokens: Mean: 116.916 - Median: 118.0\n",
      "Description Tokens: Mean: 8561.637 - Median: 5910.0\n",
      "Claims Tokens: Mean: 1082.717 - Median: 892.0\n",
      "Description Paragraphs Tokens: Mean: 137.32235713 - Median: 112.0\n",
      "Claims Paragraphs: Mean: 17.019 - Median: 15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-16 01:05:57,120 : INFO : Batch creation working on 100000\n",
      "\n",
      "2017-03-16 01:05:57,343 : INFO : Doc: 100000 -> Total Lines to write:        4\n",
      "2017-03-16 01:05:58,003 : INFO : Finished batch 20000 of size 1000 in 5m 14s\n",
      "2017-03-16 01:05:58,006 : INFO : For index 20000, the actual number of lines written is: 4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Abstract Sentences: 3\n",
      "Average Desc Sentences: 266\n",
      "Average Desc Paragraphs: 61\n",
      "Average Claims Sentences: 17\n",
      "Abstract Tokens: Mean: 118.616 - Median: 121.0\n",
      "Description Tokens: Mean: 8539.343 - Median: 5729.0\n",
      "Claims Tokens: Mean: 1096.825 - Median: 894.0\n",
      "Description Paragraphs Tokens: Mean: 138.749581607 - Median: 113.0\n",
      "Claims Paragraphs: Mean: 17.846 - Median: 15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-16 01:05:59,193 : INFO : Batch creation working on 110000\n",
      "\n",
      "2017-03-16 01:06:00,006 : INFO : Doc: 110000 -> Total Lines to write:        4\n",
      "2017-03-16 01:06:07,053 : INFO : Finished batch 30000 of size 1000 in 5m 23s\n",
      "2017-03-16 01:06:07,059 : INFO : For index 30000, the actual number of lines written is: 4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Abstract Sentences: 3\n",
      "Average Desc Sentences: 271\n",
      "Average Desc Paragraphs: 64\n",
      "Average Claims Sentences: 17\n",
      "Abstract Tokens: Mean: 118.392 - Median: 118.0\n",
      "Description Tokens: Mean: 8803.666 - Median: 6171.5\n",
      "Claims Tokens: Mean: 1099.404 - Median: 923.0\n",
      "Description Paragraphs Tokens: Mean: 137.510012183 - Median: 113.0\n",
      "Claims Paragraphs: Mean: 17.875 - Median: 16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-16 01:06:08,003 : INFO : Batch creation working on 120000\n",
      "\n",
      "2017-03-16 01:06:08,152 : INFO : Doc: 120000 -> Total Lines to write:        4\n",
      "2017-03-16 01:06:14,886 : INFO : Finished batch 10000 of size 1000 in 5m 31s\n",
      "2017-03-16 01:06:14,891 : INFO : For index 10000, the actual number of lines written is: 4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Abstract Sentences: 3\n",
      "Average Desc Sentences: 272\n",
      "Average Desc Paragraphs: 63\n",
      "Average Claims Sentences: 19\n",
      "Abstract Tokens: Mean: 117.434 - Median: 116.0\n",
      "Description Tokens: Mean: 8971.263 - Median: 5658.5\n",
      "Claims Tokens: Mean: 1131.289 - Median: 896.0\n",
      "Description Paragraphs Tokens: Mean: 141.313113334 - Median: 114.0\n",
      "Claims Paragraphs: Mean: 19.008 - Median: 16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-16 01:06:18,493 : INFO : Finished batch 70000 of size 1000 in 5m 35s\n",
      "2017-03-16 01:06:18,497 : INFO : For index 70000, the actual number of lines written is: 4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Abstract Sentences: 3\n",
      "Average Desc Sentences: 282\n",
      "Average Desc Paragraphs: 66\n",
      "Average Claims Sentences: 16\n",
      "Abstract Tokens: Mean: 116.501 - Median: 117.0\n",
      "Description Tokens: Mean: 9176.843 - Median: 6447.5\n",
      "Claims Tokens: Mean: 1058.939 - Median: 924.5\n",
      "Description Paragraphs Tokens: Mean: 137.616864615 - Median: 112.0\n",
      "Claims Paragraphs: Mean: 16.587 - Median: 16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-16 01:06:41,087 : INFO : Finished batch 60000 of size 1000 in 5m 57s\n",
      "2017-03-16 01:06:41,089 : INFO : For index 60000, the actual number of lines written is: 4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Abstract Sentences: 3\n",
      "Average Desc Sentences: 295\n",
      "Average Desc Paragraphs: 71\n",
      "Average Claims Sentences: 16\n",
      "Abstract Tokens: Mean: 116.293 - Median: 118.0\n",
      "Description Tokens: Mean: 9770.952 - Median: 6617.0\n",
      "Claims Tokens: Mean: 1059.014 - Median: 906.5\n",
      "Description Paragraphs Tokens: Mean: 137.735438399 - Median: 112.0\n",
      "Claims Paragraphs: Mean: 16.633 - Median: 16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-16 01:07:02,979 : INFO : Finished batch 120000 of size 1000 in 0m 55s\n",
      "2017-03-16 01:07:02,982 : INFO : For index 120000, the actual number of lines written is: 624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Abstract Sentences: 3\n",
      "Average Desc Sentences: 295\n",
      "Average Desc Paragraphs: 67\n",
      "Average Claims Sentences: 34\n",
      "Abstract Tokens: Mean: 123.487179487 - Median: 123.5\n",
      "Description Tokens: Mean: 9056.32051282 - Median: 6444.0\n",
      "Claims Tokens: Mean: 2129.06410256 - Median: 1735.0\n",
      "Description Paragraphs Tokens: Mean: 135.207771079 - Median: 111.0\n",
      "Claims Paragraphs: Mean: 34.3141025641 - Median: 30.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-16 01:10:54,091 : INFO : Finished batch 80000 of size 1000 in 5m 22s\n",
      "2017-03-16 01:10:54,093 : INFO : For index 80000, the actual number of lines written is: 4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Abstract Sentences: 3\n",
      "Average Desc Sentences: 277\n",
      "Average Desc Paragraphs: 65\n",
      "Average Claims Sentences: 16\n",
      "Abstract Tokens: Mean: 114.978 - Median: 118.0\n",
      "Description Tokens: Mean: 9035.3 - Median: 6557.0\n",
      "Claims Tokens: Mean: 1052.833 - Median: 908.0\n",
      "Description Paragraphs Tokens: Mean: 137.333373866 - Median: 113.0\n",
      "Claims Paragraphs: Mean: 16.561 - Median: 16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-16 01:11:22,433 : INFO : Finished batch 90000 of size 1000 in 5m 33s\n",
      "2017-03-16 01:11:22,436 : INFO : For index 90000, the actual number of lines written is: 4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Abstract Sentences: 3\n",
      "Average Desc Sentences: 276\n",
      "Average Desc Paragraphs: 65\n",
      "Average Claims Sentences: 16\n",
      "Abstract Tokens: Mean: 116.039 - Median: 118.0\n",
      "Description Tokens: Mean: 9215.904 - Median: 6593.5\n",
      "Claims Tokens: Mean: 1066.299 - Median: 932.5\n",
      "Description Paragraphs Tokens: Mean: 141.239908046 - Median: 116.0\n",
      "Claims Paragraphs: Mean: 16.451 - Median: 16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-16 01:11:36,285 : INFO : Finished batch 100000 of size 1000 in 5m 39s\n",
      "2017-03-16 01:11:36,288 : INFO : For index 100000, the actual number of lines written is: 4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Abstract Sentences: 3\n",
      "Average Desc Sentences: 287\n",
      "Average Desc Paragraphs: 67\n",
      "Average Claims Sentences: 16\n",
      "Abstract Tokens: Mean: 113.675 - Median: 117.0\n",
      "Description Tokens: Mean: 9544.564 - Median: 6973.5\n",
      "Claims Tokens: Mean: 1108.746 - Median: 977.5\n",
      "Description Paragraphs Tokens: Mean: 141.208486211 - Median: 116.0\n",
      "Claims Paragraphs: Mean: 16.438 - Median: 17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-16 01:11:54,205 : INFO : Finished batch 110000 of size 1000 in 5m 55s\n",
      "2017-03-16 01:11:54,208 : INFO : For index 110000, the actual number of lines written is: 4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Abstract Sentences: 3\n",
      "Average Desc Sentences: 311\n",
      "Average Desc Paragraphs: 73\n",
      "Average Claims Sentences: 16\n",
      "Abstract Tokens: Mean: 114.923 - Median: 118.0\n",
      "Description Tokens: Mean: 10124.423 - Median: 7225.5\n",
      "Claims Tokens: Mean: 1053.8 - Median: 936.5\n",
      "Description Paragraphs Tokens: Mean: 138.761056974 - Median: 115.0\n",
      "Claims Paragraphs: Mean: 16.433 - Median: 16.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pool = ThreadPool(8) # use just 6 because every batch requires a lot of memory\n",
    "    # +1 since range is end-exclusive\n",
    "    batches = range(0, (divmod(SAMPLE_SIZE, BATCH_SIZE)[0]+1) * BATCH_SIZE, BATCH_SIZE*10 )\n",
    "    indices = pool.map(multithreaded_extended_batch_creation, batches)\n",
    "    pool.close()\n",
    "    pool.terminate()\n",
    "finally:\n",
    "    pool.close()\n",
    "    pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
