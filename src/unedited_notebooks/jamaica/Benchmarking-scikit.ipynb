{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "import cPickle as pickle\n",
    "\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "\n",
    "from thesis.utils.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_DOCUMENTS = 5\n",
    "TOP_N_FEATURES = 10000\n",
    "\n",
    "RANDOM_SEED = 10000\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_ITERATIONS = 10\n",
    "SVM_CONVERGENCE = 0.001\n",
    "SVM_REG = 0.01\n",
    "SVM_CLASS_WEIGHTS = None\n",
    "SVM_MODEL_NAME = 'svm_iter_{}_reg_{}_classweights_{}'.format(SVM_ITERATIONS, SVM_REG, str(SVM_CLASS_WEIGHTS))\n",
    "\n",
    "CLASSIFIER_FILE = '{}_classifier.pkl'\n",
    "VALIDATION_METRICS_FILENAME= '{}_validation_metrics.pkl'\n",
    "TRAINING_METRICS_FILENAME = '{}_training_metrics.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_location = \"/big/s/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "svm_location = root_location + \"benchmarking_svm/\"\n",
    "\n",
    "\n",
    "training_file = root_location + \"docs_output.json\"\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "training_docs_list_file = exports_location + \"training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"validation_docs_list.pkl\"\n",
    "test_docs_list_file = exports_location + \"test_docs_list.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OneHotEncoder():\n",
    "    \n",
    "    def __init__(self, classifications):\n",
    "        self.classifications = classifications\n",
    "        self.one_hot_indices = {}\n",
    "\n",
    "        # convert character classifications to bit vectors\n",
    "        for i, clssf in enumerate(classifications):\n",
    "            bits = [0] * len(classifications)\n",
    "            bits[i] = 1\n",
    "            self.one_hot_indices[clssf] = i\n",
    "    \n",
    "    def get_label_vector(self, labels):\n",
    "        \"\"\"\n",
    "        classes: array of string with the classes assigned to the instance\n",
    "        \"\"\"\n",
    "        output_vector = [0] * len(self.classifications)\n",
    "        for label in labels:\n",
    "            index = self.one_hot_indices[label]\n",
    "            output_vector[index] = 1\n",
    "            \n",
    "        return output_vector\n",
    "\n",
    "def get_label_data(classifications, doc_ids, doc_classification_map):\n",
    "    one_hot_encoder = OneHotEncoder(classifications)\n",
    "    data_labels = []\n",
    "    for i, doc_id in enumerate(doc_ids):\n",
    "        #if len(doc_classification_map[doc_id]) > 20: info(\"PROOOOBBBBBBBBBBBLEM \"+  str(doc_classification_map[doc_id]))\n",
    "        eligible_classifications = [clssf for clssf in doc_classification_map[doc_id] if clssf in classifications]\n",
    "        data_labels.append(one_hot_encoder.get_label_vector(eligible_classifications))\n",
    "        #if i % 1000 == 0: info(i)\n",
    "    data_labels = np.array(data_labels)\n",
    "    return data_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Classification Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.6 s, sys: 3.85 s, total: 43.5 s\n",
      "Wall time: 48.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifications = sections\n",
    "classifications_type = \"sections\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 08:46:27,306 : INFO : =============== bm25 Being Evaluated ================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 12s, sys: 3min 44s, total: 15min 57s\n",
      "Wall time: 17min 17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 09:40:50,730 : INFO : Training Classifier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.1 s, sys: 12.6 s, total: 27.7 s\n",
      "Wall time: 20min 15s\n",
      "[[1 0 0 ..., 0 0 0]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " ..., \n",
      " [0 0 0 ..., 0 1 0]\n",
      " [1 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 1]]\n",
      "(1286325, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 09:44:40,654 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 17s, sys: 32.3 s, total: 3min 49s\n",
      "Wall time: 3min 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 09:44:55,894 : INFO : Calculating training metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.7 s, sys: 1.52 s, total: 15.2 s\n",
      "Wall time: 15.2 s\n",
      "[[1 0 0 ..., 0 0 0]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " ..., \n",
      " [0 0 0 ..., 0 1 0]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 1]]\n",
      "** Training Metrics: Cov Err: 3.521, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.703, Top 3: 0.835, Top 5: 0.861, \n",
      "\t\t F1 Micro: 0.732, F1 Macro: 0.570, Total Pos: 1,203,912\n",
      "CPU times: user 2min 59s, sys: 10.8 s, total: 3min 10s\n",
      "Wall time: 3min 12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 09:50:03,732 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.44 s, sys: 440 ms, total: 2.88 s\n",
      "Wall time: 2.87 s\n",
      "CPU times: user 3.19 s, sys: 152 ms, total: 3.34 s\n",
      "Wall time: 3.34 s\n",
      "[[0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 1 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 1 0]]\n",
      "** Validation Metrics: Cov Err: 4.737, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.590, Top 3: 0.741, Top 5: 0.766, \n",
      "\t\t F1 Micro: 0.614, F1 Macro: 0.391, Total Pos: 202,286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 09:50:37,449 : INFO : =============== tf_idf Being Evaluated ================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 16s, sys: 1min 32s, total: 13min 49s\n",
      "Wall time: 14min 36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 10:05:33,128 : INFO : Training Classifier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.8 s, sys: 2.66 s, total: 14.4 s\n",
      "Wall time: 14.7 s\n",
      "[[0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " [0 1 0 ..., 1 0 0]\n",
      " ..., \n",
      " [1 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " [0 1 0 ..., 0 0 0]]\n",
      "(1286325, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 10:08:32,816 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 54s, sys: 4.73 s, total: 2min 59s\n",
      "Wall time: 2min 59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 10:08:45,530 : INFO : Calculating training metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 s, sys: 140 ms, total: 12.7 s\n",
      "Wall time: 12.7 s\n",
      "[[0 1 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 1 0 1]\n",
      " ..., \n",
      " [0 1 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " [0 1 0 ..., 1 0 0]]\n",
      "** Training Metrics: Cov Err: 3.682, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.682, Top 3: 0.831, Top 5: 0.855, \n",
      "\t\t F1 Micro: 0.711, F1 Macro: 0.602, Total Pos: 1,200,163\n",
      "CPU times: user 3min 9s, sys: 6.18 s, total: 3min 15s\n",
      "Wall time: 3min 27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 10:14:06,444 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.43 s, sys: 396 ms, total: 2.82 s\n",
      "Wall time: 2.8 s\n",
      "CPU times: user 3.09 s, sys: 28 ms, total: 3.12 s\n",
      "Wall time: 3.12 s\n",
      "[[0 0 0 ..., 0 0 1]\n",
      " [1 0 1 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " ..., \n",
      " [1 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 1 0 ..., 0 0 0]]\n",
      "** Validation Metrics: Cov Err: 3.800, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.662, Top 3: 0.816, Top 5: 0.841, \n",
      "\t\t F1 Micro: 0.697, F1 Macro: 0.566, Total Pos: 294,836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 10:14:37,281 : INFO : =============== sublinear_tf_idf Being Evaluated ================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 10s, sys: 1min 10s, total: 12min 21s\n",
      "Wall time: 13min 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 10:28:18,652 : INFO : Training Classifier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.5 s, sys: 2.93 s, total: 14.4 s\n",
      "Wall time: 14.3 s\n",
      "[[0 1 0 ..., 0 1 1]\n",
      " [0 0 1 ..., 0 0 0]\n",
      " [0 0 1 ..., 0 0 1]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 1]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]]\n",
      "(1286325, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 10:32:32,905 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 3s, sys: 10.1 s, total: 4min 13s\n",
      "Wall time: 4min 14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 10:33:03,900 : INFO : Calculating training metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.2 s, sys: 1.73 s, total: 30.9 s\n",
      "Wall time: 31 s\n",
      "[[0 1 0 ..., 0 0 0]\n",
      " [0 0 1 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 1]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "** Training Metrics: Cov Err: 3.911, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.663, Top 3: 0.804, Top 5: 0.829, \n",
      "\t\t F1 Micro: 0.699, F1 Macro: 0.506, Total Pos: 1,078,561\n",
      "CPU times: user 2min 46s, sys: 15.5 s, total: 3min 1s\n",
      "Wall time: 3min 24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 10:39:40,777 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.48 s, sys: 592 ms, total: 6.07 s\n",
      "Wall time: 6.15 s\n",
      "CPU times: user 8.73 s, sys: 88 ms, total: 8.82 s\n",
      "Wall time: 8.86 s\n",
      "[[1 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "** Validation Metrics: Cov Err: 3.921, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.662, Top 3: 0.803, Top 5: 0.828, \n",
      "\t\t F1 Micro: 0.698, F1 Macro: 0.505, Total Pos: 269,447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 10:40:36,613 : INFO : =============== sublinear_tf Being Evaluated ================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 11s, sys: 7min 55s, total: 20min 6s\n",
      "Wall time: 21min 54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 11:03:04,655 : INFO : Training Classifier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.4 s, sys: 6.2 s, total: 28.6 s\n",
      "Wall time: 28.7 s\n",
      "[[0 0 0 ..., 1 0 0]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]]\n",
      "(1286325, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 11:12:08,680 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 9s, sys: 17.9 s, total: 7min 26s\n",
      "Wall time: 9min 4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 11:12:49,606 : INFO : Calculating training metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 s, sys: 776 ms, total: 20.7 s\n",
      "Wall time: 40.9 s\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]]\n",
      "** Training Metrics: Cov Err: 3.892, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.660, Top 3: 0.803, Top 5: 0.829, \n",
      "\t\t F1 Micro: 0.698, F1 Macro: 0.500, Total Pos: 1,096,052\n",
      "CPU times: user 3min 3s, sys: 14.4 s, total: 3min 17s\n",
      "Wall time: 4min 40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 11:22:31,233 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.23 s, sys: 408 ms, total: 3.64 s\n",
      "Wall time: 5.87 s\n",
      "CPU times: user 8.45 s, sys: 148 ms, total: 8.6 s\n",
      "Wall time: 9.96 s\n",
      "[[0 0 0 ..., 0 0 1]\n",
      " [0 1 0 ..., 0 0 0]\n",
      " [0 0 1 ..., 0 0 0]\n",
      " ..., \n",
      " [1 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 1 0]]\n",
      "** Validation Metrics: Cov Err: 3.898, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.660, Top 3: 0.803, Top 5: 0.828, \n",
      "\t\t F1 Micro: 0.698, F1 Macro: 0.499, Total Pos: 274,088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 11:23:40,369 : INFO : =============== tf Being Evaluated ================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 1s, sys: 14min 21s, total: 28min 22s\n",
      "Wall time: 43min 35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 12:07:48,436 : INFO : Training Classifier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.6 s, sys: 2.71 s, total: 17.3 s\n",
      "Wall time: 26.4 s\n",
      "[[0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 1]\n",
      " [0 0 0 ..., 0 1 0]]\n",
      "(1286325, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 12:17:15,860 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 34s, sys: 13.9 s, total: 5min 48s\n",
      "Wall time: 9min 27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 12:18:08,051 : INFO : Calculating training metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40 s, sys: 728 ms, total: 40.7 s\n",
      "Wall time: 52.2 s\n",
      "[[0 0 0 ..., 0 1 1]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 1 0]]\n",
      "** Training Metrics: Cov Err: 3.784, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.667, Top 3: 0.837, Top 5: 0.864, \n",
      "\t\t F1 Micro: 0.686, F1 Macro: 0.568, Total Pos: 1,278,646\n",
      "CPU times: user 3min 34s, sys: 38 s, total: 4min 12s\n",
      "Wall time: 8min 28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-14 12:31:42,216 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.5 s, sys: 468 ms, total: 4.97 s\n",
      "Wall time: 6.25 s\n",
      "CPU times: user 7.47 s, sys: 84 ms, total: 7.55 s\n",
      "Wall time: 12.4 s\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " ..., \n",
      " [0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 1 0]]\n",
      "** Validation Metrics: Cov Err: 3.804, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.665, Top 3: 0.836, Top 5: 0.863, \n",
      "\t\t F1 Micro: 0.684, F1 Macro: 0.566, Total Pos: 319,296\n"
     ]
    }
   ],
   "source": [
    "data_types = [\"bm25\", \"tf_idf\", \"sublinear_tf_idf\", \"sublinear_tf\", \"tf\"]\n",
    "# data_types = [\"tf\"]\n",
    "for data_type in data_types:\n",
    "    info(\"=============== {} Being Evaluated ================\".format(data_type))\n",
    "    \n",
    "    data_training_location = exports_location + \"{}_training_sparse_data.pkl\".format(data_type)\n",
    "    data_training_docids_location = exports_location + \"{}_training_sparse_docids.pkl\".format(data_type)\n",
    "    data_validation_location = exports_location + \"{}_validation_sparse_data.pkl\".format(data_type)\n",
    "    data_validation_docids_location = exports_location + \"{}_validation_sparse_docids.pkl\".format(data_type)\n",
    "    \n",
    "    # Get the training data\n",
    "    %time X = pickle.load(open(data_training_location, \"r\"))\n",
    "    training_data_docids = pickle.load(open(data_training_docids_location, \"r\"))\n",
    "    %time y = get_label_data(classifications, training_data_docids, doc_classification_map)\n",
    "    \n",
    "    print y\n",
    "    print y.shape\n",
    "\n",
    "    info('Training Classifier')\n",
    "    clf = OneVsRestClassifier(linear_model.SGDClassifier(loss='hinge', penalty='l2', \n",
    "                                                         #alpha is the 1/C parameter\n",
    "                                                         alpha=SVM_REG, fit_intercept=True, n_iter=SVM_ITERATIONS,\n",
    "                                                         #n_jobs=-1 means use all cpus\n",
    "                                                         shuffle=True, verbose=0, n_jobs=1,\n",
    "                                                         #eta0 is the learning rate when we use constant configuration\n",
    "                                                         random_state=SVM_SEED, learning_rate='optimal', eta0=0.0, \n",
    "                                                         class_weight=SVM_CLASS_WEIGHTS, warm_start=False), n_jobs=1)\n",
    "    %time clf.fit(X,y)\n",
    "    \n",
    "    # Training Metrics\n",
    "    info('Evaluating on Training Data')\n",
    "    %time yp = clf.predict(X)\n",
    "    print yp\n",
    "    info('Calculating training metrics')\n",
    "    training_metrics = get_metrics(y, yp, yp)\n",
    "    print \"** Training Metrics: Cov Err: {:.3f}, Avg Labels: {:.3f}, \\n\\t\\t Top 1: {:.3f}, Top 3: {:.3f}, Top 5: {:.3f}, \\n\\t\\t F1 Micro: {:.3f}, F1 Macro: {:.3f}, Total Pos: {:,d}\".format(\n",
    "        training_metrics['coverage_error'], training_metrics['average_num_of_labels'], \n",
    "        training_metrics['top_1'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "        training_metrics['f1_micro'], training_metrics['f1_macro'], training_metrics['total_positive'])\n",
    "    \n",
    "    # Get the validation data\n",
    "    %time Xv = pickle.load(open(data_validation_location,'r'))\n",
    "    validation_data_docids = pickle.load(open(data_validation_docids_location, \"r\"))\n",
    "    %time yv = get_label_data(classifications, validation_data_docids, doc_classification_map)\n",
    "    \n",
    "    # Validation Metrics\n",
    "    info('Evaluating on Validation Data')\n",
    "    %time yvp = clf.predict(Xv)\n",
    "    print yvp\n",
    "    validation_metrics = get_metrics(yv, yvp, yvp)\n",
    "    print \"** Validation Metrics: Cov Err: {:.3f}, Avg Labels: {:.3f}, \\n\\t\\t Top 1: {:.3f}, Top 3: {:.3f}, Top 5: {:.3f}, \\n\\t\\t F1 Micro: {:.3f}, F1 Macro: {:.3f}, Total Pos: {:,d}\".format(\n",
    "        validation_metrics['coverage_error'], validation_metrics['average_num_of_labels'], \n",
    "        validation_metrics['top_1'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "        validation_metrics['f1_micro'], validation_metrics['f1_macro'], validation_metrics['total_positive'])\n",
    "    \n",
    "    # Dump the classifier and metrics\n",
    "    data_folder = os.path.join(svm_location, SVM_MODEL_NAME, data_type)\n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "    pickle.dump(clf, open(os.path.join(data_folder, CLASSIFIER_FILE.format(classifications_type)), \"w\"))\n",
    "    pickle.dump(training_metrics, open(os.path.join(data_folder, TRAINING_METRICS_FILENAME.format(classifications_type)), \"w\"))\n",
    "    pickle.dump(validation_metrics, open(os.path.join(data_folder, VALIDATION_METRICS_FILENAME.format(classifications_type)), \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_type = \"bm25\"\n",
    "classifications = sections\n",
    "classifications_type = \"sections\"\n",
    "data_training_location = exports_location + \"{}_training_sparse_data.pkl\".format(data_type)\n",
    "data_training_docids_location = exports_location + \"{}_training_sparse_docids.pkl\".format(data_type)\n",
    "data_validation_location = exports_location + \"{}_validation_sparse_data.pkl\".format(data_type)\n",
    "data_validation_docids_location = exports_location + \"{}_validation_sparse_docids.pkl\".format(data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 59s, sys: 5min 26s, total: 17min 25s\n",
      "Wall time: 20min 53s\n",
      "CPU times: user 22.4 s, sys: 6.44 s, total: 28.9 s\n",
      "Wall time: 29.5 s\n",
      "CPU times: user 12min 24s, sys: 5min 33s, total: 17min 58s\n",
      "Wall time: 21min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%time X = pickle.load(open(data_training_location, \"r\"))\n",
    "training_data_docids = pickle.load(open(data_training_docids_location, \"r\"))\n",
    "%time y = get_label_data(classifications, training_data_docids, doc_classification_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4d6baf1a67bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-03 23:41:22,437 : INFO : Training Classifier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ..., 0 0 0]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " ..., \n",
      " [0 0 0 ..., 0 1 0]\n",
      " [1 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 1]]\n",
      "(1286325, 8)\n"
     ]
    }
   ],
   "source": [
    "print y\n",
    "print y.shape\n",
    "\n",
    "# try class weights\n",
    "# try warm start and evaluate after every iter\n",
    "\n",
    "info('Training Classifier')\n",
    "clf = OneVsRestClassifier(linear_model.SGDClassifier(loss='hinge', penalty='l2', \n",
    "                                                     #alpha is the 1/C parameter\n",
    "                                                     alpha=SVM_REG, fit_intercept=True, n_iter=SVM_ITERATIONS,\n",
    "                                                     #n_jobs=-1 means use all cpus\n",
    "                                                     shuffle=True, verbose=1, n_jobs=1,\n",
    "                                                     #eta0 is the learning rate when we use constant configuration\n",
    "                                                     random_state=SVM_SEED, learning_rate='optimal', eta0=0.0, \n",
    "                                                     class_weight=SVM_CLASS_WEIGHTS, warm_start=False), n_jobs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 5.12, NNZs: 9968, Bias: -0.470338, T: 1286325, Avg. loss: 0.943358\n",
      "Total training time: 6.43 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.83, NNZs: 9971, Bias: -0.475705, T: 2572650, Avg. loss: 0.615495\n",
      "Total training time: 9.84 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.40, NNZs: 9976, Bias: -0.477937, T: 3858975, Avg. loss: 0.496949\n",
      "Total training time: 13.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.21, NNZs: 9978, Bias: -0.479755, T: 5145300, Avg. loss: 0.435043\n",
      "Total training time: 16.14 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.10, NNZs: 9978, Bias: -0.480922, T: 6431625, Avg. loss: 0.396697\n",
      "Total training time: 18.90 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.02, NNZs: 9978, Bias: -0.482003, T: 7717950, Avg. loss: 0.370547\n",
      "Total training time: 21.81 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.98, NNZs: 9978, Bias: -0.482518, T: 9004275, Avg. loss: 0.351453\n",
      "Total training time: 26.43 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.95, NNZs: 9981, Bias: -0.483186, T: 10290600, Avg. loss: 0.336957\n",
      "Total training time: 29.96 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.91, NNZs: 9981, Bias: -0.483793, T: 11576925, Avg. loss: 0.325487\n",
      "Total training time: 33.13 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.90, NNZs: 9981, Bias: -0.484366, T: 12863250, Avg. loss: 0.316200\n",
      "Total training time: 36.36 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.89, NNZs: 9981, Bias: -0.484856, T: 14149575, Avg. loss: 0.308536\n",
      "Total training time: 39.49 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.87, NNZs: 9982, Bias: -0.485236, T: 15435900, Avg. loss: 0.302066\n",
      "Total training time: 42.24 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.86, NNZs: 9982, Bias: -0.485542, T: 16722225, Avg. loss: 0.296558\n",
      "Total training time: 45.17 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.85, NNZs: 9985, Bias: -0.485900, T: 18008550, Avg. loss: 0.291796\n",
      "Total training time: 48.08 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.84, NNZs: 9985, Bias: -0.486229, T: 19294875, Avg. loss: 0.287641\n",
      "Total training time: 51.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.83, NNZs: 9985, Bias: -0.486499, T: 20581200, Avg. loss: 0.283983\n",
      "Total training time: 53.97 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.83, NNZs: 9985, Bias: -0.486759, T: 21867525, Avg. loss: 0.280737\n",
      "Total training time: 56.91 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.83, NNZs: 9985, Bias: -0.487004, T: 23153850, Avg. loss: 0.277807\n",
      "Total training time: 59.63 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.82, NNZs: 9985, Bias: -0.487217, T: 24440175, Avg. loss: 0.275189\n",
      "Total training time: 62.25 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.82, NNZs: 9985, Bias: -0.487428, T: 25726500, Avg. loss: 0.272818\n",
      "Total training time: 64.99 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.82, NNZs: 9985, Bias: -0.487607, T: 27012825, Avg. loss: 0.270656\n",
      "Total training time: 67.81 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2.81, NNZs: 9985, Bias: -0.487808, T: 28299150, Avg. loss: 0.268694\n",
      "Total training time: 70.62 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 2.81, NNZs: 9985, Bias: -0.488016, T: 29585475, Avg. loss: 0.266872\n",
      "Total training time: 73.44 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 2.81, NNZs: 9985, Bias: -0.488178, T: 30871800, Avg. loss: 0.265208\n",
      "Total training time: 76.24 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 2.80, NNZs: 9985, Bias: -0.488358, T: 32158125, Avg. loss: 0.263677\n",
      "Total training time: 79.05 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.80, NNZs: 9989, Bias: -0.488537, T: 33444450, Avg. loss: 0.262246\n",
      "Total training time: 81.86 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 2.80, NNZs: 9989, Bias: -0.488663, T: 34730775, Avg. loss: 0.260919\n",
      "Total training time: 84.68 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 2.80, NNZs: 9989, Bias: -0.488858, T: 36017100, Avg. loss: 0.259695\n",
      "Total training time: 87.47 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 2.80, NNZs: 9989, Bias: -0.488995, T: 37303425, Avg. loss: 0.258544\n",
      "Total training time: 90.22 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 2.80, NNZs: 9989, Bias: -0.489075, T: 38589750, Avg. loss: 0.257463\n",
      "Total training time: 93.00 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.489219, T: 39876075, Avg. loss: 0.256452\n",
      "Total training time: 95.78 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.489320, T: 41162400, Avg. loss: 0.255494\n",
      "Total training time: 98.46 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.489449, T: 42448725, Avg. loss: 0.254597\n",
      "Total training time: 101.11 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.489548, T: 43735050, Avg. loss: 0.253751\n",
      "Total training time: 103.78 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.489643, T: 45021375, Avg. loss: 0.252954\n",
      "Total training time: 106.45 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.489791, T: 46307700, Avg. loss: 0.252197\n",
      "Total training time: 109.19 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.489911, T: 47594025, Avg. loss: 0.251474\n",
      "Total training time: 111.94 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.489995, T: 48880350, Avg. loss: 0.250788\n",
      "Total training time: 114.74 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.490092, T: 50166675, Avg. loss: 0.250137\n",
      "Total training time: 117.55 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.490186, T: 51453000, Avg. loss: 0.249518\n",
      "Total training time: 120.29 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.490272, T: 52739325, Avg. loss: 0.248927\n",
      "Total training time: 123.02 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.490369, T: 54025650, Avg. loss: 0.248366\n",
      "Total training time: 125.84 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.490467, T: 55311975, Avg. loss: 0.247829\n",
      "Total training time: 128.62 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.490547, T: 56598300, Avg. loss: 0.247313\n",
      "Total training time: 131.39 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.490619, T: 57884625, Avg. loss: 0.246821\n",
      "Total training time: 134.18 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.490713, T: 59170950, Avg. loss: 0.246351\n",
      "Total training time: 137.02 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.490797, T: 60457275, Avg. loss: 0.245899\n",
      "Total training time: 139.83 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.490883, T: 61743600, Avg. loss: 0.245466\n",
      "Total training time: 142.62 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.490954, T: 63029925, Avg. loss: 0.245048\n",
      "Total training time: 145.40 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.491044, T: 64316250, Avg. loss: 0.244648\n",
      "Total training time: 148.20 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.491105, T: 65602575, Avg. loss: 0.244257\n",
      "Total training time: 150.89 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.491192, T: 66888900, Avg. loss: 0.243887\n",
      "Total training time: 153.51 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.491273, T: 68175225, Avg. loss: 0.243527\n",
      "Total training time: 156.12 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.491357, T: 69461550, Avg. loss: 0.243181\n",
      "Total training time: 158.74 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.491414, T: 70747875, Avg. loss: 0.242845\n",
      "Total training time: 161.37 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.491476, T: 72034200, Avg. loss: 0.242521\n",
      "Total training time: 164.01 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.491547, T: 73320525, Avg. loss: 0.242207\n",
      "Total training time: 166.70 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.491603, T: 74606850, Avg. loss: 0.241902\n",
      "Total training time: 169.40 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.491675, T: 75893175, Avg. loss: 0.241609\n",
      "Total training time: 172.10 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.491731, T: 77179500, Avg. loss: 0.241327\n",
      "Total training time: 174.83 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.491790, T: 78465825, Avg. loss: 0.241052\n",
      "Total training time: 177.55 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.491843, T: 79752150, Avg. loss: 0.240785\n",
      "Total training time: 180.25 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.491918, T: 81038475, Avg. loss: 0.240528\n",
      "Total training time: 182.95 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.491978, T: 82324800, Avg. loss: 0.240278\n",
      "Total training time: 185.75 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.492042, T: 83611125, Avg. loss: 0.240036\n",
      "Total training time: 188.57 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.492097, T: 84897450, Avg. loss: 0.239801\n",
      "Total training time: 191.36 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.492162, T: 86183775, Avg. loss: 0.239573\n",
      "Total training time: 194.14 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.492203, T: 87470100, Avg. loss: 0.239348\n",
      "Total training time: 196.95 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.492262, T: 88756425, Avg. loss: 0.239131\n",
      "Total training time: 199.77 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.492313, T: 90042750, Avg. loss: 0.238920\n",
      "Total training time: 202.58 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.492351, T: 91329075, Avg. loss: 0.238713\n",
      "Total training time: 205.39 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.492422, T: 92615400, Avg. loss: 0.238514\n",
      "Total training time: 208.14 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.492471, T: 93901725, Avg. loss: 0.238319\n",
      "Total training time: 210.77 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.492523, T: 95188050, Avg. loss: 0.238131\n",
      "Total training time: 213.44 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 2.79, NNZs: 9989, Bias: -0.492559, T: 96474375, Avg. loss: 0.237945\n",
      "Total training time: 216.10 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.492626, T: 97760700, Avg. loss: 0.237765\n",
      "Total training time: 218.77 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.492668, T: 99047025, Avg. loss: 0.237589\n",
      "Total training time: 221.45 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.492723, T: 100333350, Avg. loss: 0.237420\n",
      "Total training time: 224.15 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.492764, T: 101619675, Avg. loss: 0.237250\n",
      "Total training time: 226.91 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.492815, T: 102906000, Avg. loss: 0.237085\n",
      "Total training time: 229.68 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.492857, T: 104192325, Avg. loss: 0.236927\n",
      "Total training time: 232.40 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.492896, T: 105478650, Avg. loss: 0.236771\n",
      "Total training time: 235.17 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.492933, T: 106764975, Avg. loss: 0.236619\n",
      "Total training time: 237.92 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.492983, T: 108051300, Avg. loss: 0.236471\n",
      "Total training time: 240.65 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.493019, T: 109337625, Avg. loss: 0.236325\n",
      "Total training time: 243.38 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.493057, T: 110623950, Avg. loss: 0.236182\n",
      "Total training time: 246.13 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.493113, T: 111910275, Avg. loss: 0.236044\n",
      "Total training time: 248.89 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.493146, T: 113196600, Avg. loss: 0.235909\n",
      "Total training time: 251.67 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.493195, T: 114482925, Avg. loss: 0.235775\n",
      "Total training time: 254.46 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.493238, T: 115769250, Avg. loss: 0.235645\n",
      "Total training time: 257.26 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.493284, T: 117055575, Avg. loss: 0.235519\n",
      "Total training time: 260.08 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.493315, T: 118341900, Avg. loss: 0.235393\n",
      "Total training time: 262.88 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.493349, T: 119628225, Avg. loss: 0.235271\n",
      "Total training time: 265.64 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.493406, T: 120914550, Avg. loss: 0.235152\n",
      "Total training time: 268.28 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.493450, T: 122200875, Avg. loss: 0.235035\n",
      "Total training time: 270.89 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.493477, T: 123487200, Avg. loss: 0.234919\n",
      "Total training time: 273.58 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.493511, T: 124773525, Avg. loss: 0.234808\n",
      "Total training time: 276.27 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.493549, T: 126059850, Avg. loss: 0.234697\n",
      "Total training time: 278.95 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.493590, T: 127346175, Avg. loss: 0.234587\n",
      "Total training time: 281.65 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 2.78, NNZs: 9989, Bias: -0.493632, T: 128632500, Avg. loss: 0.234480\n",
      "Total training time: 284.45 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4.74, NNZs: 9987, Bias: -0.302059, T: 1286325, Avg. loss: 1.183958\n",
      "Total training time: 2.87 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.68, NNZs: 9992, Bias: -0.293277, T: 2572650, Avg. loss: 0.834706\n",
      "Total training time: 5.71 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.32, NNZs: 9992, Bias: -0.286519, T: 3858975, Avg. loss: 0.709333\n",
      "Total training time: 8.52 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.18, NNZs: 9992, Bias: -0.280833, T: 5145300, Avg. loss: 0.644326\n",
      "Total training time: 11.35 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.08, NNZs: 9992, Bias: -0.276503, T: 6431625, Avg. loss: 0.604211\n",
      "Total training time: 14.24 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.04, NNZs: 9992, Bias: -0.272519, T: 7717950, Avg. loss: 0.576920\n",
      "Total training time: 17.12 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.99, NNZs: 9992, Bias: -0.269125, T: 9004275, Avg. loss: 0.557098\n",
      "Total training time: 19.99 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.97, NNZs: 9992, Bias: -0.266225, T: 10290600, Avg. loss: 0.542105\n",
      "Total training time: 22.93 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.94, NNZs: 9992, Bias: -0.263559, T: 11576925, Avg. loss: 0.530182\n",
      "Total training time: 25.88 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.92, NNZs: 9992, Bias: -0.261301, T: 12863250, Avg. loss: 0.520645\n",
      "Total training time: 28.81 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.91, NNZs: 9992, Bias: -0.259130, T: 14149575, Avg. loss: 0.512752\n",
      "Total training time: 31.70 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.90, NNZs: 9992, Bias: -0.257055, T: 15435900, Avg. loss: 0.506103\n",
      "Total training time: 34.61 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.89, NNZs: 9992, Bias: -0.255284, T: 16722225, Avg. loss: 0.500451\n",
      "Total training time: 37.49 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.88, NNZs: 9992, Bias: -0.253611, T: 18008550, Avg. loss: 0.495561\n",
      "Total training time: 40.29 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.88, NNZs: 9992, Bias: -0.251974, T: 19294875, Avg. loss: 0.491317\n",
      "Total training time: 43.06 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.87, NNZs: 9992, Bias: -0.250517, T: 20581200, Avg. loss: 0.487557\n",
      "Total training time: 45.82 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.87, NNZs: 9992, Bias: -0.249112, T: 21867525, Avg. loss: 0.484231\n",
      "Total training time: 48.60 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.86, NNZs: 9992, Bias: -0.247831, T: 23153850, Avg. loss: 0.481245\n",
      "Total training time: 51.40 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.86, NNZs: 9992, Bias: -0.246617, T: 24440175, Avg. loss: 0.478597\n",
      "Total training time: 54.22 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.85, NNZs: 9992, Bias: -0.245432, T: 25726500, Avg. loss: 0.476179\n",
      "Total training time: 57.12 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.85, NNZs: 9992, Bias: -0.244307, T: 27012825, Avg. loss: 0.473977\n",
      "Total training time: 60.02 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2.85, NNZs: 9992, Bias: -0.243272, T: 28299150, Avg. loss: 0.471970\n",
      "Total training time: 62.88 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 2.85, NNZs: 9992, Bias: -0.242269, T: 29585475, Avg. loss: 0.470132\n",
      "Total training time: 65.84 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 2.85, NNZs: 9992, Bias: -0.241314, T: 30871800, Avg. loss: 0.468447\n",
      "Total training time: 68.72 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 2.85, NNZs: 9992, Bias: -0.240416, T: 32158125, Avg. loss: 0.466881\n",
      "Total training time: 71.61 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.84, NNZs: 9992, Bias: -0.239506, T: 33444450, Avg. loss: 0.465419\n",
      "Total training time: 74.53 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 2.84, NNZs: 9992, Bias: -0.238692, T: 34730775, Avg. loss: 0.464074\n",
      "Total training time: 77.41 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 2.84, NNZs: 9992, Bias: -0.237868, T: 36017100, Avg. loss: 0.462825\n",
      "Total training time: 80.28 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 2.84, NNZs: 9992, Bias: -0.237049, T: 37303425, Avg. loss: 0.461653\n",
      "Total training time: 83.17 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 2.84, NNZs: 9992, Bias: -0.236295, T: 38589750, Avg. loss: 0.460558\n",
      "Total training time: 86.10 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.235565, T: 39876075, Avg. loss: 0.459536\n",
      "Total training time: 88.99 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 2.84, NNZs: 9992, Bias: -0.234845, T: 41162400, Avg. loss: 0.458572\n",
      "Total training time: 91.88 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.234156, T: 42448725, Avg. loss: 0.457663\n",
      "Total training time: 94.80 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.233486, T: 43735050, Avg. loss: 0.456805\n",
      "Total training time: 97.62 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.232844, T: 45021375, Avg. loss: 0.455996\n",
      "Total training time: 100.41 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.232199, T: 46307700, Avg. loss: 0.455234\n",
      "Total training time: 103.20 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.231618, T: 47594025, Avg. loss: 0.454513\n",
      "Total training time: 105.98 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.230988, T: 48880350, Avg. loss: 0.453814\n",
      "Total training time: 108.76 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.230475, T: 50166675, Avg. loss: 0.453165\n",
      "Total training time: 111.54 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.229884, T: 51453000, Avg. loss: 0.452533\n",
      "Total training time: 114.41 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.229327, T: 52739325, Avg. loss: 0.451936\n",
      "Total training time: 117.34 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.228805, T: 54025650, Avg. loss: 0.451371\n",
      "Total training time: 120.24 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.228297, T: 55311975, Avg. loss: 0.450822\n",
      "Total training time: 123.09 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.227817, T: 56598300, Avg. loss: 0.450308\n",
      "Total training time: 126.01 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.227336, T: 57884625, Avg. loss: 0.449808\n",
      "Total training time: 128.92 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.226844, T: 59170950, Avg. loss: 0.449324\n",
      "Total training time: 131.74 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.226343, T: 60457275, Avg. loss: 0.448867\n",
      "Total training time: 134.59 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.225900, T: 61743600, Avg. loss: 0.448432\n",
      "Total training time: 137.49 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.225459, T: 63029925, Avg. loss: 0.448007\n",
      "Total training time: 140.39 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.225038, T: 64316250, Avg. loss: 0.447605\n",
      "Total training time: 143.28 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.224592, T: 65602575, Avg. loss: 0.447213\n",
      "Total training time: 146.21 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.224173, T: 66888900, Avg. loss: 0.446839\n",
      "Total training time: 149.09 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.223755, T: 68175225, Avg. loss: 0.446482\n",
      "Total training time: 151.97 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.223362, T: 69461550, Avg. loss: 0.446133\n",
      "Total training time: 154.80 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.222971, T: 70747875, Avg. loss: 0.445792\n",
      "Total training time: 157.60 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 2.83, NNZs: 9992, Bias: -0.222574, T: 72034200, Avg. loss: 0.445468\n",
      "Total training time: 160.38 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.222216, T: 73320525, Avg. loss: 0.445157\n",
      "Total training time: 163.15 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.221823, T: 74606850, Avg. loss: 0.444852\n",
      "Total training time: 165.89 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.221471, T: 75893175, Avg. loss: 0.444557\n",
      "Total training time: 168.68 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.221107, T: 77179500, Avg. loss: 0.444270\n",
      "Total training time: 171.55 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.220748, T: 78465825, Avg. loss: 0.443993\n",
      "Total training time: 174.42 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.220403, T: 79752150, Avg. loss: 0.443726\n",
      "Total training time: 177.34 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.220066, T: 81038475, Avg. loss: 0.443468\n",
      "Total training time: 180.27 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.219725, T: 82324800, Avg. loss: 0.443217\n",
      "Total training time: 183.16 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.219404, T: 83611125, Avg. loss: 0.442973\n",
      "Total training time: 186.03 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.219082, T: 84897450, Avg. loss: 0.442737\n",
      "Total training time: 188.95 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.218750, T: 86183775, Avg. loss: 0.442501\n",
      "Total training time: 191.87 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.218456, T: 87470100, Avg. loss: 0.442282\n",
      "Total training time: 194.78 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.218134, T: 88756425, Avg. loss: 0.442064\n",
      "Total training time: 197.73 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.217834, T: 90042750, Avg. loss: 0.441852\n",
      "Total training time: 200.63 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.217546, T: 91329075, Avg. loss: 0.441647\n",
      "Total training time: 203.53 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.217242, T: 92615400, Avg. loss: 0.441447\n",
      "Total training time: 206.46 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.216955, T: 93901725, Avg. loss: 0.441250\n",
      "Total training time: 209.39 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.216683, T: 95188050, Avg. loss: 0.441059\n",
      "Total training time: 212.29 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.216375, T: 96474375, Avg. loss: 0.440875\n",
      "Total training time: 215.11 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.216087, T: 97760700, Avg. loss: 0.440692\n",
      "Total training time: 217.92 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.215824, T: 99047025, Avg. loss: 0.440514\n",
      "Total training time: 220.68 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.215557, T: 100333350, Avg. loss: 0.440341\n",
      "Total training time: 223.41 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.215292, T: 101619675, Avg. loss: 0.440174\n",
      "Total training time: 226.21 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.215022, T: 102906000, Avg. loss: 0.440012\n",
      "Total training time: 229.06 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.214744, T: 104192325, Avg. loss: 0.439853\n",
      "Total training time: 231.99 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.214484, T: 105478650, Avg. loss: 0.439696\n",
      "Total training time: 234.91 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.214249, T: 106764975, Avg. loss: 0.439545\n",
      "Total training time: 237.85 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.214003, T: 108051300, Avg. loss: 0.439398\n",
      "Total training time: 240.80 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.213753, T: 109337625, Avg. loss: 0.439253\n",
      "Total training time: 243.72 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.213506, T: 110623950, Avg. loss: 0.439111\n",
      "Total training time: 246.64 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.213259, T: 111910275, Avg. loss: 0.438973\n",
      "Total training time: 249.59 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.213028, T: 113196600, Avg. loss: 0.438836\n",
      "Total training time: 252.48 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.212783, T: 114482925, Avg. loss: 0.438701\n",
      "Total training time: 255.27 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.212555, T: 115769250, Avg. loss: 0.438575\n",
      "Total training time: 257.85 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.212326, T: 117055575, Avg. loss: 0.438449\n",
      "Total training time: 260.41 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.212109, T: 118341900, Avg. loss: 0.438324\n",
      "Total training time: 262.90 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.211882, T: 119628225, Avg. loss: 0.438200\n",
      "Total training time: 265.38 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.211664, T: 120914550, Avg. loss: 0.438079\n",
      "Total training time: 267.84 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.211446, T: 122200875, Avg. loss: 0.437962\n",
      "Total training time: 270.26 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.211225, T: 123487200, Avg. loss: 0.437847\n",
      "Total training time: 272.65 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.211020, T: 124773525, Avg. loss: 0.437736\n",
      "Total training time: 275.03 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.210789, T: 126059850, Avg. loss: 0.437626\n",
      "Total training time: 277.42 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.210590, T: 127346175, Avg. loss: 0.437519\n",
      "Total training time: 279.80 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 2.82, NNZs: 9992, Bias: -0.210379, T: 128632500, Avg. loss: 0.437411\n",
      "Total training time: 282.17 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5.46, NNZs: 9992, Bias: -0.425420, T: 1286325, Avg. loss: 1.215967\n",
      "Total training time: 2.29 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.68, NNZs: 9992, Bias: -0.433723, T: 2572650, Avg. loss: 0.746308\n",
      "Total training time: 4.61 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.10, NNZs: 9992, Bias: -0.438387, T: 3858975, Avg. loss: 0.574948\n",
      "Total training time: 6.93 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.79, NNZs: 9992, Bias: -0.441950, T: 5145300, Avg. loss: 0.485286\n",
      "Total training time: 9.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.61, NNZs: 9992, Bias: -0.444480, T: 6431625, Avg. loss: 0.429391\n",
      "Total training time: 11.52 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.51, NNZs: 9992, Bias: -0.446756, T: 7717950, Avg. loss: 0.391118\n",
      "Total training time: 13.83 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.43, NNZs: 9992, Bias: -0.448508, T: 9004275, Avg. loss: 0.363227\n",
      "Total training time: 16.13 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.37, NNZs: 9992, Bias: -0.450011, T: 10290600, Avg. loss: 0.341991\n",
      "Total training time: 18.43 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.34, NNZs: 9992, Bias: -0.451359, T: 11576925, Avg. loss: 0.325188\n",
      "Total training time: 20.74 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.30, NNZs: 9992, Bias: -0.452497, T: 12863250, Avg. loss: 0.311584\n",
      "Total training time: 23.04 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.27, NNZs: 9992, Bias: -0.453568, T: 14149575, Avg. loss: 0.300324\n",
      "Total training time: 25.37 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.25, NNZs: 9992, Bias: -0.454530, T: 15435900, Avg. loss: 0.290829\n",
      "Total training time: 27.70 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.24, NNZs: 9992, Bias: -0.455349, T: 16722225, Avg. loss: 0.282709\n",
      "Total training time: 30.07 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.22, NNZs: 9992, Bias: -0.456186, T: 18008550, Avg. loss: 0.275706\n",
      "Total training time: 32.45 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.20, NNZs: 9992, Bias: -0.456898, T: 19294875, Avg. loss: 0.269587\n",
      "Total training time: 34.84 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.19, NNZs: 9992, Bias: -0.457609, T: 20581200, Avg. loss: 0.264179\n",
      "Total training time: 37.21 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.18, NNZs: 9992, Bias: -0.458208, T: 21867525, Avg. loss: 0.259378\n",
      "Total training time: 39.57 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.17, NNZs: 9992, Bias: -0.458843, T: 23153850, Avg. loss: 0.255082\n",
      "Total training time: 41.90 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.16, NNZs: 9992, Bias: -0.459458, T: 24440175, Avg. loss: 0.251230\n",
      "Total training time: 44.15 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.16, NNZs: 9992, Bias: -0.459997, T: 25726500, Avg. loss: 0.247730\n",
      "Total training time: 46.40 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.16, NNZs: 9992, Bias: -0.460535, T: 27012825, Avg. loss: 0.244548\n",
      "Total training time: 48.68 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2.15, NNZs: 9992, Bias: -0.461010, T: 28299150, Avg. loss: 0.241661\n",
      "Total training time: 50.92 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 2.15, NNZs: 9992, Bias: -0.461493, T: 29585475, Avg. loss: 0.238994\n",
      "Total training time: 53.19 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 2.14, NNZs: 9992, Bias: -0.461958, T: 30871800, Avg. loss: 0.236544\n",
      "Total training time: 55.45 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 2.14, NNZs: 9992, Bias: -0.462372, T: 32158125, Avg. loss: 0.234277\n",
      "Total training time: 57.75 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.13, NNZs: 9992, Bias: -0.462772, T: 33444450, Avg. loss: 0.232178\n",
      "Total training time: 60.08 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 2.13, NNZs: 9992, Bias: -0.463180, T: 34730775, Avg. loss: 0.230224\n",
      "Total training time: 62.40 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 2.13, NNZs: 9992, Bias: -0.463575, T: 36017100, Avg. loss: 0.228403\n",
      "Total training time: 64.72 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 2.13, NNZs: 9992, Bias: -0.463946, T: 37303425, Avg. loss: 0.226707\n",
      "Total training time: 67.04 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 2.12, NNZs: 9992, Bias: -0.464303, T: 38589750, Avg. loss: 0.225117\n",
      "Total training time: 69.38 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 2.12, NNZs: 9992, Bias: -0.464639, T: 39876075, Avg. loss: 0.223623\n",
      "Total training time: 71.67 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 2.12, NNZs: 9992, Bias: -0.464947, T: 41162400, Avg. loss: 0.222210\n",
      "Total training time: 73.98 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 2.12, NNZs: 9992, Bias: -0.465288, T: 42448725, Avg. loss: 0.220887\n",
      "Total training time: 76.30 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 2.12, NNZs: 9992, Bias: -0.465579, T: 43735050, Avg. loss: 0.219640\n",
      "Total training time: 78.61 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 2.12, NNZs: 9992, Bias: -0.465850, T: 45021375, Avg. loss: 0.218458\n",
      "Total training time: 80.94 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 2.12, NNZs: 9992, Bias: -0.466120, T: 46307700, Avg. loss: 0.217340\n",
      "Total training time: 83.26 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 2.11, NNZs: 9992, Bias: -0.466424, T: 47594025, Avg. loss: 0.216280\n",
      "Total training time: 85.58 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 2.11, NNZs: 9992, Bias: -0.466662, T: 48880350, Avg. loss: 0.215266\n",
      "Total training time: 87.89 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 2.11, NNZs: 9992, Bias: -0.466986, T: 50166675, Avg. loss: 0.214314\n",
      "Total training time: 90.21 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 2.11, NNZs: 9992, Bias: -0.467235, T: 51453000, Avg. loss: 0.213398\n",
      "Total training time: 92.54 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 2.11, NNZs: 9992, Bias: -0.467512, T: 52739325, Avg. loss: 0.212529\n",
      "Total training time: 94.85 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 2.11, NNZs: 9992, Bias: -0.467740, T: 54025650, Avg. loss: 0.211698\n",
      "Total training time: 97.16 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 2.11, NNZs: 9992, Bias: -0.467991, T: 55311975, Avg. loss: 0.210902\n",
      "Total training time: 99.44 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 2.11, NNZs: 9992, Bias: -0.468226, T: 56598300, Avg. loss: 0.210146\n",
      "Total training time: 101.67 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 2.11, NNZs: 9992, Bias: -0.468437, T: 57884625, Avg. loss: 0.209416\n",
      "Total training time: 103.95 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 2.11, NNZs: 9992, Bias: -0.468669, T: 59170950, Avg. loss: 0.208717\n",
      "Total training time: 106.22 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.468886, T: 60457275, Avg. loss: 0.208051\n",
      "Total training time: 108.50 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 2.11, NNZs: 9992, Bias: -0.469118, T: 61743600, Avg. loss: 0.207411\n",
      "Total training time: 110.77 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.469330, T: 63029925, Avg. loss: 0.206794\n",
      "Total training time: 113.09 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 2.11, NNZs: 9992, Bias: -0.469530, T: 64316250, Avg. loss: 0.206199\n",
      "Total training time: 115.45 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.469717, T: 65602575, Avg. loss: 0.205620\n",
      "Total training time: 117.78 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.469922, T: 66888900, Avg. loss: 0.205073\n",
      "Total training time: 120.12 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.470103, T: 68175225, Avg. loss: 0.204539\n",
      "Total training time: 122.46 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.470302, T: 69461550, Avg. loss: 0.204027\n",
      "Total training time: 124.80 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.470474, T: 70747875, Avg. loss: 0.203535\n",
      "Total training time: 127.13 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.470655, T: 72034200, Avg. loss: 0.203057\n",
      "Total training time: 129.48 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.470833, T: 73320525, Avg. loss: 0.202596\n",
      "Total training time: 131.82 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.470999, T: 74606850, Avg. loss: 0.202146\n",
      "Total training time: 134.17 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.471178, T: 75893175, Avg. loss: 0.201712\n",
      "Total training time: 136.53 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.471336, T: 77179500, Avg. loss: 0.201292\n",
      "Total training time: 138.87 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.471504, T: 78465825, Avg. loss: 0.200886\n",
      "Total training time: 141.23 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.471663, T: 79752150, Avg. loss: 0.200494\n",
      "Total training time: 143.57 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.471836, T: 81038475, Avg. loss: 0.200116\n",
      "Total training time: 145.92 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.471977, T: 82324800, Avg. loss: 0.199744\n",
      "Total training time: 148.29 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.472133, T: 83611125, Avg. loss: 0.199384\n",
      "Total training time: 150.66 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.472268, T: 84897450, Avg. loss: 0.199033\n",
      "Total training time: 153.02 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.472428, T: 86183775, Avg. loss: 0.198694\n",
      "Total training time: 155.33 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.472576, T: 87470100, Avg. loss: 0.198362\n",
      "Total training time: 157.63 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.472720, T: 88756425, Avg. loss: 0.198039\n",
      "Total training time: 159.89 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.472856, T: 90042750, Avg. loss: 0.197727\n",
      "Total training time: 162.16 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.472996, T: 91329075, Avg. loss: 0.197422\n",
      "Total training time: 164.40 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.473122, T: 92615400, Avg. loss: 0.197126\n",
      "Total training time: 166.66 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.473252, T: 93901725, Avg. loss: 0.196841\n",
      "Total training time: 168.94 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.473391, T: 95188050, Avg. loss: 0.196562\n",
      "Total training time: 171.25 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.473525, T: 96474375, Avg. loss: 0.196289\n",
      "Total training time: 173.58 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.473661, T: 97760700, Avg. loss: 0.196022\n",
      "Total training time: 175.92 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.473790, T: 99047025, Avg. loss: 0.195762\n",
      "Total training time: 178.25 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.473915, T: 100333350, Avg. loss: 0.195510\n",
      "Total training time: 180.58 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.474035, T: 101619675, Avg. loss: 0.195262\n",
      "Total training time: 182.89 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.474158, T: 102906000, Avg. loss: 0.195021\n",
      "Total training time: 185.20 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.474297, T: 104192325, Avg. loss: 0.194787\n",
      "Total training time: 187.53 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.474411, T: 105478650, Avg. loss: 0.194560\n",
      "Total training time: 189.87 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.474523, T: 106764975, Avg. loss: 0.194335\n",
      "Total training time: 192.24 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.474656, T: 108051300, Avg. loss: 0.194118\n",
      "Total training time: 194.58 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.474761, T: 109337625, Avg. loss: 0.193904\n",
      "Total training time: 197.02 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.474885, T: 110623950, Avg. loss: 0.193694\n",
      "Total training time: 199.57 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.474991, T: 111910275, Avg. loss: 0.193489\n",
      "Total training time: 201.97 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.475110, T: 113196600, Avg. loss: 0.193288\n",
      "Total training time: 204.34 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.475206, T: 114482925, Avg. loss: 0.193091\n",
      "Total training time: 206.69 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.475323, T: 115769250, Avg. loss: 0.192898\n",
      "Total training time: 209.02 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.475426, T: 117055575, Avg. loss: 0.192711\n",
      "Total training time: 211.31 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.475531, T: 118341900, Avg. loss: 0.192527\n",
      "Total training time: 213.58 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.475647, T: 119628225, Avg. loss: 0.192347\n",
      "Total training time: 215.84 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.475736, T: 120914550, Avg. loss: 0.192172\n",
      "Total training time: 218.10 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.475834, T: 122200875, Avg. loss: 0.191998\n",
      "Total training time: 220.38 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.475937, T: 123487200, Avg. loss: 0.191830\n",
      "Total training time: 222.64 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.476033, T: 124773525, Avg. loss: 0.191664\n",
      "Total training time: 224.94 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.476126, T: 126059850, Avg. loss: 0.191501\n",
      "Total training time: 227.26 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.476228, T: 127346175, Avg. loss: 0.191339\n",
      "Total training time: 229.65 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 2.10, NNZs: 9992, Bias: -0.476342, T: 128632500, Avg. loss: 0.191183\n",
      "Total training time: 232.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 32.20, NNZs: 9992, Bias: 0.639200, T: 1286325, Avg. loss: 7.636219\n",
      "Total training time: 2.27 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 20.46, NNZs: 9992, Bias: 0.619619, T: 2572650, Avg. loss: 4.231423\n",
      "Total training time: 4.49 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 15.90, NNZs: 9992, Bias: 0.605794, T: 3858975, Avg. loss: 2.992658\n",
      "Total training time: 6.71 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 13.51, NNZs: 9992, Bias: 0.595348, T: 5145300, Avg. loss: 2.344167\n",
      "Total training time: 8.94 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.08, NNZs: 9992, Bias: 0.586106, T: 6431625, Avg. loss: 1.942608\n",
      "Total training time: 11.15 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 11.14, NNZs: 9992, Bias: 0.580237, T: 7717950, Avg. loss: 1.668575\n",
      "Total training time: 13.36 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 10.40, NNZs: 9992, Bias: 0.574362, T: 9004275, Avg. loss: 1.468484\n",
      "Total training time: 15.56 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 9.84, NNZs: 9992, Bias: 0.568879, T: 10290600, Avg. loss: 1.316204\n",
      "Total training time: 17.77 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 9.44, NNZs: 9992, Bias: 0.564692, T: 11576925, Avg. loss: 1.196529\n",
      "Total training time: 19.97 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 9.07, NNZs: 9992, Bias: 0.560799, T: 12863250, Avg. loss: 1.099338\n",
      "Total training time: 22.18 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 8.77, NNZs: 9992, Bias: 0.556219, T: 14149575, Avg. loss: 1.019064\n",
      "Total training time: 24.38 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 8.51, NNZs: 9992, Bias: 0.552804, T: 15435900, Avg. loss: 0.951755\n",
      "Total training time: 26.58 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 8.28, NNZs: 9992, Bias: 0.549444, T: 16722225, Avg. loss: 0.893974\n",
      "Total training time: 28.78 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 8.10, NNZs: 9992, Bias: 0.546331, T: 18008550, Avg. loss: 0.844293\n",
      "Total training time: 30.98 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 7.92, NNZs: 9992, Bias: 0.543541, T: 19294875, Avg. loss: 0.800780\n",
      "Total training time: 33.14 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 7.79, NNZs: 9992, Bias: 0.540808, T: 20581200, Avg. loss: 0.762643\n",
      "Total training time: 35.26 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 7.68, NNZs: 9992, Bias: 0.538225, T: 21867525, Avg. loss: 0.729039\n",
      "Total training time: 37.39 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 7.57, NNZs: 9992, Bias: 0.535713, T: 23153850, Avg. loss: 0.698880\n",
      "Total training time: 39.51 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 7.48, NNZs: 9992, Bias: 0.533435, T: 24440175, Avg. loss: 0.671711\n",
      "Total training time: 41.64 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 7.38, NNZs: 9992, Bias: 0.530911, T: 25726500, Avg. loss: 0.647247\n",
      "Total training time: 43.77 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 7.30, NNZs: 9992, Bias: 0.529097, T: 27012825, Avg. loss: 0.624913\n",
      "Total training time: 45.91 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 7.24, NNZs: 9992, Bias: 0.527021, T: 28299150, Avg. loss: 0.604472\n",
      "Total training time: 48.14 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 7.17, NNZs: 9992, Bias: 0.524932, T: 29585475, Avg. loss: 0.585816\n",
      "Total training time: 50.36 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 7.12, NNZs: 9992, Bias: 0.523085, T: 30871800, Avg. loss: 0.568699\n",
      "Total training time: 52.59 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 7.05, NNZs: 9992, Bias: 0.521234, T: 32158125, Avg. loss: 0.552779\n",
      "Total training time: 54.80 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 7.00, NNZs: 9992, Bias: 0.519173, T: 33444450, Avg. loss: 0.538137\n",
      "Total training time: 57.04 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 6.96, NNZs: 9992, Bias: 0.517590, T: 34730775, Avg. loss: 0.524548\n",
      "Total training time: 59.25 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 6.92, NNZs: 9992, Bias: 0.515975, T: 36017100, Avg. loss: 0.511809\n",
      "Total training time: 61.47 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 6.88, NNZs: 9992, Bias: 0.514245, T: 37303425, Avg. loss: 0.499963\n",
      "Total training time: 63.68 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 6.85, NNZs: 9992, Bias: 0.512666, T: 38589750, Avg. loss: 0.488905\n",
      "Total training time: 65.90 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 6.82, NNZs: 9992, Bias: 0.511271, T: 39876075, Avg. loss: 0.478526\n",
      "Total training time: 68.13 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 6.78, NNZs: 9992, Bias: 0.509839, T: 41162400, Avg. loss: 0.468738\n",
      "Total training time: 70.35 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 6.75, NNZs: 9992, Bias: 0.508324, T: 42448725, Avg. loss: 0.459480\n",
      "Total training time: 72.58 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 6.73, NNZs: 9992, Bias: 0.507147, T: 43735050, Avg. loss: 0.450858\n",
      "Total training time: 74.82 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 6.71, NNZs: 9992, Bias: 0.505703, T: 45021375, Avg. loss: 0.442675\n",
      "Total training time: 77.04 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 6.69, NNZs: 9992, Bias: 0.504369, T: 46307700, Avg. loss: 0.434965\n",
      "Total training time: 79.27 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 6.66, NNZs: 9992, Bias: 0.503019, T: 47594025, Avg. loss: 0.427618\n",
      "Total training time: 81.49 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 6.64, NNZs: 9992, Bias: 0.501739, T: 48880350, Avg. loss: 0.420617\n",
      "Total training time: 83.70 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 6.62, NNZs: 9992, Bias: 0.500566, T: 50166675, Avg. loss: 0.413956\n",
      "Total training time: 85.90 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 6.60, NNZs: 9992, Bias: 0.499472, T: 51453000, Avg. loss: 0.407575\n",
      "Total training time: 88.08 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 6.58, NNZs: 9992, Bias: 0.498403, T: 52739325, Avg. loss: 0.401530\n",
      "Total training time: 90.24 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 6.56, NNZs: 9992, Bias: 0.497075, T: 54025650, Avg. loss: 0.395801\n",
      "Total training time: 92.38 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 6.54, NNZs: 9992, Bias: 0.496115, T: 55311975, Avg. loss: 0.390294\n",
      "Total training time: 94.53 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 6.53, NNZs: 9992, Bias: 0.495039, T: 56598300, Avg. loss: 0.385044\n",
      "Total training time: 96.67 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 6.51, NNZs: 9992, Bias: 0.493937, T: 57884625, Avg. loss: 0.380013\n",
      "Total training time: 98.81 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 6.50, NNZs: 9992, Bias: 0.492816, T: 59170950, Avg. loss: 0.375236\n",
      "Total training time: 100.95 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 6.48, NNZs: 9992, Bias: 0.491846, T: 60457275, Avg. loss: 0.370610\n",
      "Total training time: 103.13 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 6.47, NNZs: 9992, Bias: 0.490836, T: 61743600, Avg. loss: 0.366183\n",
      "Total training time: 105.32 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 6.46, NNZs: 9992, Bias: 0.489791, T: 63029925, Avg. loss: 0.361924\n",
      "Total training time: 107.47 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 6.45, NNZs: 9992, Bias: 0.488862, T: 64316250, Avg. loss: 0.357834\n",
      "Total training time: 109.61 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 6.44, NNZs: 9992, Bias: 0.487983, T: 65602575, Avg. loss: 0.353879\n",
      "Total training time: 111.75 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 6.43, NNZs: 9992, Bias: 0.487105, T: 66888900, Avg. loss: 0.350093\n",
      "Total training time: 113.88 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 6.41, NNZs: 9992, Bias: 0.486218, T: 68175225, Avg. loss: 0.346426\n",
      "Total training time: 116.01 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 6.40, NNZs: 9992, Bias: 0.485451, T: 69461550, Avg. loss: 0.342885\n",
      "Total training time: 118.14 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 6.39, NNZs: 9992, Bias: 0.484585, T: 70747875, Avg. loss: 0.339484\n",
      "Total training time: 120.29 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 6.38, NNZs: 9992, Bias: 0.483748, T: 72034200, Avg. loss: 0.336186\n",
      "Total training time: 122.43 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 6.37, NNZs: 9992, Bias: 0.482907, T: 73320525, Avg. loss: 0.333010\n",
      "Total training time: 124.57 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 6.36, NNZs: 9992, Bias: 0.482002, T: 74606850, Avg. loss: 0.329950\n",
      "Total training time: 126.71 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 6.35, NNZs: 9992, Bias: 0.481136, T: 75893175, Avg. loss: 0.326981\n",
      "Total training time: 128.86 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 6.34, NNZs: 9992, Bias: 0.480425, T: 77179500, Avg. loss: 0.324094\n",
      "Total training time: 131.00 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 6.34, NNZs: 9992, Bias: 0.479507, T: 78465825, Avg. loss: 0.321320\n",
      "Total training time: 133.14 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 6.33, NNZs: 9992, Bias: 0.478763, T: 79752150, Avg. loss: 0.318632\n",
      "Total training time: 135.29 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 6.33, NNZs: 9992, Bias: 0.478104, T: 81038475, Avg. loss: 0.316025\n",
      "Total training time: 137.43 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 6.32, NNZs: 9992, Bias: 0.477322, T: 82324800, Avg. loss: 0.313499\n",
      "Total training time: 139.58 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 6.31, NNZs: 9992, Bias: 0.476532, T: 83611125, Avg. loss: 0.311043\n",
      "Total training time: 141.72 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 6.31, NNZs: 9992, Bias: 0.475907, T: 84897450, Avg. loss: 0.308653\n",
      "Total training time: 143.85 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 6.30, NNZs: 9992, Bias: 0.475238, T: 86183775, Avg. loss: 0.306342\n",
      "Total training time: 145.94 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 6.30, NNZs: 9992, Bias: 0.474491, T: 87470100, Avg. loss: 0.304097\n",
      "Total training time: 148.20 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 6.29, NNZs: 9992, Bias: 0.473768, T: 88756425, Avg. loss: 0.301920\n",
      "Total training time: 150.46 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 6.29, NNZs: 9992, Bias: 0.473137, T: 90042750, Avg. loss: 0.299807\n",
      "Total training time: 152.65 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 6.28, NNZs: 9992, Bias: 0.472399, T: 91329075, Avg. loss: 0.297746\n",
      "Total training time: 154.77 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 6.28, NNZs: 9992, Bias: 0.471776, T: 92615400, Avg. loss: 0.295723\n",
      "Total training time: 156.88 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 6.27, NNZs: 9992, Bias: 0.471165, T: 93901725, Avg. loss: 0.293756\n",
      "Total training time: 159.08 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 6.26, NNZs: 9992, Bias: 0.470397, T: 95188050, Avg. loss: 0.291846\n",
      "Total training time: 161.26 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 6.26, NNZs: 9992, Bias: 0.469873, T: 96474375, Avg. loss: 0.289976\n",
      "Total training time: 163.44 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 6.26, NNZs: 9992, Bias: 0.469238, T: 97760700, Avg. loss: 0.288179\n",
      "Total training time: 165.61 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 6.25, NNZs: 9992, Bias: 0.468582, T: 99047025, Avg. loss: 0.286411\n",
      "Total training time: 167.80 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 6.25, NNZs: 9992, Bias: 0.467986, T: 100333350, Avg. loss: 0.284707\n",
      "Total training time: 169.98 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 6.24, NNZs: 9992, Bias: 0.467417, T: 101619675, Avg. loss: 0.283021\n",
      "Total training time: 172.15 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 6.24, NNZs: 9992, Bias: 0.466813, T: 102906000, Avg. loss: 0.281393\n",
      "Total training time: 174.33 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 6.24, NNZs: 9992, Bias: 0.466135, T: 104192325, Avg. loss: 0.279790\n",
      "Total training time: 176.51 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 6.23, NNZs: 9992, Bias: 0.465612, T: 105478650, Avg. loss: 0.278231\n",
      "Total training time: 178.72 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 6.23, NNZs: 9992, Bias: 0.465029, T: 106764975, Avg. loss: 0.276706\n",
      "Total training time: 180.92 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 6.22, NNZs: 9992, Bias: 0.464374, T: 108051300, Avg. loss: 0.275220\n",
      "Total training time: 183.12 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 6.22, NNZs: 9992, Bias: 0.463889, T: 109337625, Avg. loss: 0.273759\n",
      "Total training time: 185.31 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 6.22, NNZs: 9992, Bias: 0.463369, T: 110623950, Avg. loss: 0.272330\n",
      "Total training time: 187.50 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 6.21, NNZs: 9992, Bias: 0.462806, T: 111910275, Avg. loss: 0.270943\n",
      "Total training time: 189.69 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 6.21, NNZs: 9992, Bias: 0.462247, T: 113196600, Avg. loss: 0.269581\n",
      "Total training time: 191.90 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 6.21, NNZs: 9992, Bias: 0.461687, T: 114482925, Avg. loss: 0.268254\n",
      "Total training time: 194.08 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 6.20, NNZs: 9992, Bias: 0.461148, T: 115769250, Avg. loss: 0.266959\n",
      "Total training time: 196.26 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 6.20, NNZs: 9992, Bias: 0.460682, T: 117055575, Avg. loss: 0.265679\n",
      "Total training time: 198.46 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 6.20, NNZs: 9992, Bias: 0.460169, T: 118341900, Avg. loss: 0.264436\n",
      "Total training time: 200.63 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 6.19, NNZs: 9992, Bias: 0.459608, T: 119628225, Avg. loss: 0.263211\n",
      "Total training time: 202.75 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 6.19, NNZs: 9992, Bias: 0.459085, T: 120914550, Avg. loss: 0.262011\n",
      "Total training time: 204.88 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 6.19, NNZs: 9992, Bias: 0.458623, T: 122200875, Avg. loss: 0.260821\n",
      "Total training time: 207.04 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 6.18, NNZs: 9992, Bias: 0.458214, T: 123487200, Avg. loss: 0.259671\n",
      "Total training time: 209.20 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 6.18, NNZs: 9992, Bias: 0.457677, T: 124773525, Avg. loss: 0.258550\n",
      "Total training time: 211.35 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 6.18, NNZs: 9992, Bias: 0.457177, T: 126059850, Avg. loss: 0.257441\n",
      "Total training time: 213.52 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 6.17, NNZs: 9992, Bias: 0.456707, T: 127346175, Avg. loss: 0.256347\n",
      "Total training time: 215.75 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 6.17, NNZs: 9992, Bias: 0.456235, T: 128632500, Avg. loss: 0.255289\n",
      "Total training time: 217.98 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 9.98, NNZs: 9146, Bias: 0.336372, T: 1286325, Avg. loss: 1.784511\n",
      "Total training time: 2.26 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.99, NNZs: 9188, Bias: 0.319001, T: 2572650, Avg. loss: 1.081081\n",
      "Total training time: 4.50 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.99, NNZs: 9220, Bias: 0.307448, T: 3858975, Avg. loss: 0.826925\n",
      "Total training time: 6.75 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.52, NNZs: 9240, Bias: 0.298833, T: 5145300, Avg. loss: 0.694060\n",
      "Total training time: 9.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.22, NNZs: 9251, Bias: 0.292224, T: 6431625, Avg. loss: 0.611900\n",
      "Total training time: 11.25 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 5.00, NNZs: 9260, Bias: 0.286435, T: 7717950, Avg. loss: 0.555798\n",
      "Total training time: 13.50 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 4.87, NNZs: 9262, Bias: 0.282237, T: 9004275, Avg. loss: 0.514977\n",
      "Total training time: 15.74 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 4.76, NNZs: 9263, Bias: 0.278360, T: 10290600, Avg. loss: 0.483633\n",
      "Total training time: 17.99 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 4.68, NNZs: 9263, Bias: 0.275006, T: 11576925, Avg. loss: 0.459231\n",
      "Total training time: 20.25 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 4.63, NNZs: 9264, Bias: 0.271903, T: 12863250, Avg. loss: 0.439617\n",
      "Total training time: 22.50 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 4.57, NNZs: 9265, Bias: 0.269018, T: 14149575, Avg. loss: 0.423112\n",
      "Total training time: 24.74 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 4.53, NNZs: 9265, Bias: 0.266374, T: 15435900, Avg. loss: 0.409364\n",
      "Total training time: 27.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 4.51, NNZs: 9266, Bias: 0.263961, T: 16722225, Avg. loss: 0.397592\n",
      "Total training time: 29.26 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 4.49, NNZs: 9266, Bias: 0.261694, T: 18008550, Avg. loss: 0.387502\n",
      "Total training time: 31.51 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 4.45, NNZs: 9266, Bias: 0.259471, T: 19294875, Avg. loss: 0.378639\n",
      "Total training time: 33.79 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 4.43, NNZs: 9266, Bias: 0.257724, T: 20581200, Avg. loss: 0.370829\n",
      "Total training time: 36.02 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 4.41, NNZs: 9266, Bias: 0.255884, T: 21867525, Avg. loss: 0.363972\n",
      "Total training time: 38.22 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 4.40, NNZs: 9267, Bias: 0.254172, T: 23153850, Avg. loss: 0.357820\n",
      "Total training time: 40.40 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 4.38, NNZs: 9267, Bias: 0.252539, T: 24440175, Avg. loss: 0.352267\n",
      "Total training time: 42.56 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 4.37, NNZs: 9267, Bias: 0.251055, T: 25726500, Avg. loss: 0.347219\n",
      "Total training time: 44.70 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 4.37, NNZs: 9267, Bias: 0.249638, T: 27012825, Avg. loss: 0.342727\n",
      "Total training time: 46.86 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 4.35, NNZs: 9267, Bias: 0.248340, T: 28299150, Avg. loss: 0.338580\n",
      "Total training time: 49.04 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 4.35, NNZs: 9267, Bias: 0.247130, T: 29585475, Avg. loss: 0.334771\n",
      "Total training time: 51.22 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 4.34, NNZs: 9268, Bias: 0.245813, T: 30871800, Avg. loss: 0.331295\n",
      "Total training time: 53.43 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 4.34, NNZs: 9268, Bias: 0.244794, T: 32158125, Avg. loss: 0.328092\n",
      "Total training time: 55.69 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 4.33, NNZs: 9268, Bias: 0.243604, T: 33444450, Avg. loss: 0.325125\n",
      "Total training time: 57.94 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 4.32, NNZs: 9268, Bias: 0.242506, T: 34730775, Avg. loss: 0.322330\n",
      "Total training time: 60.20 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 4.32, NNZs: 9268, Bias: 0.241507, T: 36017100, Avg. loss: 0.319744\n",
      "Total training time: 62.45 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 4.31, NNZs: 9268, Bias: 0.240473, T: 37303425, Avg. loss: 0.317311\n",
      "Total training time: 64.70 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 4.31, NNZs: 9268, Bias: 0.239514, T: 38589750, Avg. loss: 0.315060\n",
      "Total training time: 66.95 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 4.30, NNZs: 9268, Bias: 0.238557, T: 39876075, Avg. loss: 0.312945\n",
      "Total training time: 69.18 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 4.30, NNZs: 9268, Bias: 0.237711, T: 41162400, Avg. loss: 0.310957\n",
      "Total training time: 71.40 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 4.29, NNZs: 9268, Bias: 0.236833, T: 42448725, Avg. loss: 0.309081\n",
      "Total training time: 73.64 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 4.29, NNZs: 9268, Bias: 0.235915, T: 43735050, Avg. loss: 0.307333\n",
      "Total training time: 75.89 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 4.29, NNZs: 9268, Bias: 0.235105, T: 45021375, Avg. loss: 0.305651\n",
      "Total training time: 78.16 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 4.28, NNZs: 9268, Bias: 0.234286, T: 46307700, Avg. loss: 0.304048\n",
      "Total training time: 80.42 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 4.28, NNZs: 9268, Bias: 0.233554, T: 47594025, Avg. loss: 0.302552\n",
      "Total training time: 82.69 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 4.28, NNZs: 9268, Bias: 0.232730, T: 48880350, Avg. loss: 0.301118\n",
      "Total training time: 84.95 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 4.28, NNZs: 9268, Bias: 0.231974, T: 50166675, Avg. loss: 0.299762\n",
      "Total training time: 87.23 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 4.27, NNZs: 9268, Bias: 0.231238, T: 51453000, Avg. loss: 0.298483\n",
      "Total training time: 89.50 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 4.27, NNZs: 9268, Bias: 0.230515, T: 52739325, Avg. loss: 0.297251\n",
      "Total training time: 91.75 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 4.27, NNZs: 9268, Bias: 0.229807, T: 54025650, Avg. loss: 0.296073\n",
      "Total training time: 93.99 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 4.27, NNZs: 9268, Bias: 0.229105, T: 55311975, Avg. loss: 0.294956\n",
      "Total training time: 96.19 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 4.27, NNZs: 9268, Bias: 0.228456, T: 56598300, Avg. loss: 0.293889\n",
      "Total training time: 98.38 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 4.27, NNZs: 9268, Bias: 0.227838, T: 57884625, Avg. loss: 0.292855\n",
      "Total training time: 100.57 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 4.27, NNZs: 9268, Bias: 0.227230, T: 59170950, Avg. loss: 0.291865\n",
      "Total training time: 102.76 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 4.26, NNZs: 9268, Bias: 0.226580, T: 60457275, Avg. loss: 0.290910\n",
      "Total training time: 104.94 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 4.26, NNZs: 9268, Bias: 0.225990, T: 61743600, Avg. loss: 0.290000\n",
      "Total training time: 107.13 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 4.26, NNZs: 9268, Bias: 0.225398, T: 63029925, Avg. loss: 0.289116\n",
      "Total training time: 109.35 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 4.26, NNZs: 9268, Bias: 0.224847, T: 64316250, Avg. loss: 0.288292\n",
      "Total training time: 111.57 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 4.26, NNZs: 9268, Bias: 0.224287, T: 65602575, Avg. loss: 0.287480\n",
      "Total training time: 113.80 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 4.26, NNZs: 9268, Bias: 0.223730, T: 66888900, Avg. loss: 0.286723\n",
      "Total training time: 116.03 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 4.26, NNZs: 9268, Bias: 0.223188, T: 68175225, Avg. loss: 0.285982\n",
      "Total training time: 118.27 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 4.26, NNZs: 9268, Bias: 0.222685, T: 69461550, Avg. loss: 0.285268\n",
      "Total training time: 120.49 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 4.26, NNZs: 9268, Bias: 0.222139, T: 70747875, Avg. loss: 0.284569\n",
      "Total training time: 122.72 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 4.26, NNZs: 9268, Bias: 0.221657, T: 72034200, Avg. loss: 0.283896\n",
      "Total training time: 124.95 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 4.25, NNZs: 9268, Bias: 0.221145, T: 73320525, Avg. loss: 0.283243\n",
      "Total training time: 127.18 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 4.25, NNZs: 9268, Bias: 0.220679, T: 74606850, Avg. loss: 0.282612\n",
      "Total training time: 129.46 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 4.25, NNZs: 9268, Bias: 0.220196, T: 75893175, Avg. loss: 0.282004\n",
      "Total training time: 131.72 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 4.25, NNZs: 9268, Bias: 0.219715, T: 77179500, Avg. loss: 0.281413\n",
      "Total training time: 133.97 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 4.25, NNZs: 9268, Bias: 0.219282, T: 78465825, Avg. loss: 0.280842\n",
      "Total training time: 136.23 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 4.25, NNZs: 9268, Bias: 0.218792, T: 79752150, Avg. loss: 0.280290\n",
      "Total training time: 138.51 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 4.25, NNZs: 9268, Bias: 0.218339, T: 81038475, Avg. loss: 0.279753\n",
      "Total training time: 140.77 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 4.25, NNZs: 9268, Bias: 0.217882, T: 82324800, Avg. loss: 0.279231\n",
      "Total training time: 143.02 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 4.25, NNZs: 9268, Bias: 0.217442, T: 83611125, Avg. loss: 0.278733\n",
      "Total training time: 145.30 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 4.25, NNZs: 9268, Bias: 0.217006, T: 84897450, Avg. loss: 0.278248\n",
      "Total training time: 147.56 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 4.25, NNZs: 9268, Bias: 0.216617, T: 86183775, Avg. loss: 0.277778\n",
      "Total training time: 149.78 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 4.25, NNZs: 9268, Bias: 0.216258, T: 87470100, Avg. loss: 0.277313\n",
      "Total training time: 151.95 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 4.25, NNZs: 9268, Bias: 0.215820, T: 88756425, Avg. loss: 0.276871\n",
      "Total training time: 154.10 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 4.25, NNZs: 9268, Bias: 0.215427, T: 90042750, Avg. loss: 0.276433\n",
      "Total training time: 156.24 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.215026, T: 91329075, Avg. loss: 0.276016\n",
      "Total training time: 158.39 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.214626, T: 92615400, Avg. loss: 0.275611\n",
      "Total training time: 160.54 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.214258, T: 93901725, Avg. loss: 0.275205\n",
      "Total training time: 162.71 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.213870, T: 95188050, Avg. loss: 0.274818\n",
      "Total training time: 164.90 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.213475, T: 96474375, Avg. loss: 0.274438\n",
      "Total training time: 167.13 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.213142, T: 97760700, Avg. loss: 0.274064\n",
      "Total training time: 169.35 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.212775, T: 99047025, Avg. loss: 0.273706\n",
      "Total training time: 171.57 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.212418, T: 100333350, Avg. loss: 0.273353\n",
      "Total training time: 173.80 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.212057, T: 101619675, Avg. loss: 0.273005\n",
      "Total training time: 176.03 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.211711, T: 102906000, Avg. loss: 0.272662\n",
      "Total training time: 178.25 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.211375, T: 104192325, Avg. loss: 0.272330\n",
      "Total training time: 180.46 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.211046, T: 105478650, Avg. loss: 0.272005\n",
      "Total training time: 182.69 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.210673, T: 106764975, Avg. loss: 0.271692\n",
      "Total training time: 184.91 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.210324, T: 108051300, Avg. loss: 0.271386\n",
      "Total training time: 187.14 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.210032, T: 109337625, Avg. loss: 0.271086\n",
      "Total training time: 189.39 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.209662, T: 110623950, Avg. loss: 0.270798\n",
      "Total training time: 191.60 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.209383, T: 111910275, Avg. loss: 0.270512\n",
      "Total training time: 193.83 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.209056, T: 113196600, Avg. loss: 0.270231\n",
      "Total training time: 196.04 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.208724, T: 114482925, Avg. loss: 0.269954\n",
      "Total training time: 198.26 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.208425, T: 115769250, Avg. loss: 0.269693\n",
      "Total training time: 200.46 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.208112, T: 117055575, Avg. loss: 0.269432\n",
      "Total training time: 202.67 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.207835, T: 118341900, Avg. loss: 0.269170\n",
      "Total training time: 204.89 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.207512, T: 119628225, Avg. loss: 0.268920\n",
      "Total training time: 207.07 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.207223, T: 120914550, Avg. loss: 0.268673\n",
      "Total training time: 209.22 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.206921, T: 122200875, Avg. loss: 0.268431\n",
      "Total training time: 211.39 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.206643, T: 123487200, Avg. loss: 0.268197\n",
      "Total training time: 213.55 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.206361, T: 124773525, Avg. loss: 0.267958\n",
      "Total training time: 215.70 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 4.23, NNZs: 9268, Bias: 0.206066, T: 126059850, Avg. loss: 0.267724\n",
      "Total training time: 217.85 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.205783, T: 127346175, Avg. loss: 0.267503\n",
      "Total training time: 220.01 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 4.24, NNZs: 9268, Bias: 0.205524, T: 128632500, Avg. loss: 0.267278\n",
      "Total training time: 222.26 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5.58, NNZs: 9040, Bias: -0.393420, T: 1286325, Avg. loss: 1.117822\n",
      "Total training time: 2.30 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.30, NNZs: 9121, Bias: -0.383499, T: 2572650, Avg. loss: 0.745482\n",
      "Total training time: 4.57 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.90, NNZs: 9157, Bias: -0.376713, T: 3858975, Avg. loss: 0.611987\n",
      "Total training time: 6.85 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.71, NNZs: 9172, Bias: -0.371072, T: 5145300, Avg. loss: 0.542672\n",
      "Total training time: 9.13 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.58, NNZs: 9190, Bias: -0.366387, T: 6431625, Avg. loss: 0.499903\n",
      "Total training time: 11.41 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.52, NNZs: 9197, Bias: -0.362734, T: 7717950, Avg. loss: 0.470924\n",
      "Total training time: 13.71 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.46, NNZs: 9204, Bias: -0.359343, T: 9004275, Avg. loss: 0.449792\n",
      "Total training time: 15.99 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.43, NNZs: 9206, Bias: -0.356381, T: 10290600, Avg. loss: 0.433758\n",
      "Total training time: 18.28 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.40, NNZs: 9213, Bias: -0.353763, T: 11576925, Avg. loss: 0.421124\n",
      "Total training time: 20.55 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 3.38, NNZs: 9218, Bias: -0.351298, T: 12863250, Avg. loss: 0.410894\n",
      "Total training time: 22.81 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 3.37, NNZs: 9219, Bias: -0.349134, T: 14149575, Avg. loss: 0.402447\n",
      "Total training time: 25.10 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 3.36, NNZs: 9219, Bias: -0.347220, T: 15435900, Avg. loss: 0.395412\n",
      "Total training time: 27.37 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 3.34, NNZs: 9220, Bias: -0.345423, T: 16722225, Avg. loss: 0.389382\n",
      "Total training time: 29.63 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 3.33, NNZs: 9220, Bias: -0.343808, T: 18008550, Avg. loss: 0.384204\n",
      "Total training time: 31.90 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 3.33, NNZs: 9221, Bias: -0.342222, T: 19294875, Avg. loss: 0.379673\n",
      "Total training time: 34.18 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 3.32, NNZs: 9221, Bias: -0.340692, T: 20581200, Avg. loss: 0.375670\n",
      "Total training time: 36.48 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 3.31, NNZs: 9221, Bias: -0.339368, T: 21867525, Avg. loss: 0.372118\n",
      "Total training time: 38.75 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 3.31, NNZs: 9221, Bias: -0.338063, T: 23153850, Avg. loss: 0.368972\n",
      "Total training time: 40.96 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 3.31, NNZs: 9223, Bias: -0.336815, T: 24440175, Avg. loss: 0.366132\n",
      "Total training time: 43.17 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 3.30, NNZs: 9223, Bias: -0.335638, T: 25726500, Avg. loss: 0.363569\n",
      "Total training time: 45.40 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 3.30, NNZs: 9223, Bias: -0.334553, T: 27012825, Avg. loss: 0.361241\n",
      "Total training time: 47.62 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 3.30, NNZs: 9223, Bias: -0.333461, T: 28299150, Avg. loss: 0.359101\n",
      "Total training time: 49.83 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 3.30, NNZs: 9223, Bias: -0.332443, T: 29585475, Avg. loss: 0.357149\n",
      "Total training time: 52.05 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 3.29, NNZs: 9223, Bias: -0.331531, T: 30871800, Avg. loss: 0.355359\n",
      "Total training time: 54.33 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 3.29, NNZs: 9223, Bias: -0.330565, T: 32158125, Avg. loss: 0.353716\n",
      "Total training time: 56.59 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 3.29, NNZs: 9223, Bias: -0.329703, T: 33444450, Avg. loss: 0.352183\n",
      "Total training time: 58.84 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 3.29, NNZs: 9223, Bias: -0.328833, T: 34730775, Avg. loss: 0.350760\n",
      "Total training time: 61.12 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 3.29, NNZs: 9223, Bias: -0.327991, T: 36017100, Avg. loss: 0.349434\n",
      "Total training time: 63.38 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 3.29, NNZs: 9223, Bias: -0.327205, T: 37303425, Avg. loss: 0.348203\n",
      "Total training time: 65.68 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 3.28, NNZs: 9223, Bias: -0.326445, T: 38589750, Avg. loss: 0.347044\n",
      "Total training time: 67.95 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 3.28, NNZs: 9223, Bias: -0.325701, T: 39876075, Avg. loss: 0.345955\n",
      "Total training time: 70.22 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 3.28, NNZs: 9223, Bias: -0.324945, T: 41162400, Avg. loss: 0.344937\n",
      "Total training time: 72.50 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 3.28, NNZs: 9223, Bias: -0.324264, T: 42448725, Avg. loss: 0.343981\n",
      "Total training time: 74.79 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 3.28, NNZs: 9223, Bias: -0.323576, T: 43735050, Avg. loss: 0.343059\n",
      "Total training time: 77.07 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 3.28, NNZs: 9223, Bias: -0.322929, T: 45021375, Avg. loss: 0.342208\n",
      "Total training time: 79.36 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 3.28, NNZs: 9223, Bias: -0.322278, T: 46307700, Avg. loss: 0.341389\n",
      "Total training time: 81.64 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 3.28, NNZs: 9223, Bias: -0.321658, T: 47594025, Avg. loss: 0.340611\n",
      "Total training time: 83.93 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 3.28, NNZs: 9223, Bias: -0.321083, T: 48880350, Avg. loss: 0.339883\n",
      "Total training time: 86.22 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.320478, T: 50166675, Avg. loss: 0.339176\n",
      "Total training time: 88.50 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.319918, T: 51453000, Avg. loss: 0.338513\n",
      "Total training time: 90.78 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.319366, T: 52739325, Avg. loss: 0.337871\n",
      "Total training time: 93.08 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.318847, T: 54025650, Avg. loss: 0.337268\n",
      "Total training time: 95.35 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.318304, T: 55311975, Avg. loss: 0.336680\n",
      "Total training time: 97.56 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.317796, T: 56598300, Avg. loss: 0.336135\n",
      "Total training time: 99.77 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.317305, T: 57884625, Avg. loss: 0.335614\n",
      "Total training time: 101.97 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.316841, T: 59170950, Avg. loss: 0.335116\n",
      "Total training time: 104.17 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.316335, T: 60457275, Avg. loss: 0.334623\n",
      "Total training time: 106.40 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.315898, T: 61743600, Avg. loss: 0.334169\n",
      "Total training time: 108.61 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.315430, T: 63029925, Avg. loss: 0.333723\n",
      "Total training time: 110.84 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.314984, T: 64316250, Avg. loss: 0.333292\n",
      "Total training time: 113.12 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.314547, T: 65602575, Avg. loss: 0.332877\n",
      "Total training time: 115.40 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.314129, T: 66888900, Avg. loss: 0.332483\n",
      "Total training time: 117.68 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.313731, T: 68175225, Avg. loss: 0.332103\n",
      "Total training time: 119.95 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.313314, T: 69461550, Avg. loss: 0.331736\n",
      "Total training time: 122.21 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.312912, T: 70747875, Avg. loss: 0.331373\n",
      "Total training time: 124.47 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.312528, T: 72034200, Avg. loss: 0.331025\n",
      "Total training time: 126.75 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.312116, T: 73320525, Avg. loss: 0.330690\n",
      "Total training time: 129.07 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.311749, T: 74606850, Avg. loss: 0.330366\n",
      "Total training time: 131.36 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.311378, T: 75893175, Avg. loss: 0.330055\n",
      "Total training time: 133.66 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.310971, T: 77179500, Avg. loss: 0.329753\n",
      "Total training time: 135.92 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.310647, T: 78465825, Avg. loss: 0.329466\n",
      "Total training time: 138.21 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.310287, T: 79752150, Avg. loss: 0.329183\n",
      "Total training time: 140.49 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.309906, T: 81038475, Avg. loss: 0.328906\n",
      "Total training time: 142.78 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.309585, T: 82324800, Avg. loss: 0.328644\n",
      "Total training time: 145.05 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.309229, T: 83611125, Avg. loss: 0.328388\n",
      "Total training time: 147.31 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.308917, T: 84897450, Avg. loss: 0.328143\n",
      "Total training time: 149.55 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.308580, T: 86183775, Avg. loss: 0.327901\n",
      "Total training time: 151.74 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.308273, T: 87470100, Avg. loss: 0.327665\n",
      "Total training time: 153.96 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.307951, T: 88756425, Avg. loss: 0.327438\n",
      "Total training time: 156.18 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.307645, T: 90042750, Avg. loss: 0.327217\n",
      "Total training time: 158.38 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.307339, T: 91329075, Avg. loss: 0.326998\n",
      "Total training time: 160.58 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.307019, T: 92615400, Avg. loss: 0.326789\n",
      "Total training time: 162.77 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.306720, T: 93901725, Avg. loss: 0.326580\n",
      "Total training time: 164.97 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.306444, T: 95188050, Avg. loss: 0.326380\n",
      "Total training time: 167.20 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.306125, T: 96474375, Avg. loss: 0.326180\n",
      "Total training time: 169.47 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.305874, T: 97760700, Avg. loss: 0.325993\n",
      "Total training time: 171.75 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.305583, T: 99047025, Avg. loss: 0.325807\n",
      "Total training time: 174.05 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.305287, T: 100333350, Avg. loss: 0.325623\n",
      "Total training time: 176.32 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.305023, T: 101619675, Avg. loss: 0.325446\n",
      "Total training time: 178.58 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.304749, T: 102906000, Avg. loss: 0.325272\n",
      "Total training time: 180.85 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.304473, T: 104192325, Avg. loss: 0.325101\n",
      "Total training time: 183.12 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.304225, T: 105478650, Avg. loss: 0.324940\n",
      "Total training time: 185.41 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.303962, T: 106764975, Avg. loss: 0.324784\n",
      "Total training time: 187.69 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.303711, T: 108051300, Avg. loss: 0.324628\n",
      "Total training time: 189.98 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.303461, T: 109337625, Avg. loss: 0.324472\n",
      "Total training time: 192.28 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.303207, T: 110623950, Avg. loss: 0.324326\n",
      "Total training time: 194.57 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 3.27, NNZs: 9223, Bias: -0.302939, T: 111910275, Avg. loss: 0.324180\n",
      "Total training time: 196.87 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.302700, T: 113196600, Avg. loss: 0.324037\n",
      "Total training time: 199.18 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.302460, T: 114482925, Avg. loss: 0.323896\n",
      "Total training time: 201.50 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.302225, T: 115769250, Avg. loss: 0.323760\n",
      "Total training time: 203.80 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.301997, T: 117055575, Avg. loss: 0.323622\n",
      "Total training time: 206.07 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.301765, T: 118341900, Avg. loss: 0.323491\n",
      "Total training time: 208.28 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.301531, T: 119628225, Avg. loss: 0.323364\n",
      "Total training time: 210.48 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.301310, T: 120914550, Avg. loss: 0.323243\n",
      "Total training time: 212.69 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.301074, T: 122200875, Avg. loss: 0.323118\n",
      "Total training time: 214.89 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.300855, T: 123487200, Avg. loss: 0.322998\n",
      "Total training time: 217.07 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.300649, T: 124773525, Avg. loss: 0.322879\n",
      "Total training time: 219.28 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.300430, T: 126059850, Avg. loss: 0.322764\n",
      "Total training time: 221.47 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.300210, T: 127346175, Avg. loss: 0.322651\n",
      "Total training time: 223.71 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 3.26, NNZs: 9223, Bias: -0.299996, T: 128632500, Avg. loss: 0.322542\n",
      "Total training time: 225.98 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.65, NNZs: 9971, Bias: -0.248218, T: 1286325, Avg. loss: 0.862183\n",
      "Total training time: 2.42 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.93, NNZs: 9971, Bias: -0.266175, T: 2572650, Avg. loss: 0.636292\n",
      "Total training time: 4.88 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.72, NNZs: 9974, Bias: -0.276258, T: 3858975, Avg. loss: 0.554734\n",
      "Total training time: 7.35 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.60, NNZs: 9985, Bias: -0.283874, T: 5145300, Avg. loss: 0.512141\n",
      "Total training time: 9.80 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.53, NNZs: 9985, Bias: -0.289579, T: 6431625, Avg. loss: 0.485717\n",
      "Total training time: 12.26 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.49, NNZs: 9985, Bias: -0.294225, T: 7717950, Avg. loss: 0.467635\n",
      "Total training time: 14.72 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.47, NNZs: 9985, Bias: -0.298039, T: 9004275, Avg. loss: 0.454482\n",
      "Total training time: 17.20 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.44, NNZs: 9985, Bias: -0.301320, T: 10290600, Avg. loss: 0.444460\n",
      "Total training time: 19.67 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.42, NNZs: 9985, Bias: -0.304257, T: 11576925, Avg. loss: 0.436528\n",
      "Total training time: 22.14 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.40, NNZs: 9985, Bias: -0.306785, T: 12863250, Avg. loss: 0.430123\n",
      "Total training time: 24.61 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.40, NNZs: 9985, Bias: -0.309058, T: 14149575, Avg. loss: 0.424805\n",
      "Total training time: 27.09 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.39, NNZs: 9985, Bias: -0.311184, T: 15435900, Avg. loss: 0.420330\n",
      "Total training time: 29.56 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.39, NNZs: 9985, Bias: -0.313064, T: 16722225, Avg. loss: 0.416500\n",
      "Total training time: 32.03 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.37, NNZs: 9985, Bias: -0.314904, T: 18008550, Avg. loss: 0.413182\n",
      "Total training time: 34.47 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.37, NNZs: 9985, Bias: -0.316484, T: 19294875, Avg. loss: 0.410283\n",
      "Total training time: 36.86 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.37, NNZs: 9985, Bias: -0.318044, T: 20581200, Avg. loss: 0.407734\n",
      "Total training time: 39.28 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.37, NNZs: 9985, Bias: -0.319471, T: 21867525, Avg. loss: 0.405464\n",
      "Total training time: 41.69 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.37, NNZs: 9985, Bias: -0.320788, T: 23153850, Avg. loss: 0.403430\n",
      "Total training time: 44.07 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.37, NNZs: 9985, Bias: -0.322019, T: 24440175, Avg. loss: 0.401600\n",
      "Total training time: 46.47 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.36, NNZs: 9985, Bias: -0.323209, T: 25726500, Avg. loss: 0.399941\n",
      "Total training time: 48.87 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.36, NNZs: 9985, Bias: -0.324335, T: 27012825, Avg. loss: 0.398437\n",
      "Total training time: 51.29 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2.35, NNZs: 9985, Bias: -0.325390, T: 28299150, Avg. loss: 0.397059\n",
      "Total training time: 53.72 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 2.36, NNZs: 9985, Bias: -0.326399, T: 29585475, Avg. loss: 0.395797\n",
      "Total training time: 56.19 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 2.36, NNZs: 9985, Bias: -0.327385, T: 30871800, Avg. loss: 0.394631\n",
      "Total training time: 58.66 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 2.35, NNZs: 9985, Bias: -0.328299, T: 32158125, Avg. loss: 0.393553\n",
      "Total training time: 61.11 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.35, NNZs: 9985, Bias: -0.329199, T: 33444450, Avg. loss: 0.392548\n",
      "Total training time: 63.54 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 2.35, NNZs: 9985, Bias: -0.330076, T: 34730775, Avg. loss: 0.391614\n",
      "Total training time: 66.01 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 2.35, NNZs: 9985, Bias: -0.330843, T: 36017100, Avg. loss: 0.390744\n",
      "Total training time: 68.46 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 2.35, NNZs: 9985, Bias: -0.331677, T: 37303425, Avg. loss: 0.389934\n",
      "Total training time: 70.93 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.332401, T: 38589750, Avg. loss: 0.389171\n",
      "Total training time: 73.40 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.333107, T: 39876075, Avg. loss: 0.388456\n",
      "Total training time: 75.87 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.333832, T: 41162400, Avg. loss: 0.387784\n",
      "Total training time: 78.33 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.334498, T: 42448725, Avg. loss: 0.387150\n",
      "Total training time: 80.80 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.335171, T: 43735050, Avg. loss: 0.386551\n",
      "Total training time: 83.28 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.335815, T: 45021375, Avg. loss: 0.385984\n",
      "Total training time: 85.74 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.336427, T: 46307700, Avg. loss: 0.385446\n",
      "Total training time: 88.20 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.337018, T: 47594025, Avg. loss: 0.384937\n",
      "Total training time: 90.63 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.337603, T: 48880350, Avg. loss: 0.384453\n",
      "Total training time: 92.99 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.338181, T: 50166675, Avg. loss: 0.383991\n",
      "Total training time: 95.37 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.338739, T: 51453000, Avg. loss: 0.383553\n",
      "Total training time: 97.78 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.339246, T: 52739325, Avg. loss: 0.383132\n",
      "Total training time: 100.15 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.339789, T: 54025650, Avg. loss: 0.382732\n",
      "Total training time: 102.54 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.340275, T: 55311975, Avg. loss: 0.382348\n",
      "Total training time: 104.94 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.340777, T: 56598300, Avg. loss: 0.381984\n",
      "Total training time: 107.32 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.341266, T: 57884625, Avg. loss: 0.381633\n",
      "Total training time: 109.77 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.341714, T: 59170950, Avg. loss: 0.381296\n",
      "Total training time: 112.28 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.342206, T: 60457275, Avg. loss: 0.380974\n",
      "Total training time: 114.82 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.342627, T: 61743600, Avg. loss: 0.380662\n",
      "Total training time: 117.30 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.343064, T: 63029925, Avg. loss: 0.380363\n",
      "Total training time: 119.76 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.343507, T: 64316250, Avg. loss: 0.380076\n",
      "Total training time: 122.21 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.343903, T: 65602575, Avg. loss: 0.379799\n",
      "Total training time: 124.68 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.344314, T: 66888900, Avg. loss: 0.379531\n",
      "Total training time: 127.18 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.344720, T: 68175225, Avg. loss: 0.379274\n",
      "Total training time: 129.66 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.345125, T: 69461550, Avg. loss: 0.379026\n",
      "Total training time: 132.14 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.345517, T: 70747875, Avg. loss: 0.378786\n",
      "Total training time: 134.63 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.345879, T: 72034200, Avg. loss: 0.378553\n",
      "Total training time: 137.11 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.346272, T: 73320525, Avg. loss: 0.378331\n",
      "Total training time: 139.56 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.346629, T: 74606850, Avg. loss: 0.378113\n",
      "Total training time: 142.00 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.346982, T: 75893175, Avg. loss: 0.377901\n",
      "Total training time: 144.44 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.347333, T: 77179500, Avg. loss: 0.377697\n",
      "Total training time: 146.88 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.347670, T: 78465825, Avg. loss: 0.377500\n",
      "Total training time: 149.22 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.348022, T: 79752150, Avg. loss: 0.377307\n",
      "Total training time: 151.58 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.348359, T: 81038475, Avg. loss: 0.377122\n",
      "Total training time: 153.95 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.348677, T: 82324800, Avg. loss: 0.376941\n",
      "Total training time: 156.31 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.348987, T: 83611125, Avg. loss: 0.376766\n",
      "Total training time: 158.66 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.349315, T: 84897450, Avg. loss: 0.376595\n",
      "Total training time: 161.02 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.349636, T: 86183775, Avg. loss: 0.376431\n",
      "Total training time: 163.40 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.349944, T: 87470100, Avg. loss: 0.376270\n",
      "Total training time: 165.81 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.350246, T: 88756425, Avg. loss: 0.376113\n",
      "Total training time: 168.24 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.350543, T: 90042750, Avg. loss: 0.375961\n",
      "Total training time: 170.67 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.350836, T: 91329075, Avg. loss: 0.375814\n",
      "Total training time: 173.16 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.351117, T: 92615400, Avg. loss: 0.375669\n",
      "Total training time: 175.59 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.351402, T: 93901725, Avg. loss: 0.375528\n",
      "Total training time: 178.05 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.351678, T: 95188050, Avg. loss: 0.375391\n",
      "Total training time: 180.52 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.351953, T: 96474375, Avg. loss: 0.375258\n",
      "Total training time: 182.98 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.352230, T: 97760700, Avg. loss: 0.375127\n",
      "Total training time: 185.45 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.352502, T: 99047025, Avg. loss: 0.375001\n",
      "Total training time: 187.92 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.352764, T: 100333350, Avg. loss: 0.374877\n",
      "Total training time: 190.40 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 2.34, NNZs: 9985, Bias: -0.353016, T: 101619675, Avg. loss: 0.374755\n",
      "Total training time: 192.89 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.353276, T: 102906000, Avg. loss: 0.374637\n",
      "Total training time: 195.34 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.353522, T: 104192325, Avg. loss: 0.374521\n",
      "Total training time: 197.82 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.353780, T: 105478650, Avg. loss: 0.374408\n",
      "Total training time: 200.26 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.354028, T: 106764975, Avg. loss: 0.374298\n",
      "Total training time: 202.69 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.354255, T: 108051300, Avg. loss: 0.374189\n",
      "Total training time: 205.06 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.354497, T: 109337625, Avg. loss: 0.374083\n",
      "Total training time: 207.43 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.354723, T: 110623950, Avg. loss: 0.373979\n",
      "Total training time: 209.81 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.354971, T: 111910275, Avg. loss: 0.373880\n",
      "Total training time: 212.19 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.355200, T: 113196600, Avg. loss: 0.373780\n",
      "Total training time: 214.55 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.355421, T: 114482925, Avg. loss: 0.373682\n",
      "Total training time: 216.92 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.355649, T: 115769250, Avg. loss: 0.373587\n",
      "Total training time: 219.29 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.355878, T: 117055575, Avg. loss: 0.373495\n",
      "Total training time: 221.67 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.356078, T: 118341900, Avg. loss: 0.373403\n",
      "Total training time: 224.11 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.356304, T: 119628225, Avg. loss: 0.373314\n",
      "Total training time: 226.57 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.356513, T: 120914550, Avg. loss: 0.373226\n",
      "Total training time: 229.02 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.356733, T: 122200875, Avg. loss: 0.373141\n",
      "Total training time: 231.43 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.356937, T: 123487200, Avg. loss: 0.373057\n",
      "Total training time: 233.85 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.357142, T: 124773525, Avg. loss: 0.372975\n",
      "Total training time: 236.28 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.357340, T: 126059850, Avg. loss: 0.372894\n",
      "Total training time: 238.73 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.357554, T: 127346175, Avg. loss: 0.372815\n",
      "Total training time: 241.17 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 2.33, NNZs: 9985, Bias: -0.357757, T: 128632500, Avg. loss: 0.372737\n",
      "Total training time: 243.59 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.48, NNZs: 9097, Bias: 0.158547, T: 1286325, Avg. loss: 0.852378\n",
      "Total training time: 2.36 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.84, NNZs: 9167, Bias: 0.142342, T: 2572650, Avg. loss: 0.624732\n",
      "Total training time: 4.75 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.64, NNZs: 9187, Bias: 0.131371, T: 3858975, Avg. loss: 0.542721\n",
      "Total training time: 7.11 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.57, NNZs: 9203, Bias: 0.124077, T: 5145300, Avg. loss: 0.499844\n",
      "Total training time: 9.51 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.50, NNZs: 9208, Bias: 0.118272, T: 6431625, Avg. loss: 0.473360\n",
      "Total training time: 11.91 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.47, NNZs: 9216, Bias: 0.113490, T: 7717950, Avg. loss: 0.455234\n",
      "Total training time: 14.31 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.45, NNZs: 9217, Bias: 0.109371, T: 9004275, Avg. loss: 0.442025\n",
      "Total training time: 16.65 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.43, NNZs: 9217, Bias: 0.105936, T: 10290600, Avg. loss: 0.431948\n",
      "Total training time: 18.98 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.42, NNZs: 9218, Bias: 0.103105, T: 11576925, Avg. loss: 0.423990\n",
      "Total training time: 21.31 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.40, NNZs: 9228, Bias: 0.100414, T: 12863250, Avg. loss: 0.417563\n",
      "Total training time: 23.61 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.40, NNZs: 9229, Bias: 0.098003, T: 14149575, Avg. loss: 0.412233\n",
      "Total training time: 25.92 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.40, NNZs: 9229, Bias: 0.095849, T: 15435900, Avg. loss: 0.407740\n",
      "Total training time: 28.22 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.38, NNZs: 9230, Bias: 0.093847, T: 16722225, Avg. loss: 0.403913\n",
      "Total training time: 30.52 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.38, NNZs: 9235, Bias: 0.092059, T: 18008550, Avg. loss: 0.400590\n",
      "Total training time: 32.83 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.38, NNZs: 9236, Bias: 0.090321, T: 19294875, Avg. loss: 0.397695\n",
      "Total training time: 35.13 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.38, NNZs: 9236, Bias: 0.088683, T: 20581200, Avg. loss: 0.395150\n",
      "Total training time: 37.48 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.37, NNZs: 9236, Bias: 0.087240, T: 21867525, Avg. loss: 0.392872\n",
      "Total training time: 39.84 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.37, NNZs: 9236, Bias: 0.085853, T: 23153850, Avg. loss: 0.390845\n",
      "Total training time: 42.20 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.37, NNZs: 9236, Bias: 0.084546, T: 24440175, Avg. loss: 0.389019\n",
      "Total training time: 44.57 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.37, NNZs: 9237, Bias: 0.083279, T: 25726500, Avg. loss: 0.387367\n",
      "Total training time: 46.93 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.37, NNZs: 9237, Bias: 0.082136, T: 27012825, Avg. loss: 0.385857\n",
      "Total training time: 49.31 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2.37, NNZs: 9237, Bias: 0.080991, T: 28299150, Avg. loss: 0.384485\n",
      "Total training time: 51.71 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 2.37, NNZs: 9237, Bias: 0.079923, T: 29585475, Avg. loss: 0.383223\n",
      "Total training time: 54.10 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 2.37, NNZs: 9237, Bias: 0.078909, T: 30871800, Avg. loss: 0.382059\n",
      "Total training time: 56.51 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 2.36, NNZs: 9237, Bias: 0.077923, T: 32158125, Avg. loss: 0.380983\n",
      "Total training time: 58.92 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.36, NNZs: 9237, Bias: 0.077027, T: 33444450, Avg. loss: 0.379982\n",
      "Total training time: 61.32 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 2.36, NNZs: 9237, Bias: 0.076131, T: 34730775, Avg. loss: 0.379050\n",
      "Total training time: 63.71 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 2.36, NNZs: 9237, Bias: 0.075262, T: 36017100, Avg. loss: 0.378186\n",
      "Total training time: 66.07 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 2.36, NNZs: 9237, Bias: 0.074413, T: 37303425, Avg. loss: 0.377373\n",
      "Total training time: 68.45 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 2.36, NNZs: 9237, Bias: 0.073632, T: 38589750, Avg. loss: 0.376616\n",
      "Total training time: 70.79 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 2.36, NNZs: 9237, Bias: 0.072839, T: 39876075, Avg. loss: 0.375909\n",
      "Total training time: 73.14 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 2.36, NNZs: 9237, Bias: 0.072128, T: 41162400, Avg. loss: 0.375238\n",
      "Total training time: 75.45 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 2.36, NNZs: 9237, Bias: 0.071372, T: 42448725, Avg. loss: 0.374607\n",
      "Total training time: 77.77 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 2.36, NNZs: 9237, Bias: 0.070666, T: 43735050, Avg. loss: 0.374012\n",
      "Total training time: 80.10 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 2.36, NNZs: 9237, Bias: 0.070009, T: 45021375, Avg. loss: 0.373447\n",
      "Total training time: 82.42 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 2.36, NNZs: 9237, Bias: 0.069360, T: 46307700, Avg. loss: 0.372911\n",
      "Total training time: 84.72 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 2.36, NNZs: 9237, Bias: 0.068709, T: 47594025, Avg. loss: 0.372407\n",
      "Total training time: 87.08 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 2.36, NNZs: 9237, Bias: 0.068102, T: 48880350, Avg. loss: 0.371925\n",
      "Total training time: 89.41 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.067508, T: 50166675, Avg. loss: 0.371467\n",
      "Total training time: 91.72 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.066937, T: 51453000, Avg. loss: 0.371026\n",
      "Total training time: 94.04 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 2.36, NNZs: 9237, Bias: 0.066373, T: 52739325, Avg. loss: 0.370610\n",
      "Total training time: 96.37 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 2.36, NNZs: 9237, Bias: 0.065824, T: 54025650, Avg. loss: 0.370211\n",
      "Total training time: 98.77 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.065270, T: 55311975, Avg. loss: 0.369832\n",
      "Total training time: 101.15 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 2.36, NNZs: 9237, Bias: 0.064772, T: 56598300, Avg. loss: 0.369465\n",
      "Total training time: 103.54 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.064264, T: 57884625, Avg. loss: 0.369115\n",
      "Total training time: 105.92 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.063743, T: 59170950, Avg. loss: 0.368780\n",
      "Total training time: 108.32 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.063275, T: 60457275, Avg. loss: 0.368458\n",
      "Total training time: 110.71 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.062778, T: 61743600, Avg. loss: 0.368150\n",
      "Total training time: 113.10 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.062314, T: 63029925, Avg. loss: 0.367852\n",
      "Total training time: 115.47 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.061869, T: 64316250, Avg. loss: 0.367565\n",
      "Total training time: 117.87 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.061395, T: 65602575, Avg. loss: 0.367291\n",
      "Total training time: 120.26 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.060962, T: 66888900, Avg. loss: 0.367025\n",
      "Total training time: 122.66 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.060540, T: 68175225, Avg. loss: 0.366770\n",
      "Total training time: 125.03 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.060109, T: 69461550, Avg. loss: 0.366522\n",
      "Total training time: 127.41 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.059707, T: 70747875, Avg. loss: 0.366282\n",
      "Total training time: 129.77 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.059292, T: 72034200, Avg. loss: 0.366051\n",
      "Total training time: 132.06 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.058916, T: 73320525, Avg. loss: 0.365828\n",
      "Total training time: 134.35 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.058521, T: 74606850, Avg. loss: 0.365611\n",
      "Total training time: 136.62 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.058150, T: 75893175, Avg. loss: 0.365403\n",
      "Total training time: 138.90 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.057766, T: 77179500, Avg. loss: 0.365201\n",
      "Total training time: 141.20 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.057391, T: 78465825, Avg. loss: 0.365004\n",
      "Total training time: 143.50 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.057039, T: 79752150, Avg. loss: 0.364814\n",
      "Total training time: 145.79 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.056681, T: 81038475, Avg. loss: 0.364629\n",
      "Total training time: 148.05 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.056322, T: 82324800, Avg. loss: 0.364449\n",
      "Total training time: 150.32 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.055972, T: 83611125, Avg. loss: 0.364276\n",
      "Total training time: 152.59 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.055645, T: 84897450, Avg. loss: 0.364107\n",
      "Total training time: 154.90 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.055316, T: 86183775, Avg. loss: 0.363942\n",
      "Total training time: 157.26 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.054988, T: 87470100, Avg. loss: 0.363782\n",
      "Total training time: 159.61 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.054666, T: 88756425, Avg. loss: 0.363626\n",
      "Total training time: 161.96 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.054350, T: 90042750, Avg. loss: 0.363474\n",
      "Total training time: 164.34 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.054039, T: 91329075, Avg. loss: 0.363326\n",
      "Total training time: 166.71 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.053729, T: 92615400, Avg. loss: 0.363184\n",
      "Total training time: 169.10 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.053435, T: 93901725, Avg. loss: 0.363044\n",
      "Total training time: 171.47 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.053134, T: 95188050, Avg. loss: 0.362909\n",
      "Total training time: 173.86 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.052842, T: 96474375, Avg. loss: 0.362776\n",
      "Total training time: 176.27 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.052553, T: 97760700, Avg. loss: 0.362646\n",
      "Total training time: 178.65 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.052264, T: 99047025, Avg. loss: 0.362520\n",
      "Total training time: 181.06 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.051973, T: 100333350, Avg. loss: 0.362398\n",
      "Total training time: 183.46 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.051698, T: 101619675, Avg. loss: 0.362277\n",
      "Total training time: 185.85 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.051414, T: 102906000, Avg. loss: 0.362159\n",
      "Total training time: 188.20 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.051160, T: 104192325, Avg. loss: 0.362044\n",
      "Total training time: 190.48 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.050894, T: 105478650, Avg. loss: 0.361933\n",
      "Total training time: 192.77 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.050637, T: 106764975, Avg. loss: 0.361823\n",
      "Total training time: 195.04 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.050378, T: 108051300, Avg. loss: 0.361716\n",
      "Total training time: 197.32 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.050114, T: 109337625, Avg. loss: 0.361612\n",
      "Total training time: 199.59 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.049856, T: 110623950, Avg. loss: 0.361509\n",
      "Total training time: 201.86 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.049611, T: 111910275, Avg. loss: 0.361408\n",
      "Total training time: 204.17 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.049369, T: 113196600, Avg. loss: 0.361311\n",
      "Total training time: 206.46 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.049116, T: 114482925, Avg. loss: 0.361215\n",
      "Total training time: 208.73 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.048888, T: 115769250, Avg. loss: 0.361120\n",
      "Total training time: 211.00 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.048651, T: 117055575, Avg. loss: 0.361028\n",
      "Total training time: 213.29 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.048409, T: 118341900, Avg. loss: 0.360938\n",
      "Total training time: 215.63 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.048173, T: 119628225, Avg. loss: 0.360850\n",
      "Total training time: 217.97 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.047954, T: 120914550, Avg. loss: 0.360762\n",
      "Total training time: 220.32 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.047728, T: 122200875, Avg. loss: 0.360678\n",
      "Total training time: 222.72 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.047502, T: 123487200, Avg. loss: 0.360595\n",
      "Total training time: 225.08 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.047285, T: 124773525, Avg. loss: 0.360514\n",
      "Total training time: 227.44 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.047057, T: 126059850, Avg. loss: 0.360434\n",
      "Total training time: 229.80 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 2.35, NNZs: 9237, Bias: 0.046846, T: 127346175, Avg. loss: 0.360355\n",
      "Total training time: 232.16 seconds.\n",
      "-- Epoch 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-04 00:14:09,647 : INFO : Evaluating on Training Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 2.35, NNZs: 9237, Bias: 0.046628, T: 128632500, Avg. loss: 0.360278\n",
      "Total training time: 234.53 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-04 00:14:23,416 : INFO : Calculating training metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 ..., 0 0 0]\n",
      " [1 0 0 ..., 1 0 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " ..., \n",
      " [1 0 0 ..., 0 1 0]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 1]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown format code 'd' for object of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e7853687d354>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'# Training Metrics\\nclf.fit(X,y)\\ninfo(\\'Evaluating on Training Data\\')\\nyp = clf.predict(X)\\nprint yp\\ninfo(\\'Calculating training metrics\\')\\ntraining_metrics = get_metrics(y, yp, yp)\\nprint \"** Training Metrics: Cov Err: {:.3f}, Avg Labels: {:.3f}, \\\\n\\\\t\\\\t Top 1: {:.3f}, Top 3: {:.3f}, Top 5: {:.3f}, \\\\n\\\\t\\\\t F1 Micro: {:.3f}, Total Pos: {:,d}\".format(\\n    training_metrics[\\'coverage_error\\'], training_metrics[\\'average_num_of_labels\\'], \\n    training_metrics[\\'top_1\\'], training_metrics[\\'top_3\\'], training_metrics[\\'top_5\\'], \\n    training_metrics[\\'f1_micro\\'], training_metrics[\\'f1_macro\\'], training_metrics[\\'total_positive\\'])'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/s/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/s/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/s/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown format code 'd' for object of type 'float'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training Metrics\n",
    "clf.fit(X,y)\n",
    "info('Evaluating on Training Data')\n",
    "yp = clf.predict(X)\n",
    "print yp\n",
    "info('Calculating training metrics')\n",
    "training_metrics = get_metrics(y, yp, yp)\n",
    "print \"** Training Metrics: Cov Err: {:.3f}, Avg Labels: {:.3f}, \\n\\t\\t Top 1: {:.3f}, Top 3: {:.3f}, Top 5: {:.3f}, \\n\\t\\t F1 Micro: {:.3f}, F1 Macro: {:.3f}, Total Pos: {:,d}\".format(\n",
    "    training_metrics['coverage_error'], training_metrics['average_num_of_labels'], \n",
    "    training_metrics['top_1'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "    training_metrics['f1_micro'], training_metrics['f1_macro'], training_metrics['total_positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 56s, sys: 5.49 s, total: 3min 1s\n",
      "Wall time: 3min 4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-04 00:30:20,277 : INFO : 0\n",
      "2017-01-04 00:30:20,308 : INFO : 1000\n",
      "2017-01-04 00:30:20,319 : INFO : PROOOOBBBBBBBBBBBLEM [u'B', u'B-27', u'B-27-B', u'B-27-L', u'B-27-M', u'B-29', u'B-29-C', u'B-32', u'B-32-B', u'B-41', u'B-41-M', u'B-44', u'B-44-C', u'D', u'D-06', u'D-06-N', u'E', u'E-04', u'E-04-B', u'E-04-F', u'G', u'G-03', u'G-03-G']\n",
      "2017-01-04 00:30:20,321 : INFO : 2000\n",
      "2017-01-04 00:30:20,352 : INFO : 3000\n",
      "2017-01-04 00:30:20,360 : INFO : PROOOOBBBBBBBBBBBLEM [u'B', u'B-05', u'B-05-D', u'B-23', u'B-23-K', u'B-32', u'B-32-B', u'C', u'C-05', u'C-05-D', u'C-08', u'C-08-J', u'C-23', u'C-23-C', u'D', u'D-06', u'D-06-N', u'G', u'G-03', u'G-03-B', u'H', u'H-05', u'H-05-H']\n",
      "2017-01-04 00:30:20,365 : INFO : 4000\n",
      "2017-01-04 00:30:20,376 : INFO : 5000\n",
      "2017-01-04 00:30:20,386 : INFO : 6000\n",
      "2017-01-04 00:30:20,397 : INFO : 7000\n",
      "2017-01-04 00:30:20,408 : INFO : 8000\n",
      "2017-01-04 00:30:20,419 : INFO : 9000\n",
      "2017-01-04 00:30:20,429 : INFO : 10000\n",
      "2017-01-04 00:30:20,440 : INFO : 11000\n",
      "2017-01-04 00:30:20,451 : INFO : 12000\n",
      "2017-01-04 00:30:20,461 : INFO : 13000\n",
      "2017-01-04 00:30:20,471 : INFO : 14000\n",
      "2017-01-04 00:30:20,481 : INFO : 15000\n",
      "2017-01-04 00:30:20,489 : INFO : PROOOOBBBBBBBBBBBLEM [u'A', u'A-01', u'A-01-J', u'A-21', u'A-21-C', u'A-23', u'A-23-G', u'A-23-P', u'B', u'B-28', u'B-28-B', u'B-29', u'B-29-C', u'B-29-D', u'B-31', u'B-31-B', u'B-31-D', u'B-32', u'B-32-B', u'B-44', u'B-44-C', u'B-65', u'B-65-C', u'D', u'D-06', u'D-06-N']\n",
      "2017-01-04 00:30:20,492 : INFO : 16000\n",
      "2017-01-04 00:30:20,501 : INFO : 17000\n",
      "2017-01-04 00:30:20,511 : INFO : 18000\n",
      "2017-01-04 00:30:20,521 : INFO : 19000\n",
      "2017-01-04 00:30:20,532 : INFO : 20000\n",
      "2017-01-04 00:30:20,541 : INFO : 21000\n",
      "2017-01-04 00:30:20,551 : INFO : 22000\n",
      "2017-01-04 00:30:20,561 : INFO : 23000\n",
      "2017-01-04 00:30:20,570 : INFO : 24000\n",
      "2017-01-04 00:30:20,580 : INFO : 25000\n",
      "2017-01-04 00:30:20,589 : INFO : 26000\n",
      "2017-01-04 00:30:20,598 : INFO : 27000\n",
      "2017-01-04 00:30:20,607 : INFO : PROOOOBBBBBBBBBBBLEM [u'A', u'A-47', u'A-47-G', u'A-61', u'A-61-K', u'B', u'B-28', u'B-28-B', u'B-29', u'B-29-D', u'B-32', u'B-32-B', u'C', u'C-04', u'C-04-B', u'C-08', u'C-08-B', u'D', u'D-21', u'D-21-H', u'F', u'F-16', u'F-16-L']\n",
      "2017-01-04 00:30:20,609 : INFO : 28000\n",
      "2017-01-04 00:30:20,618 : INFO : 29000\n",
      "2017-01-04 00:30:20,627 : INFO : 30000\n",
      "2017-01-04 00:30:20,636 : INFO : 31000\n",
      "2017-01-04 00:30:20,645 : INFO : 32000\n",
      "2017-01-04 00:30:20,654 : INFO : 33000\n",
      "2017-01-04 00:30:20,663 : INFO : 34000\n",
      "2017-01-04 00:30:20,672 : INFO : 35000\n",
      "2017-01-04 00:30:20,681 : INFO : 36000\n",
      "2017-01-04 00:30:20,691 : INFO : 37000\n",
      "2017-01-04 00:30:20,700 : INFO : 38000\n",
      "2017-01-04 00:30:20,709 : INFO : 39000\n",
      "2017-01-04 00:30:20,718 : INFO : 40000\n",
      "2017-01-04 00:30:20,727 : INFO : 41000\n",
      "2017-01-04 00:30:20,736 : INFO : 42000\n",
      "2017-01-04 00:30:20,744 : INFO : 43000\n",
      "2017-01-04 00:30:20,753 : INFO : 44000\n",
      "2017-01-04 00:30:20,763 : INFO : 45000\n",
      "2017-01-04 00:30:20,771 : INFO : 46000\n",
      "2017-01-04 00:30:20,780 : INFO : 47000\n",
      "2017-01-04 00:30:20,789 : INFO : 48000\n",
      "2017-01-04 00:30:20,797 : INFO : 49000\n",
      "2017-01-04 00:30:20,801 : INFO : PROOOOBBBBBBBBBBBLEM [u'A', u'A-61', u'A-61-K', u'A-61-L', u'B', u'B-29', u'B-29-C', u'C', u'C-04', u'C-04-B', u'C-08', u'C-08-B', u'C-08-F', u'C-08-G', u'C-08-K', u'C-08-L', u'C-09', u'C-09-D', u'G', u'G-03', u'G-03-F']\n",
      "2017-01-04 00:30:20,807 : INFO : 50000\n",
      "2017-01-04 00:30:20,815 : INFO : 51000\n",
      "2017-01-04 00:30:20,824 : INFO : 52000\n",
      "2017-01-04 00:30:20,833 : INFO : 53000\n",
      "2017-01-04 00:30:20,842 : INFO : 54000\n",
      "2017-01-04 00:30:20,843 : INFO : PROOOOBBBBBBBBBBBLEM [u'A', u'A-01', u'A-01-G', u'B', u'B-01', u'B-01-F', u'B-29', u'B-29-C', u'B-29-D', u'B-60', u'B-60-C', u'C', u'C-04', u'C-04-B', u'C-08', u'C-08-G', u'C-08-K', u'C-08-L', u'C-09', u'C-09-D', u'H', u'H-01', u'H-01-B', u'H-01-M']\n",
      "2017-01-04 00:30:20,851 : INFO : 55000\n",
      "2017-01-04 00:30:20,860 : INFO : 56000\n",
      "2017-01-04 00:30:20,868 : INFO : 57000\n",
      "2017-01-04 00:30:20,877 : INFO : 58000\n",
      "2017-01-04 00:30:20,885 : INFO : 59000\n",
      "2017-01-04 00:30:20,894 : INFO : 60000\n",
      "2017-01-04 00:30:20,902 : INFO : 61000\n",
      "2017-01-04 00:30:20,912 : INFO : 62000\n",
      "2017-01-04 00:30:20,920 : INFO : 63000\n",
      "2017-01-04 00:30:20,922 : INFO : PROOOOBBBBBBBBBBBLEM [u'A', u'A-61', u'A-61-K', u'A-61-L', u'B', u'B-41', u'B-41-J', u'C', u'C-04', u'C-04-B', u'C-08', u'C-08-F', u'C-08-J', u'C-08-K', u'C-09', u'C-09-D', u'G', u'G-01', u'G-01-D', u'H', u'H-05', u'H-05-B']\n",
      "2017-01-04 00:30:20,929 : INFO : 64000\n",
      "2017-01-04 00:30:20,938 : INFO : 65000\n",
      "2017-01-04 00:30:20,946 : INFO : 66000\n",
      "2017-01-04 00:30:20,955 : INFO : 67000\n",
      "2017-01-04 00:30:20,963 : INFO : 68000\n",
      "2017-01-04 00:30:20,970 : INFO : PROOOOBBBBBBBBBBBLEM [u'B', u'B-29', u'B-29-D', u'E', u'E-05', u'E-05-D', u'F', u'F-21', u'F-21-V', u'G', u'G-06', u'G-06-F', u'G-06-T', u'G-09', u'G-09-G', u'H', u'H-04', u'H-04-M', u'H-04-N', u'H-05', u'H-05-K']\n",
      "2017-01-04 00:30:20,972 : INFO : 69000\n",
      "2017-01-04 00:30:20,981 : INFO : 70000\n",
      "2017-01-04 00:30:20,990 : INFO : 71000\n",
      "2017-01-04 00:30:20,998 : INFO : 72000\n",
      "2017-01-04 00:30:21,007 : INFO : 73000\n",
      "2017-01-04 00:30:21,015 : INFO : 74000\n",
      "2017-01-04 00:30:21,023 : INFO : 75000\n",
      "2017-01-04 00:30:21,032 : INFO : 76000\n",
      "2017-01-04 00:30:21,040 : INFO : 77000\n",
      "2017-01-04 00:30:21,048 : INFO : 78000\n",
      "2017-01-04 00:30:21,057 : INFO : 79000\n",
      "2017-01-04 00:30:21,066 : INFO : 80000\n",
      "2017-01-04 00:30:21,074 : INFO : 81000\n",
      "2017-01-04 00:30:21,082 : INFO : 82000\n",
      "2017-01-04 00:30:21,091 : INFO : 83000\n",
      "2017-01-04 00:30:21,099 : INFO : 84000\n",
      "2017-01-04 00:30:21,107 : INFO : 85000\n",
      "2017-01-04 00:30:21,115 : INFO : 86000\n",
      "2017-01-04 00:30:21,124 : INFO : 87000\n",
      "2017-01-04 00:30:21,133 : INFO : 88000\n",
      "2017-01-04 00:30:21,141 : INFO : 89000\n",
      "2017-01-04 00:30:21,149 : INFO : 90000\n",
      "2017-01-04 00:30:21,157 : INFO : 91000\n",
      "2017-01-04 00:30:21,166 : INFO : 92000\n",
      "2017-01-04 00:30:21,176 : INFO : 93000\n",
      "2017-01-04 00:30:21,186 : INFO : 94000\n",
      "2017-01-04 00:30:21,197 : INFO : 95000\n",
      "2017-01-04 00:30:21,207 : INFO : 96000\n",
      "2017-01-04 00:30:21,217 : INFO : 97000\n",
      "2017-01-04 00:30:21,227 : INFO : 98000\n",
      "2017-01-04 00:30:21,237 : INFO : 99000\n",
      "2017-01-04 00:30:21,246 : INFO : 100000\n",
      "2017-01-04 00:30:21,254 : INFO : 101000\n",
      "2017-01-04 00:30:21,262 : INFO : 102000\n",
      "2017-01-04 00:30:21,270 : INFO : 103000\n",
      "2017-01-04 00:30:21,280 : INFO : 104000\n",
      "2017-01-04 00:30:21,288 : INFO : 105000\n",
      "2017-01-04 00:30:21,291 : INFO : PROOOOBBBBBBBBBBBLEM [u'A', u'A-61', u'A-61-N', u'B', u'B-01', u'B-01-D', u'B-44', u'B-44-C', u'C', u'C-03', u'C-03-C', u'C-23', u'C-23-F', u'G', u'G-21', u'G-21-G', u'G-21-K', u'H', u'H-01', u'H-01-J', u'H-01-L']\n",
      "2017-01-04 00:30:21,297 : INFO : 106000\n",
      "2017-01-04 00:30:21,305 : INFO : 107000\n",
      "2017-01-04 00:30:21,313 : INFO : 108000\n",
      "2017-01-04 00:30:21,321 : INFO : 109000\n",
      "2017-01-04 00:30:21,329 : INFO : 110000\n",
      "2017-01-04 00:30:21,337 : INFO : 111000\n",
      "2017-01-04 00:30:21,346 : INFO : 112000\n",
      "2017-01-04 00:30:21,354 : INFO : 113000\n",
      "2017-01-04 00:30:21,362 : INFO : 114000\n",
      "2017-01-04 00:30:21,370 : INFO : 115000\n",
      "2017-01-04 00:30:21,379 : INFO : 116000\n",
      "2017-01-04 00:30:21,387 : INFO : 117000\n",
      "2017-01-04 00:30:21,395 : INFO : 118000\n",
      "2017-01-04 00:30:21,403 : INFO : 119000\n",
      "2017-01-04 00:30:21,411 : INFO : 120000\n",
      "2017-01-04 00:30:21,420 : INFO : 121000\n",
      "2017-01-04 00:30:21,428 : INFO : 122000\n",
      "2017-01-04 00:30:21,436 : INFO : 123000\n",
      "2017-01-04 00:30:21,439 : INFO : PROOOOBBBBBBBBBBBLEM [u'B', u'B-32', u'B-32-B', u'B-64', u'B-64-C', u'B-64-D', u'C', u'C-08', u'C-08-G', u'C-08-J', u'C-08-K', u'C-08-L', u'D', u'D-03', u'D-03-D', u'E', u'E-06', u'E-06-B', u'G', u'G-02', u'G-02-B', u'G-02-F']\n",
      "2017-01-04 00:30:21,444 : INFO : 124000\n",
      "2017-01-04 00:30:21,452 : INFO : 125000\n",
      "2017-01-04 00:30:21,460 : INFO : 126000\n",
      "2017-01-04 00:30:21,469 : INFO : 127000\n",
      "2017-01-04 00:30:21,477 : INFO : 128000\n",
      "2017-01-04 00:30:21,486 : INFO : 129000\n",
      "2017-01-04 00:30:21,495 : INFO : 130000\n",
      "2017-01-04 00:30:21,504 : INFO : 131000\n",
      "2017-01-04 00:30:21,513 : INFO : 132000\n",
      "2017-01-04 00:30:21,522 : INFO : 133000\n",
      "2017-01-04 00:30:21,531 : INFO : 134000\n",
      "2017-01-04 00:30:21,539 : INFO : 135000\n",
      "2017-01-04 00:30:21,548 : INFO : 136000\n",
      "2017-01-04 00:30:21,558 : INFO : 137000\n",
      "2017-01-04 00:30:21,566 : INFO : 138000\n",
      "2017-01-04 00:30:21,575 : INFO : 139000\n",
      "2017-01-04 00:30:21,583 : INFO : 140000\n",
      "2017-01-04 00:30:21,590 : INFO : 141000\n",
      "2017-01-04 00:30:21,599 : INFO : 142000\n",
      "2017-01-04 00:30:21,607 : INFO : 143000\n",
      "2017-01-04 00:30:21,615 : INFO : 144000\n",
      "2017-01-04 00:30:21,623 : INFO : 145000\n",
      "2017-01-04 00:30:21,632 : INFO : 146000\n",
      "2017-01-04 00:30:21,640 : INFO : 147000\n",
      "2017-01-04 00:30:21,648 : INFO : 148000\n",
      "2017-01-04 00:30:21,655 : INFO : PROOOOBBBBBBBBBBBLEM [u'A', u'A-47', u'A-47-B', u'A-47-G', u'A-47-K', u'E', u'E-04', u'E-04-G', u'F', u'F-16', u'F-16-L', u'F-21', u'F-21-V', u'G', u'G-02', u'G-02-F', u'G-09', u'G-09-F', u'H', u'H-04', u'H-04-N', u'H-05', u'H-05-K']\n",
      "2017-01-04 00:30:21,657 : INFO : 149000\n",
      "2017-01-04 00:30:21,665 : INFO : 150000\n",
      "2017-01-04 00:30:21,673 : INFO : 151000\n",
      "2017-01-04 00:30:21,681 : INFO : 152000\n",
      "2017-01-04 00:30:21,689 : INFO : 153000\n",
      "2017-01-04 00:30:21,698 : INFO : 154000\n",
      "2017-01-04 00:30:21,706 : INFO : 155000\n",
      "2017-01-04 00:30:21,714 : INFO : 156000\n",
      "2017-01-04 00:30:21,722 : INFO : 157000\n",
      "2017-01-04 00:30:21,730 : INFO : 158000\n",
      "2017-01-04 00:30:21,738 : INFO : 159000\n",
      "2017-01-04 00:30:21,746 : INFO : 160000\n",
      "2017-01-04 00:30:21,754 : INFO : 161000\n",
      "2017-01-04 00:30:21,762 : INFO : 162000\n",
      "2017-01-04 00:30:21,770 : INFO : 163000\n",
      "2017-01-04 00:30:21,778 : INFO : 164000\n",
      "2017-01-04 00:30:21,786 : INFO : 165000\n",
      "2017-01-04 00:30:21,795 : INFO : 166000\n",
      "2017-01-04 00:30:21,803 : INFO : 167000\n",
      "2017-01-04 00:30:21,811 : INFO : 168000\n",
      "2017-01-04 00:30:21,818 : INFO : 169000\n",
      "2017-01-04 00:30:21,826 : INFO : 170000\n",
      "2017-01-04 00:30:21,833 : INFO : PROOOOBBBBBBBBBBBLEM [u'A', u'A-61', u'A-61-K', u'B', u'B-29', u'B-29-C', u'B-29-D', u'B-32', u'B-32-B', u'B-41', u'B-41-M', u'B-60', u'B-60-C', u'C', u'C-03', u'C-03-C', u'C-08', u'C-08-F', u'C-08-G', u'C-08-J', u'C-08-K', u'C-08-L', u'D', u'D-21', u'D-21-H', u'H', u'H-05', u'H-05-B']\n",
      "2017-01-04 00:30:21,836 : INFO : 171000\n",
      "2017-01-04 00:30:21,844 : INFO : 172000\n",
      "2017-01-04 00:30:21,852 : INFO : 173000\n",
      "2017-01-04 00:30:21,860 : INFO : 174000\n",
      "2017-01-04 00:30:21,868 : INFO : 175000\n",
      "2017-01-04 00:30:21,875 : INFO : 176000\n",
      "2017-01-04 00:30:21,884 : INFO : 177000\n",
      "2017-01-04 00:30:21,893 : INFO : 178000\n",
      "2017-01-04 00:30:21,899 : INFO : PROOOOBBBBBBBBBBBLEM [u'B', u'B-27', u'B-27-J', u'B-28', u'B-28-B', u'B-29', u'B-29-B', u'B-29-C', u'B-60', u'B-60-C', u'C', u'C-01', u'C-01-B', u'C-04', u'C-04-B', u'C-08', u'C-08-F', u'C-08-G', u'C-08-J', u'C-08-K', u'C-08-L', u'D', u'D-04', u'D-04-H', u'D-21', u'D-21-H', u'H', u'H-01', u'H-01-B', u'H-01-M']\n",
      "2017-01-04 00:30:21,903 : INFO : 179000\n",
      "2017-01-04 00:30:21,912 : INFO : 180000\n",
      "2017-01-04 00:30:21,921 : INFO : 181000\n",
      "2017-01-04 00:30:21,930 : INFO : 182000\n",
      "2017-01-04 00:30:21,939 : INFO : 183000\n",
      "2017-01-04 00:30:21,947 : INFO : 184000\n",
      "2017-01-04 00:30:21,956 : INFO : 185000\n",
      "2017-01-04 00:30:21,965 : INFO : 186000\n",
      "2017-01-04 00:30:21,973 : INFO : 187000\n",
      "2017-01-04 00:30:21,982 : INFO : 188000\n",
      "2017-01-04 00:30:21,989 : INFO : 189000\n",
      "2017-01-04 00:30:21,997 : INFO : 190000\n",
      "2017-01-04 00:30:22,005 : INFO : 191000\n",
      "2017-01-04 00:30:22,013 : INFO : 192000\n",
      "2017-01-04 00:30:22,021 : INFO : 193000\n",
      "2017-01-04 00:30:22,029 : INFO : 194000\n",
      "2017-01-04 00:30:22,037 : INFO : 195000\n",
      "2017-01-04 00:30:22,046 : INFO : 196000\n",
      "2017-01-04 00:30:22,053 : INFO : 197000\n",
      "2017-01-04 00:30:22,061 : INFO : 198000\n",
      "2017-01-04 00:30:22,069 : INFO : 199000\n",
      "2017-01-04 00:30:22,077 : INFO : 200000\n",
      "2017-01-04 00:30:22,085 : INFO : 201000\n",
      "2017-01-04 00:30:22,093 : INFO : 202000\n",
      "2017-01-04 00:30:22,101 : INFO : 203000\n",
      "2017-01-04 00:30:22,110 : INFO : 204000\n",
      "2017-01-04 00:30:22,117 : INFO : 205000\n",
      "2017-01-04 00:30:22,125 : INFO : 206000\n",
      "2017-01-04 00:30:22,133 : INFO : 207000\n",
      "2017-01-04 00:30:22,141 : INFO : 208000\n",
      "2017-01-04 00:30:22,149 : INFO : 209000\n",
      "2017-01-04 00:30:22,157 : INFO : 210000\n",
      "2017-01-04 00:30:22,165 : INFO : 211000\n",
      "2017-01-04 00:30:22,173 : INFO : 212000\n",
      "2017-01-04 00:30:22,182 : INFO : 213000\n",
      "2017-01-04 00:30:22,190 : INFO : 214000\n",
      "2017-01-04 00:30:22,198 : INFO : 215000\n",
      "2017-01-04 00:30:22,206 : INFO : 216000\n",
      "2017-01-04 00:30:22,213 : INFO : 217000\n",
      "2017-01-04 00:30:22,221 : INFO : 218000\n",
      "2017-01-04 00:30:22,229 : INFO : 219000\n",
      "2017-01-04 00:30:22,237 : INFO : 220000\n",
      "2017-01-04 00:30:22,242 : INFO : PROOOOBBBBBBBBBBBLEM [u'G', u'G-05', u'G-05-F', u'G-06', u'G-06-F', u'G-06-G', u'G-11', u'G-11-C', u'H', u'H-01', u'H-01-F', u'H-03', u'H-03-B', u'H-03-F', u'H-03-G', u'H-03-H', u'H-03-J', u'H-03-K', u'H-03-M', u'H-04', u'H-04-B']\n",
      "2017-01-04 00:30:22,246 : INFO : 221000\n",
      "2017-01-04 00:30:22,254 : INFO : 222000\n",
      "2017-01-04 00:30:22,262 : INFO : 223000\n",
      "2017-01-04 00:30:22,270 : INFO : 224000\n",
      "2017-01-04 00:30:22,278 : INFO : 225000\n",
      "2017-01-04 00:30:22,286 : INFO : 226000\n",
      "2017-01-04 00:30:22,293 : INFO : 227000\n",
      "2017-01-04 00:30:22,301 : INFO : 228000\n",
      "2017-01-04 00:30:22,309 : INFO : 229000\n",
      "2017-01-04 00:30:22,318 : INFO : 230000\n",
      "2017-01-04 00:30:22,326 : INFO : 231000\n",
      "2017-01-04 00:30:22,334 : INFO : 232000\n",
      "2017-01-04 00:30:22,341 : INFO : 233000\n",
      "2017-01-04 00:30:22,349 : INFO : 234000\n",
      "2017-01-04 00:30:22,357 : INFO : 235000\n",
      "2017-01-04 00:30:22,365 : INFO : 236000\n",
      "2017-01-04 00:30:22,373 : INFO : 237000\n",
      "2017-01-04 00:30:22,382 : INFO : 238000\n",
      "2017-01-04 00:30:22,390 : INFO : 239000\n",
      "2017-01-04 00:30:22,398 : INFO : 240000\n",
      "2017-01-04 00:30:22,405 : INFO : 241000\n",
      "2017-01-04 00:30:22,413 : INFO : 242000\n",
      "2017-01-04 00:30:22,421 : INFO : 243000\n",
      "2017-01-04 00:30:22,429 : INFO : 244000\n",
      "2017-01-04 00:30:22,436 : INFO : 245000\n",
      "2017-01-04 00:30:22,445 : INFO : 246000\n",
      "2017-01-04 00:30:22,451 : INFO : PROOOOBBBBBBBBBBBLEM [u'A', u'A-62', u'A-62-C', u'B', u'B-01', u'B-01-J', u'B-21', u'B-21-C', u'B-23', u'B-23-F', u'B-23-H', u'B-24', u'B-24-B', u'B-26', u'B-26-D', u'B-31', u'B-31-B', u'B-32', u'B-32-B', u'G', u'G-11', u'G-11-B']\n",
      "2017-01-04 00:30:22,454 : INFO : 247000\n",
      "2017-01-04 00:30:22,459 : INFO : PROOOOBBBBBBBBBBBLEM [u'A', u'A-61', u'A-61-F', u'B', u'B-22', u'B-22-F', u'B-24', u'B-24-D', u'B-27', u'B-27-N', u'B-29', u'B-29-C', u'B-31', u'B-31-B', u'B-32', u'B-32-B', u'B-65', u'B-65-C', u'G', u'G-05', u'G-05-G']\n",
      "2017-01-04 00:30:22,462 : INFO : 248000\n",
      "2017-01-04 00:30:22,470 : INFO : 249000\n",
      "2017-01-04 00:30:22,478 : INFO : 250000\n",
      "2017-01-04 00:30:22,486 : INFO : 251000\n",
      "2017-01-04 00:30:22,494 : INFO : 252000\n",
      "2017-01-04 00:30:22,501 : INFO : 253000\n",
      "2017-01-04 00:30:22,509 : INFO : 254000\n",
      "2017-01-04 00:30:22,518 : INFO : 255000\n",
      "2017-01-04 00:30:22,526 : INFO : 256000\n",
      "2017-01-04 00:30:22,533 : INFO : 257000\n",
      "2017-01-04 00:30:22,541 : INFO : 258000\n",
      "2017-01-04 00:30:22,549 : INFO : 259000\n",
      "2017-01-04 00:30:22,557 : INFO : 260000\n",
      "2017-01-04 00:30:22,564 : INFO : 261000\n",
      "2017-01-04 00:30:22,572 : INFO : 262000\n",
      "2017-01-04 00:30:22,581 : INFO : 263000\n",
      "2017-01-04 00:30:22,589 : INFO : 264000\n",
      "2017-01-04 00:30:22,597 : INFO : 265000\n",
      "2017-01-04 00:30:22,605 : INFO : 266000\n",
      "2017-01-04 00:30:22,613 : INFO : 267000\n",
      "2017-01-04 00:30:22,620 : INFO : PROOOOBBBBBBBBBBBLEM [u'A', u'A-01', u'A-01-J', u'A-21', u'A-21-C', u'A-23', u'A-23-G', u'A-23-P', u'A-47', u'A-47-J', u'B', u'B-01', u'B-01-J', u'B-28', u'B-28-B', u'B-29', u'B-29-C', u'B-29-D', u'B-31', u'B-31-F', u'B-32', u'B-32-B', u'C', u'C-04', u'C-04-B', u'D', u'D-02', u'D-02-G']\n",
      "2017-01-04 00:30:22,621 : INFO : 268000\n",
      "2017-01-04 00:30:22,629 : INFO : 269000\n",
      "2017-01-04 00:30:22,637 : INFO : 270000\n",
      "2017-01-04 00:30:22,644 : INFO : 271000\n",
      "2017-01-04 00:30:22,653 : INFO : 272000\n",
      "2017-01-04 00:30:22,661 : INFO : 273000\n",
      "2017-01-04 00:30:22,669 : INFO : 274000\n",
      "2017-01-04 00:30:22,676 : INFO : 275000\n",
      "2017-01-04 00:30:22,684 : INFO : 276000\n",
      "2017-01-04 00:30:22,692 : INFO : 277000\n",
      "2017-01-04 00:30:22,700 : INFO : 278000\n",
      "2017-01-04 00:30:22,707 : INFO : 279000\n",
      "2017-01-04 00:30:22,713 : INFO : PROOOOBBBBBBBBBBBLEM [u'A', u'A-61', u'A-61-K', u'B', u'B-05', u'B-05-D', u'B-29', u'B-29-C', u'B-29-D', u'B-41', u'B-41-J', u'C', u'C-03', u'C-03-C', u'C-07', u'C-07-D', u'C-08', u'C-08-F', u'C-08-G', u'C-08-J', u'C-09', u'C-09-D', u'G', u'G-01', u'G-01-D', u'G-03', u'G-03-C', u'G-03-F', u'H', u'H-05', u'H-05-B']\n",
      "2017-01-04 00:30:22,717 : INFO : 280000\n",
      "2017-01-04 00:30:22,725 : INFO : 281000\n",
      "2017-01-04 00:30:22,732 : INFO : 282000\n",
      "2017-01-04 00:30:22,740 : INFO : 283000\n",
      "2017-01-04 00:30:22,748 : INFO : 284000\n",
      "2017-01-04 00:30:22,755 : INFO : 285000\n",
      "2017-01-04 00:30:22,763 : INFO : 286000\n",
      "2017-01-04 00:30:22,771 : INFO : 287000\n",
      "2017-01-04 00:30:22,780 : INFO : 288000\n",
      "2017-01-04 00:30:22,787 : INFO : 289000\n",
      "2017-01-04 00:30:22,795 : INFO : 290000\n",
      "2017-01-04 00:30:22,803 : INFO : 291000\n",
      "2017-01-04 00:30:22,811 : INFO : 292000\n",
      "2017-01-04 00:30:22,818 : INFO : 293000\n",
      "2017-01-04 00:30:22,822 : INFO : PROOOOBBBBBBBBBBBLEM [u'A', u'A-01', u'A-01-N', u'A-61', u'A-61-K', u'B', u'B-22', u'B-22-F', u'C', u'C-01', u'C-01-B', u'C-08', u'C-08-K', u'C-09', u'C-09-D', u'C-21', u'C-21-B', u'C-22', u'C-22-B', u'C-22-C', u'H', u'H-01', u'H-01-M']\n",
      "2017-01-04 00:30:22,827 : INFO : 294000\n",
      "2017-01-04 00:30:22,835 : INFO : 295000\n",
      "2017-01-04 00:30:22,842 : INFO : 296000\n",
      "2017-01-04 00:30:22,851 : INFO : 297000\n",
      "2017-01-04 00:30:22,859 : INFO : 298000\n",
      "2017-01-04 00:30:22,866 : INFO : 299000\n",
      "2017-01-04 00:30:22,874 : INFO : 300000\n",
      "2017-01-04 00:30:22,878 : INFO : PROOOOBBBBBBBBBBBLEM [u'A', u'A-61', u'A-61-K', u'B', u'B-05', u'B-05-D', u'B-60', u'B-60-C', u'C', u'C-04', u'C-04-B', u'C-08', u'C-08-G', u'C-08-K', u'C-08-L', u'C-09', u'C-09-D', u'G', u'G-02', u'G-02-B', u'G-02-F', u'H', u'H-01', u'H-01-C', u'H-04', u'H-04-N']\n",
      "2017-01-04 00:30:22,883 : INFO : 301000\n",
      "2017-01-04 00:30:22,890 : INFO : 302000\n",
      "2017-01-04 00:30:22,898 : INFO : 303000\n",
      "2017-01-04 00:30:22,906 : INFO : 304000\n",
      "2017-01-04 00:30:22,914 : INFO : 305000\n",
      "2017-01-04 00:30:22,922 : INFO : 306000\n",
      "2017-01-04 00:30:22,930 : INFO : 307000\n",
      "2017-01-04 00:30:22,937 : INFO : 308000\n",
      "2017-01-04 00:30:22,945 : INFO : 309000\n",
      "2017-01-04 00:30:22,953 : INFO : 310000\n",
      "2017-01-04 00:30:22,961 : INFO : 311000\n",
      "2017-01-04 00:30:22,968 : INFO : 312000\n",
      "2017-01-04 00:30:22,976 : INFO : 313000\n",
      "2017-01-04 00:30:22,985 : INFO : 314000\n",
      "2017-01-04 00:30:22,992 : INFO : 315000\n",
      "2017-01-04 00:30:23,000 : INFO : 316000\n",
      "2017-01-04 00:30:23,008 : INFO : 317000\n",
      "2017-01-04 00:30:23,015 : INFO : 318000\n",
      "2017-01-04 00:30:23,023 : INFO : 319000\n",
      "2017-01-04 00:30:23,031 : INFO : 320000\n",
      "2017-01-04 00:30:23,038 : INFO : 321000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.02 s, sys: 448 ms, total: 3.47 s\n",
      "Wall time: 3.2 s\n",
      "CPU times: user 2min 59s, sys: 6 s, total: 3min 5s\n",
      "Wall time: 3min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%time Xv = pickle.load(open(data_validation_location,'r'))\n",
    "validation_data_docids = pickle.load(open(data_validation_docids_location, \"r\"))\n",
    "%time yv = get_label_data(classifications, validation_data_docids, doc_classification_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Training Metrics: Cov Err: 2.600, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.686, Top 3: 0.931, Top 5: 0.972, \n",
      "\t\t F1 Micro: 0.687, F1 Macro: 0.580, Total Pos: 2,340,112\n"
     ]
    }
   ],
   "source": [
    "print \"** Training Metrics: Cov Err: {:.3f}, Avg Labels: {:.3f}, \\n\\t\\t Top 1: {:.3f}, Top 3: {:.3f}, Top 5: {:.3f}, \\n\\t\\t F1 Micro: {:.3f}, F1 Macro: {:.3f}, Total Pos: {:,d}\".format(\n",
    "    training_metrics['coverage_error'], training_metrics['average_num_of_labels'], \n",
    "    training_metrics['top_1'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "    training_metrics['f1_micro'], training_metrics['f1_macro'], training_metrics['total_positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321473"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_data_docids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-04 00:36:08,808 : INFO : Evaluating on Validation Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 1 1]\n",
      " [0 0 0 ..., 0 1 0]\n",
      " [0 1 0 ..., 0 0 0]\n",
      " ..., \n",
      " [1 1 1 ..., 0 0 0]\n",
      " [0 1 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 1 0]]\n",
      "** Validation Metrics: Cov Err: 2.815, Avg Labels: 1.150, \n",
      "\t\t Top 1: 0.656, Top 3: 0.921, Top 5: 0.963, \n",
      "\t\t F1 Micro: 0.650, F1 Macro: 0.578, Total Pos: 617,435\n"
     ]
    }
   ],
   "source": [
    "info('Evaluating on Validation Data')\n",
    "yvp = clf.predict(Xv)\n",
    "print yvp\n",
    "validation_metrics = get_metrics(yv, yvp, yvp)\n",
    "print \"** Validation Metrics: Cov Err: {:.3f}, Avg Labels: {:.3f}, \\n\\t\\t Top 1: {:.3f}, Top 3: {:.3f}, Top 5: {:.3f}, \\n\\t\\t F1 Micro: {:.3f}, F1 Macro: {:.3f}, Total Pos: {:,d}\".format(\n",
    "    validation_metrics['coverage_error'], validation_metrics['average_num_of_labels'], \n",
    "    validation_metrics['top_1'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "    validation_metrics['f1_micro'], validation_metrics['f1_macro'], validation_metrics['total_positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = os.path.join(svm_location, SVM_MODEL_NAME, data_type)\n",
    "os.mkdir(data_folder)\n",
    "pickle.dump(clf, open(data_folder, CLASSIFIER_FILE.format(classifications_type)))\n",
    "pickle.dump(training_metrics, open(data_folder, TRAINING_METRICS_FILENAME.format(classifications_type)))\n",
    "pickle.dump(validation_metrics, open(data_folder, VALIDATION_METRICS_FILENAME.format(classifications_type)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
