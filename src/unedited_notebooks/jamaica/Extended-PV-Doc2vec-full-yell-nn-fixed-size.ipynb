{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: TITAN X (Pascal) (CNMeM is disabled, cuDNN 5105)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple, defaultdict\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "from functools import partial\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "from thesis.utils.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_SEED = 1234\n",
    "DOC2VEC_SEED = 1234\n",
    "WORD2VEC_SEED = 1234\n",
    "NN_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_WORD_COUNT = 50\n",
    "MIN_SIZE = 0\n",
    "NUM_CORES = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GLOBAL_VARS = namedtuple('GLOBAL_VARS', ['MODEL_NAME', 'DOC2VEC_MODEL_NAME', 'DOC2VEC_MODEL', \n",
    "                                         'SVM_MODEL_NAME', 'NN_MODEL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_MODEL = \"vocab_model\"\n",
    "MODEL_PREFIX = \"model\"\n",
    "VALIDATION_MATRIX = \"validation_matrix.pkl\"\n",
    "VALIDATION_DICT = \"validation_dict.pkl\"\n",
    "TEST_MATRIX = \"test_matrix.pkl\"\n",
    "TEST_DICT = \"test_dict.pkl\"\n",
    "METRICS = \"metrics.pkl\"\n",
    "CLASSIFIER = \"classifier.pkl\"\n",
    "TYPE_CLASSIFIER= \"{}_classifier.pkl\"\n",
    "\n",
    "TRAINING_DATA_MATRIX = \"X.npy\"\n",
    "TRAINING_LABELS_MATRIX = \"y_{}.npy\"\n",
    "VALIDATION_DATA_MATRIX = \"Xv.npy\"\n",
    "VALIDATION_LABELS_MATRIX = \"yv_{}.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_PARAMETER_SEARCH_PREFIX = \"lstm_{}_level_{}_batch_{}_nn_parameter_searches.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_location = \"/mnt/data2/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "doc2vec_model_save_location = os.path.join(\"/mnt/data/shalaby/\", \"parameter_search_doc2vec_models_extended_abs_desc_claims_all_levels_2\", \"full\")\n",
    "nn_parameter_search_location = os.path.join(root_location, \"nn_parameter_search_extended_abs_desc_claims_all_levels_2\")\n",
    "if not os.path.exists(doc2vec_model_save_location):\n",
    "    os.makedirs(doc2vec_model_save_location)\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL)):\n",
    "    os.makedirs(os.path.join(doc2vec_model_save_location, VOCAB_MODEL))\n",
    "if not os.path.exists(nn_parameter_search_location):\n",
    "    os.makedirs(nn_parameter_search_location)\n",
    "\n",
    "#training_file = root_location + \"docs_output.json\"\n",
    "training_file = root_location + 'docs_output.json'\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "valid_classes_file = exports_location + \"valid_classes.pkl\"\n",
    "valid_subclasses_file = exports_location + \"valid_subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "training_docs_list_file = exports_location + \"extended_pv_training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"extended_pv_validation_docs_list.pkl\"\n",
    "test_docs_list_file = exports_location + \"extended_pv_test_docs_list.pkl\"\n",
    "\n",
    "preprocessed_location = \"/mnt/data/shalaby/\" + \"preprocessed_data/extended_pv_abs_desc_claims_all_levels_2/\"\n",
    "\n",
    "training_preprocessed_files_prefix = preprocessed_location + \"extended_pv_training_docs_data_preprocessed-\"\n",
    "validation_preprocessed_files_prefix = preprocessed_location + \"extended_pv_validation_docs_data_preprocessed-\"\n",
    "test_preprocessed_files_prefix = preprocessed_location + \"extended_pv_test_docs_data_preprocessed-\"\n",
    "\n",
    "word2vec_questions_file = result = root_location + 'tensorflow/word2vec/questions-words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.4 s, sys: 680 ms, total: 16.1 s\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "valid_classes = pickle.load(open(valid_classes_file))\n",
    "valid_subclasses = pickle.load(open(valid_subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120156"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29675"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37771"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ensure_disk_location_exists(location):\n",
    "    if not os.path.exists(location):\n",
    "        os.makedirs(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OneHotEncoder():\n",
    "    \n",
    "    def __init__(self, classifications):\n",
    "        self.classifications = classifications\n",
    "        self.one_hot_indices = {}\n",
    "\n",
    "        # convert character classifications to bit vectors\n",
    "        for i, clssf in enumerate(classifications):\n",
    "            bits = [0] * len(classifications)\n",
    "            bits[i] = 1\n",
    "            self.one_hot_indices[clssf] = i\n",
    "    \n",
    "    def get_label_vector(self, labels):\n",
    "        \"\"\"\n",
    "        classes: array of string with the classes assigned to the instance\n",
    "        \"\"\"\n",
    "        output_vector = [0] * len(self.classifications)\n",
    "        for label in labels:\n",
    "            index = self.one_hot_indices[label]\n",
    "            output_vector[index] = 1\n",
    "            \n",
    "        return output_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Document, Paragraph and Sentence Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FixedDocumentsStatsGenerator(object):\n",
    "    def __init__(self, filename_prefix):\n",
    "        self.filename_prefix = filename_prefix\n",
    "        self.docids = []\n",
    "        self.doc_parts = defaultdict(list)\n",
    "        self.doc_part_chunks = defaultdict(list)\n",
    "        self.curr_doc_index = 0\n",
    "        self.batch_end = -1\n",
    "    def load_new_batch_in_memory(self):\n",
    "        info(\"Loading new batch for index: {}\".format(self.curr_doc_index))\n",
    "        true_docs_count = 0\n",
    "        try:\n",
    "            with open(self.filename_prefix + str(self.curr_doc_index)) as preproc_file:\n",
    "                for line in preproc_file:\n",
    "                    line_array = line.split(\" \", 1)\n",
    "                    entity_id = line_array[0].strip()\n",
    "                    if self.is_doc(entity_id):\n",
    "                        self.docids.append(entity_id)\n",
    "                        true_docs_count+= 1\n",
    "                    elif self.is_doc_part(entity_id):\n",
    "                        self.doc_parts[self.get_doc_id(entity_id)].append(entity_id)\n",
    "                    elif self.is_doc_part_chunk(entity_id):\n",
    "                        self.doc_part_chunks[self.get_doc_id(entity_id)].append(entity_id)\n",
    "            self.batch_end = self.curr_doc_index + true_docs_count - 1 \n",
    "            info(\"Finished loading new batch of {} documents\".format(true_docs_count))\n",
    "        except IOError:\n",
    "            info(\"No more batches to load, exiting at index: {}\".format(self.curr_doc_index))\n",
    "            raise StopIteration()\n",
    "    def get_stats(self):\n",
    "        try:\n",
    "            while True:\n",
    "                if self.curr_doc_index > self.batch_end:\n",
    "                    self.load_new_batch_in_memory()\n",
    "                self.curr_doc_index = self.batch_end + 1\n",
    "        except StopIteration:\n",
    "            pass\n",
    "            \n",
    "    def get_doc_id(self, entity_id):\n",
    "        return entity_id.split(\"_\")[0]\n",
    "    def get_entity_parts(self, entity_id):\n",
    "        return entity_id.split(\"_\")\n",
    "    def is_doc(self, entity_id):\n",
    "        parts = self.get_entity_parts(entity_id)\n",
    "        if len(parts) == 1:\n",
    "            return True\n",
    "        return False\n",
    "    def is_doc_part(self, entity_id):\n",
    "        parts = self.get_entity_parts(entity_id)\n",
    "        if len(parts) == 2:\n",
    "            return True\n",
    "        return False\n",
    "    def is_doc_part_chunk(self, entity_id):\n",
    "        parts = self.get_entity_parts(entity_id)\n",
    "        if len(parts) == 3:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_doc_vector(entity_id):\n",
    "    if entity_id in doc2vec_model.docvecs:\n",
    "        if DOC2VEC_MMAP:\n",
    "            normal_array = []\n",
    "            normal_array[:] = doc2vec_model.docvecs[entity_id][:]\n",
    "            return normal_array\n",
    "        else:\n",
    "            return doc2vec_model.docvecs[entity_id]\n",
    "    else:\n",
    "        # some claims have low token count, so they cant fill out the whole 16 spots\n",
    "        return ZERO_VECTOR\n",
    "\n",
    "def data_generator(doc_stats, doc_id):\n",
    "    yield get_doc_vector(doc_id)\n",
    "    for part_id in doc_stats.doc_parts[doc_id]:\n",
    "        yield get_doc_vector(part_id)\n",
    "    for part_id in doc_stats.doc_part_chunks[doc_id]:\n",
    "        yield get_doc_vector(part_id)\n",
    "    yield ZERO_VECTOR\n",
    "\n",
    "def validation_data_generator(doc_stats, validation_dict, doc_id):\n",
    "    yield validation_dict[doc_id]\n",
    "    for part_id in doc_stats.doc_parts[doc_id]:\n",
    "        yield validation_dict[part_id]\n",
    "    for part_id in doc_stats.doc_part_chunks[doc_id]:\n",
    "        yield get_doc_vector(part_id)\n",
    "    yield ZERO_VECTOR\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_training_data(doc2vec_model, classifications, classifications_type, doc_stats, sequence_size, embedding_size):\n",
    "    \n",
    "    def get_training_y_labels():\n",
    "        if not os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                       TRAINING_LABELS_MATRIX.format(classifications_type))):\n",
    "            info(\"Creating Training Labels\")\n",
    "            one_hot_encoder = OneHotEncoder(classifications)\n",
    "            classifications_set = set(classifications)\n",
    "            training_labels_mat = np.zeros((len(training_docs_list), len(classifications)), dtype=np.int8)\n",
    "            for i, doc_id in enumerate(training_docs_list):\n",
    "                eligible_classifications = set(doc_classification_map[doc_id]) & classifications_set\n",
    "                training_labels_mat[i][:] = one_hot_encoder.get_label_vector(eligible_classifications)\n",
    "        else:    \n",
    "            training_labels_mat = np.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                            TRAINING_LABELS_MATRIX.format(classifications_type))))\n",
    "        return training_labels_mat\n",
    "\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, TRAINING_DATA_MATRIX)):\n",
    "        info(\"Creating Training Data\")\n",
    "        one_hot_encoder = OneHotEncoder(classifications)\n",
    "        classifications_set = set(classifications)\n",
    "        # 1st level: document level\n",
    "        training_data = np.ndarray((len(training_docs_list), sequence_size, embedding_size), dtype=np.float32)\n",
    "        info(\"Training Data shape: {}\".format(training_data.shape))\n",
    "        training_labels_mat = np.zeros((len(training_docs_list), len(classifications)), dtype=np.int8)\n",
    "        for i, doc_id in enumerate(training_docs_list):\n",
    "            data_gen = data_generator(doc_stats, doc_id)\n",
    "            # 2nd level: constituents\n",
    "            for j in range(sequence_size):\n",
    "                #3rd level: feature vectors\n",
    "                training_data[i][j] = data_gen.next()\n",
    "            eligible_classifications = set(doc_classification_map[doc_id]) & classifications_set\n",
    "            training_labels_mat[i][:] = one_hot_encoder.get_label_vector(eligible_classifications)\n",
    "            if i % 10000 == 0:\n",
    "                info(\"Finished {} in training\".format(i))\n",
    "        \n",
    "        info(\"Saving Training Data to file...\")\n",
    "        np.save(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                  TRAINING_DATA_MATRIX), \"w\"), training_data)\n",
    "        np.save(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                  TRAINING_LABELS_MATRIX.format(classifications_type)), \"w\"), training_labels_mat)\n",
    "    else:\n",
    "        info(\"Loading Training Data from file\")\n",
    "        training_data = np.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                  TRAINING_DATA_MATRIX)))\n",
    "        training_labels_mat = get_training_y_labels()\n",
    "        \n",
    "    return training_data, training_labels_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_data(validation_dict, classifications, classifications_type, doc_stats, sequence_size, embedding_size):\n",
    "    \n",
    "    def get_validation_y_labels():\n",
    "        if not os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                       VALIDATION_LABELS_MATRIX.format(classifications_type))):\n",
    "            info(\"Creating Validation Labels\")\n",
    "            one_hot_encoder = OneHotEncoder(classifications)\n",
    "            classifications_set = set(classifications)\n",
    "            validation_labels_mat = np.zeros((len(validation_docs_list), len(classifications)), dtype=np.int8)\n",
    "            for i, doc_id in enumerate(validation_docs_list):\n",
    "                eligible_classifications = set(doc_classification_map[doc_id]) & classifications_set\n",
    "                validation_labels_mat[i][:] = one_hot_encoder.get_label_vector(eligible_classifications)\n",
    "        else:    \n",
    "            validation_labels_mat = np.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                        VALIDATION_LABELS_MATRIX.format(classifications_type))))\n",
    "        return validation_labels_mat\n",
    "\n",
    "    \n",
    "    if not os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_DATA_MATRIX)):\n",
    "        info(\"Creating Validation Data\")\n",
    "        one_hot_encoder = OneHotEncoder(classifications)\n",
    "        classifications_set = set(classifications)\n",
    "        # 1st level: document level\n",
    "        validation_data = np.ndarray((len(validation_docs_list), sequence_size, embedding_size), dtype=np.float32)\n",
    "        info(\"Validation Data shape: {}\".format(validation_data.shape))\n",
    "        validation_labels_mat = np.zeros((len(validation_docs_list), len(classifications)), dtype=np.int8)\n",
    "        for i, doc_id in enumerate(validation_docs_list):\n",
    "            data_gen = validation_data_generator(doc_stats, validation_dict, doc_id)\n",
    "            # 2nd level: constituents\n",
    "            for j in range(sequence_size):\n",
    "                #3d level: feature vectors\n",
    "                validation_data[i][j]= data_gen.next()\n",
    "            eligible_classifications = set(doc_classification_map[doc_id]) & classifications_set\n",
    "            validation_labels_mat[i][:] = one_hot_encoder.get_label_vector(eligible_classifications)\n",
    "            if i % 10000 == 0:\n",
    "                info(\"Finished {} in validation\".format(i))\n",
    "        \n",
    "        info(\"Saving Validation Data to file...\")\n",
    "        np.save(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                  VALIDATION_DATA_MATRIX), \"w\"), validation_data)\n",
    "        np.save(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                  VALIDATION_LABELS_MATRIX.format(classifications_type)), \"w\"), validation_labels_mat)\n",
    "    else:\n",
    "        info(\"Loading Validation Data from file\")\n",
    "        validation_data = np.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                  VALIDATION_DATA_MATRIX)))\n",
    "        validation_labels_mat = get_validation_y_labels()\n",
    "        \n",
    "    return validation_data, validation_labels_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Masking\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifications = sections\n",
    "classifications_type = 'sections'\n",
    "classifier_file = TYPE_CLASSIFIER.format(classifications_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VALIDATION_METRICS_FILENAME= '{}_validation_metrics.pkl'.format(classifications_type)\n",
    "TRAINING_METRICS_FILENAME = '{}_training_metrics.pkl'.format(classifications_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOC2VEC_SIZE = 200\n",
    "DOC2VEC_WINDOW = 2\n",
    "DOC2VEC_MAX_VOCAB_SIZE = None\n",
    "DOC2VEC_SAMPLE = 1e-3\n",
    "DOC2VEC_TYPE = 1\n",
    "DOC2VEC_HIERARCHICAL_SAMPLE = 0\n",
    "DOC2VEC_NEGATIVE_SAMPLE_SIZE = 10\n",
    "DOC2VEC_CONCAT = 0\n",
    "DOC2VEC_MEAN = 1\n",
    "DOC2VEC_TRAIN_WORDS = 0\n",
    "DOC2VEC_EPOCHS = 1 # we do our training manually one epoch at a time\n",
    "DOC2VEC_MAX_EPOCHS = 8\n",
    "REPORT_DELAY = 20 # report the progress every x seconds\n",
    "REPORT_VOCAB_PROGRESS = 100000 # report vocab progress every x documents\n",
    "\n",
    "DOC2VEC_MMAP = 'r'\n",
    "# DOC2VEC_MMAP = None\n",
    "\n",
    "ZERO_VECTOR = [0] * DOC2VEC_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8\n"
     ]
    }
   ],
   "source": [
    "placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}'.format(DOC2VEC_SIZE, \n",
    "                                                                DOC2VEC_WINDOW, \n",
    "                                                                'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                                DOC2VEC_TRAIN_WORDS,\n",
    "                                                                DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                                str(DOC2VEC_MAX_VOCAB_SIZE))\n",
    "GLOBAL_VARS.DOC2VEC_MODEL_NAME = placeholder_model_name\n",
    "placeholder_model_name = os.path.join(placeholder_model_name, \"epoch_{}\")\n",
    "\n",
    "epoch = 8\n",
    "\n",
    "GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "doc2vec_model = None\n",
    "print GLOBAL_VARS.MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 08:47:43,109 : INFO : loading Doc2Vec object from /mnt/data/shalaby/parameter_search_doc2vec_models_extended_abs_desc_claims_all_levels_2/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/shalaby/parameter_search_doc2vec_models_extended_abs_desc_claims_all_levels_2/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 08:48:53,424 : INFO : loading docvecs recursively from /mnt/data/shalaby/parameter_search_doc2vec_models_extended_abs_desc_claims_all_levels_2/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.docvecs.* with mmap=r\n",
      "2017-03-20 08:48:53,426 : INFO : loading doctag_syn0 from /mnt/data/shalaby/parameter_search_doc2vec_models_extended_abs_desc_claims_all_levels_2/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.docvecs.doctag_syn0.npy with mmap=r\n",
      "2017-03-20 08:48:53,428 : INFO : loading syn1neg from /mnt/data/shalaby/parameter_search_doc2vec_models_extended_abs_desc_claims_all_levels_2/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.syn1neg.npy with mmap=r\n",
      "2017-03-20 08:48:53,429 : INFO : loading syn0 from /mnt/data/shalaby/parameter_search_doc2vec_models_extended_abs_desc_claims_all_levels_2/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/model.syn0.npy with mmap=r\n",
      "2017-03-20 08:48:53,431 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-03-20 08:48:53,433 : INFO : setting ignored attribute cum_table to None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41 s, sys: 28.9 s, total: 1min 9s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)\n",
    "if os.path.exists(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX)):\n",
    "    doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX), mmap=DOC2VEC_MMAP)\n",
    "    doc2vec_model.workers = NUM_CORES\n",
    "    GLOBAL_VARS.DOC2VEC_MODEL = doc2vec_model\n",
    "else:\n",
    "    info(\"Couldnt find the doc2vec model with epoch {}\".format(epoch))\n",
    "    raise Exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_doc_stats_file = os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \"extended_pv_training_doc_stats.pkl\")\n",
    "validation_doc_stats_file = os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, \"extended_pv_validation_doc_stats.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create/Load Training Document Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PARTS_LEVEL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 10:14:52,153 : INFO : Creating Training Document Stats\n",
      "2017-03-20 10:14:52,155 : INFO : Loading new batch for index: 0\n",
      "2017-03-20 10:15:04,161 : INFO : Finished loading new batch of 10000 documents\n",
      "2017-03-20 10:15:04,164 : INFO : Loading new batch for index: 10000\n",
      "2017-03-20 10:15:16,567 : INFO : Finished loading new batch of 10000 documents\n",
      "2017-03-20 10:15:16,569 : INFO : Loading new batch for index: 20000\n",
      "2017-03-20 10:15:27,014 : INFO : Finished loading new batch of 10000 documents\n",
      "2017-03-20 10:15:27,015 : INFO : Loading new batch for index: 30000\n",
      "2017-03-20 10:15:37,173 : INFO : Finished loading new batch of 10000 documents\n",
      "2017-03-20 10:15:37,175 : INFO : Loading new batch for index: 40000\n",
      "2017-03-20 10:15:47,363 : INFO : Finished loading new batch of 10000 documents\n",
      "2017-03-20 10:15:47,365 : INFO : Loading new batch for index: 50000\n",
      "2017-03-20 10:15:57,379 : INFO : Finished loading new batch of 10000 documents\n",
      "2017-03-20 10:15:57,381 : INFO : Loading new batch for index: 60000\n",
      "2017-03-20 10:16:07,267 : INFO : Finished loading new batch of 10000 documents\n",
      "2017-03-20 10:16:07,268 : INFO : Loading new batch for index: 70000\n",
      "2017-03-20 10:16:16,558 : INFO : Finished loading new batch of 10000 documents\n",
      "2017-03-20 10:16:16,560 : INFO : Loading new batch for index: 80000\n",
      "2017-03-20 10:16:21,142 : INFO : Finished loading new batch of 10000 documents\n",
      "2017-03-20 10:16:21,143 : INFO : Loading new batch for index: 90000\n",
      "2017-03-20 10:16:25,680 : INFO : Finished loading new batch of 10000 documents\n",
      "2017-03-20 10:16:25,681 : INFO : Loading new batch for index: 100000\n",
      "2017-03-20 10:16:39,443 : INFO : Finished loading new batch of 10000 documents\n",
      "2017-03-20 10:16:39,445 : INFO : Loading new batch for index: 110000\n",
      "2017-03-20 10:16:53,189 : INFO : Finished loading new batch of 10000 documents\n",
      "2017-03-20 10:16:53,192 : INFO : Loading new batch for index: 120000\n",
      "2017-03-20 10:16:53,427 : INFO : Finished loading new batch of 156 documents\n",
      "2017-03-20 10:16:53,429 : INFO : Loading new batch for index: 120156\n",
      "2017-03-20 10:16:53,430 : INFO : No more batches to load, exiting at index: 120156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 15.8 s, total: 1min 46s\n",
      "Wall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.exists(training_doc_stats_file):\n",
    "    info(\"Creating Training Document Stats\")\n",
    "    doc_stats = FixedDocumentsStatsGenerator(training_preprocessed_files_prefix)\n",
    "    doc_stats.get_stats()\n",
    "    pickle.dump(doc_stats, open(training_doc_stats_file, \"w\"))\n",
    "else:\n",
    "    info(\"Loading Training Document Stats\")\n",
    "    doc_stats = pickle.load(open(training_doc_stats_file, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts: 3\n",
      "Chunks: 74\n",
      "Max Size: 78\n"
     ]
    }
   ],
   "source": [
    "MAX_PARTS = int(np.max([len(doc_stats.doc_parts[d]) for d in doc_stats.docids]))\n",
    "print \"Parts: {}\".format(MAX_PARTS)\n",
    "MAX_PART_CHUNKS = int(np.max([len(doc_stats.doc_part_chunks[d]) for d in doc_stats.docids]))\n",
    "print \"Chunks: {}\".format(MAX_PART_CHUNKS)\n",
    "# +1 is for the document vector\n",
    "MAX_SIZE = MAX_PARTS + MAX_PART_CHUNKS + 1\n",
    "print \"Max Size: {}\".format(MAX_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Training Data Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 10:19:55,187 : INFO : Creating Training Data\n",
      "2017-03-20 10:19:55,189 : INFO : Training Data shape: (120156, 78, 200)\n",
      "2017-03-20 10:19:55,201 : INFO : Finished 0 in training\n",
      "2017-03-20 10:20:29,667 : INFO : Finished 10000 in training\n",
      "2017-03-20 10:21:05,452 : INFO : Finished 20000 in training\n",
      "2017-03-20 10:21:40,453 : INFO : Finished 30000 in training\n",
      "2017-03-20 10:22:16,031 : INFO : Finished 40000 in training\n",
      "2017-03-20 10:22:51,824 : INFO : Finished 50000 in training\n",
      "2017-03-20 10:23:26,160 : INFO : Finished 60000 in training\n",
      "2017-03-20 10:24:00,336 : INFO : Finished 70000 in training\n",
      "2017-03-20 10:24:34,654 : INFO : Finished 80000 in training\n",
      "2017-03-20 10:25:08,972 : INFO : Finished 90000 in training\n",
      "2017-03-20 10:25:43,246 : INFO : Finished 100000 in training\n",
      "2017-03-20 10:26:17,805 : INFO : Finished 110000 in training\n",
      "2017-03-20 10:26:52,372 : INFO : Finished 120000 in training\n",
      "2017-03-20 10:26:52,905 : INFO : Saving Training Data to file...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 28s, sys: 31.6 s, total: 7min\n",
      "Wall time: 7min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X, y = get_training_data(doc2vec_model, classifications, classifications_type, doc_stats, MAX_SIZE, DOC2VEC_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_docs_list.index('06984387')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7497734528\n",
      "(120156, 78, 200)\n",
      "(120156, 8)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print sys.getsizeof(X)\n",
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Validation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_dict = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create/Load Validation Doc Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Validation Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 17s, sys: 3.4 s, total: 3min 20s\n",
      "Wall time: 3min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "validation_dict = pickle.load(open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_DICT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 10:49:22,518 : INFO : Loading new batch for index: 0\n",
      "2017-03-20 10:49:34,065 : INFO : Finished loading new batch of 10000 documents\n",
      "2017-03-20 10:49:34,067 : INFO : Loading new batch for index: 10000\n",
      "2017-03-20 10:49:46,515 : INFO : Finished loading new batch of 10000 documents\n",
      "2017-03-20 10:49:46,518 : INFO : Loading new batch for index: 20000\n",
      "2017-03-20 10:49:57,773 : INFO : Finished loading new batch of 9675 documents\n",
      "2017-03-20 10:49:57,775 : INFO : Loading new batch for index: 29675\n",
      "2017-03-20 10:49:57,776 : INFO : No more batches to load, exiting at index: 29675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.3 s, sys: 4.53 s, total: 26.9 s\n",
      "Wall time: 39.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.exists(validation_doc_stats_file) or True:\n",
    "    validation_doc_stats = FixedDocumentsStatsGenerator(validation_preprocessed_files_prefix)\n",
    "    validation_doc_stats.get_stats()\n",
    "    pickle.dump(validation_doc_stats, open(validation_doc_stats_file, \"w\"))\n",
    "else:\n",
    "    info(\"Loading Validation Document Stats\")\n",
    "    validation_doc_stats = pickle.load(open(validation_doc_stats_file, \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Validation Data Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 10:58:37,429 : INFO : Creating Validation Data\n",
      "2017-03-20 10:58:37,432 : INFO : Validation Data shape: (29675, 78, 200)\n",
      "2017-03-20 10:58:37,436 : INFO : Finished 0 in validation\n",
      "2017-03-20 10:58:43,473 : INFO : Finished 10000 in validation\n",
      "2017-03-20 10:58:49,195 : INFO : Finished 20000 in validation\n",
      "2017-03-20 10:58:54,662 : INFO : Saving Validation Data to file...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 s, sys: 4.93 s, total: 18.4 s\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Xv, yv = get_validation_data(validation_dict, classifications, classifications_type, validation_doc_stats, \n",
    "                             MAX_SIZE, DOC2VEC_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del validation_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_keras_rnn_model(input_size, output_size, lstm_output_size, w_dropout_do, u_dropout_do, \n",
    "                           stack_layers=1, conv_size=None):\n",
    "    \n",
    "    model= Sequential()\n",
    "#     model.add(Masking(mask_value=0., input_shape=(MAX_SIZE, input_size)))\n",
    "    if conv_size:\n",
    "        model.add(Convolution1D(nb_filter=conv_size, input_shape=(MAX_SIZE, input_size), filter_length=3, \n",
    "                                border_mode='same', activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_length=2))\n",
    "    for i in range(stack_layers):\n",
    "        model.add(LSTM(lstm_output_size, input_dim=input_size, dropout_W=w_dropout_do, dropout_U=u_dropout_do,\n",
    "                       return_sequences=False if i+1 == stack_layers else True,\n",
    "                  name='lstm_{}_u-drop_{}_w-drop_{}_layer_{}'.format(lstm_output_size, str(u_dropout_do), str(w_dropout_do), str(i+1))))\n",
    "    model.add(Dense(output_size, activation='sigmoid', name='sigmoid_output'))\n",
    "    model.compile(optimizer=NN_OPTIMIZER, loss='binary_crossentropy')\n",
    "    return model\n",
    "    \n",
    "#     lstm = LSTM(lstm_output_size, input_dim=input_size, dropout_W=w_dropout_do, dropout_U=u_dropout_do)\n",
    "    \n",
    "#     pooling = GlobalAveragePooling1D()(lstm)\n",
    "    \n",
    "#     sigmoid_output = Dense(output_size, activation='sigmoid', name='sigmoid_output')(pooling)\n",
    "\n",
    "#     model = Model(input=doc_input, output=sigmoid_output)\n",
    "#     model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stopper_deltas = {\n",
    "    'sections': 0.0001,\n",
    "    'classes': 0.00001,\n",
    "    'subclasses': 0.00001\n",
    "}\n",
    "early_stopper_patience = {\n",
    "    'sections': 10,\n",
    "    'classes': 10,\n",
    "    'subclasses': 10\n",
    "}\n",
    "epochs_before_validation = {\n",
    "    'sections': 10,\n",
    "    'classes': 50,\n",
    "    'subclasses': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_OUTPUT_NEURONS = len(classifications)\n",
    "\n",
    "EARLY_STOPPER_MIN_DELTA = early_stopper_deltas[classifications_type]\n",
    "EARLY_STOPPER_PATIENCE = early_stopper_patience[classifications_type]\n",
    "\n",
    "NN_MAX_EPOCHS = 200\n",
    "NN_RANDOM_SEARCH_BUDGET = 12\n",
    "NN_PARAM_SAMPLE_SEED = 1234\n",
    "\n",
    "NN_BATCH_SIZE = 2048\n",
    "\n",
    "MODEL_VERBOSITY = 1\n",
    "\n",
    "NN_OPTIMIZER = 'rmsprop'\n",
    "# NN_OPTIMIZER = 'adam'\n",
    "\n",
    "to_skip = []\n",
    "\n",
    "load_existing_results = True\n",
    "save_results = True\n",
    "\n",
    "\n",
    "lstm_output_sizes = [200,300,500]\n",
    "w_dropout_options = [0.4,0.6]\n",
    "u_dropout_options = [0.4,0.6]\n",
    "stack_layers_options = [1]\n",
    "conv_size_options = [None]\n",
    "\n",
    "\n",
    "\n",
    "# Uncomment for Specific Configuration\n",
    "NN_RANDOM_SEARCH_BUDGET = 1\n",
    "lstm_output_sizes = [500]\n",
    "w_dropout_options = [0.6]\n",
    "u_dropout_options = [0.4]\n",
    "stack_layers_options = [1]\n",
    "conv_size_options = [None]\n",
    "# conv_size_options = [None, 32,100,200,300]\n",
    "\n",
    "\n",
    "np.random.seed(NN_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MetricsCallback(keras.callbacks.Callback):\n",
    "    \n",
    "    EPOCHS_BEFORE_VALIDATION = epochs_before_validation[classifications_type]\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.epoch_index = 0\n",
    "        self.val_loss_reductions = 0\n",
    "        self.metrics_dict = {}\n",
    "        self.best_val_loss = np.iinfo(np.int32).max\n",
    "        self.best_weights = None\n",
    "        self.best_validation_metrics = None\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epoch_index += 1\n",
    "        if logs['val_loss'] < self.best_val_loss:\n",
    "            self.val_loss_reductions += 1\n",
    "            self.best_val_loss = logs['val_loss']\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            print '\\r    \\r' # to remove the previous line of verbose output of model fit\n",
    "            time.sleep(0.2)\n",
    "            info('Found lower val loss for epoch {} => {}'.format(self.epoch_index, round(logs['val_loss'], 5)))\n",
    "            if self.val_loss_reductions % MetricsCallback.EPOCHS_BEFORE_VALIDATION == 0:\n",
    "                \n",
    "                info('Validation Loss Reduced {} times'.format(self.val_loss_reductions))\n",
    "                info('Evaluating on Validation Data')\n",
    "                yvp = self.model.predict(Xv)\n",
    "                yvp_binary = get_binary_0_5(yvp)\n",
    "                info('Generating Validation Metrics')\n",
    "                validation_metrics = get_metrics(yv, yvp, yvp_binary)\n",
    "                print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "                    validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "                    validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "                self.metrics_dict[self.epoch_index] = validation_metrics\n",
    "#                 self.best_validation_metrics = validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 15:47:46,567 : INFO : No Previous results exist in /mnt/data2/shalaby/nn_parameter_search_extended_abs_desc_claims_all_levels_2/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None/epoch_8/lstm_sections_level_2_batch_2048_nn_parameter_searches.pkl\n",
      "2017-03-20 15:47:46,570 : INFO : ***************************************************************************************\n",
      "2017-03-20 15:47:46,571 : INFO : lstm_optimizer_rmsprop_size_500_w-drop_0.6_u-drop_0.4_stack_1_conv_None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_500_u-drop_0.4_w-drop_0.6_l (None, 500)           1402000     lstm_input_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "sigmoid_output (Dense)           (None, 8)             4008        lstm_500_u-drop_0.4_w-drop_0.6_la\n",
      "====================================================================================================\n",
      "Total params: 1406008\n",
      "____________________________________________________________________________________________________\n",
      "Train on 120156 samples, validate on 29675 samples\n",
      "Epoch 1/200\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 15:49:24,072 : INFO : Found lower val loss for epoch 1 => 0.36241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 83s - loss: 0.3918 - val_loss: 0.3624\n",
      "Epoch 2/200\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 15:50:49,788 : INFO : Found lower val loss for epoch 2 => 0.35994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 85s - loss: 0.3617 - val_loss: 0.3599\n",
      "Epoch 3/200\n",
      "120156/120156 [==============================] - 87s - loss: 0.3393 - val_loss: 0.3667\n",
      "Epoch 4/200\n",
      "120156/120156 [==============================] - 89s - loss: 0.3281 - val_loss: 0.3897\n",
      "Epoch 5/200\n",
      "120156/120156 [==============================] - 90s - loss: 0.3166 - val_loss: 0.3894\n",
      "Epoch 6/200\n",
      "120156/120156 [==============================] - 90s - loss: 0.3101 - val_loss: 0.3730\n",
      "Epoch 7/200\n",
      "120156/120156 [==============================] - 90s - loss: 0.3038 - val_loss: 0.4167\n",
      "Epoch 8/200\n",
      "120156/120156 [==============================] - 90s - loss: 0.2992 - val_loss: 0.4073\n",
      "Epoch 9/200\n",
      "120156/120156 [==============================] - 90s - loss: 0.2942 - val_loss: 0.3911\n",
      "Epoch 10/200\n",
      "120156/120156 [==============================] - 90s - loss: 0.2892 - val_loss: 0.3649\n",
      "Epoch 11/200\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 16:04:21,275 : INFO : Found lower val loss for epoch 11 => 0.35107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 91s - loss: 0.2839 - val_loss: 0.3511\n",
      "Epoch 12/200\n",
      "120156/120156 [==============================] - 91s - loss: 0.2799 - val_loss: 0.3684\n",
      "Epoch 13/200\n",
      "120156/120156 [==============================] - 91s - loss: 0.2762 - val_loss: 0.4077\n",
      "Epoch 14/200\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 16:08:55,970 : INFO : Found lower val loss for epoch 14 => 0.31955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120156/120156 [==============================] - 91s - loss: 0.2724 - val_loss: 0.3196\n",
      "Epoch 15/200\n",
      "120156/120156 [==============================] - 90s - loss: 0.2686 - val_loss: 0.3506\n",
      "Epoch 16/200\n",
      "120156/120156 [==============================] - 90s - loss: 0.2651 - val_loss: 0.3776\n",
      "Epoch 17/200\n",
      "120156/120156 [==============================] - 91s - loss: 0.2625 - val_loss: 0.3303\n",
      "Epoch 18/200\n",
      "120156/120156 [==============================] - 91s - loss: 0.2596 - val_loss: 0.3610\n",
      "Epoch 19/200\n",
      "120156/120156 [==============================] - 90s - loss: 0.2567 - val_loss: 0.3529\n",
      "Epoch 20/200\n",
      "120156/120156 [==============================] - 91s - loss: 0.2536 - val_loss: 0.3793\n",
      "Epoch 21/200\n",
      "120156/120156 [==============================] - 90s - loss: 0.2519 - val_loss: 0.4083\n",
      "Epoch 22/200\n",
      "120156/120156 [==============================] - 91s - loss: 0.2492 - val_loss: 0.3496\n",
      "Epoch 23/200\n",
      "120156/120156 [==============================] - 91s - loss: 0.2465 - val_loss: 0.3698\n",
      "Epoch 24/200\n",
      " 43008/120156 [=========>....................] - ETA: 52s - loss: 0.2451"
     ]
    }
   ],
   "source": [
    "param_sampler = ParameterSampler({\n",
    "    'lstm_output_size':lstm_output_sizes,\n",
    "    'w_dropout':w_dropout_options,\n",
    "    'u_dropout':u_dropout_options,\n",
    "    'stack_layers':stack_layers_options,\n",
    "    'conv_size':conv_size_options,\n",
    "}, n_iter=NN_RANDOM_SEARCH_BUDGET, random_state=NN_PARAM_SAMPLE_SEED)\n",
    "\n",
    "param_results_dict = {}\n",
    "if load_existing_results:\n",
    "    param_results_path = os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                       NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, PARTS_LEVEL, NN_BATCH_SIZE)))\n",
    "    if os.path.exists(param_results_path):\n",
    "        param_results_dict = pickle.load(open(param_results_path))\n",
    "    else:\n",
    "        info('No Previous results exist in {}'.format(param_results_path))\n",
    "        \n",
    "# create nn parameter search directory\n",
    "if not os.path.exists(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME)):\n",
    "    os.makedirs(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME))\n",
    "        \n",
    "for parameters in param_sampler:\n",
    "    start_time = time.time()\n",
    "    lstm_output_size = parameters['lstm_output_size']\n",
    "    w_dropout_do = parameters['w_dropout']\n",
    "    u_dropout_do = parameters['u_dropout']\n",
    "    stack_layers = parameters['stack_layers']\n",
    "    conv_size = parameters['conv_size']\n",
    "\n",
    "    GLOBAL_VARS.NN_MODEL_NAME = 'lstm_optimizer_{}_size_{}_w-drop_{}_u-drop_{}_stack_{}_conv_{}'.format(NN_OPTIMIZER,\n",
    "        lstm_output_size,  w_dropout_do, u_dropout_do, stack_layers, str(conv_size)\n",
    "    )\n",
    "\n",
    "    if GLOBAL_VARS.NN_MODEL_NAME in param_results_dict.keys() or GLOBAL_VARS.NN_MODEL_NAME in to_skip:\n",
    "        print \"skipping: {}\".format(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "        continue\n",
    "#         if first_hidden_layer_size < DOC2VEC_SIZE or second_hidden_layer_size < NN_OUTPUT_NEURONS:\n",
    "#             print \"skipping: {} due to 1st layer size {} < {} or 2nd layer size {} < {}\".format(GLOBAL_VARS.NN_MODEL_NAME,\n",
    "#                                                                                                 first_hidden_layer_size, DOC2VEC_SIZE, \n",
    "#                                                                                                 second_hidden_layer_size, NN_OUTPUT_NEURONS)\n",
    "#             continue\n",
    "\n",
    "\n",
    "    info('***************************************************************************************')\n",
    "    info(GLOBAL_VARS.NN_MODEL_NAME)\n",
    "\n",
    "    model = create_keras_rnn_model(DOC2VEC_SIZE, NN_OUTPUT_NEURONS, \n",
    "                                  lstm_output_size, w_dropout_do, u_dropout_do, stack_layers, conv_size)\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    early_stopper = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=EARLY_STOPPER_MIN_DELTA, \\\n",
    "                                                  patience=EARLY_STOPPER_PATIENCE, verbose=1, mode='auto')\n",
    "    metrics_callback = MetricsCallback()\n",
    "\n",
    "\n",
    "\n",
    "    # Model Fitting\n",
    "    %time history = model.fit(x=X, y=y, validation_data=(Xv,yv), batch_size=NN_BATCH_SIZE, \\\n",
    "                              nb_epoch=NN_MAX_EPOCHS, verbose=MODEL_VERBOSITY, \\\n",
    "                              callbacks=[early_stopper, metrics_callback])\n",
    "    \n",
    "    info('Evaluating on Training Data')\n",
    "    yp = model.predict(X)\n",
    "    yp_binary = get_binary_0_5(yp)\n",
    "    #print yvp\n",
    "    info('Generating Training Metrics')\n",
    "    training_metrics = get_metrics(y, yp, yp_binary)\n",
    "    print \"****** Training Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    training_metrics['coverage_error'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "    training_metrics['f1_micro'], training_metrics['f1_macro'])\n",
    "    \n",
    "    \n",
    "    info('Evaluating on Validation Data using saved best weights')\n",
    "    model.set_weights(metrics_callback.best_weights)\n",
    "    yvp = model.predict(Xv)\n",
    "    yvp_binary = get_binary_0_5(yvp)\n",
    "    #print yvp\n",
    "    info('Generating Validation Metrics')\n",
    "    validation_metrics = get_metrics(yv, yvp, yvp_binary)\n",
    "    print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "        validation_metrics['coverage_error'], validation_metrics['top_3'], validation_metrics['top_5'], \n",
    "        validation_metrics['f1_micro'], validation_metrics['f1_macro'])\n",
    "    best_validation_metrics = validation_metrics\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME] = dict()\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['best_validation_metrics'] = best_validation_metrics\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['epochs'] = len(history.history['val_loss'])\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['best_weights'] = metrics_callback.best_weights\n",
    "#         param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['last_weights'] = last_model_weights\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    param_results_dict[GLOBAL_VARS.NN_MODEL_NAME]['duration'] =  duration\n",
    "\n",
    "    del history, metrics_callback, model\n",
    "\n",
    "if save_results:\n",
    "    pickle.dump(param_results_dict, open(os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                                   NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, PARTS_LEVEL, NN_BATCH_SIZE))), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(param_results_dict, open(os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                                   NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, PARTS_LEVEL, NN_BATCH_SIZE))), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NameError: name 'model' is not defined\n"
     ]
    }
   ],
   "source": [
    "%xdel model\n",
    "import gc\n",
    "for i in range(3): gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lstm_optimizer_rmsprop_size_200_w-drop_0.4_u-drop_0.2',\n",
       " 'lstm_optimizer_adam_size_300_w-drop_0.2_u-drop_0.3',\n",
       " 'lstm_optimizer_adam_size_200_w-drop_0.3_u-drop_0.2',\n",
       " 'lstm_optimizer_adam_size_200_w-drop_0.2_u-drop_0.3',\n",
       " 'lstm_optimizer_adam_size_200_w-drop_0.2_u-drop_0.2',\n",
       " 'lstm_optimizer_rmsprop_size_200_w-drop_0.2_u-drop_0.4']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_results_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(param_results_dict, open(os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                                   NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE))), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # create nn parameter search directory\n",
    "    if not os.path.exists(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME)):\n",
    "        os.makedirs(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(param_results_dict, open(os.path.join(os.path.join(nn_parameter_search_location, GLOBAL_VARS.MODEL_NAME, \n",
    "                                                                   NN_PARAMETER_SEARCH_PREFIX.format(classifications_type, NN_BATCH_SIZE))), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-09 03:51:31,120 : INFO : Generating Training Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 6s, sys: 1min 35s, total: 5min 41s\n",
      "Wall time: 5min 41s\n",
      "CPU times: user 136 ms, sys: 40 ms, total: 176 ms\n",
      "Wall time: 172 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/stud/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Training Metrics: Cov Err: 2.940 | Top 3: 0.705 | Top 5: 0.910 | F1 Micro: 0.000 | F1 Macro: 0.000\n",
      "CPU times: user 4min 9s, sys: 1min 35s, total: 5min 44s\n",
      "Wall time: 5min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%time yp = model.predict(X)\n",
    "%time yp_binary = get_binary_0_5(yp)\n",
    "#print yvp\n",
    "info('Generating Training Metrics')\n",
    "training_metrics = get_metrics(y, yp, yp_binary)\n",
    "print \"****** Training Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    training_metrics['coverage_error'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "    training_metrics['f1_micro'], training_metrics['f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2937    \n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2882    \n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2843    \n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2808    \n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2779    \n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2743    \n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2729    \n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2684    \n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2687    \n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 44s - loss: 0.2713    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f253497d250>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X, y=y, batch_size=NN_BATCH_SIZE, \\\n",
    "                              nb_epoch=10, verbose=MODEL_VERBOSITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-07 15:30:31,828 : INFO : Generating Training Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.9 s, sys: 17.3 s, total: 56.2 s\n",
      "Wall time: 56.2 s\n",
      "CPU times: user 20 ms, sys: 4 ms, total: 24 ms\n",
      "Wall time: 23.7 ms\n",
      "****** Validation Metrics: Cov Err: 2.181 | Top 3: 0.853 | Top 5: 0.954 | F1 Micro: 0.541 | F1 Macro: 0.403\n",
      "CPU times: user 39.3 s, sys: 17.3 s, total: 56.7 s\n",
      "Wall time: 56.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%time yp = model.predict(X)\n",
    "%time yp_binary = get_binary_0_5(yp)\n",
    "#print yvp\n",
    "info('Generating Training Metrics')\n",
    "training_metrics = get_metrics(y, yp, yp_binary)\n",
    "print \"****** Training Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    training_metrics['coverage_error'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "    training_metrics['f1_micro'], training_metrics['f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2643    \n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2611    \n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2612    \n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2650    \n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2597    \n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2572    \n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2551    \n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2519    \n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2484    \n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2497    \n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2501    \n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2461    \n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2460    \n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2423    \n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2408    \n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2403    \n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2477    \n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2427    \n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2396    \n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 44s - loss: 0.2371    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2534933d90>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X, y=y, batch_size=NN_BATCH_SIZE, \\\n",
    "                              nb_epoch=20, verbose=MODEL_VERBOSITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-07 16:05:49,006 : INFO : Generating Training Metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38 s, sys: 18.4 s, total: 56.4 s\n",
      "Wall time: 56.4 s\n",
      "CPU times: user 24 ms, sys: 0 ns, total: 24 ms\n",
      "Wall time: 23.7 ms\n",
      "****** Validation Metrics: Cov Err: 1.980 | Top 3: 0.887 | Top 5: 0.963 | F1 Micro: 0.628 | F1 Macro: 0.467\n",
      "CPU times: user 38.4 s, sys: 18.4 s, total: 56.9 s\n",
      "Wall time: 56.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%time yp = model.predict(X)\n",
    "%time yp_binary = get_binary_0_5(yp)\n",
    "#print yvp\n",
    "info('Generating Training Metrics')\n",
    "training_metrics = get_metrics(y, yp, yp_binary)\n",
    "print \"****** Validation Metrics: Cov Err: {:.3f} | Top 3: {:.3f} | Top 5: {:.3f} | F1 Micro: {:.3f} | F1 Macro: {:.3f}\".format(\n",
    "    training_metrics['coverage_error'], training_metrics['top_3'], training_metrics['top_5'], \n",
    "    training_metrics['f1_micro'], training_metrics['f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
