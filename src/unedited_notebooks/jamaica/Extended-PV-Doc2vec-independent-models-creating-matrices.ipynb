{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of training and validation matrices for classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple, defaultdict\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "import gzip\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "from functools import partial\n",
    "\n",
    "import seaborn\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "from thesis.utils.metrics import *\n",
    "from thesis.utils.classification import *\n",
    "from thesis.utils.file import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables used throughout the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CORES = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GLOBAL_VARS = namedtuple('GLOBAL_VARS', ['MODEL_NAME', 'DOC2VEC_MODEL_NAME', 'DOC2VEC_MODEL', 'DOC2VEC_RAW_MODEL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_MODEL = \"vocab_model\"\n",
    "MODEL_PREFIX = \"model\"\n",
    "VALIDATION_MATRIX = \"validation_matrix.pkl\"\n",
    "VALIDATION_DICT = \"validation_dict.pkl\"\n",
    "TEST_MATRIX = \"test_matrix.pkl\"\n",
    "TEST_DICT = \"test_dict.pkl\"\n",
    "METRICS = \"metrics.pkl\"\n",
    "CLASSIFIER = \"classifier.pkl\"\n",
    "TYPE_CLASSIFIER= \"{}_classifier.pkl\"\n",
    "\n",
    "TRAINING_DATA_MATRIX = \"X_level_{}.npy\"\n",
    "TRAINING_LABELS_MATRIX = \"y_{}.npy\"\n",
    "VALIDATION_DATA_MATRIX = \"Xv_level_{}.npy\"\n",
    "VALIDATION_LABELS_MATRIX = \"yv_{}.npy\"\n",
    "TEST_DATA_MATRIX = \"Xt_level_{}.npy\"\n",
    "TEST_LABELS_MATRIX = \"yt_{}.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GZIP_EXTENSION = \".gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_location = \"/mnt/virtual-machines/data/\"\n",
    "big_data_location = \"/mnt/virtual-machines/data/\"\n",
    "\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "matrices_save_location = big_data_location + \"extended_pv_matrices\"\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "valid_classes_file = exports_location + \"valid_classes.pkl\"\n",
    "valid_subclasses_file = exports_location + \"valid_subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "training_docs_list_file = exports_location + \"training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"validation_docs_list.pkl\"\n",
    "test_docs_list_file = exports_location + \"test_docs_list.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load general data required for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.2 s, sys: 1.32 s, total: 19.6 s\n",
      "Wall time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "valid_classes = pickle.load(open(valid_classes_file))\n",
    "valid_subclasses = pickle.load(open(valid_subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286325"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321473"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401877"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables for generation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_ABSTRACT_CHUNKS = 3\n",
    "NUM_DESC_CHUNKS = 23\n",
    "NUM_CLAIMS_CHUNKS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEVEL_1_ID = \"{}\"\n",
    "LEVEL_2_ID = \"{}_{}\"\n",
    "LEVEL_3_ID = \"{}_{}_part-{}\"\n",
    "\n",
    "PART_LEVEL_NAME = \"{}_{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOCUMENT_ORDER = [\n",
    "    (1, \"document\"), \n",
    "    (2, \"abstract\"), (3, \"abstract\"), \n",
    "    (2, \"description\"), (3, \"description\"), \n",
    "    (2, \"claims\"), (3, \"claims\")\n",
    "]\n",
    "DOCUMENT_PART_SIZES = {\n",
    "    \n",
    "    \"1_document\": 1,\n",
    "    \"2_abstract\": 1,\n",
    "    \"2_description\": 1,\n",
    "    \"2_claims\": 1,\n",
    "    \"3_abstract\": NUM_ABSTRACT_CHUNKS,\n",
    "    \"3_description\": NUM_DESC_CHUNKS,\n",
    "    \"3_claims\": NUM_CLAIMS_CHUNKS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOC2VEC_SIZE = 200\n",
    "DOC2VEC_WINDOW = 2\n",
    "DOC2VEC_MAX_VOCAB_SIZE = None\n",
    "DOC2VEC_SAMPLE = 1e-3\n",
    "DOC2VEC_TYPE = 1\n",
    "DOC2VEC_HIERARCHICAL_SAMPLE = 0\n",
    "DOC2VEC_NEGATIVE_SAMPLE_SIZE = 10\n",
    "DOC2VEC_CONCAT = 0\n",
    "DOC2VEC_MEAN = 1\n",
    "DOC2VEC_TRAIN_WORDS = 0\n",
    "DOC2VEC_EPOCHS = 1 # we do our training manually one epoch at a time\n",
    "DOC2VEC_MAX_EPOCHS = 8\n",
    "REPORT_DELAY = 20 # report the progress every x seconds\n",
    "REPORT_VOCAB_PROGRESS = 100000 # report vocab progress every x documents\n",
    "\n",
    "DOC2VEC_MMAP = 'r'\n",
    "\n",
    "DOC2VEC_EPOCH = 1\n",
    "\n",
    "\n",
    "raw_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}'.format(DOC2VEC_SIZE,\n",
    "                        DOC2VEC_WINDOW,\n",
    "                        'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                        DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                        DOC2VEC_TRAIN_WORDS,\n",
    "                        DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                        str(DOC2VEC_MAX_VOCAB_SIZE)\n",
    "                        )\n",
    "raw_model_name = os.path.join(raw_model_name, \"epoch_{}\")\n",
    "GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME = raw_model_name.format(DOC2VEC_EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities for data matrix filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_part_ids(doc_id, part_level, part_name):\n",
    "    \"\"\"\n",
    "    Returns the ids to look for, for a given document id, part level and part name\n",
    "    ex get_part_ids(x, 3, \"abstract) => [\"x_abstract_part-1\", \"x_abstract_part-2\", \"x_abstract_part-3\", ...]\n",
    "    \"\"\"\n",
    "    if part_name == \"document\": \n",
    "        return [LEVEL_1_ID.format(doc_id)]\n",
    "    elif part_level == 2:\n",
    "        return [LEVEL_2_ID.format(doc_id, part_name)]\n",
    "    elif part_level == 3:\n",
    "        ids = []\n",
    "        for i in range(DOCUMENT_PART_SIZES[PART_LEVEL_NAME.format(part_level, part_name)]):\n",
    "            ids.append(LEVEL_3_ID.format(doc_id, part_name, i+1))\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sequence_insert_location(my_part_level, my_part_name, max_level):\n",
    "    \"\"\"\n",
    "    for a given level and name, determines where its position in the sequence begins\n",
    "    \"\"\"\n",
    "    assert DOCUMENT_PART_SIZES.get(PART_LEVEL_NAME.format(my_part_level, my_part_name)) is not None\n",
    "    loc = 0\n",
    "    for part_level, part_name in DOCUMENT_ORDER:\n",
    "        if part_level <= max_level:\n",
    "            if part_level == my_part_level and part_name == my_part_name:\n",
    "                break\n",
    "            else:\n",
    "                loc += DOCUMENT_PART_SIZES[PART_LEVEL_NAME.format(part_level, part_name)]\n",
    "    return loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Training and Validation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEVEL_TO_GENERATE = 2\n",
    "EMBEDDING_SIZE = DOC2VEC_SIZE\n",
    "ZERO_VECTOR = [0] * DOC2VEC_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "sequence_size = sum([DOCUMENT_PART_SIZES[\"{}_{}\".format(part_level, part_name)] for part_level, part_name in DOCUMENT_ORDER if part_level <= LEVEL_TO_GENERATE])\n",
    "print sequence_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 21:27:03,276 : INFO : ********** Generating Matrices for LEVEL:2 ************\n",
      "2017-04-18 21:27:03,278 : INFO : ======== Working on Level: 1 => document\n",
      "2017-04-18 21:27:03,279 : INFO : Loading Doc2vec model: doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_1_document/epoch_1\n",
      "2017-04-18 21:27:03,279 : INFO : loading Doc2Vec object from /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_1_document/epoch_1/model\n",
      "2017-04-18 21:27:09,467 : INFO : loading docvecs recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_1_document/epoch_1/model.docvecs.* with mmap=r\n",
      "2017-04-18 21:27:09,468 : INFO : loading doctag_syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_1_document/epoch_1/model.docvecs.doctag_syn0.npy with mmap=r\n",
      "2017-04-18 21:27:09,470 : INFO : loading wv recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_1_document/epoch_1/model.wv.* with mmap=r\n",
      "2017-04-18 21:27:09,470 : INFO : loading syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_1_document/epoch_1/model.wv.syn0.npy with mmap=r\n",
      "2017-04-18 21:27:09,471 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-18 21:27:09,472 : INFO : loading syn1neg from /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_1_document/epoch_1/model.syn1neg.npy with mmap=r\n",
      "2017-04-18 21:27:09,473 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-18 21:27:09,473 : INFO : loaded /mnt/virtual-machines/data/parameter_search_doc2vec_models_1_document/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_1_document/epoch_1/model\n",
      "2017-04-18 21:27:11,005 : INFO : Loading Validation Dict\n",
      "2017-04-18 21:27:33,963 : INFO : Filling training matrix\n",
      "2017-04-18 21:27:42,755 : INFO : Filling validation matrix\n",
      "2017-04-18 21:27:44,045 : INFO : ======== Working on Level: 2 => abstract\n",
      "2017-04-18 21:27:44,052 : INFO : Loading Doc2vec model: doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_abstract/epoch_1\n",
      "2017-04-18 21:27:44,067 : INFO : loading Doc2Vec object from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_abstract/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_abstract/epoch_1/model\n",
      "2017-04-18 21:27:51,070 : INFO : loading docvecs recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_abstract/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_abstract/epoch_1/model.docvecs.* with mmap=r\n",
      "2017-04-18 21:27:51,071 : INFO : loading doctag_syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_abstract/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_abstract/epoch_1/model.docvecs.doctag_syn0.npy with mmap=r\n",
      "2017-04-18 21:27:51,072 : INFO : loading wv recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_abstract/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_abstract/epoch_1/model.wv.* with mmap=r\n",
      "2017-04-18 21:27:51,072 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-18 21:27:51,073 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-18 21:27:51,074 : INFO : loaded /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_abstract/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_abstract/epoch_1/model\n",
      "2017-04-18 21:27:51,756 : INFO : Loading Validation Dict\n",
      "2017-04-18 21:28:14,084 : INFO : Filling training matrix\n",
      "2017-04-18 21:28:22,771 : INFO : Filling validation matrix\n",
      "2017-04-18 21:28:23,871 : INFO : ======== Working on Level: 2 => description\n",
      "2017-04-18 21:28:23,880 : INFO : Loading Doc2vec model: doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_description/epoch_1\n",
      "2017-04-18 21:28:23,881 : INFO : loading Doc2Vec object from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_description/epoch_1/model\n",
      "2017-04-18 21:28:30,146 : INFO : loading docvecs recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_description/epoch_1/model.docvecs.* with mmap=r\n",
      "2017-04-18 21:28:30,147 : INFO : loading doctag_syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_description/epoch_1/model.docvecs.doctag_syn0.npy with mmap=r\n",
      "2017-04-18 21:28:30,148 : INFO : loading wv recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_description/epoch_1/model.wv.* with mmap=r\n",
      "2017-04-18 21:28:30,148 : INFO : loading syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_description/epoch_1/model.wv.syn0.npy with mmap=r\n",
      "2017-04-18 21:28:30,149 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-18 21:28:30,150 : INFO : loading syn1neg from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_description/epoch_1/model.syn1neg.npy with mmap=r\n",
      "2017-04-18 21:28:30,151 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-18 21:28:30,151 : INFO : loaded /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_description/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_description/epoch_1/model\n",
      "2017-04-18 21:28:31,670 : INFO : Loading Validation Dict\n",
      "2017-04-18 21:28:54,066 : INFO : Filling training matrix\n",
      "2017-04-18 21:29:02,028 : INFO : Filling validation matrix\n",
      "2017-04-18 21:29:02,931 : INFO : ======== Working on Level: 2 => claims\n",
      "2017-04-18 21:29:02,941 : INFO : Loading Doc2vec model: doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_claims/epoch_1\n",
      "2017-04-18 21:29:02,946 : INFO : loading Doc2Vec object from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_claims/epoch_1/model\n",
      "2017-04-18 21:29:07,962 : INFO : loading docvecs recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_claims/epoch_1/model.docvecs.* with mmap=r\n",
      "2017-04-18 21:29:07,963 : INFO : loading doctag_syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_claims/epoch_1/model.docvecs.doctag_syn0.npy with mmap=r\n",
      "2017-04-18 21:29:07,965 : INFO : loading wv recursively from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_claims/epoch_1/model.wv.* with mmap=r\n",
      "2017-04-18 21:29:07,965 : INFO : loading syn0 from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_claims/epoch_1/model.wv.syn0.npy with mmap=r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 21:29:07,966 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-18 21:29:07,967 : INFO : loading syn1neg from /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_claims/epoch_1/model.syn1neg.npy with mmap=r\n",
      "2017-04-18 21:29:07,967 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-18 21:29:07,968 : INFO : loaded /mnt/virtual-machines/data/parameter_search_doc2vec_models_2_claims/full/doc2vec_size_200_w_2_type_dm_concat_0_mean_1_trainwords_0_hs_0_neg_10_vocabsize_None_model_2_claims/epoch_1/model\n",
      "2017-04-18 21:29:08,827 : INFO : Loading Validation Dict\n",
      "2017-04-18 21:29:30,857 : INFO : Filling training matrix\n",
      "2017-04-18 21:29:39,070 : INFO : Filling validation matrix\n",
      "2017-04-18 21:29:40,193 : INFO : Saving training matrix\n",
      "2017-04-18 21:29:42,940 : INFO : Saving validation matrix\n"
     ]
    }
   ],
   "source": [
    "X_data = np.ndarray((len(training_docs_list), sequence_size, EMBEDDING_SIZE), dtype=np.float32)\n",
    "Xv_data = np.ndarray((len(validation_docs_list), sequence_size, EMBEDDING_SIZE), dtype=np.float32)\n",
    "\n",
    "info(\"********** Generating Matrices for LEVEL:{} ************\".format(LEVEL_TO_GENERATE))\n",
    "\n",
    "for part_level, part_name in DOCUMENT_ORDER:\n",
    "    if part_level <= LEVEL_TO_GENERATE:\n",
    "        \n",
    "        info(\"======== Working on Level: {} => {}\".format(part_level, part_name))\n",
    "        \n",
    "        sequence_insert_location = get_sequence_insert_location(part_level, part_name, LEVEL_TO_GENERATE)\n",
    "        \n",
    "        \n",
    "        doc2vec_model_save_location = os.path.join(root_location,\n",
    "                                                   \"parameter_search_doc2vec_models_\" + str(part_level) + '_' + part_name,\n",
    "                                                   \"full\")\n",
    "        \n",
    "        placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}_model_{}'.format(DOC2VEC_SIZE,\n",
    "                                                                DOC2VEC_WINDOW,\n",
    "                                                                'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                                DOC2VEC_TRAIN_WORDS,\n",
    "                                                                DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                                str(DOC2VEC_MAX_VOCAB_SIZE),\n",
    "                                                                str(part_level) + '_' + part_name\n",
    "                                                                )\n",
    "        GLOBAL_VARS.DOC2VEC_MODEL_NAME = placeholder_model_name\n",
    "        placeholder_model_name = os.path.join(placeholder_model_name, \"epoch_{}\")\n",
    "        epoch = DOC2VEC_EPOCH\n",
    "        GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "        \n",
    "        \n",
    "        info(\"Loading Doc2vec model: {}\".format(GLOBAL_VARS.MODEL_NAME))\n",
    "        doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX), mmap=DOC2VEC_MMAP)\n",
    "        \n",
    "        \n",
    "        info(\"Loading Validation Dict\")\n",
    "        validation_dict = dict(pickle.load(gzip.open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_DICT + GZIP_EXTENSION))))\n",
    "        part_level_name = PART_LEVEL_NAME.format(part_level, part_name)\n",
    "        \n",
    "        \n",
    "        def fill_matrix(data_matrix, source_dict, docs_list, start_location, use_get=False):\n",
    "            \"\"\"\n",
    "            use_get is for doc2vec_model.docvecs since it doesnt support .get(), so we catch the exception and\n",
    "            fill with zeros in that case. This should really happen very rarely (if ever) so this exception handling\n",
    "            should not be a drain on performance\n",
    "            \"\"\"\n",
    "            for i, doc_id in enumerate(docs_list):\n",
    "                child_ids = get_part_ids(doc_id, part_level, part_name)\n",
    "\n",
    "                j = start_location\n",
    "                for child_id in child_ids:\n",
    "                    try:\n",
    "                        if not use_get or source_dict.get(child_id) is not None:\n",
    "                            data_matrix[i][j] = source_dict[child_id]\n",
    "                        else:\n",
    "                            info(\"ZERO_VECTOR for {}\".format(child_id))\n",
    "                            data_matrix[i][j] = ZERO_VECTOR\n",
    "                    except:\n",
    "                        info(\"ZERO_VECTOR for {}\".format(child_id))\n",
    "                        data_matrix[i][j] = ZERO_VECTOR\n",
    "                    j+= 1\n",
    "        \n",
    "        info(\"Filling training matrix\")\n",
    "        fill_matrix(X_data, doc2vec_model.docvecs, training_docs_list, sequence_insert_location, use_get=False)\n",
    "        info(\"Filling validation matrix\")\n",
    "        fill_matrix(Xv_data, validation_dict, validation_docs_list, sequence_insert_location, use_get=True)\n",
    "        \n",
    "        \n",
    "ensure_disk_location_exists(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME))\n",
    "info(\"Saving training matrix\")\n",
    "np.save(open(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME, \n",
    "                          TRAINING_DATA_MATRIX.format(LEVEL_TO_GENERATE)), \"w\"), X_data)\n",
    "info(\"Saving validation matrix\")\n",
    "np.save(open(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME, \n",
    "                          VALIDATION_DATA_MATRIX.format(LEVEL_TO_GENERATE)), \"w\"), Xv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate test matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEVEL_TO_GENERATE = 3\n",
    "EMBEDDING_SIZE = DOC2VEC_SIZE\n",
    "ZERO_VECTOR = [0] * DOC2VEC_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "sequence_size = sum([DOCUMENT_PART_SIZES[\"{}_{}\".format(part_level, part_name)] for part_level, part_name in DOCUMENT_ORDER if part_level <= LEVEL_TO_GENERATE])\n",
    "print sequence_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-13 01:00:31,855 : INFO : ********** Generating Matrices for LEVEL:3 ************\n",
      "2017-04-13 01:00:31,856 : INFO : ======== Working on Level: 1 => document\n",
      "2017-04-13 01:00:31,857 : INFO : Loading Test Dict\n",
      "2017-04-13 01:01:08,159 : INFO : Filling test matrix\n",
      "2017-04-13 01:01:10,003 : INFO : ======== Working on Level: 2 => abstract\n",
      "2017-04-13 01:01:10,006 : INFO : Loading Test Dict\n",
      "2017-04-13 01:01:45,889 : INFO : Filling test matrix\n",
      "2017-04-13 01:01:47,582 : INFO : ======== Working on Level: 3 => abstract\n",
      "2017-04-13 01:01:47,583 : INFO : Loading Test Dict\n",
      "2017-04-13 01:03:38,426 : INFO : Filling test matrix\n",
      "2017-04-13 01:03:42,300 : INFO : ======== Working on Level: 2 => description\n",
      "2017-04-13 01:03:42,309 : INFO : Loading Test Dict\n",
      "2017-04-13 01:04:19,430 : INFO : Filling test matrix\n",
      "2017-04-13 01:04:21,060 : INFO : ======== Working on Level: 3 => description\n",
      "2017-04-13 01:04:21,065 : INFO : Loading Test Dict\n",
      "2017-04-13 01:18:28,382 : INFO : Filling test matrix\n",
      "2017-04-13 01:18:53,298 : INFO : ======== Working on Level: 2 => claims\n",
      "2017-04-13 01:18:53,300 : INFO : Loading Test Dict\n",
      "2017-04-13 01:19:43,208 : INFO : Filling test matrix\n",
      "2017-04-13 01:19:44,827 : INFO : ======== Working on Level: 3 => claims\n",
      "2017-04-13 01:19:44,831 : INFO : Loading Test Dict\n",
      "2017-04-13 01:22:10,799 : INFO : Filling test matrix\n",
      "2017-04-13 01:22:12,089 : INFO : ZERO_VECTOR for 07371868_claims_part-2\n",
      "2017-04-13 01:22:12,094 : INFO : ZERO_VECTOR for 07371868_claims_part-3\n",
      "2017-04-13 01:22:12,098 : INFO : ZERO_VECTOR for 07371868_claims_part-4\n",
      "2017-04-13 01:22:13,649 : INFO : ZERO_VECTOR for 08822687_claims_part-2\n",
      "2017-04-13 01:22:13,652 : INFO : ZERO_VECTOR for 08822687_claims_part-3\n",
      "2017-04-13 01:22:13,655 : INFO : ZERO_VECTOR for 08822687_claims_part-4\n",
      "2017-04-13 01:22:15,461 : INFO : Saving test matrix\n"
     ]
    }
   ],
   "source": [
    "Xt_data = np.ndarray((len(test_docs_list), sequence_size, EMBEDDING_SIZE), dtype=np.float32)\n",
    "\n",
    "info(\"********** Generating Matrices for LEVEL:{} ************\".format(LEVEL_TO_GENERATE))\n",
    "\n",
    "for part_level, part_name in DOCUMENT_ORDER:\n",
    "    if part_level <= LEVEL_TO_GENERATE:\n",
    "        \n",
    "        info(\"======== Working on Level: {} => {}\".format(part_level, part_name))\n",
    "        \n",
    "        sequence_insert_location = get_sequence_insert_location(part_level, part_name, LEVEL_TO_GENERATE)\n",
    "        \n",
    "        \n",
    "        doc2vec_model_save_location = os.path.join(root_location,\n",
    "                                                   \"parameter_search_doc2vec_models_\" + str(part_level) + '_' + part_name,\n",
    "                                                   \"full\")\n",
    "        \n",
    "        placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}_model_{}'.format(DOC2VEC_SIZE,\n",
    "                                                                DOC2VEC_WINDOW,\n",
    "                                                                'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                                DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                                DOC2VEC_TRAIN_WORDS,\n",
    "                                                                DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                                str(DOC2VEC_MAX_VOCAB_SIZE),\n",
    "                                                                str(part_level) + '_' + part_name\n",
    "                                                                )\n",
    "        GLOBAL_VARS.DOC2VEC_MODEL_NAME = placeholder_model_name\n",
    "        placeholder_model_name = os.path.join(placeholder_model_name, \"epoch_{}\")\n",
    "        epoch = DOC2VEC_EPOCH\n",
    "        GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "        \n",
    "        \n",
    "        info(\"Loading Test Dict\")\n",
    "        test_dict = dict(pickle.load(gzip.open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, TEST_DICT))))\n",
    "        #test_dict = dict(pickle.load(gzip.open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, TEST_DICT + GZIP_EXTENSION))))\n",
    "        part_level_name = PART_LEVEL_NAME.format(part_level, part_name)\n",
    "        \n",
    "        \n",
    "        def fill_matrix(data_matrix, source_dict, docs_list, start_location, use_get=False):\n",
    "            \"\"\"\n",
    "            use_get is for doc2vec_model.docvecs since it doesnt support .get(), so we catch the exception and\n",
    "            fill with zeros in that case. This should really happen very rarely (if ever) so this exception handling\n",
    "            should not be a drain on performance\n",
    "            \"\"\"\n",
    "            for i, doc_id in enumerate(docs_list):\n",
    "                child_ids = get_part_ids(doc_id, part_level, part_name)\n",
    "\n",
    "                j = start_location\n",
    "                for child_id in child_ids:\n",
    "                    try:\n",
    "                        if not use_get or source_dict.get(child_id) is not None:\n",
    "                            data_matrix[i][j] = source_dict[child_id]\n",
    "                        else:\n",
    "                            info(\"ZERO_VECTOR for {}\".format(child_id))\n",
    "                            data_matrix[i][j] = ZERO_VECTOR\n",
    "                    except:\n",
    "                        info(\"ZERO_VECTOR for {}\".format(child_id))\n",
    "                        data_matrix[i][j] = ZERO_VECTOR\n",
    "                    j+= 1\n",
    "        \n",
    "        info(\"Filling test matrix\")\n",
    "        fill_matrix(Xt_data, test_dict, test_docs_list, sequence_insert_location, use_get=True)\n",
    "        \n",
    "        \n",
    "ensure_disk_location_exists(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME))\n",
    "info(\"Saving test matrix\")\n",
    "np.save(open(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME, \n",
    "                          TEST_DATA_MATRIX.format(LEVEL_TO_GENERATE)), \"w\"), Xt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training and validation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_labels(classifications, docs_list):\n",
    "    one_hot_encoder = OneHotEncoder(classifications)\n",
    "    classifications_set = set(classifications)\n",
    "    labels_mat = np.zeros((len(docs_list), len(classifications)), dtype=np.int8)\n",
    "    for i, doc_id in enumerate(docs_list):\n",
    "        eligible_classifications = set(doc_classification_map[doc_id]) & classifications_set\n",
    "        labels_mat[i][:] = one_hot_encoder.get_label_vector(eligible_classifications)\n",
    "    return labels_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-18 21:58:05,482 : INFO : Creating Training Labels for sections\n",
      "2017-04-18 21:58:09,454 : INFO : Creating Validation Labels for sections\n",
      "2017-04-18 21:58:10,486 : INFO : Creating Test Labels for sections\n"
     ]
    }
   ],
   "source": [
    "classifications_to_create = [\n",
    "    (\"sections\", sections),\n",
    "#     (\"classes\", valid_classes),\n",
    "#     (\"subclasses\", valid_subclasses)\n",
    "]\n",
    "\n",
    "for classifications_type, classifications in classifications_to_create:\n",
    "    info(\"Creating Training Labels for {}\".format(classifications_type))\n",
    "    y = create_labels(classifications, training_docs_list)\n",
    "    info(\"Creating Validation Labels for {}\".format(classifications_type))\n",
    "    yv = create_labels(classifications, validation_docs_list)\n",
    "    info(\"Creating Test Labels for {}\".format(classifications_type))\n",
    "    yt = create_labels(classifications, test_docs_list)\n",
    "    \n",
    "    ensure_disk_location_exists(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME))\n",
    "    np.save(open(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME, \n",
    "                                  TRAINING_LABELS_MATRIX.format(classifications_type)), \"w\"), y)\n",
    "    np.save(open(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME, \n",
    "                                  VALIDATION_LABELS_MATRIX.format(classifications_type)), \"w\"), yv)\n",
    "    np.save(open(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME, \n",
    "                                  TEST_LABELS_MATRIX.format(classifications_type)), \"w\"), yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
