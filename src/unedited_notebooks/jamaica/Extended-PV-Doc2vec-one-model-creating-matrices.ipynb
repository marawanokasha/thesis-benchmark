{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of training and validation matrices for classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple, defaultdict\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "import gzip\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "from functools import partial\n",
    "\n",
    "import seaborn\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "from thesis.utils.metrics import *\n",
    "from thesis.utils.classification import *\n",
    "from thesis.utils.file import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables used throughout the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CORES = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GLOBAL_VARS = namedtuple('GLOBAL_VARS', ['MODEL_NAME', 'DOC2VEC_MODEL_NAME', 'DOC2VEC_MODEL', 'DOC2VEC_RAW_MODEL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_MODEL = \"vocab_model\"\n",
    "MODEL_PREFIX = \"model\"\n",
    "VALIDATION_MATRIX = \"validation_matrix.pkl\"\n",
    "VALIDATION_DICT = \"validation_dict.pkl\"\n",
    "TEST_MATRIX = \"test_matrix.pkl\"\n",
    "TEST_DICT = \"test_dict.pkl\"\n",
    "METRICS = \"metrics.pkl\"\n",
    "CLASSIFIER = \"classifier.pkl\"\n",
    "TYPE_CLASSIFIER= \"{}_classifier.pkl\"\n",
    "\n",
    "TRAINING_DATA_MATRIX = \"X_level_{}.npy\"\n",
    "TRAINING_LABELS_MATRIX = \"y_{}.npy\"\n",
    "VALIDATION_DATA_MATRIX = \"Xv_level_{}.npy\"\n",
    "VALIDATION_LABELS_MATRIX = \"yv_{}.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GZIP_EXTENSION = \".gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_location = \"/mnt/virtual-machines/data/\"\n",
    "big_data_location = \"/mnt/virtual-machines/data/\"\n",
    "\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "doc2vec_model_save_location = os.path.join(root_location, \"parameter_search_doc2vec_models_extended_abs_desc_claims_full_chunks\", \"full\")\n",
    "\n",
    "matrices_save_location = big_data_location + \"extended_pv_matrices/one_model/\"\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "valid_classes_file = exports_location + \"valid_classes.pkl\"\n",
    "valid_subclasses_file = exports_location + \"valid_subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "training_docs_list_file = exports_location + \"training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"validation_docs_list.pkl\"\n",
    "test_docs_list_file = exports_location + \"test_docs_list.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load general data required for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23 s, sys: 2.77 s, total: 25.8 s\n",
      "Wall time: 25.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "valid_classes = pickle.load(open(valid_classes_file))\n",
    "valid_subclasses = pickle.load(open(valid_subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286325"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321473"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401877"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables for generation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_ABSTRACT_CHUNKS = 3\n",
    "NUM_DESC_CHUNKS = 23\n",
    "NUM_CLAIMS_CHUNKS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEVEL_1_ID = \"{}\"\n",
    "LEVEL_2_ID = \"{}_{}\"\n",
    "LEVEL_3_ID = \"{}_{}_part-{}\"\n",
    "\n",
    "PART_LEVEL_NAME = \"{}_{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOCUMENT_ORDER = [\n",
    "    (1, \"document\"), \n",
    "    (2, \"abstract\"), (3, \"abstract\"), \n",
    "    (2, \"description\"), (3, \"description\"), \n",
    "    (2, \"claims\"), (3, \"claims\")\n",
    "]\n",
    "DOCUMENT_PART_SIZES = {\n",
    "    \n",
    "    \"1_document\": 1,\n",
    "    \"2_abstract\": 1,\n",
    "    \"2_description\": 1,\n",
    "    \"2_claims\": 1,\n",
    "    \"3_abstract\": NUM_ABSTRACT_CHUNKS,\n",
    "    \"3_description\": NUM_DESC_CHUNKS,\n",
    "    \"3_claims\": NUM_CLAIMS_CHUNKS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOC2VEC_SIZE = 200\n",
    "DOC2VEC_WINDOW = 2\n",
    "DOC2VEC_MAX_VOCAB_SIZE = None\n",
    "DOC2VEC_SAMPLE = 1e-3\n",
    "DOC2VEC_TYPE = 1\n",
    "DOC2VEC_HIERARCHICAL_SAMPLE = 0\n",
    "DOC2VEC_NEGATIVE_SAMPLE_SIZE = 10\n",
    "DOC2VEC_CONCAT = 0\n",
    "DOC2VEC_MEAN = 1\n",
    "DOC2VEC_TRAIN_WORDS = 0\n",
    "DOC2VEC_EPOCHS = 1 # we do our training manually one epoch at a time\n",
    "DOC2VEC_MAX_EPOCHS = 8\n",
    "REPORT_DELAY = 20 # report the progress every x seconds\n",
    "REPORT_VOCAB_PROGRESS = 100000 # report vocab progress every x documents\n",
    "\n",
    "DOC2VEC_MMAP = 'r'\n",
    "\n",
    "DOC2VEC_EPOCH = 8\n",
    "\n",
    "\n",
    "raw_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}'.format(DOC2VEC_SIZE,\n",
    "                        DOC2VEC_WINDOW,\n",
    "                        'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                        DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                        DOC2VEC_TRAIN_WORDS,\n",
    "                        DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                        str(DOC2VEC_MAX_VOCAB_SIZE)\n",
    "                        )\n",
    "raw_model_name = os.path.join(raw_model_name, \"epoch_{}\")\n",
    "GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME = raw_model_name.format(DOC2VEC_EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities for data matrix filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_part_ids(doc_id, part_level, part_name):\n",
    "    \"\"\"\n",
    "    Returns the ids to look for, for a given document id, part level and part name\n",
    "    ex get_part_ids(x, 3, \"abstract) => [\"x_abstract_part-1\", \"x_abstract_part-2\", \"x_abstract_part-3\", ...]\n",
    "    \"\"\"\n",
    "    if part_name == \"document\": \n",
    "        return [LEVEL_1_ID.format(doc_id)]\n",
    "    elif part_level == 2:\n",
    "        return [LEVEL_2_ID.format(doc_id, part_name)]\n",
    "    elif part_level == 3:\n",
    "        ids = []\n",
    "        for i in range(DOCUMENT_PART_SIZES[PART_LEVEL_NAME.format(part_level, part_name)]):\n",
    "            ids.append(LEVEL_3_ID.format(doc_id, part_name, i+1))\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sequence_insert_location(my_part_level, my_part_name, max_level):\n",
    "    \"\"\"\n",
    "    for a given level and name, determines where its position in the sequence begins\n",
    "    \"\"\"\n",
    "    assert DOCUMENT_PART_SIZES.get(PART_LEVEL_NAME.format(my_part_level, my_part_name)) is not None\n",
    "    loc = 0\n",
    "    for part_level, part_name in DOCUMENT_ORDER:\n",
    "        if part_level <= max_level:\n",
    "            if part_level == my_part_level and part_name == my_part_name:\n",
    "                break\n",
    "            else:\n",
    "                loc += DOCUMENT_PART_SIZES[PART_LEVEL_NAME.format(part_level, part_name)]\n",
    "    return loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Training and Validation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEVEL_TO_GENERATE = 3\n",
    "EMBEDDING_SIZE = DOC2VEC_SIZE\n",
    "ZERO_VECTOR = [0] * DOC2VEC_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "sequence_size = sum([DOCUMENT_PART_SIZES[\"{}_{}\".format(part_level, part_name)] for part_level, part_name in DOCUMENT_ORDER if part_level <= LEVEL_TO_GENERATE])\n",
    "print sequence_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Doc2vec and validation dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "info(\"********** Generating Matrices for LEVEL:{} ************\".format(LEVEL_TO_GENERATE))\n",
    "\n",
    "\n",
    "placeholder_model_name = 'doc2vec_size_{}_w_{}_type_{}_concat_{}_mean_{}_trainwords_{}_hs_{}_neg_{}_vocabsize_{}'.format(DOC2VEC_SIZE,\n",
    "                                                        DOC2VEC_WINDOW,\n",
    "                                                        'dm' if DOC2VEC_TYPE == 1 else 'pv-dbow',\n",
    "                                                        DOC2VEC_CONCAT, DOC2VEC_MEAN,\n",
    "                                                        DOC2VEC_TRAIN_WORDS,\n",
    "                                                        DOC2VEC_HIERARCHICAL_SAMPLE,DOC2VEC_NEGATIVE_SAMPLE_SIZE,\n",
    "                                                        str(DOC2VEC_MAX_VOCAB_SIZE)\n",
    "                                                        )\n",
    "GLOBAL_VARS.DOC2VEC_MODEL_NAME = placeholder_model_name\n",
    "placeholder_model_name = os.path.join(placeholder_model_name, \"epoch_{}\")\n",
    "epoch = DOC2VEC_EPOCH\n",
    "GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(epoch)\n",
    "\n",
    "\n",
    "info(\"Loading Doc2vec model: {}\".format(GLOBAL_VARS.MODEL_NAME))\n",
    "doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX), mmap=DOC2VEC_MMAP)\n",
    "\n",
    "\n",
    "info(\"Loading Validation Dict\")\n",
    "validation_dict = dict(pickle.load(gzip.open(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, VALIDATION_DICT + GZIP_EXTENSION))))\n",
    "part_level_name = PART_LEVEL_NAME.format(part_level, part_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually filling out the matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-12 11:47:09,631 : INFO : ======== Working on Level: 1 => document\n",
      "2017-04-12 11:47:09,632 : INFO : Filling training matrix\n",
      "2017-04-12 11:47:21,159 : INFO : Filling validation matrix\n",
      "2017-04-12 11:47:23,547 : INFO : ======== Working on Level: 2 => abstract\n",
      "2017-04-12 11:47:23,548 : INFO : Filling training matrix\n",
      "2017-04-12 11:47:35,232 : INFO : Filling validation matrix\n",
      "2017-04-12 11:47:37,003 : INFO : ======== Working on Level: 3 => abstract\n",
      "2017-04-12 11:47:37,003 : INFO : Filling training matrix\n",
      "2017-04-12 11:47:59,824 : INFO : Filling validation matrix\n",
      "2017-04-12 11:48:03,334 : INFO : ======== Working on Level: 2 => description\n",
      "2017-04-12 11:48:03,335 : INFO : Filling training matrix\n",
      "2017-04-12 11:48:13,663 : INFO : Filling validation matrix\n",
      "2017-04-12 11:48:15,762 : INFO : ======== Working on Level: 3 => description\n",
      "2017-04-12 11:48:15,763 : INFO : Filling training matrix\n",
      "2017-04-12 11:50:56,486 : INFO : Filling validation matrix\n",
      "2017-04-12 11:51:11,729 : INFO : ======== Working on Level: 2 => claims\n",
      "2017-04-12 11:51:11,731 : INFO : Filling training matrix\n",
      "2017-04-12 11:51:25,024 : INFO : Filling validation matrix\n",
      "2017-04-12 11:51:27,411 : INFO : ======== Working on Level: 3 => claims\n",
      "2017-04-12 11:51:27,412 : INFO : Filling training matrix\n",
      "2017-04-12 11:51:58,643 : INFO : Filling validation matrix\n",
      "2017-04-12 11:52:02,708 : INFO : Saving training matrix\n",
      "2017-04-12 11:52:31,530 : INFO : Saving validation matrix\n"
     ]
    }
   ],
   "source": [
    "X_data = np.ndarray((len(training_docs_list), sequence_size, EMBEDDING_SIZE), dtype=np.float32)\n",
    "Xv_data = np.ndarray((len(validation_docs_list), sequence_size, EMBEDDING_SIZE), dtype=np.float32)\n",
    "\n",
    "\n",
    "for part_level, part_name in DOCUMENT_ORDER:\n",
    "    if part_level <= LEVEL_TO_GENERATE:\n",
    "        \n",
    "        info(\"======== Working on Level: {} => {}\".format(part_level, part_name))\n",
    "        \n",
    "        sequence_insert_location = get_sequence_insert_location(part_level, part_name, LEVEL_TO_GENERATE)\n",
    "        \n",
    "        \n",
    "        def fill_matrix(data_matrix, source_dict, docs_list, start_location, use_get=False):\n",
    "            \"\"\"\n",
    "            use_get is for doc2vec_model.docvecs since it doesnt support .get(), so we catch the exception and\n",
    "            fill with zeros in that case. This should really happen very rarely (if ever) so this exception handling\n",
    "            should not be a drain on performance\n",
    "            \"\"\"\n",
    "            for i, doc_id in enumerate(docs_list):\n",
    "                child_ids = get_part_ids(doc_id, part_level, part_name)\n",
    "\n",
    "                j = start_location\n",
    "                for child_id in child_ids:\n",
    "                    try:\n",
    "                        if not use_get or source_dict.get(child_id) is not None:\n",
    "                            data_matrix[i][j] = source_dict[child_id]\n",
    "                        else:\n",
    "                            info(\"ZERO_VECTOR for {}\".format(child_id))\n",
    "                            data_matrix[i][j] = ZERO_VECTOR\n",
    "                    except:\n",
    "                        info(\"ZERO_VECTOR for {}\".format(child_id))\n",
    "                        data_matrix[i][j] = ZERO_VECTOR\n",
    "                    j+= 1\n",
    "        \n",
    "        info(\"Filling training matrix\")\n",
    "        fill_matrix(X_data, doc2vec_model.docvecs, training_docs_list, sequence_insert_location, use_get=False)\n",
    "        info(\"Filling validation matrix\")\n",
    "        fill_matrix(Xv_data, validation_dict, validation_docs_list, sequence_insert_location, use_get=True)\n",
    "        \n",
    "        \n",
    "ensure_disk_location_exists(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME))\n",
    "info(\"Saving training matrix\")\n",
    "np.save(open(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME, \n",
    "                          TRAINING_DATA_MATRIX.format(LEVEL_TO_GENERATE)), \"w\"), X_data)\n",
    "info(\"Saving validation matrix\")\n",
    "np.save(open(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME, \n",
    "                          VALIDATION_DATA_MATRIX.format(LEVEL_TO_GENERATE)), \"w\"), Xv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training and validation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_labels(classifications, docs_list):\n",
    "    one_hot_encoder = OneHotEncoder(classifications)\n",
    "    classifications_set = set(classifications)\n",
    "    labels_mat = np.zeros((len(docs_list), len(classifications)), dtype=np.int8)\n",
    "    for i, doc_id in enumerate(docs_list):\n",
    "        eligible_classifications = set(doc_classification_map[doc_id]) & classifications_set\n",
    "        labels_mat[i][:] = one_hot_encoder.get_label_vector(eligible_classifications)\n",
    "    return labels_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-12 11:54:04,057 : INFO : Creating Training Labels for sections\n",
      "2017-04-12 11:54:09,223 : INFO : Creating Validation Labels for sections\n",
      "2017-04-12 11:54:10,551 : INFO : Creating Training Labels for classes\n",
      "2017-04-12 11:54:29,035 : INFO : Creating Validation Labels for classes\n",
      "2017-04-12 11:54:34,075 : INFO : Creating Training Labels for subclasses\n",
      "2017-04-12 11:55:37,394 : INFO : Creating Validation Labels for subclasses\n"
     ]
    }
   ],
   "source": [
    "classifications_to_create = [\n",
    "    (\"sections\", sections),\n",
    "    (\"classes\", valid_classes),\n",
    "    (\"subclasses\", valid_subclasses)\n",
    "]\n",
    "\n",
    "for classifications_type, classifications in classifications_to_create:\n",
    "    info(\"Creating Training Labels for {}\".format(classifications_type))\n",
    "    y = create_labels(classifications, training_docs_list)\n",
    "    info(\"Creating Validation Labels for {}\".format(classifications_type))\n",
    "    yv = create_labels(classifications, validation_docs_list)\n",
    "    \n",
    "    ensure_disk_location_exists(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME))\n",
    "    np.save(open(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME, \n",
    "                                  TRAINING_LABELS_MATRIX.format(classifications_type)), \"w\"), y)\n",
    "    np.save(open(os.path.join(matrices_save_location, GLOBAL_VARS.DOC2VEC_RAW_MODEL_NAME, \n",
    "                                  VALIDATION_LABELS_MATRIX.format(classifications_type)), \"w\"), yv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
