{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import cPickle as pickle\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "\n",
    "from multiprocessing import Pool as ThreadPool\n",
    "import itertools\n",
    "\n",
    "import nltk\n",
    "\n",
    "from thesis.utils.text import get_sentences, sentence_wordtokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATIO = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_location = \"/big/s/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "# training_file = root_location + 'docs_output_training_validation_documents_' + str(SAMPLE_RATIO)\n",
    "training_file = root_location + 'docs_output.json'\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "# training_docs_list_file = exports_location + \"training_documents_\" + str(SAMPLE_RATIO) + \"_sample.pkl\"\n",
    "# validation_docs_list_file = exports_location + \"validation_documents_\" + str(SAMPLE_RATIO) + \"_sample.pkl\"\n",
    "training_docs_list_file = exports_location + \"extended_pv_training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"extended_pv_validation_docs_list.pkl\"\n",
    "test_docs_list_file = exports_location + \"extended_pv_test_docs_list.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.3 s, sys: 4.58 s, total: 32.9 s\n",
      "Wall time: 32.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120156"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29675"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37771"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_SIZE = 0\n",
    "NUMBER_INDICATOR = \"number_inidicator\"\n",
    "CURRENCY_INDICATOR = \"currency_inidicator\"\n",
    "CHEMICAL_INDICATOR = \"chemical_inidicator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', 'text', '.', '?']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"[\\w']+|[.,!?;]\", ',text\".?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', 'U.S.', '')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"^([,!?]*)(.+?)([,!?]?)$\", \"U.S.\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "punctuation_to_strip = '\"#%&\\'();:*+-/<=>@[\\\\]^_`{|}~'\n",
    "def stemtokenizer(text):\n",
    "    \"\"\" MAIN FUNCTION to get clean stems out of a text. A list of clean stems are returned \"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\s+', gaps=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stems = []  # result\n",
    "    for token in tokens:\n",
    "        stem = token.lower()\n",
    "        stem = stem.strip(punctuation_to_strip)\n",
    "        if stem:\n",
    "            if is_number(stem):\n",
    "                token_stems = [NUMBER_INDICATOR]\n",
    "            elif is_currency(stem):\n",
    "                token_stems = [CURRENCY_INDICATOR]\n",
    "            elif is_chemical(stem):\n",
    "                token_stems = [CHEMICAL_INDICATOR]\n",
    "            else:\n",
    "                stem = stem.strip(punctuation_to_strip)\n",
    "                token_stems = list(re.findall(r\"^([.,!?;:]*)(.+?)([.,!?;:]?)$\", stem)[0])\n",
    "                if len(token_stems):\n",
    "                    if token_stems[1] in abbrev_types_set:\n",
    "                        token_stems[1] = token_stems[1] + token_stems[2]\n",
    "                        token_stems[2] = \"\"\n",
    "                \n",
    "            for stem in token_stems:   \n",
    "                if stem and len(stem) >= MIN_SIZE:\n",
    "                    # extract uni-grams\n",
    "                    stems.append(stem)\n",
    "    del tokens\n",
    "    return stems\n",
    "\n",
    "def is_number(str):\n",
    "    \"\"\" Returns true if given string is a number (float or int)\"\"\"\n",
    "    try:\n",
    "        float(str.replace(\",\", \"\"))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_currency(str):\n",
    "    return str[0] == \"$\"\n",
    "\n",
    "def is_chemical(strg):\n",
    "    return (strg.count(\"-\") > 2 and len(strg) >= 25) or len(strg) >= 40\n",
    "\n",
    "MIN_SENT_LENGTH = 50\n",
    "MAX_NON_TABLE_ROW_LENGTH = 100\n",
    "\n",
    "def is_table_row(sent):\n",
    "    return len(sent) < MAX_NON_TABLE_ROW_LENGTH and sent.count('.') > 2\n",
    "\n",
    "def min_sentence_length_enforcer(sentences):\n",
    "    # initialize with '' so we dont have to add a condition to be evaluated in the loop\n",
    "    new_sentences = ['']\n",
    "    for sent in sentences:\n",
    "        if len(sent) < MIN_SENT_LENGTH or is_table_row(sent):\n",
    "            new_sentences[-1] += ' ' + sent\n",
    "        else:\n",
    "            new_sentences.append(sent)\n",
    "    if len(new_sentences[0]) == 0: new_sentences = new_sentences[1:]\n",
    "    return new_sentences\n",
    "\n",
    "def get_sentences(text):\n",
    "    sents = sentence_tokenizer.tokenize(text)\n",
    "    sents = min_sentence_length_enforcer(sents)\n",
    "    return sents\n",
    "\n",
    "reload(nltk.data)\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "extra_abbrv = [u'u.s', u'fig', u'figs', u'no', u'ser',\n",
    "               u'jan', u'feb', u'mar', u'apr', u'may', u'jun', u'jul', u'aug', u'sep', u'oct', u'nov', u'dec',\n",
    "u'proc', u'natl', u'sci', u'al', u'biochem', u'mol', u'res', u'biophys', u'commun', u'acad', \n",
    "u'chem', u'med', u'biol', u'enzymol', u'Am', u'Soc', u'pat', u'nos', u'id', u'seq', \n",
    "u'gen', u'ed', u'publ', u'cell', u'ii', u'iii', u'iv', u'viral', u'dis', u'infect', \n",
    "u'rev', u'supp', u'dev', u'pp', u'genet', u'pp', u'nucl', u'pub', u'etc', u'virol', \n",
    "u'u.s. pat', u'u.s. ser', u'u.s. patent', u'ann', u'microbiol', u'environ', u'U.S',\n",
    "u'curr', u'vol', u'enz', u'struct', u'exp', u'approx', u'int', u'oncol', u'appl', \n",
    "u'math', u'adv', u'u.s.c', u'et', u'app', u'biosci', u'molec']\n",
    "extra_abbrv.extend([str(i) for i in range(1,40)])\n",
    "extra_abbrv.extend(list('abcdefghijklmnopqrstuvwxyz'))\n",
    "sentence_tokenizer._params.abbrev_types.update(extra_abbrv)\n",
    "abbrev_types_set = set(sentence_tokenizer._params.abbrev_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dd = \"CROSS REFERENCE TO RELATED APPLICATIONS This application is a Divisional of U.S. application Ser. No. 09/466,778, filed on Dec. 20, 1999, now U.S. Pat. No. 6,872,546, which claims benefit under 35 U.S.C. \\xa7 119(e) of U.S.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dd = \"The invention encompasses full-length WF-HABP polynucleotides; host cell expression systems; encompasses full-length WF-HABP polypeptides (including fragments, variants, derivatives and analogs thereof); encompasses full-length WF-HABP fusion proteins; antibodies encompasses full-length WF-HABP; agonists and antagonists encompasses full-length WF-HABP; and other compounds that modulate encompasses full-length WF-HABP gene expression or encompasses full-length WF-HABP activity; that can be used for diagnosis, drug screening, and treatment or prevention of disorders which include, but are not limited to, vascular disorders and conditions, congenital pain insensitivity, inflammation, ischemia, host defense dysfunction, immune surveillance dysfunction, neural disorders, arthritis, edema, multiple sclerosis, autoimmunity, immune dysfunction, cancers, metastasis, integumentary disorders, and allergy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'invention',\n",
       " 'encompasses',\n",
       " 'full-length',\n",
       " 'wf-habp',\n",
       " 'polynucleotides',\n",
       " 'host',\n",
       " 'cell',\n",
       " 'expression',\n",
       " 'systems',\n",
       " 'encompasses',\n",
       " 'full-length',\n",
       " 'wf-habp',\n",
       " 'polypeptides',\n",
       " 'including',\n",
       " 'fragments',\n",
       " ',',\n",
       " 'variants',\n",
       " ',',\n",
       " 'derivatives',\n",
       " 'and',\n",
       " 'analogs',\n",
       " 'thereof',\n",
       " 'encompasses',\n",
       " 'full-length',\n",
       " 'wf-habp',\n",
       " 'fusion',\n",
       " 'proteins',\n",
       " 'antibodies',\n",
       " 'encompasses',\n",
       " 'full-length',\n",
       " 'wf-habp',\n",
       " 'agonists',\n",
       " 'and',\n",
       " 'antagonists',\n",
       " 'encompasses',\n",
       " 'full-length',\n",
       " 'wf-habp',\n",
       " 'and',\n",
       " 'other',\n",
       " 'compounds',\n",
       " 'that',\n",
       " 'modulate',\n",
       " 'encompasses',\n",
       " 'full-length',\n",
       " 'wf-habp',\n",
       " 'gene',\n",
       " 'expression',\n",
       " 'or',\n",
       " 'encompasses',\n",
       " 'full-length',\n",
       " 'wf-habp',\n",
       " 'activity',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'for',\n",
       " 'diagnosis',\n",
       " ',',\n",
       " 'drug',\n",
       " 'screening',\n",
       " ',',\n",
       " 'and',\n",
       " 'treatment',\n",
       " 'or',\n",
       " 'prevention',\n",
       " 'of',\n",
       " 'disorders',\n",
       " 'which',\n",
       " 'include',\n",
       " ',',\n",
       " 'but',\n",
       " 'are',\n",
       " 'not',\n",
       " 'limited',\n",
       " 'to',\n",
       " ',',\n",
       " 'vascular',\n",
       " 'disorders',\n",
       " 'and',\n",
       " 'conditions',\n",
       " ',',\n",
       " 'congenital',\n",
       " 'pain',\n",
       " 'insensitivity',\n",
       " ',',\n",
       " 'inflammation',\n",
       " ',',\n",
       " 'ischemia',\n",
       " ',',\n",
       " 'host',\n",
       " 'defense',\n",
       " 'dysfunction',\n",
       " ',',\n",
       " 'immune',\n",
       " 'surveillance',\n",
       " 'dysfunction',\n",
       " ',',\n",
       " 'neural',\n",
       " 'disorders',\n",
       " ',',\n",
       " 'arthritis',\n",
       " ',',\n",
       " 'edema',\n",
       " ',',\n",
       " 'multiple',\n",
       " 'sclerosis',\n",
       " ',',\n",
       " 'autoimmunity',\n",
       " ',',\n",
       " 'immune',\n",
       " 'dysfunction',\n",
       " ',',\n",
       " 'cancers',\n",
       " ',',\n",
       " 'metastasis',\n",
       " ',',\n",
       " 'integumentary',\n",
       " 'disorders',\n",
       " ',',\n",
       " 'and',\n",
       " 'allergy',\n",
       " '.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemtokenizer(dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Separate Training and Validation Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000\n",
    "\n",
    "preprocessed_location = root_location + \"preprocessed_data/\"\n",
    "TRAINING_PREPROCESSED_FILES_PREFIX = preprocessed_location + \"extended_pv_training_docs_data_preprocessed-\"\n",
    "VALIDATION_PREPROCESSED_FILES_PREFIX = preprocessed_location + \"extended_pv_validation_docs_data_preprocessed-\"\n",
    "TEST_PREPROCESSED_FILES_PREFIX = preprocessed_location + \"extended_pv_test_docs_data_preprocessed-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SENTENCE_ID = \"{}_s{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_batch(file_prefix, batch_lines, batch_start):\n",
    "    if len(batch_lines):\n",
    "        print \"writing batch %d\" % batch_start\n",
    "        with open(file_prefix + str(batch_start), 'w') as batch_file:\n",
    "            for line in batch_lines:\n",
    "                batch_file.write((u\" \".join(line) + \"\\n\").encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Line Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line_positions = pickle.load(open(\"/big/s/shalaby/exported_data/line_positions.pkl\", \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Line Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 54s, sys: 30.6 s, total: 4min 25s\n",
      "Wall time: 4min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "line_positions = dict()\n",
    "with open(training_file) as f:\n",
    "    \n",
    "    i = 0\n",
    "    line_positions[i] = f.tell()\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        i+=1\n",
    "        if not line.strip(): continue\n",
    "        line_positions[i] = f.tell()\n",
    "        line = f.readline()\n",
    "    del line_positions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(line_positions, open(\"/big/s/shalaby/exported_data/line_positions.pkl\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2009750"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(line_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc Line Positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Doc Line Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_line_positions = pickle.load(open(exports_location + \"doc_line_positions.pkl\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2009750"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_line_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Doc Line Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 7s, sys: 35.7 s, total: 19min 43s\n",
      "Wall time: 19min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_line_positions = dict()\n",
    "with open(training_file) as f:\n",
    "\n",
    "    i = 0\n",
    "    curr_pos = f.tell()\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        doc_id, text = eval(line)\n",
    "        if not line.strip(): continue\n",
    "        doc_line_positions[doc_id] = curr_pos\n",
    "        curr_pos = f.tell()\n",
    "        line = f.readline()\n",
    "        i+=1\n",
    "        if i % 1000:\n",
    "            info(\"Finished: {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(doc_line_positions, open(exports_location + \"doc_line_positions.pkl\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOCS_LIST = training_docs_list\n",
    "FILE_PREFIX = TRAINING_PREPROCESSED_FILES_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def multithreaded_batch_creation(start_index):\n",
    "\n",
    "    if os.path.exists(FILE_PREFIX + str(start_index)):\n",
    "        info(\"Batch {} already exists, skipping..\".format(start_index))\n",
    "        return\n",
    "    \n",
    "    info(\"Batch creation working on {}\\n\".format(start_index))\n",
    "    with open(training_file) as file_obj:\n",
    "        token_lines = []\n",
    "        start_time = time.time()\n",
    "        for doc_index, curr_doc_id in enumerate(DOCS_LIST[start_index:]):\n",
    "            file_obj.seek(doc_line_positions[curr_doc_id])\n",
    "            line = file_obj.readline()\n",
    "            (doc_id, text) = eval(line)\n",
    "\n",
    "            sent_tokens_list = []\n",
    "            doc_tokens_list = [doc_id]\n",
    "\n",
    "            # get the tokenized sentences\n",
    "            sentences = get_sentences(text)\n",
    "            for i,sent in enumerate(sentences):\n",
    "                sentence_id = SENTENCE_ID.format(doc_id, i+1)\n",
    "                sent_tokens_list.append([sentence_id] + sentence_wordtokenizer(sent))\n",
    "\n",
    "            # create the document tokens list from the sentence token lists instead of tokenizing the whole document again\n",
    "            for sent_tokens in sent_tokens_list:\n",
    "                doc_tokens_list.extend(sent_tokens[1:])\n",
    "\n",
    "            # now add the document tokens and the sentence tokens to the list that will be written to the file\n",
    "            token_lines.append(doc_tokens_list)\n",
    "            token_lines.extend(sent_tokens_list)\n",
    "\n",
    "            if doc_index % 1000 == 0: info(\"Doc:{:6} -> Total Lines to write: {:8}\".format(start_index + doc_index, len(token_lines)))\n",
    "            if doc_index >= BATCH_SIZE - 1:\n",
    "                break\n",
    "    duration = time.time() - start_time\n",
    "    info(\"Finished batch {} of size {:d} in {:.0f}m {:.0f}s\".format(start_index, BATCH_SIZE, * divmod(duration, 60)))\n",
    "    info(\"For index {}, the actual number of lines written is: {}\".format(start_index, len(token_lines)))\n",
    "    \n",
    "    write_batch(FILE_PREFIX, token_lines, start_index)\n",
    "    del token_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(divmod(SAMPLE_SIZE, BATCH_SIZE)[0]+1) * BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches = range(0, (divmod(SAMPLE_SIZE, BATCH_SIZE)[0]+1) * BATCH_SIZE, BATCH_SIZE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-05 03:15:59,531 : INFO : Batch creation working on 40000\n",
      "\n",
      "2017-03-05 03:15:59,531 : INFO : Batch creation working on 20000\n",
      "\n",
      "2017-03-05 03:15:59,531 : INFO : Batch creation working on 30000\n",
      "\n",
      "2017-03-05 03:15:59,531 : INFO : Batch creation working on 0\n",
      "\n",
      "2017-03-05 03:15:59,531 : INFO : Batch creation working on 10000\n",
      "\n",
      "2017-03-05 03:15:59,533 : INFO : Batch creation working on 90000\n",
      "\n",
      "2017-03-05 03:15:59,532 : INFO : Batch creation working on 60000\n",
      "\n",
      "2017-03-05 03:15:59,533 : INFO : Batch creation working on 70000\n",
      "\n",
      "2017-03-05 03:15:59,532 : INFO : Batch creation working on 50000\n",
      "\n",
      "2017-03-05 03:15:59,533 : INFO : Batch creation working on 80000\n",
      "\n",
      "2017-03-05 03:15:59,534 : INFO : Batch creation working on 100000\n",
      "\n",
      "2017-03-05 03:15:59,536 : INFO : Batch creation working on 110000\n",
      "\n",
      "2017-03-05 03:15:59,536 : INFO : Batch creation working on 120000\n",
      "\n",
      "2017-03-05 03:15:59,613 : INFO : 58\n",
      "2017-03-05 03:15:59,656 : INFO : 103\n",
      "2017-03-05 03:15:59,656 : INFO : 127\n",
      "2017-03-05 03:15:59,663 : INFO : 135\n",
      "2017-03-05 03:15:59,682 : INFO : 120\n",
      "2017-03-05 03:15:59,696 : INFO : 172\n",
      "2017-03-05 03:15:59,710 : INFO : 204\n",
      "2017-03-05 03:15:59,717 : INFO : 214\n",
      "2017-03-05 03:15:59,746 : INFO : 180\n",
      "2017-03-05 03:15:59,752 : INFO : 273\n",
      "2017-03-05 03:15:59,755 : INFO : 236\n",
      "2017-03-05 03:16:00,012 : INFO : 422\n",
      "2017-03-05 03:16:00,058 : INFO : 602\n",
      "2017-03-05 03:16:35,762 : INFO : Finished batch 120000 of size 10000 in 0m 36s\n",
      "2017-03-05 03:16:35,765 : INFO : For index 120000, the actual number of lines written is: 39051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 120000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-05 03:19:15,160 : INFO : 203424\n",
      "2017-03-05 03:19:18,040 : INFO : 228307\n",
      "2017-03-05 03:19:24,655 : INFO : 241057\n",
      "2017-03-05 03:19:29,227 : INFO : 229779\n",
      "2017-03-05 03:19:33,072 : INFO : 242969\n",
      "2017-03-05 03:19:33,568 : INFO : 229559\n",
      "2017-03-05 03:19:38,542 : INFO : 233298\n",
      "2017-03-05 03:19:39,541 : INFO : 234948\n",
      "2017-03-05 03:19:42,106 : INFO : 254152\n",
      "2017-03-05 03:19:43,240 : INFO : 244518\n",
      "2017-03-05 03:19:44,998 : INFO : 256308\n",
      "2017-03-05 03:20:04,229 : INFO : 279122\n",
      "2017-03-05 03:22:19,521 : INFO : 400542\n",
      "2017-03-05 03:22:49,276 : INFO : 461417\n",
      "2017-03-05 03:23:03,220 : INFO : 476057\n",
      "2017-03-05 03:23:03,576 : INFO : 447324\n",
      "2017-03-05 03:23:05,930 : INFO : 454121\n",
      "2017-03-05 03:23:08,328 : INFO : 484385\n",
      "2017-03-05 03:23:10,075 : INFO : 457522\n",
      "2017-03-05 03:23:15,167 : INFO : 497750\n",
      "2017-03-05 03:23:24,213 : INFO : 505281\n",
      "2017-03-05 03:23:26,933 : INFO : 485703\n",
      "2017-03-05 03:23:38,007 : INFO : 497345\n",
      "2017-03-05 03:24:02,592 : INFO : 547861\n",
      "2017-03-05 03:25:46,599 : INFO : 605961\n",
      "2017-03-05 03:26:26,753 : INFO : 689996\n",
      "2017-03-05 03:26:48,806 : INFO : 670807\n",
      "2017-03-05 03:26:52,297 : INFO : 675801\n",
      "2017-03-05 03:26:54,474 : INFO : 682427\n",
      "2017-03-05 03:26:57,109 : INFO : 722944\n",
      "2017-03-05 03:26:59,150 : INFO : 733008\n",
      "2017-03-05 03:27:12,355 : INFO : 735236\n",
      "2017-03-05 03:27:15,411 : INFO : 711204\n",
      "2017-03-05 03:27:37,172 : INFO : 776026\n",
      "2017-03-05 03:27:53,098 : INFO : 755113\n",
      "2017-03-05 03:28:23,009 : INFO : 826345\n",
      "2017-03-05 03:29:41,858 : INFO : 830734\n",
      "2017-03-05 03:30:40,179 : INFO : 939501\n",
      "2017-03-05 03:30:43,984 : INFO : 885197\n",
      "2017-03-05 03:30:59,782 : INFO : 909571\n",
      "2017-03-05 03:31:02,990 : INFO : 966029\n",
      "2017-03-05 03:31:04,371 : INFO : 921332\n",
      "2017-03-05 03:31:15,079 : INFO : 972208\n",
      "2017-03-05 03:31:21,456 : INFO : 991928\n",
      "2017-03-05 03:31:23,518 : INFO : 950658\n",
      "2017-03-05 03:31:53,447 : INFO : 1028998\n",
      "2017-03-05 03:32:27,844 : INFO : 1025464\n",
      "2017-03-05 03:32:40,405 : INFO : 1082767\n",
      "2017-03-05 03:33:37,660 : INFO : 1053745\n",
      "2017-03-05 03:34:24,522 : INFO : 1092397\n",
      "2017-03-05 03:34:51,901 : INFO : 1127879\n",
      "2017-03-05 03:34:52,374 : INFO : 1181727\n",
      "2017-03-05 03:35:04,763 : INFO : 1145506\n",
      "2017-03-05 03:35:14,063 : INFO : 1203180\n",
      "2017-03-05 03:36:38,848 : INFO : 1223268\n",
      "2017-03-05 03:36:34,933 : INFO : 1229755\n",
      "2017-03-05 03:36:41,834 : INFO : 1192007\n",
      "2017-03-05 03:37:40,314 : INFO : 1282331\n",
      "2017-03-05 03:43:33,260 : INFO : 1295491\n",
      "2017-03-05 03:43:51,729 : INFO : 1351488\n",
      "2017-03-05 03:44:53,223 : INFO : 1315290\n",
      "2017-03-05 03:47:21,999 : INFO : 1327550\n",
      "2017-03-05 03:48:21,500 : INFO : 1354513\n",
      "2017-03-05 03:49:21,961 : INFO : 1391063\n",
      "2017-03-05 03:49:45,403 : INFO : 1442106\n",
      "2017-03-05 03:49:53,154 : INFO : 1423486\n",
      "2017-03-05 03:50:05,404 : INFO : 1430774\n",
      "2017-03-05 03:50:16,966 : INFO : 1462101\n",
      "2017-03-05 03:50:24,680 : INFO : 1473724\n",
      "2017-03-05 03:52:18,809 : INFO : 1525028\n",
      "2017-03-05 03:53:44,967 : INFO : 1543647\n",
      "2017-03-05 03:54:58,733 : INFO : 1541001\n",
      "2017-03-05 03:55:05,858 : INFO : 1644625\n",
      "2017-03-05 03:55:21,081 : INFO : 1560524\n",
      "2017-03-05 03:56:53,408 : INFO : 1572585\n",
      "2017-03-05 03:57:52,886 : INFO : 1619870\n",
      "2017-03-05 03:59:08,884 : INFO : 1679659\n",
      "2017-03-05 03:59:13,250 : INFO : 1679486\n",
      "2017-03-05 03:59:18,814 : INFO : 1672861\n",
      "2017-03-05 03:59:29,683 : INFO : 1701374\n",
      "2017-03-05 03:59:40,097 : INFO : 1730884\n",
      "2017-03-05 04:00:23,904 : INFO : 1797834\n",
      "2017-03-05 04:01:40,701 : INFO : 1745424\n",
      "2017-03-05 04:02:46,054 : INFO : 1782013\n",
      "2017-03-05 04:03:49,138 : INFO : 1796206\n",
      "2017-03-05 04:04:21,441 : INFO : 1796829\n",
      "2017-03-05 04:04:31,611 : INFO : 1943458\n",
      "2017-03-05 04:04:52,750 : INFO : 1852163\n",
      "2017-03-05 04:07:03,659 : INFO : 1925673\n",
      "2017-03-05 04:09:32,760 : INFO : 1957235\n",
      "2017-03-05 04:10:23,215 : INFO : 1977262\n",
      "2017-03-05 04:10:26,371 : INFO : 1941709\n",
      "2017-03-05 04:10:29,504 : INFO : 1922152\n",
      "2017-03-05 04:11:14,360 : INFO : 1995787\n",
      "2017-03-05 04:11:20,875 : INFO : 2049099\n",
      "2017-03-05 04:12:29,926 : INFO : 2014914\n",
      "2017-03-05 04:13:33,249 : INFO : 2046055\n",
      "2017-03-05 04:14:30,767 : INFO : 2010290\n",
      "2017-03-05 04:15:11,952 : INFO : 2072053\n",
      "2017-03-05 04:16:16,474 : INFO : 2226060\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    pool = ThreadPool(16)\n",
    "    # +1 since range is end-exclusive\n",
    "    batches = range(0, (divmod(SAMPLE_SIZE, BATCH_SIZE)[0]+1) * BATCH_SIZE, BATCH_SIZE )\n",
    "    indices = pool.map(multithreaded_batch_creation, batches)\n",
    "    pool.close()\n",
    "    pool.terminate()\n",
    "finally:\n",
    "    pool.close()\n",
    "    pool.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOCS_LIST = validation_docs_list\n",
    "FILE_PREFIX = VALIDATION_PREPROCESSED_FILES_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-04 18:02:17,370 : INFO : Batch creation working on 20000\n",
      "\n",
      "2017-03-04 18:02:17,370 : INFO : Batch creation working on 10000\n",
      "\n",
      "2017-03-04 18:02:17,369 : INFO : Batch creation working on 0\n",
      "\n",
      "2017-03-04 18:02:17,662 : INFO : 56\n",
      "2017-03-04 18:02:17,852 : INFO : 65\n",
      "2017-03-04 18:02:17,978 : INFO : 309\n",
      "2017-03-04 18:05:18,878 : INFO : 216333\n",
      "2017-03-04 18:05:40,903 : INFO : 245047\n",
      "2017-03-04 18:05:42,567 : INFO : 257709\n",
      "2017-03-04 18:08:13,537 : INFO : 427212\n",
      "2017-03-04 18:08:56,333 : INFO : 495800\n",
      "2017-03-04 18:08:59,727 : INFO : 507128\n",
      "2017-03-04 18:11:19,354 : INFO : 639322\n",
      "2017-03-04 18:12:06,057 : INFO : 732933\n",
      "2017-03-04 18:12:21,122 : INFO : 758304\n",
      "2017-03-04 18:14:23,184 : INFO : 846145\n",
      "2017-03-04 18:15:06,734 : INFO : 958711\n",
      "2017-03-04 18:15:45,331 : INFO : 1014866\n",
      "2017-03-04 18:17:29,865 : INFO : 1061395\n",
      "2017-03-04 18:18:10,942 : INFO : 1195757\n",
      "2017-03-04 18:19:11,428 : INFO : 1277012\n",
      "2017-03-04 18:20:39,682 : INFO : 1265578\n",
      "2017-03-04 18:21:37,980 : INFO : 1417192\n",
      "2017-03-04 18:23:03,644 : INFO : 1526815\n",
      "2017-03-04 18:24:15,072 : INFO : 1481366\n",
      "2017-03-04 18:25:11,154 : INFO : 1648977\n",
      "2017-03-04 18:26:57,336 : INFO : 1791293\n",
      "2017-03-04 18:27:40,647 : INFO : 1702348\n",
      "2017-03-04 18:28:38,081 : INFO : 1886698\n",
      "2017-03-04 18:30:31,395 : INFO : 2043121\n",
      "2017-03-04 18:30:56,970 : INFO : 1917108\n",
      "2017-03-04 18:31:54,036 : INFO : 2116564\n",
      "2017-03-04 18:34:09,381 : INFO : 2296299\n",
      "2017-03-04 18:34:25,052 : INFO : Finished batch of 10000 in 32m 8s\n",
      "2017-03-04 18:34:25,054 : INFO : For index 0, the actual number of lines written is: 2149311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-04 18:35:25,363 : INFO : Finished batch of 10000 in 33m 8s\n",
      "2017-03-04 18:35:25,365 : INFO : For index 10000, the actual number of lines written is: 2349706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-04 18:36:30,630 : INFO : Finished batch of 10000 in 34m 13s\n",
      "2017-03-04 18:36:30,632 : INFO : For index 20000, the actual number of lines written is: 2438579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 20000\n",
      "CPU times: user 1.66 s, sys: 1.49 s, total: 3.15 s\n",
      "Wall time: 34min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    pool = ThreadPool(16)\n",
    "    # +1 since range is end-exclusive\n",
    "    batches = range(0, (divmod(SAMPLE_SIZE, BATCH_SIZE)[0]+1) * BATCH_SIZE, BATCH_SIZE )\n",
    "    indices = pool.map(multithreaded_batch_creation, batches)\n",
    "    pool.close()\n",
    "    pool.terminate()\n",
    "except e as Exception:\n",
    "    pool.close()\n",
    "    pool.terminate()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DOCS_LIST = test_docs_list\n",
    "FILE_PREFIX = TEST_PREPROCESSED_FILES_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = len(test_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "try:\n",
    "    pool = ThreadPool(16)\n",
    "    # +1 since range is end-exclusive\n",
    "    batches = range(0, (divmod(SAMPLE_SIZE, BATCH_SIZE)[0]+1) * BATCH_SIZE, BATCH_SIZE )\n",
    "    indices = pool.map(multithreaded_batch_creation, batches)\n",
    "    pool.close()\n",
    "    pool.terminate()\n",
    "except e as Exception:\n",
    "    pool.close()\n",
    "    pool.terminate()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool.close()\n",
    "pool.terminate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
