{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import cPickle as pickle\n",
    "import string\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "\n",
    "from multiprocessing import Pool as ThreadPool\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_location = \"/big/s/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "#training_file = root_location + 'docs_output_training_validation_documents_' + str(SAMPLE_RATIO)\n",
    "training_file = root_location + 'docs_output.json'\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "# training_docs_list_file = exports_location + \"training_documents_\" + str(SAMPLE_RATIO) + \"_sample.pkl\"\n",
    "# validation_docs_list_file = exports_location + \"validation_documents_\" + str(SAMPLE_RATIO) + \"_sample.pkl\"\n",
    "training_docs_list_file = exports_location + \"training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"validation_docs_list.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_docs_additional_list_file = exports_location + \"balanced_additional_training_docs_list.pkl\"\n",
    "validation_docs_additional_list_file = exports_location + \"balanced_additional_validation_docs_list.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.8 s, sys: 880 ms, total: 26.7 s\n",
      "Wall time: 26.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "\n",
    "additional_training_docs_list = pickle.load(open(training_docs_additional_list_file))\n",
    "additional_validation_docs_list = pickle.load(open(validation_docs_additional_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "783"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(additional_training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemtokenizer(text):\n",
    "    \"\"\" MAIN FUNCTION to get clean stems out of a text. A list of clean stems are returned \"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\s+', gaps=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stems = []  # result\n",
    "    for token in tokens:\n",
    "        stem = token.lower()\n",
    "        stem = stem.strip(string.punctuation)\n",
    "        if stem:\n",
    "            if is_number(stem):\n",
    "                stem = NUMBER_INDICATOR\n",
    "            elif is_currency(stem):\n",
    "                stem = CURRENCY_INDICATOR\n",
    "            elif is_chemical(stem):\n",
    "                stem = CHEMICAL_INDICATOR\n",
    "            else:\n",
    "                stem = stem.strip(string.punctuation)\n",
    "            if stem and len(stem) >= MIN_SIZE:\n",
    "                # extract uni-grams\n",
    "                stems.append(stem)\n",
    "    del tokens\n",
    "    return stems\n",
    "\n",
    "def is_number(str):\n",
    "    \"\"\" Returns true if given string is a number (float or int)\"\"\"\n",
    "    try:\n",
    "        float(str.replace(\",\", \"\"))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_currency(str):\n",
    "    return str[0] == \"$\"\n",
    "\n",
    "def is_chemical(str):\n",
    "    return str.count(\"-\") > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_SIZE = 0\n",
    "NUMBER_INDICATOR = \"number_inidicator\"\n",
    "CURRENCY_INDICATOR = \"currency_inidicator\"\n",
    "CHEMICAL_INDICATOR = \"chemical_inidicator\"\n",
    "\n",
    "\n",
    "BATCH_SIZE = 10000\n",
    "# TRAINING_PREPROCESSED_FILES_PREFIX = \"/mnt/data2/shalaby/training_docs_sample_%s_data_preprocessed-\" % str(SAMPLE_RATIO)\n",
    "# TRAINING_PREPROCESSED_DOCIDS_FILES_PREFIX = \"/mnt/data2/shalaby/training_docs_sample_%s_docids_preprocessed-\" % str(SAMPLE_RATIO)\n",
    "# VALIDATION_PREPROCESSED_FILES_PREFIX = \"/mnt/data2/shalaby/validation_docs_sample_%s_data_preprocessed-\" % str(SAMPLE_RATIO)\n",
    "# VALIDATION_PREPROCESSED_DOCIDS_FILES_PREFIX = \"/mnt/data2/shalaby/validation_docs_sample_%s_docids_preprocessed-\" % str(SAMPLE_RATIO)\n",
    "\n",
    "TRAINING_PREPROCESSED_FILES_PREFIX = \"/big/s/shalaby/preprocessed_data/training_docs_additional_data_preprocessed\"\n",
    "VALIDATION_PREPROCESSED_FILES_PREFIX = \"/big/s/shalaby/preprocessed_data/validation_docs_additional_data_preprocessed\"\n",
    "\n",
    "\n",
    "TRAINING_MERGED_PREPROCESSED_FILES_PREFIX = \"/big/s/shalaby/preprocessed_data/training_docs_merged_data_preprocessed-\"\n",
    "TRAINING_MERGED_PREPROCESSED_DOCIDS_FILES_PREFIX = \"/big/s/shalaby/preprocessed_data/training_docs_merged_docids_preprocessed-\"\n",
    "VALIDATION_MERGED_PREPROCESSED_FILES_PREFIX = \"/big/s/shalaby/preprocessed_data/validation_docs_merged_data_preprocessed-\"\n",
    "VALIDATION_MERGED_PREPROCESSED_DOCIDS_FILES_PREFIX = \"/big/s/shalaby/preprocessed_data/validation_docs_merged_docids_preprocessed-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_batch(file_name, batch_lines):\n",
    "    if len(batch_lines):\n",
    "        with open(file_name, 'w') as batch_file:\n",
    "            for line in batch_lines:\n",
    "                batch_file.write((u\" \".join(line) + \"\\n\").encode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 16s, sys: 1min 14s, total: 6min 31s\n",
      "Wall time: 14min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_line_positions = dict()\n",
    "with open(training_file) as f:\n",
    "    doc_position = f.tell()\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        if not line.strip(): continue\n",
    "        doc_id = line[3:11]\n",
    "        doc_line_positions[doc_id] = doc_position\n",
    "        doc_position = f.tell()\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Training Batch File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(training_file) as f:\n",
    "    token_lines = []\n",
    "    for training_doc in additional_training_docs_list:\n",
    "        f.seek(doc_line_positions[training_doc])\n",
    "        line = f.readline()\n",
    "        (doc_id, text) = eval(line)\n",
    "        token_lines.append(stemtokenizer(text))\n",
    "    write_batch(TRAINING_PREPROCESSED_FILES_PREFIX, token_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Validation Batch File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.7 s, sys: 24 ms, total: 1.72 s\n",
      "Wall time: 1.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(training_file) as f:\n",
    "    token_lines = []\n",
    "    for validation_doc in additional_validation_docs_list:\n",
    "        f.seek(doc_line_positions[validation_doc])\n",
    "        line = f.readline()\n",
    "        (doc_id, text) = eval(line)\n",
    "        token_lines.append(stemtokenizer(text))\n",
    "    write_batch(VALIDATION_PREPROCESSED_FILES_PREFIX, token_lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
