{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "import cPickle as pickle\n",
    "\n",
    "from sklearn.metrics import coverage_error\n",
    "from scipy.sparse import dok_matrix\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IS_SAMPLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MIN_DOCUMENTS = 5\n",
    "TOP_N_FEATURES = 10000\n",
    "\n",
    "SVM_ITERATIONS = 1000\n",
    "SVM_REG = 0.1\n",
    "\n",
    "BM25_K = 1.5  # controls power of tf component\n",
    "BM25_b = 0.75  # controls the BM25 length normalization\n",
    "\n",
    "RANDOM_SEED = 10000\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_postings(postings_list1, postings_list2):\n",
    "    # key could be either a doc id or a term\n",
    "    for key in postings_list2:\n",
    "        if postings_list1.get(key):\n",
    "            postings_list1[key] += postings_list2[key]\n",
    "        else:\n",
    "            postings_list1[key] = postings_list2[key]\n",
    "    return postings_list1\n",
    "\n",
    "def get_term_dictionary(terms):\n",
    "    \"\"\"\n",
    "    Maps string terms to indexes in an array\n",
    "    \"\"\"\n",
    "    term_dictionary = {}\n",
    "    term_array = [None] * len(terms)\n",
    "    def put(key):\n",
    "        hashvalue = hashfunction(key, len(term_array))\n",
    "        if term_array[hashvalue] == None:\n",
    "            term_array[hashvalue] = key\n",
    "            return hashvalue\n",
    "        else:\n",
    "            nextslot = rehash(hashvalue, len(term_array))\n",
    "            while term_array[nextslot] != None:\n",
    "                nextslot = rehash(nextslot, len(term_array))\n",
    "            if term_array[nextslot] == None:\n",
    "                term_array[nextslot] = key\n",
    "                return nextslot\n",
    "    def hashfunction(key, size):\n",
    "        return hash(key) % size\n",
    "    def rehash(oldhash, size):\n",
    "        return (oldhash + 1) % size\n",
    "    i = 0\n",
    "    for term in terms:\n",
    "        corresponding_index = put(term)\n",
    "        term_dictionary[term] = corresponding_index\n",
    "        i+=1\n",
    "        if i%10000 == 0: print \"finished \" + str(i)\n",
    "    return term_dictionary\n",
    "\n",
    "def jsonKV2str(x):\n",
    "    \"\"\"\n",
    "    Change string keys to int\n",
    "    \"\"\"\n",
    "    if isinstance(x, dict):\n",
    "            #return {doc_id:{int(term_id):x[doc_id][term_id] for term_id in x[doc_id]} for doc_id in x }\n",
    "        \n",
    "            return {int(k):(int(v) if isinstance(v, unicode) else v) for k,v in x.items()}\n",
    "    return x\n",
    "\n",
    "def get_json(json_postings):\n",
    "    return json.loads(json_postings)\n",
    "\n",
    "def get_json_convert_num(json_postings):\n",
    "    return json.loads(json_postings, object_hook=jsonKV2str)\n",
    "\n",
    "def get_doc_index(term, postings_list, term_dictionary):\n",
    "    #return [(doc_id, {term: postings_list[doc_id]}) for doc_id in postings_list]\n",
    "    return [(doc_id, {term_dictionary[term]: postings_list[doc_id]}) for doc_id in postings_list]\n",
    "\n",
    "def get_classes(ipc_classification):\n",
    "    sections = []\n",
    "    classes = []\n",
    "    subclasses = []\n",
    "    for classification in ipc_classification:\n",
    "        # we do the check because some documents have repetitions\n",
    "        section_name = classification['section']\n",
    "        class_name = classification['section'] + \"-\" + classification['class']\n",
    "        subclass_name = classification['section'] + \"-\" + classification['class'] + \"-\" + classification['subclass']\n",
    "        if section_name not in sections:\n",
    "            sections.append(section_name)\n",
    "        if class_name not in classes:\n",
    "            classes.append(class_name)\n",
    "        if subclass_name not in subclasses:\n",
    "            subclasses.append(subclass_name)\n",
    "    return {\"sections\": sections, \"classes\": classes, \"subclasses\": subclasses}\n",
    "\n",
    "\n",
    "def compare_classifications(x,y):\n",
    "    len_comp = cmp(len(x), len(y))\n",
    "    if len_comp == 0:\n",
    "        return cmp(x,y)\n",
    "    return len_comp\n",
    "\n",
    "\n",
    "def create_doc_index(term_index, term_dictionary):\n",
    "    return term_index \\\n",
    "        .flatMap(lambda (term, postings_list): get_doc_index(term, postings_list, term_dictionary)) \\\n",
    "        .reduceByKey(lambda x, y: merge_postings(x, y))\n",
    "\n",
    "\n",
    "def get_rf_stats(postings, classification):\n",
    "    a_plus_c = set(postings.keys())\n",
    "    a_plus_b = set(classifications_index[classification])\n",
    "    # first intersection is to get (a), second difference is to get (c) (checkout tf-rf paper for reference)\n",
    "    a = a_plus_c.intersection(a_plus_b)\n",
    "    c = a_plus_c.difference(a_plus_b)\n",
    "    size_a = len(a)\n",
    "    size_c = len(c)\n",
    "    return size_a, size_c\n",
    "\n",
    "\n",
    "def get_rf_postings(classification):\n",
    "    def get_rf_postings_internal(postings):\n",
    "        size_a, size_c = get_rf_stats(postings, classification)\n",
    "        return {docId: calculate_rf(size_a, size_c)\n",
    "                for docId, tf in postings.items()}\n",
    "    return get_rf_postings_internal\n",
    "\n",
    "\n",
    "def get_tf_rf_postings(classification):\n",
    "    def get_tf_rf_postings_internal(postings):\n",
    "        size_a, size_c = get_rf_stats(postings, classification)\n",
    "        return {docId: calculate_tf_rf(tf, size_a, size_c)\n",
    "                for docId, tf in postings.items()}\n",
    "    return get_tf_rf_postings_internal\n",
    "\n",
    "def get_error(svm, test_vectors):\n",
    "    labelsAndPreds = test_vectors.map(lambda p: (p.label, svm.predict(p.features)))\n",
    "    trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(test_vectors.count())\n",
    "    return trainErr\n",
    "\n",
    "\n",
    "def get_labeled_points_from_doc_index(doc_index, doc_classification_map, number_of_terms):\n",
    "    docs_with_classes = doc_index.map(lambda (doc_id, terms): (doc_id, (terms, doc_classification_map[doc_id])))\n",
    "    training_vectors = docs_with_classes.map(\n",
    "        lambda (doc_id, (term_list, classifications)): get_training_vector(classification, term_list,\n",
    "                                                                           classifications, number_of_terms))\n",
    "    return training_vectors\n",
    "\n",
    "get_binary = lambda x: 1 if x > 0 else 0\n",
    "get_binary = np.vectorize(get_binary)\n",
    "\n",
    "def get_row_top_N(y_score_row, y_true_row):\n",
    "    desc_score_indices = np.argsort(y_score_row)[::-1]\n",
    "    # print y_score_row\n",
    "    # print y_true_row\n",
    "    true_indices = np.where(y_true_row ==1)[0]\n",
    "    # print desc_score_indices\n",
    "    found = 0\n",
    "    top_N = 0\n",
    "    for i, score in enumerate(desc_score_indices):\n",
    "        if score in true_indices:\n",
    "            found += 1\n",
    "            if found == len(true_indices):\n",
    "                top_N = i + 1\n",
    "    # print top_N\n",
    "    return top_N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input/Output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATIO = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sc = SparkContext(\"\", \"Generate Inverted Index Job\")\n",
    "es_server = \"deka.cip.ifi.lmu.de\"\n",
    "es_port = \"9200\"\n",
    "\n",
    "original_parent_save_location = \"hdfs://deka.cip.ifi.lmu.de/extended_pv/\"\n",
    "save_parent_location = original_parent_save_location\n",
    "\n",
    "root_location = \"/big/s/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "\n",
    "docs_output = save_parent_location + \"docs_output\"\n",
    "postings_list_output = save_parent_location + \"postings_list_full.json\"\n",
    "\n",
    "accepted_terms_list_output = original_parent_save_location + \"accepted_terms_list_{}.pkl\"\n",
    "accepted_terms_with_scores_list_output = original_parent_save_location + \"accepted_terms_with_scores_list_{}.pkl\"\n",
    "postings_list_chi_selected_output = original_parent_save_location + \"postings_list_{}.json\"\n",
    "term_df_map_output = original_parent_save_location + \"term_df_map_output_{}.json\"\n",
    "doc_index_chi_selected_output = original_parent_save_location + \"doc_index_for_postings_{}.json\"\n",
    "term_dictionary_output = original_parent_save_location + \"term_dictionary_{}.pkl\"\n",
    "\n",
    "\n",
    "postings_list_training_chi_selected_output = save_parent_location + \"training_postings_list_{}.json\"\n",
    "postings_list_validation_chi_selected_output = save_parent_location + \"validation_postings_list_{}.json\"\n",
    "postings_list_test_chi_selected_output = save_parent_location + \"test_postings_list_{}.json\"\n",
    "\n",
    "\n",
    "# data location\n",
    "preprocessed_docs_location = root_location + \"preprocessed_data/\" + \"extended_pv_docs_only_for_spark\"\n",
    "\n",
    "\n",
    "# Classification objects, unrelated to sample size\n",
    "classifications_index_output = exports_location + \"extended_pv_classifications_index.pkl\"\n",
    "doc_classifications_map_file = exports_location + \"extended_pv_doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "valid_classes_file = exports_location + \"valid_classes.pkl\"\n",
    "valid_subclasses_file = exports_location + \"valid_subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "doc_lengths_map_output = exports_location + \"extended_pv_doc_lengths_map.pkl\"\n",
    "training_docs_list_file = exports_location + \"extended_pv_training_docs_list_\" + str(SAMPLE_RATIO) + \".pkl\"\n",
    "validation_docs_list_file = exports_location + \"extended_pv_validation_docs_list_\" + str(SAMPLE_RATIO) + \".pkl\"\n",
    "test_docs_list_file = exports_location + \"extended_pv_test_docs_list_\" + str(SAMPLE_RATIO) + \".pkl\"\n",
    "\n",
    "\n",
    "training_predictions_sections_output = save_parent_location + \"training_predictions_sections_list.pkl\"\n",
    "training_labels_sections_list_output = save_parent_location + \"training_labels_sections_list.pkl\"\n",
    "valdiation_predictions_sections_output = save_parent_location + \"validation_predictions_sections_list.pkl\"\n",
    "validation_labels_sections_list_output = save_parent_location + \"validation_labels_sections_list.pkl\"\n",
    "\n",
    "\n",
    "test_postings_list_output = save_parent_location + \"test_postings_list_50000.json\"\n",
    "training_errors_output = save_parent_location + \"training_errors.json\"\n",
    "model_output = save_parent_location + \"models/\" + \"iter_\" + str(SVM_ITERATIONS) + \"_reg_\" + str(SVM_REG) + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model_name(method, classification, reg=SVM_REG, iterations=SVM_ITERATIONS):\n",
    "    return save_parent_location + \"models/\" + \"iter_\" + str(iterations) + \"_reg_\" + str(reg) + \"/\" + method + \"_\" + classification + \"_model.svm\"\n",
    "def get_data_output_name(method, no_of_features=TOP_N_FEATURES, data_type=\"training\"):\n",
    "    return save_parent_location + \"models/\" + data_type + \"_data/\" + method  + \"_data.json\"\n",
    "def get_save_location(location, sample=False):\n",
    "    if sample:\n",
    "        return location.replace(save_parent_location, sample_save_parent_location)\n",
    "    return location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Classification Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.2 s, sys: 8 ms, total: 10.2 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "valid_classes = pickle.load(open(valid_classes_file))\n",
    "valid_subclasses = pickle.load(open(valid_subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))\n",
    "classifications_index = pickle.load(open(classifications_index_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extended_pv_docs = set(training_docs_list) | set(validation_docs_list) | set(test_docs_list)\n",
    "doc_count = len(extended_pv_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395509"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Sparse data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_written_doc_index(term_index, name, data_type=\"training\"):\n",
    "    doc_index = create_doc_index(term_index, term_dictionary)\n",
    "    output_name = get_data_output_name(name, data_type=data_type)\n",
    "    doc_index.map(lambda postings: json.dumps(postings)).repartition(100).saveAsTextFile(output_name)\n",
    "    doc_index = sc.textFile(output_name).map(get_json_convert_num)# .cache()\n",
    "    return doc_index\n",
    "\n",
    "def read_written_doc_index(name, data_type=\"training\"):\n",
    "    output_name = get_data_output_name(name, data_type=data_type)\n",
    "    doc_index = sc.textFile(output_name).map(get_json_convert_num)\n",
    "    return doc_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_type = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_doc_index_training = read_written_doc_index(\"tf\", data_type)\n",
    "sublinear_tf_doc_index_training = read_written_doc_index(\"tf-sublinear\", data_type)\n",
    "tf_idf_doc_index_training = read_written_doc_index(\"tf-idf\", data_type)\n",
    "sublinear_tf_idf_doc_index_training = read_written_doc_index(\"sublinear-tf-idf\", data_type)\n",
    "bm25_doc_index_training = read_written_doc_index(\"bm25\", data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.12 s, sys: 764 ms, total: 6.88 s\n",
      "Wall time: 15.8 s\n",
      "CPU times: user 42.6 s, sys: 4.08 s, total: 46.7 s\n",
      "Wall time: 46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ll = [\n",
    "    (\"bm25\", bm25_doc_index_training), \n",
    "    (\"sublinear_tf\", sublinear_tf_doc_index_training), \n",
    "    (\"tf_idf\", tf_idf_doc_index_training), \n",
    "    (\"tf\", tf_doc_index_training),\n",
    "    (\"sublinear_tf_idf\", sublinear_tf_idf_doc_index_training)\n",
    "]\n",
    "for (name, doc_ind) in ll:\n",
    "    v = sklearn.feature_extraction.DictVectorizer(sparse=True, dtype=float)\n",
    "    %time yy = doc_ind.collect()\n",
    "    list_dicts = [y[1] for y in yy]\n",
    "    doc_ids = [y[0] for y in yy]\n",
    "    %time X = v.fit_transform(list_dicts)\n",
    "    %time pickle.dump(X, open(\"/big/s/shalaby/exported_data/extened_pv_benchmarking_data/{}_{}_sparse_data.pkl\".format(name, data_type),\"w\"))\n",
    "    pickle.dump(doc_ids, open(\"/big/s/shalaby/exported_data/extened_pv_benchmarking_data/{}_{}_sparse_docids.pkl\".format(name, data_type),\"w\"))\n",
    "    del yy\n",
    "    del X\n",
    "    print name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.6.1)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
