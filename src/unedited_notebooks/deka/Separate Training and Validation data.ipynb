{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import cPickle as pickle\n",
    "import string\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "import logging\n",
    "from logging import info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATIO = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_location = \"/big/s/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "training_file = root_location + 'docs_output_training_validation_documents_' + str(SAMPLE_RATIO)\n",
    "training_file = root_location + 'docs_output.json'\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "# training_docs_list_file = exports_location + \"training_documents_\" + str(SAMPLE_RATIO) + \"_sample.pkl\"\n",
    "# validation_docs_list_file = exports_location + \"validation_documents_\" + str(SAMPLE_RATIO) + \"_sample.pkl\"\n",
    "training_docs_list_file = exports_location + \"training_docs_list.pkl\"\n",
    "validation_docs_list_file = exports_location + \"validation_docs_list.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.8 s, sys: 1.05 s, total: 27.8 s\n",
      "Wall time: 28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286325"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321473"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemtokenizer(text):\n",
    "    \"\"\" MAIN FUNCTION to get clean stems out of a text. A list of clean stems are returned \"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\s+', gaps=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stems = []  # result\n",
    "    for token in tokens:\n",
    "        stem = token.lower()\n",
    "        stem = stem.strip(string.punctuation)\n",
    "        if stem:\n",
    "            if is_number(stem):\n",
    "                stem = NUMBER_INDICATOR\n",
    "            elif is_currency(stem):\n",
    "                stem = CURRENCY_INDICATOR\n",
    "            elif is_chemical(stem):\n",
    "                stem = CHEMICAL_INDICATOR\n",
    "            else:\n",
    "                stem = stem.strip(string.punctuation)\n",
    "            if stem and len(stem) >= MIN_SIZE:\n",
    "                # extract uni-grams\n",
    "                stems.append(stem)\n",
    "    del tokens\n",
    "    return stems\n",
    "\n",
    "def is_number(str):\n",
    "    \"\"\" Returns true if given string is a number (float or int)\"\"\"\n",
    "    try:\n",
    "        float(str.replace(\",\", \"\"))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_currency(str):\n",
    "    return str[0] == \"$\"\n",
    "\n",
    "def is_chemical(str):\n",
    "    return str.count(\"-\") > 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Separate Training and Validation Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MIN_SIZE = 0\n",
    "NUMBER_INDICATOR = \"number_inidicator\"\n",
    "CURRENCY_INDICATOR = \"currency_inidicator\"\n",
    "CHEMICAL_INDICATOR = \"chemical_inidicator\"\n",
    "\n",
    "\n",
    "BATCH_SIZE = 10000\n",
    "# TRAINING_PREPROCESSED_FILES_PREFIX = \"/mnt/data2/shalaby/training_docs_sample_%s_data_preprocessed-\" % str(SAMPLE_RATIO)\n",
    "# TRAINING_PREPROCESSED_DOCIDS_FILES_PREFIX = \"/mnt/data2/shalaby/training_docs_sample_%s_docids_preprocessed-\" % str(SAMPLE_RATIO)\n",
    "# VALIDATION_PREPROCESSED_FILES_PREFIX = \"/mnt/data2/shalaby/validation_docs_sample_%s_data_preprocessed-\" % str(SAMPLE_RATIO)\n",
    "# VALIDATION_PREPROCESSED_DOCIDS_FILES_PREFIX = \"/mnt/data2/shalaby/validation_docs_sample_%s_docids_preprocessed-\" % str(SAMPLE_RATIO)\n",
    "\n",
    "TRAINING_PREPROCESSED_FILES_PREFIX = \"/big/s/shalaby/preprocessed_data/training_docs_data_preprocessed-\"\n",
    "TRAINING_PREPROCESSED_DOCIDS_FILES_PREFIX = \"/big/s/shalaby/preprocessed_data/training_docs_docids_preprocessed-\"\n",
    "VALIDATION_PREPROCESSED_FILES_PREFIX = \"/big/s/shalaby/preprocessed_data/validation_docs_data_preprocessed-\"\n",
    "VALIDATION_PREPROCESSED_DOCIDS_FILES_PREFIX = \"/big/s/shalaby/preprocessed_data/validation_docs_docids_preprocessed-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_batch(file_prefix, doc_files_prefix, batch_lines, doc_ids, batch_start):\n",
    "    if len(batch_lines):\n",
    "        print \"writing batch %d\" % batch_start\n",
    "        with open(file_prefix + str(batch_start), 'w') as batch_file:\n",
    "            for line in batch_lines:\n",
    "                batch_file.write((u\" \".join(line) + \"\\n\").encode('utf-8'))\n",
    "        pickle.dump(doc_ids, open(doc_files_prefix + str(batch_start), 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 7s, sys: 1min 12s, total: 6min 20s\n",
      "Wall time: 13min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "line_positions = dict()\n",
    "with open(training_file) as f:\n",
    "    \n",
    "    i = 0\n",
    "    line_positions[i] = f.tell()\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        i+=1\n",
    "        if not line.strip(): continue\n",
    "        line_positions[i] = f.tell()\n",
    "        line = f.readline()\n",
    "    del line_positions[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(divmod(len(line_positions), BATCH_SIZE)[0] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "range(150000, (divmod(len(line_positions), BATCH_SIZE)[0] + 1) * BATCH_SIZE, BATCH_SIZE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multithreaded_batch_creation(start_index)\n",
    "\n",
    "batch_index = 0\n",
    "file_prefix = TRAINING_PREPROCESSED_FILES_PREFIX\n",
    "doc_file_prefix = TRAINING_PREPROCESSED_DOCIDS_FILES_PREFIX\n",
    "with open(training_file) as file_obj:\n",
    "    token_lines, doc_ids = [], []\n",
    "    start_time = time.time()\n",
    "    for line in file_obj:\n",
    "        (doc_id, text) = eval(line)\n",
    "        if doc_id in training_docs_list:\n",
    "            token_lines.append(stemtokenizer(text))\n",
    "            doc_ids.append(doc_id)\n",
    "            if len(token_lines) % 100 == 0: info(len(token_lines))\n",
    "            if len(token_lines) % BATCH_SIZE == 0:\n",
    "                duration = time.time() - start_time\n",
    "                info(\"Finished batch of {:d} in {:.0f}m {:.0f}s\".format(BATCH_SIZE, *divmod(duration, 60)))\n",
    "                start_time = time.time()\n",
    "                write_batch(file_prefix, doc_file_prefix, token_lines, doc_ids, batch_index * BATCH_SIZE)\n",
    "                batch_index += 1\n",
    "                token_lines, doc_ids = [], []\n",
    "    duration = time.time() - start_time\n",
    "    info(\"Finished batch of {:d} in {:d}m {:.0f}s\".format(BATCH_SIZE, *divmod(duration, 60)))\n",
    "    write_batch(file_prefix, doc_file_prefix, token_lines, doc_ids, batch_index * BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 01:25:14,018 : INFO : 1000\n",
      "2017-01-02 01:28:01,824 : INFO : 2000\n",
      "2017-01-02 01:31:05,860 : INFO : 3000\n",
      "2017-01-02 01:34:10,195 : INFO : 4000\n",
      "2017-01-02 01:37:09,988 : INFO : 5000\n",
      "2017-01-02 01:40:11,487 : INFO : 6000\n",
      "2017-01-02 01:43:12,095 : INFO : 7000\n",
      "2017-01-02 01:46:12,877 : INFO : 8000\n",
      "2017-01-02 01:49:08,692 : INFO : 9000\n",
      "2017-01-02 01:52:01,441 : INFO : 10000\n",
      "2017-01-02 01:52:01,443 : INFO : Finished batch of 10000 in 30m 3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 130000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 01:55:17,627 : INFO : 1000\n",
      "2017-01-02 01:58:21,699 : INFO : 2000\n",
      "2017-01-02 02:01:28,690 : INFO : 3000\n",
      "2017-01-02 02:04:28,969 : INFO : 4000\n",
      "2017-01-02 02:07:29,446 : INFO : 5000\n",
      "2017-01-02 02:10:30,742 : INFO : 6000\n",
      "2017-01-02 02:13:35,014 : INFO : 7000\n",
      "2017-01-02 02:16:34,744 : INFO : 8000\n",
      "2017-01-02 02:19:35,035 : INFO : 9000\n",
      "2017-01-02 02:22:30,112 : INFO : 10000\n",
      "2017-01-02 02:22:30,113 : INFO : Finished batch of 10000 in 30m 29s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 140000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 02:25:43,022 : INFO : 1000\n",
      "2017-01-02 02:28:37,431 : INFO : 2000\n",
      "2017-01-02 02:31:30,380 : INFO : 3000\n",
      "2017-01-02 02:34:28,167 : INFO : 4000\n",
      "2017-01-02 02:37:32,448 : INFO : 5000\n",
      "2017-01-02 02:40:32,475 : INFO : 6000\n",
      "2017-01-02 02:43:27,390 : INFO : 7000\n",
      "2017-01-02 02:46:22,148 : INFO : 8000\n",
      "2017-01-02 02:49:26,378 : INFO : 9000\n",
      "2017-01-02 02:52:24,805 : INFO : 10000\n",
      "2017-01-02 02:52:24,806 : INFO : Finished batch of 10000 in 29m 55s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 150000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 02:55:37,343 : INFO : 1000\n",
      "2017-01-02 02:58:36,167 : INFO : 2000\n",
      "2017-01-02 03:01:28,300 : INFO : 3000\n",
      "2017-01-02 03:04:26,369 : INFO : 4000\n",
      "2017-01-02 03:07:17,581 : INFO : 5000\n",
      "2017-01-02 03:10:14,510 : INFO : 6000\n",
      "2017-01-02 03:13:15,221 : INFO : 7000\n",
      "2017-01-02 03:16:15,075 : INFO : 8000\n",
      "2017-01-02 03:19:10,778 : INFO : 9000\n",
      "2017-01-02 03:22:11,914 : INFO : 10000\n",
      "2017-01-02 03:22:11,915 : INFO : Finished batch of 10000 in 29m 47s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 160000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 03:25:31,322 : INFO : 1000\n",
      "2017-01-02 03:28:33,966 : INFO : 2000\n",
      "2017-01-02 03:31:50,445 : INFO : 3000\n",
      "2017-01-02 03:34:49,097 : INFO : 4000\n",
      "2017-01-02 03:37:48,962 : INFO : 5000\n",
      "2017-01-02 03:40:51,658 : INFO : 6000\n",
      "2017-01-02 03:43:55,387 : INFO : 7000\n",
      "2017-01-02 03:47:00,618 : INFO : 8000\n",
      "2017-01-02 03:50:04,741 : INFO : 9000\n",
      "2017-01-02 03:53:09,790 : INFO : 10000\n",
      "2017-01-02 03:53:09,791 : INFO : Finished batch of 10000 in 30m 58s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 170000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 03:56:26,362 : INFO : 1000\n",
      "2017-01-02 03:59:29,682 : INFO : 2000\n",
      "2017-01-02 04:02:25,355 : INFO : 3000\n",
      "2017-01-02 04:05:22,368 : INFO : 4000\n",
      "2017-01-02 04:08:28,398 : INFO : 5000\n",
      "2017-01-02 04:11:25,195 : INFO : 6000\n",
      "2017-01-02 04:14:30,232 : INFO : 7000\n",
      "2017-01-02 04:17:33,838 : INFO : 8000\n",
      "2017-01-02 04:20:26,105 : INFO : 9000\n",
      "2017-01-02 04:23:17,971 : INFO : 10000\n",
      "2017-01-02 04:23:17,973 : INFO : Finished batch of 10000 in 30m 8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 180000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 04:26:36,169 : INFO : 1000\n",
      "2017-01-02 04:29:39,520 : INFO : 2000\n",
      "2017-01-02 04:32:33,765 : INFO : 3000\n",
      "2017-01-02 04:35:28,511 : INFO : 4000\n",
      "2017-01-02 04:38:27,592 : INFO : 5000\n",
      "2017-01-02 04:41:19,889 : INFO : 6000\n",
      "2017-01-02 04:44:16,277 : INFO : 7000\n",
      "2017-01-02 04:47:15,891 : INFO : 8000\n",
      "2017-01-02 04:50:08,330 : INFO : 9000\n",
      "2017-01-02 04:53:03,904 : INFO : 10000\n",
      "2017-01-02 04:53:03,906 : INFO : Finished batch of 10000 in 29m 46s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 190000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 04:56:24,185 : INFO : 1000\n",
      "2017-01-02 04:59:29,268 : INFO : 2000\n",
      "2017-01-02 05:02:32,869 : INFO : 3000\n",
      "2017-01-02 05:05:25,856 : INFO : 4000\n",
      "2017-01-02 05:08:28,309 : INFO : 5000\n",
      "2017-01-02 05:11:23,302 : INFO : 6000\n",
      "2017-01-02 05:14:19,956 : INFO : 7000\n",
      "2017-01-02 05:17:22,077 : INFO : 8000\n",
      "2017-01-02 05:20:25,337 : INFO : 9000\n",
      "2017-01-02 05:23:25,288 : INFO : 10000\n",
      "2017-01-02 05:23:25,290 : INFO : Finished batch of 10000 in 30m 21s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 05:26:49,203 : INFO : 1000\n",
      "2017-01-02 05:29:47,564 : INFO : 2000\n",
      "2017-01-02 05:32:46,848 : INFO : 3000\n",
      "2017-01-02 05:35:40,011 : INFO : 4000\n",
      "2017-01-02 05:38:36,640 : INFO : 5000\n",
      "2017-01-02 05:41:28,871 : INFO : 6000\n",
      "2017-01-02 05:44:16,902 : INFO : 7000\n",
      "2017-01-02 05:47:14,450 : INFO : 8000\n",
      "2017-01-02 05:50:18,108 : INFO : 9000\n",
      "2017-01-02 05:53:13,662 : INFO : 10000\n",
      "2017-01-02 05:53:13,663 : INFO : Finished batch of 10000 in 29m 48s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 210000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 05:56:38,148 : INFO : 1000\n",
      "2017-01-02 05:59:41,129 : INFO : 2000\n",
      "2017-01-02 06:02:40,957 : INFO : 3000\n",
      "2017-01-02 06:05:46,645 : INFO : 4000\n",
      "2017-01-02 06:08:45,444 : INFO : 5000\n",
      "2017-01-02 06:11:39,115 : INFO : 6000\n",
      "2017-01-02 06:14:48,232 : INFO : 7000\n",
      "2017-01-02 06:17:49,897 : INFO : 8000\n",
      "2017-01-02 06:20:51,129 : INFO : 9000\n",
      "2017-01-02 06:24:00,753 : INFO : 10000\n",
      "2017-01-02 06:24:00,754 : INFO : Finished batch of 10000 in 30m 47s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 220000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 06:27:19,864 : INFO : 1000\n",
      "2017-01-02 06:30:24,870 : INFO : 2000\n",
      "2017-01-02 06:33:25,480 : INFO : 3000\n",
      "2017-01-02 06:36:25,046 : INFO : 4000\n",
      "2017-01-02 06:39:32,354 : INFO : 5000\n",
      "2017-01-02 06:42:37,695 : INFO : 6000\n",
      "2017-01-02 06:45:33,478 : INFO : 7000\n",
      "2017-01-02 06:48:41,676 : INFO : 8000\n",
      "2017-01-02 06:51:36,503 : INFO : 9000\n",
      "2017-01-02 06:54:38,892 : INFO : 10000\n",
      "2017-01-02 06:54:38,893 : INFO : Finished batch of 10000 in 30m 38s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 230000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 06:57:49,732 : INFO : 1000\n",
      "2017-01-02 07:00:45,448 : INFO : 2000\n",
      "2017-01-02 07:03:49,348 : INFO : 3000\n",
      "2017-01-02 07:06:49,350 : INFO : 4000\n",
      "2017-01-02 07:09:51,254 : INFO : 5000\n",
      "2017-01-02 07:12:48,108 : INFO : 6000\n",
      "2017-01-02 07:15:51,689 : INFO : 7000\n",
      "2017-01-02 07:18:52,357 : INFO : 8000\n",
      "2017-01-02 07:21:46,889 : INFO : 9000\n",
      "2017-01-02 07:24:38,167 : INFO : 10000\n",
      "2017-01-02 07:24:38,168 : INFO : Finished batch of 10000 in 29m 59s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 240000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 07:27:55,958 : INFO : 1000\n",
      "2017-01-02 07:31:02,903 : INFO : 2000\n",
      "2017-01-02 07:34:01,638 : INFO : 3000\n",
      "2017-01-02 07:36:58,784 : INFO : 4000\n",
      "2017-01-02 07:40:01,335 : INFO : 5000\n",
      "2017-01-02 07:42:59,512 : INFO : 6000\n",
      "2017-01-02 07:45:57,886 : INFO : 7000\n",
      "2017-01-02 07:48:55,878 : INFO : 8000\n",
      "2017-01-02 07:51:56,025 : INFO : 9000\n",
      "2017-01-02 07:54:49,585 : INFO : 10000\n",
      "2017-01-02 07:54:49,587 : INFO : Finished batch of 10000 in 30m 11s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 07:57:58,669 : INFO : 1000\n",
      "2017-01-02 08:01:05,605 : INFO : 2000\n",
      "2017-01-02 08:04:06,415 : INFO : 3000\n",
      "2017-01-02 08:07:06,091 : INFO : 4000\n",
      "2017-01-02 08:10:13,395 : INFO : 5000\n",
      "2017-01-02 08:13:19,305 : INFO : 6000\n",
      "2017-01-02 08:16:19,811 : INFO : 7000\n",
      "2017-01-02 08:19:23,798 : INFO : 8000\n",
      "2017-01-02 08:22:25,411 : INFO : 9000\n",
      "2017-01-02 08:25:23,817 : INFO : 10000\n",
      "2017-01-02 08:25:23,819 : INFO : Finished batch of 10000 in 30m 34s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 260000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 08:28:39,651 : INFO : 1000\n",
      "2017-01-02 08:31:32,783 : INFO : 2000\n",
      "2017-01-02 08:34:24,982 : INFO : 3000\n",
      "2017-01-02 08:37:33,356 : INFO : 4000\n",
      "2017-01-02 08:40:40,323 : INFO : 5000\n",
      "2017-01-02 08:43:42,041 : INFO : 6000\n",
      "2017-01-02 08:46:42,804 : INFO : 7000\n",
      "2017-01-02 08:49:53,882 : INFO : 8000\n",
      "2017-01-02 08:52:54,550 : INFO : 9000\n",
      "2017-01-02 08:55:55,857 : INFO : 10000\n",
      "2017-01-02 08:55:55,858 : INFO : Finished batch of 10000 in 30m 32s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 270000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 08:59:16,743 : INFO : 1000\n",
      "2017-01-02 09:02:16,228 : INFO : 2000\n",
      "2017-01-02 09:05:17,590 : INFO : 3000\n",
      "2017-01-02 09:08:19,245 : INFO : 4000\n",
      "2017-01-02 09:11:25,877 : INFO : 5000\n",
      "2017-01-02 09:14:30,740 : INFO : 6000\n",
      "2017-01-02 09:17:32,114 : INFO : 7000\n",
      "2017-01-02 09:20:31,946 : INFO : 8000\n",
      "2017-01-02 09:23:29,049 : INFO : 9000\n",
      "2017-01-02 09:26:36,326 : INFO : 10000\n",
      "2017-01-02 09:26:36,327 : INFO : Finished batch of 10000 in 30m 40s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 280000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 09:29:48,330 : INFO : 1000\n",
      "2017-01-02 09:32:48,074 : INFO : 2000\n",
      "2017-01-02 09:35:50,552 : INFO : 3000\n",
      "2017-01-02 09:38:47,818 : INFO : 4000\n",
      "2017-01-02 09:41:41,060 : INFO : 5000\n",
      "2017-01-02 09:44:44,734 : INFO : 6000\n",
      "2017-01-02 09:47:45,105 : INFO : 7000\n",
      "2017-01-02 09:50:39,349 : INFO : 8000\n",
      "2017-01-02 09:53:38,466 : INFO : 9000\n",
      "2017-01-02 09:56:37,521 : INFO : 10000\n",
      "2017-01-02 09:56:37,522 : INFO : Finished batch of 10000 in 30m 1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 290000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 09:59:50,238 : INFO : 1000\n",
      "2017-01-02 10:02:44,695 : INFO : 2000\n",
      "2017-01-02 10:05:36,034 : INFO : 3000\n",
      "2017-01-02 10:08:43,817 : INFO : 4000\n",
      "2017-01-02 10:11:48,655 : INFO : 5000\n",
      "2017-01-02 10:14:55,650 : INFO : 6000\n",
      "2017-01-02 10:17:53,603 : INFO : 7000\n",
      "2017-01-02 10:21:08,518 : INFO : 8000\n",
      "2017-01-02 10:24:13,838 : INFO : 9000\n",
      "2017-01-02 10:27:08,318 : INFO : 10000\n",
      "2017-01-02 10:27:08,320 : INFO : Finished batch of 10000 in 30m 31s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 10:30:22,623 : INFO : 1000\n",
      "2017-01-02 10:33:29,183 : INFO : 2000\n",
      "2017-01-02 10:36:34,721 : INFO : 3000\n",
      "2017-01-02 10:39:30,210 : INFO : 4000\n",
      "2017-01-02 10:42:32,179 : INFO : 5000\n",
      "2017-01-02 10:45:28,704 : INFO : 6000\n",
      "2017-01-02 10:48:28,010 : INFO : 7000\n",
      "2017-01-02 10:51:27,117 : INFO : 8000\n",
      "2017-01-02 10:54:24,934 : INFO : 9000\n",
      "2017-01-02 10:57:18,872 : INFO : 10000\n",
      "2017-01-02 10:57:18,873 : INFO : Finished batch of 10000 in 30m 11s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 310000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 11:00:43,930 : INFO : 1000\n",
      "2017-01-02 11:03:49,123 : INFO : 2000\n",
      "2017-01-02 11:06:49,762 : INFO : 3000\n",
      "2017-01-02 11:09:49,345 : INFO : 4000\n",
      "2017-01-02 11:12:46,952 : INFO : 5000\n",
      "2017-01-02 11:15:43,578 : INFO : 6000\n",
      "2017-01-02 11:18:40,336 : INFO : 7000\n",
      "2017-01-02 11:21:37,621 : INFO : 8000\n",
      "2017-01-02 11:24:39,533 : INFO : 9000\n",
      "2017-01-02 11:27:47,748 : INFO : 10000\n",
      "2017-01-02 11:27:47,750 : INFO : Finished batch of 10000 in 30m 29s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 320000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 11:31:10,544 : INFO : 1000\n",
      "2017-01-02 11:34:00,545 : INFO : 2000\n",
      "2017-01-02 11:37:02,555 : INFO : 3000\n",
      "2017-01-02 11:40:02,157 : INFO : 4000\n",
      "2017-01-02 11:43:01,159 : INFO : 5000\n",
      "2017-01-02 11:45:53,854 : INFO : 6000\n",
      "2017-01-02 11:48:57,808 : INFO : 7000\n",
      "2017-01-02 11:51:57,851 : INFO : 8000\n",
      "2017-01-02 11:54:54,478 : INFO : 9000\n",
      "2017-01-02 11:57:50,195 : INFO : 10000\n",
      "2017-01-02 11:57:50,197 : INFO : Finished batch of 10000 in 30m 2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 330000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 12:01:02,940 : INFO : 1000\n",
      "2017-01-02 12:04:01,022 : INFO : 2000\n",
      "2017-01-02 12:06:49,027 : INFO : 3000\n",
      "2017-01-02 12:09:41,973 : INFO : 4000\n",
      "2017-01-02 12:12:34,362 : INFO : 5000\n",
      "2017-01-02 12:15:29,602 : INFO : 6000\n",
      "2017-01-02 12:18:19,141 : INFO : 7000\n",
      "2017-01-02 12:21:19,913 : INFO : 8000\n",
      "2017-01-02 12:24:17,318 : INFO : 9000\n",
      "2017-01-02 12:27:22,255 : INFO : 10000\n",
      "2017-01-02 12:27:22,256 : INFO : Finished batch of 10000 in 29m 32s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 340000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 12:30:40,996 : INFO : 1000\n",
      "2017-01-02 12:33:46,605 : INFO : 2000\n",
      "2017-01-02 12:36:42,259 : INFO : 3000\n",
      "2017-01-02 12:39:41,189 : INFO : 4000\n",
      "2017-01-02 12:42:32,643 : INFO : 5000\n",
      "2017-01-02 12:45:42,103 : INFO : 6000\n",
      "2017-01-02 12:48:41,482 : INFO : 7000\n",
      "2017-01-02 12:51:43,678 : INFO : 8000\n",
      "2017-01-02 12:54:38,891 : INFO : 9000\n",
      "2017-01-02 12:57:37,621 : INFO : 10000\n",
      "2017-01-02 12:57:37,622 : INFO : Finished batch of 10000 in 30m 15s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 350000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 13:01:02,370 : INFO : 1000\n",
      "2017-01-02 13:03:49,363 : INFO : 2000\n",
      "2017-01-02 13:06:43,466 : INFO : 3000\n",
      "2017-01-02 13:09:55,502 : INFO : 4000\n",
      "2017-01-02 13:13:06,757 : INFO : 5000\n",
      "2017-01-02 13:16:10,085 : INFO : 6000\n",
      "2017-01-02 13:19:08,811 : INFO : 7000\n",
      "2017-01-02 13:22:08,752 : INFO : 8000\n",
      "2017-01-02 13:25:09,036 : INFO : 9000\n",
      "2017-01-02 13:28:14,897 : INFO : 10000\n",
      "2017-01-02 13:28:14,898 : INFO : Finished batch of 10000 in 30m 37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 360000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 13:31:37,306 : INFO : 1000\n",
      "2017-01-02 13:34:39,102 : INFO : 2000\n",
      "2017-01-02 13:37:42,450 : INFO : 3000\n",
      "2017-01-02 13:40:52,968 : INFO : 4000\n",
      "2017-01-02 13:43:52,018 : INFO : 5000\n",
      "2017-01-02 13:46:54,025 : INFO : 6000\n",
      "2017-01-02 13:49:48,879 : INFO : 7000\n",
      "2017-01-02 13:52:50,589 : INFO : 8000\n",
      "2017-01-02 13:55:50,734 : INFO : 9000\n",
      "2017-01-02 13:58:52,874 : INFO : 10000\n",
      "2017-01-02 13:58:52,875 : INFO : Finished batch of 10000 in 30m 38s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 370000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 14:02:08,655 : INFO : 1000\n",
      "2017-01-02 14:05:02,855 : INFO : 2000\n",
      "2017-01-02 14:08:05,972 : INFO : 3000\n",
      "2017-01-02 14:10:58,669 : INFO : 4000\n",
      "2017-01-02 14:13:54,627 : INFO : 5000\n",
      "2017-01-02 14:16:47,415 : INFO : 6000\n",
      "2017-01-02 14:19:46,534 : INFO : 7000\n",
      "2017-01-02 14:22:51,446 : INFO : 8000\n",
      "2017-01-02 14:25:43,703 : INFO : 9000\n",
      "2017-01-02 14:28:31,684 : INFO : 10000\n",
      "2017-01-02 14:28:31,686 : INFO : Finished batch of 10000 in 29m 39s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 380000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 14:31:49,303 : INFO : 1000\n",
      "2017-01-02 14:34:40,717 : INFO : 2000\n",
      "2017-01-02 14:37:30,982 : INFO : 3000\n",
      "2017-01-02 14:40:29,462 : INFO : 4000\n",
      "2017-01-02 14:43:27,374 : INFO : 5000\n",
      "2017-01-02 14:46:22,456 : INFO : 6000\n",
      "2017-01-02 14:49:30,500 : INFO : 7000\n",
      "2017-01-02 14:52:35,224 : INFO : 8000\n",
      "2017-01-02 14:55:38,553 : INFO : 9000\n",
      "2017-01-02 14:58:36,609 : INFO : 10000\n",
      "2017-01-02 14:58:36,610 : INFO : Finished batch of 10000 in 30m 5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 390000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 15:02:03,746 : INFO : 1000\n",
      "2017-01-02 15:05:06,186 : INFO : 2000\n",
      "2017-01-02 15:08:00,698 : INFO : 3000\n",
      "2017-01-02 15:11:03,282 : INFO : 4000\n",
      "2017-01-02 15:14:10,935 : INFO : 5000\n",
      "2017-01-02 15:17:11,173 : INFO : 6000\n",
      "2017-01-02 15:20:11,225 : INFO : 7000\n",
      "2017-01-02 15:23:15,715 : INFO : 8000\n",
      "2017-01-02 15:26:11,302 : INFO : 9000\n",
      "2017-01-02 15:29:13,546 : INFO : 10000\n",
      "2017-01-02 15:29:13,547 : INFO : Finished batch of 10000 in 30m 37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 15:32:30,602 : INFO : 1000\n",
      "2017-01-02 15:35:35,697 : INFO : 2000\n",
      "2017-01-02 15:38:33,193 : INFO : 3000\n",
      "2017-01-02 15:41:38,107 : INFO : 4000\n",
      "2017-01-02 15:44:33,748 : INFO : 5000\n",
      "2017-01-02 15:47:38,495 : INFO : 6000\n",
      "2017-01-02 15:50:34,719 : INFO : 7000\n",
      "2017-01-02 15:53:33,385 : INFO : 8000\n",
      "2017-01-02 15:56:30,328 : INFO : 9000\n",
      "2017-01-02 15:59:22,759 : INFO : 10000\n",
      "2017-01-02 15:59:22,760 : INFO : Finished batch of 10000 in 30m 9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 410000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 16:02:45,071 : INFO : 1000\n",
      "2017-01-02 16:05:50,348 : INFO : 2000\n",
      "2017-01-02 16:08:50,436 : INFO : 3000\n",
      "2017-01-02 16:11:58,744 : INFO : 4000\n",
      "2017-01-02 16:15:04,110 : INFO : 5000\n",
      "2017-01-02 16:18:05,931 : INFO : 6000\n",
      "2017-01-02 16:21:03,783 : INFO : 7000\n",
      "2017-01-02 16:23:58,676 : INFO : 8000\n",
      "2017-01-02 16:27:08,718 : INFO : 9000\n",
      "2017-01-02 16:30:10,750 : INFO : 10000\n",
      "2017-01-02 16:30:10,751 : INFO : Finished batch of 10000 in 30m 48s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 420000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 16:33:30,195 : INFO : 1000\n",
      "2017-01-02 16:36:31,901 : INFO : 2000\n",
      "2017-01-02 16:39:20,999 : INFO : 3000\n",
      "2017-01-02 16:42:21,074 : INFO : 4000\n",
      "2017-01-02 16:45:16,518 : INFO : 5000\n",
      "2017-01-02 16:48:21,534 : INFO : 6000\n",
      "2017-01-02 16:51:21,858 : INFO : 7000\n",
      "2017-01-02 16:54:24,153 : INFO : 8000\n",
      "2017-01-02 16:57:23,143 : INFO : 9000\n",
      "2017-01-02 17:00:24,497 : INFO : 10000\n",
      "2017-01-02 17:00:24,498 : INFO : Finished batch of 10000 in 30m 14s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 430000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 17:03:54,134 : INFO : 1000\n",
      "2017-01-02 17:06:53,984 : INFO : 2000\n",
      "2017-01-02 17:09:58,839 : INFO : 3000\n",
      "2017-01-02 17:12:58,599 : INFO : 4000\n",
      "2017-01-02 17:16:07,067 : INFO : 5000\n",
      "2017-01-02 17:19:04,985 : INFO : 6000\n",
      "2017-01-02 17:22:04,861 : INFO : 7000\n",
      "2017-01-02 17:25:00,053 : INFO : 8000\n",
      "2017-01-02 17:28:00,722 : INFO : 9000\n",
      "2017-01-02 17:30:56,100 : INFO : 10000\n",
      "2017-01-02 17:30:56,101 : INFO : Finished batch of 10000 in 30m 32s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 440000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 17:34:17,197 : INFO : 1000\n",
      "2017-01-02 17:37:20,950 : INFO : 2000\n",
      "2017-01-02 17:40:21,228 : INFO : 3000\n",
      "2017-01-02 17:43:25,867 : INFO : 4000\n",
      "2017-01-02 17:46:29,445 : INFO : 5000\n",
      "2017-01-02 17:49:33,048 : INFO : 6000\n",
      "2017-01-02 17:52:25,184 : INFO : 7000\n",
      "2017-01-02 17:55:19,296 : INFO : 8000\n",
      "2017-01-02 17:58:16,233 : INFO : 9000\n",
      "2017-01-02 18:01:12,514 : INFO : 10000\n",
      "2017-01-02 18:01:12,516 : INFO : Finished batch of 10000 in 30m 16s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 450000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 18:04:24,221 : INFO : 1000\n",
      "2017-01-02 18:07:17,894 : INFO : 2000\n",
      "2017-01-02 18:10:15,895 : INFO : 3000\n",
      "2017-01-02 18:13:14,083 : INFO : 4000\n",
      "2017-01-02 18:16:17,182 : INFO : 5000\n",
      "2017-01-02 18:19:19,513 : INFO : 6000\n",
      "2017-01-02 18:22:26,432 : INFO : 7000\n",
      "2017-01-02 18:25:25,999 : INFO : 8000\n",
      "2017-01-02 18:28:22,524 : INFO : 9000\n",
      "2017-01-02 18:31:17,119 : INFO : 10000\n",
      "2017-01-02 18:31:17,120 : INFO : Finished batch of 10000 in 30m 5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 460000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 18:34:31,312 : INFO : 1000\n",
      "2017-01-02 18:37:25,298 : INFO : 2000\n",
      "2017-01-02 18:40:24,079 : INFO : 3000\n",
      "2017-01-02 18:43:28,680 : INFO : 4000\n",
      "2017-01-02 18:46:24,295 : INFO : 5000\n",
      "2017-01-02 18:49:25,743 : INFO : 6000\n",
      "2017-01-02 18:52:28,808 : INFO : 7000\n",
      "2017-01-02 18:55:34,370 : INFO : 8000\n",
      "2017-01-02 18:58:45,478 : INFO : 9000\n",
      "2017-01-02 19:01:46,220 : INFO : 10000\n",
      "2017-01-02 19:01:46,221 : INFO : Finished batch of 10000 in 30m 29s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 470000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 19:05:10,734 : INFO : 1000\n",
      "2017-01-02 19:08:11,669 : INFO : 2000\n",
      "2017-01-02 19:11:14,476 : INFO : 3000\n",
      "2017-01-02 19:14:14,907 : INFO : 4000\n",
      "2017-01-02 19:17:21,562 : INFO : 5000\n",
      "2017-01-02 19:20:33,357 : INFO : 6000\n",
      "2017-01-02 19:23:33,099 : INFO : 7000\n",
      "2017-01-02 19:26:32,795 : INFO : 8000\n",
      "2017-01-02 19:29:34,459 : INFO : 9000\n",
      "2017-01-02 19:32:29,922 : INFO : 10000\n",
      "2017-01-02 19:32:29,923 : INFO : Finished batch of 10000 in 30m 44s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 480000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 19:35:49,902 : INFO : 1000\n",
      "2017-01-02 19:38:48,423 : INFO : 2000\n",
      "2017-01-02 19:41:42,998 : INFO : 3000\n",
      "2017-01-02 19:44:42,654 : INFO : 4000\n",
      "2017-01-02 19:47:44,883 : INFO : 5000\n",
      "2017-01-02 19:50:39,620 : INFO : 6000\n",
      "2017-01-02 19:53:34,759 : INFO : 7000\n",
      "2017-01-02 19:56:24,709 : INFO : 8000\n",
      "2017-01-02 19:59:28,761 : INFO : 9000\n",
      "2017-01-02 20:02:25,800 : INFO : 10000\n",
      "2017-01-02 20:02:25,802 : INFO : Finished batch of 10000 in 29m 56s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 490000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 20:05:32,179 : INFO : 1000\n",
      "2017-01-02 20:08:20,960 : INFO : 2000\n",
      "2017-01-02 20:11:11,953 : INFO : 3000\n",
      "2017-01-02 20:13:58,471 : INFO : 4000\n",
      "2017-01-02 20:16:49,967 : INFO : 5000\n",
      "2017-01-02 20:19:56,260 : INFO : 6000\n",
      "2017-01-02 20:22:55,049 : INFO : 7000\n",
      "2017-01-02 20:26:04,441 : INFO : 8000\n",
      "2017-01-02 20:29:22,832 : INFO : 9000\n",
      "2017-01-02 20:32:25,900 : INFO : 10000\n",
      "2017-01-02 20:32:25,902 : INFO : Finished batch of 10000 in 30m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 20:35:43,510 : INFO : 1000\n",
      "2017-01-02 20:38:46,008 : INFO : 2000\n",
      "2017-01-02 20:41:45,464 : INFO : 3000\n",
      "2017-01-02 20:44:40,100 : INFO : 4000\n",
      "2017-01-02 20:47:40,847 : INFO : 5000\n",
      "2017-01-02 20:50:41,939 : INFO : 6000\n",
      "2017-01-02 20:53:41,134 : INFO : 7000\n",
      "2017-01-02 20:56:31,399 : INFO : 8000\n",
      "2017-01-02 20:59:37,434 : INFO : 9000\n",
      "2017-01-02 21:02:40,453 : INFO : 10000\n",
      "2017-01-02 21:02:40,455 : INFO : Finished batch of 10000 in 30m 15s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 510000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 21:05:56,866 : INFO : 1000\n",
      "2017-01-02 21:09:02,269 : INFO : 2000\n",
      "2017-01-02 21:11:57,367 : INFO : 3000\n",
      "2017-01-02 21:14:52,611 : INFO : 4000\n",
      "2017-01-02 21:17:57,187 : INFO : 5000\n",
      "2017-01-02 21:20:44,906 : INFO : 6000\n",
      "2017-01-02 21:23:53,138 : INFO : 7000\n",
      "2017-01-02 21:27:03,577 : INFO : 8000\n",
      "2017-01-02 21:30:04,936 : INFO : 9000\n",
      "2017-01-02 21:33:02,718 : INFO : 10000\n",
      "2017-01-02 21:33:02,719 : INFO : Finished batch of 10000 in 30m 22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 520000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 21:36:18,431 : INFO : 1000\n",
      "2017-01-02 21:39:24,110 : INFO : 2000\n",
      "2017-01-02 21:42:27,417 : INFO : 3000\n",
      "2017-01-02 21:45:29,348 : INFO : 4000\n",
      "2017-01-02 21:48:35,650 : INFO : 5000\n",
      "2017-01-02 21:51:39,622 : INFO : 6000\n",
      "2017-01-02 21:54:46,688 : INFO : 7000\n",
      "2017-01-02 21:57:48,224 : INFO : 8000\n",
      "2017-01-02 22:00:53,865 : INFO : 9000\n",
      "2017-01-02 22:03:48,507 : INFO : 10000\n",
      "2017-01-02 22:03:48,508 : INFO : Finished batch of 10000 in 30m 46s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 530000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 22:07:15,765 : INFO : 1000\n",
      "2017-01-02 22:10:15,989 : INFO : 2000\n",
      "2017-01-02 22:13:19,343 : INFO : 3000\n",
      "2017-01-02 22:16:16,460 : INFO : 4000\n",
      "2017-01-02 22:19:11,965 : INFO : 5000\n",
      "2017-01-02 22:22:03,189 : INFO : 6000\n",
      "2017-01-02 22:24:57,652 : INFO : 7000\n",
      "2017-01-02 22:27:53,169 : INFO : 8000\n",
      "2017-01-02 22:30:50,289 : INFO : 9000\n",
      "2017-01-02 22:33:54,997 : INFO : 10000\n",
      "2017-01-02 22:33:54,999 : INFO : Finished batch of 10000 in 30m 6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 540000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 22:37:11,002 : INFO : 1000\n",
      "2017-01-02 22:40:12,207 : INFO : 2000\n",
      "2017-01-02 22:43:12,397 : INFO : 3000\n",
      "2017-01-02 22:46:11,495 : INFO : 4000\n",
      "2017-01-02 22:49:09,655 : INFO : 5000\n",
      "2017-01-02 22:52:06,449 : INFO : 6000\n",
      "2017-01-02 22:55:10,780 : INFO : 7000\n",
      "2017-01-02 22:58:21,558 : INFO : 8000\n",
      "2017-01-02 23:01:22,135 : INFO : 9000\n",
      "2017-01-02 23:04:11,478 : INFO : 10000\n",
      "2017-01-02 23:04:11,479 : INFO : Finished batch of 10000 in 30m 16s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 550000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 23:07:21,926 : INFO : 1000\n",
      "2017-01-02 23:10:26,296 : INFO : 2000\n",
      "2017-01-02 23:13:31,276 : INFO : 3000\n",
      "2017-01-02 23:16:23,135 : INFO : 4000\n",
      "2017-01-02 23:19:30,892 : INFO : 5000\n",
      "2017-01-02 23:22:27,944 : INFO : 6000\n",
      "2017-01-02 23:25:27,168 : INFO : 7000\n",
      "2017-01-02 23:28:23,595 : INFO : 8000\n",
      "2017-01-02 23:31:27,722 : INFO : 9000\n",
      "2017-01-02 23:34:32,119 : INFO : 10000\n",
      "2017-01-02 23:34:32,121 : INFO : Finished batch of 10000 in 30m 21s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 560000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-02 23:37:53,708 : INFO : 1000\n",
      "2017-01-02 23:40:56,101 : INFO : 2000\n",
      "2017-01-02 23:43:53,676 : INFO : 3000\n",
      "2017-01-02 23:46:47,455 : INFO : 4000\n",
      "2017-01-02 23:49:44,330 : INFO : 5000\n",
      "2017-01-02 23:52:44,877 : INFO : 6000\n",
      "2017-01-02 23:55:52,862 : INFO : 7000\n",
      "2017-01-02 23:59:03,089 : INFO : 8000\n",
      "2017-01-03 00:02:12,314 : INFO : 9000\n",
      "2017-01-03 00:05:08,895 : INFO : 10000\n",
      "2017-01-03 00:05:08,896 : INFO : Finished batch of 10000 in 30m 37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 570000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-03 00:08:24,121 : INFO : 1000\n",
      "2017-01-03 00:11:29,288 : INFO : 2000\n",
      "2017-01-03 00:14:32,468 : INFO : 3000\n",
      "2017-01-03 00:17:33,442 : INFO : 4000\n",
      "2017-01-03 00:20:35,365 : INFO : 5000\n",
      "2017-01-03 00:23:36,387 : INFO : 6000\n",
      "2017-01-03 00:26:35,715 : INFO : 7000\n",
      "2017-01-03 00:29:40,753 : INFO : 8000\n",
      "2017-01-03 00:32:35,848 : INFO : 9000\n",
      "2017-01-03 00:35:31,257 : INFO : 10000\n",
      "2017-01-03 00:35:31,258 : INFO : Finished batch of 10000 in 30m 22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 580000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-03 00:38:56,022 : INFO : 1000\n",
      "2017-01-03 00:41:52,106 : INFO : 2000\n",
      "2017-01-03 00:44:53,635 : INFO : 3000\n",
      "2017-01-03 00:47:54,551 : INFO : 4000\n",
      "2017-01-03 00:50:52,444 : INFO : 5000\n",
      "2017-01-03 00:53:48,344 : INFO : 6000\n",
      "2017-01-03 00:56:37,635 : INFO : 7000\n",
      "2017-01-03 00:59:34,075 : INFO : 8000\n",
      "2017-01-03 01:02:33,885 : INFO : 9000\n",
      "2017-01-03 01:05:25,933 : INFO : 10000\n",
      "2017-01-03 01:05:25,934 : INFO : Finished batch of 10000 in 29m 55s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 590000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-03 01:08:44,106 : INFO : 1000\n",
      "2017-01-03 01:11:52,003 : INFO : 2000\n",
      "2017-01-03 01:14:50,026 : INFO : 3000\n",
      "2017-01-03 01:17:51,948 : INFO : 4000\n",
      "2017-01-03 01:20:57,639 : INFO : 5000\n",
      "2017-01-03 01:23:50,747 : INFO : 6000\n",
      "2017-01-03 01:27:03,418 : INFO : 7000\n",
      "2017-01-03 01:30:12,214 : INFO : 8000\n",
      "2017-01-03 01:33:24,545 : INFO : 9000\n",
      "2017-01-03 01:36:28,362 : INFO : 10000\n",
      "2017-01-03 01:36:28,363 : INFO : Finished batch of 10000 in 31m 2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 600000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-90d2346939b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'\\nbatch_index = 13\\nfile_prefix = TRAINING_PREPROCESSED_FILES_PREFIX\\ndoc_file_prefix = TRAINING_PREPROCESSED_DOCIDS_FILES_PREFIX\\nwith open(training_file) as file_obj:\\n    token_lines, doc_ids = [], []\\n    start_time = time.time()\\n    for index,line in enumerate(file_obj):\\n        if index < 130000: continue\\n        (doc_id, text) = eval(line)\\n        if doc_id in training_docs_list:\\n            token_lines.append(stemtokenizer(text))\\n            doc_ids.append(doc_id)\\n            if len(token_lines) % 1000 == 0: info(len(token_lines))\\n            if len(token_lines) % BATCH_SIZE == 0:\\n                duration = time.time() - start_time\\n                info(\"Finished batch of {:d} in {:.0f}m {:.0f}s\".format(BATCH_SIZE, *divmod(duration, 60)))\\n                start_time = time.time()\\n                write_batch(file_prefix, doc_file_prefix, token_lines, doc_ids, batch_index * BATCH_SIZE)\\n                batch_index += 1\\n                token_lines, doc_ids = [], []\\n    duration = time.time() - start_time\\n    info(\"Finished batch of {:d} in {:d}m {:.0f}s\".format(BATCH_SIZE, *divmod(duration, 60)))\\n    write_batch(file_prefix, doc_file_prefix, token_lines, doc_ids, batch_index * BATCH_SIZE)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/s/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/s/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/s/shalaby/.virtualenv/thesis-env/local/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-12b77c00c630>\u001b[0m in \u001b[0;36mstemtokenizer\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m                 \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNUMBER_INDICATOR\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mis_currency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-12b77c00c630>\u001b[0m in \u001b[0;36mis_number\u001b[1;34m(str)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;34m\"\"\" Returns true if given string is a number (float or int)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_index = 13\n",
    "file_prefix = TRAINING_PREPROCESSED_FILES_PREFIX\n",
    "doc_file_prefix = TRAINING_PREPROCESSED_DOCIDS_FILES_PREFIX\n",
    "with open(training_file) as file_obj:\n",
    "    token_lines, doc_ids = [], []\n",
    "    start_time = time.time()\n",
    "    for index,line in enumerate(file_obj):\n",
    "        if index < 130000: continue\n",
    "        (doc_id, text) = eval(line)\n",
    "        if doc_id in training_docs_list:\n",
    "            token_lines.append(stemtokenizer(text))\n",
    "            doc_ids.append(doc_id)\n",
    "            if len(token_lines) % 1000 == 0: info(len(token_lines))\n",
    "            if len(token_lines) % BATCH_SIZE == 0:\n",
    "                duration = time.time() - start_time\n",
    "                info(\"Finished batch of {:d} in {:.0f}m {:.0f}s\".format(BATCH_SIZE, *divmod(duration, 60)))\n",
    "                start_time = time.time()\n",
    "                write_batch(file_prefix, doc_file_prefix, token_lines, doc_ids, batch_index * BATCH_SIZE)\n",
    "                batch_index += 1\n",
    "                token_lines, doc_ids = [], []\n",
    "    duration = time.time() - start_time\n",
    "    info(\"Finished batch of {:d} in {:d}m {:.0f}s\".format(BATCH_SIZE, *divmod(duration, 60)))\n",
    "    write_batch(file_prefix, doc_file_prefix, token_lines, doc_ids, batch_index * BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 0\n",
      "CPU times: user 42.5 s, sys: 832 ms, total: 43.4 s\n",
      "Wall time: 43.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_index = 0\n",
    "file_prefix = VALIDATION_PREPROCESSED_FILES_PREFIX\n",
    "doc_file_prefix = VALIDATION_PREPROCESSED_DOCIDS_FILES_PREFIX\n",
    "with open(training_file) as file_obj:\n",
    "    token_lines, doc_ids = [], []\n",
    "    for line in file_obj:\n",
    "        (doc_id, text) = eval(line)\n",
    "        if doc_id in validation_docs_list:\n",
    "            token_lines.append(stemtokenizer(text))\n",
    "            doc_ids.append(doc_id)\n",
    "            if len(token_lines) % BATCH_SIZE == 0:\n",
    "                %time write_batch(file_prefix, doc_file_prefix, token_lines, doc_ids, batch_index * BATCH_SIZE)\n",
    "                batch_index += 1\n",
    "                token_lines, doc_ids = [], []\n",
    "    write_batch(file_prefix, doc_file_prefix, token_lines, doc_ids, batch_index * BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.53 s, sys: 856 ms, total: 2.38 s\n",
      "Wall time: 2.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "line_tokens = []\n",
    "with open(TRAINING_PREPROCESSED_FILES_PREFIX + str(0)) as preproc_file:\n",
    "    line_lengths = []\n",
    "    for line in preproc_file:\n",
    "        line_lengths.append(len(line))\n",
    "        line_tokens.append(line.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we used to write the doc id and the tokens as tuples, then do an eval on them in reading time, but this turned out to be very slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def write_batch(file_prefix, batch_lines, batch_start):\n",
    "    if len(batch_lines):\n",
    "        print \"writing batch %d\" % batch_start\n",
    "        %time pickle.dump(batch_lines, open(file_prefix + str(batch_start), 'w'))\n",
    "#         with open(file_prefix + str(batch_start), 'w') as batch_file:\n",
    "#             for line in batch_lines:\n",
    "#                 batch_file.write(str(line) + \"\\n\")\n",
    "\n",
    "batch_index = 0\n",
    "file_prefix = TRAINING_PREPROCESSED_FILES_PREFIX\n",
    "with open(training_file) as file_obj:\n",
    "    token_lines = []\n",
    "    for line in file_obj:\n",
    "        (doc_id, text) = eval(line)\n",
    "        if doc_id in training_docs_list:\n",
    "            token_lines.append((doc_id, stemtokenizer(text)))\n",
    "            if len(token_lines) % BATCH_SIZE == 0:\n",
    "                write_batch(file_prefix, token_lines, batch_index * BATCH_SIZE)\n",
    "                batch_index += 1\n",
    "                token_lines = []\n",
    "    write_batch(file_prefix, token_lines, batch_index * BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing batch 0\n",
      "writing batch 10000\n"
     ]
    }
   ],
   "source": [
    "batch_index = 0\n",
    "file_prefix = VALIDATION_PREPROCESSED_FILES_PREFIX\n",
    "with open(training_file) as file_obj:\n",
    "    token_lines = []\n",
    "    for line in file_obj:\n",
    "        (doc_id, text) = eval(line)\n",
    "        if doc_id in validation_docs_list:\n",
    "            token_lines.append((doc_id, stemtokenizer(text)))\n",
    "            if len(token_lines) % BATCH_SIZE == 0:\n",
    "                write_batch(file_prefix, token_lines, batch_index * BATCH_SIZE)\n",
    "                batch_index += 1\n",
    "                token_lines = []\n",
    "    write_batch(file_prefix, token_lines, batch_index * BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 s, sys: 1.2 s, total: 6.19 s\n",
      "Wall time: 6.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "line_tokens = []\n",
    "with open(TRAINING_PREPROCESSED_FILES_PREFIX + str(0)) as preproc_file:\n",
    "    for line in preproc_file:\n",
    "        line_tokens.append(line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['technical',\n",
       " 'field',\n",
       " 'the',\n",
       " 'present',\n",
       " 'invention',\n",
       " 'generally',\n",
       " 'relates',\n",
       " 'to',\n",
       " 'wireless',\n",
       " 'communications']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_tokens[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
