{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import coverage_error\n",
    "import sklearn.metrics\n",
    "import cPickle as pickle\n",
    "\n",
    "from thesis.utils.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IS_SAMPLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "STOP_WORDS = nltk.corpus.stopwords.words('english')\n",
    "NUMBER_INDICATOR = \"number_inidicator\"\n",
    "CURRENCY_INDICATOR = \"currency_inidicator\"\n",
    "CHEMICAL_INDICATOR = \"chemical_inidicator\"\n",
    "MIN_SIZE = 3\n",
    "MIN_DOCUMENTS = 5\n",
    "TOP_N_FEATURES = 10000\n",
    "\n",
    "SVM_ITERATIONS = 100\n",
    "SVM_CONVERGENCE = 0.001\n",
    "SVM_REG = 0.001\n",
    "\n",
    "BM25_K = 1.5  # controls power of tf component\n",
    "BM25_b = 0.75  # controls the BM25 length normalization\n",
    "\n",
    "RANDOM_SEED = 10000\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Manipulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_tokenizer(text, doc_id):\n",
    "    \"\"\" \n",
    "    Get clean stems out of a text where number, chemical, currency indicators have already been identified.\n",
    "    A list of clean stems are returned \n",
    "    \"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\s+', gaps=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stems = []  # result\n",
    "    previous_unigram = None\n",
    "    for token in tokens:\n",
    "        stem = token.lower()\n",
    "        stem = stem.strip(string.punctuation)\n",
    "        if stem:\n",
    "            if is_stopword(stem):\n",
    "                stem = None\n",
    "            if stem and len(stem) >= MIN_SIZE:\n",
    "                # extract uni-grams\n",
    "                stems.append((stem,{doc_id: 1}))\n",
    "                # extract bi-grams\n",
    "                if previous_unigram: stems.append((previous_unigram + \" \" + stem,{doc_id: 1}))\n",
    "                previous_unigram = stem\n",
    "    del tokens\n",
    "    return stems\n",
    "\n",
    "def is_stopword(word):\n",
    "    return word in STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_postings(postings_list1, postings_list2):\n",
    "    # key could be either a doc id or a term\n",
    "    for key in postings_list2:\n",
    "        if postings_list1.get(key):\n",
    "            postings_list1[key] += postings_list2[key]\n",
    "        else:\n",
    "            postings_list1[key] = postings_list2[key]\n",
    "    return postings_list1\n",
    "\n",
    "def get_term_dictionary(terms):\n",
    "    \"\"\"\n",
    "    Maps string terms to indexes in an array\n",
    "    \"\"\"\n",
    "    term_dictionary = {}\n",
    "    term_array = [None] * len(terms)\n",
    "    def put(key):\n",
    "        hashvalue = hashfunction(key, len(term_array))\n",
    "        if term_array[hashvalue] == None:\n",
    "            term_array[hashvalue] = key\n",
    "            return hashvalue\n",
    "        else:\n",
    "            nextslot = rehash(hashvalue, len(term_array))\n",
    "            while term_array[nextslot] != None:\n",
    "                nextslot = rehash(nextslot, len(term_array))\n",
    "            if term_array[nextslot] == None:\n",
    "                term_array[nextslot] = key\n",
    "                return nextslot\n",
    "    def hashfunction(key, size):\n",
    "        return hash(key) % size\n",
    "    def rehash(oldhash, size):\n",
    "        return (oldhash + 1) % size\n",
    "    i = 0\n",
    "    for term in terms:\n",
    "        corresponding_index = put(term)\n",
    "        term_dictionary[term] = corresponding_index\n",
    "        i+=1\n",
    "        if i%10000 == 0: print \"finished \" + str(i)\n",
    "    return term_dictionary\n",
    "\n",
    "def jsonKV2str(x):\n",
    "    \"\"\"\n",
    "    Change string keys to int\n",
    "    \"\"\"\n",
    "    if isinstance(x, dict):\n",
    "            #return {doc_id:{int(term_id):x[doc_id][term_id] for term_id in x[doc_id]} for doc_id in x }\n",
    "        \n",
    "            return {int(k):(int(v) if isinstance(v, unicode) else v) for k,v in x.items()}\n",
    "    return x\n",
    "\n",
    "def get_json(json_postings):\n",
    "    return json.loads(json_postings)\n",
    "\n",
    "def get_json_convert_num(json_postings):\n",
    "    return json.loads(json_postings, object_hook=jsonKV2str)\n",
    "\n",
    "def get_doc_index(term, postings_list, term_dictionary):\n",
    "    #return [(doc_id, {term: postings_list[doc_id]}) for doc_id in postings_list]\n",
    "    return [(doc_id, {term_dictionary[term]: postings_list[doc_id]}) for doc_id in postings_list]\n",
    "\n",
    "\n",
    "def calculate_sublinear_tf(tf):\n",
    "    # laplace smoothing with +1 in case of term with no documents (useful during testing)\n",
    "    return math.log10(1 + tf)\n",
    "\n",
    "\n",
    "def calculate_tf_idf(tf, df, N):\n",
    "    # laplace smoothing with +1 in case of term with no documents (useful during testing)\n",
    "    return tf * math.log10((N+1) / (df + 1))\n",
    "\n",
    "\n",
    "def calculate_sublinear_tf_idf(tf, df, N):\n",
    "    # laplace smoothing with +1 in case of term with no documents (useful during testing)\n",
    "    return calculate_sublinear_tf(tf) * math.log10((N+1) / (df + 1))\n",
    "\n",
    "\n",
    "def calculate_bm25(tf, df, N, d_len, d_avg):\n",
    "    idf = max(0, math.log10((N-df + 0.5)/(df+0.5))) # in rare cases where the df is over 50% of N, this could become -ve, so we guard against that\n",
    "    tf_comp = float(((BM25_K + 1) * tf)) / ( BM25_K * ((1-BM25_b) + BM25_b*(float(d_len)/d_avg)) + tf)\n",
    "    return tf_comp * idf\n",
    "\n",
    "\n",
    "def calculate_rf(df_relevant, df_non_relevant):\n",
    "    return math.log( (2 + (float(df_relevant)/max(1, df_non_relevant))), 2)\n",
    "\n",
    "\n",
    "def calculate_tf_rf(tf, df_relevant, df_non_relevant):\n",
    "    return tf * calculate_rf(df_relevant, df_non_relevant)\n",
    "\n",
    "\n",
    "def compare_classifications(x,y):\n",
    "    len_comp = cmp(len(x), len(y))\n",
    "    if len_comp == 0:\n",
    "        return cmp(x,y)\n",
    "    return len_comp\n",
    "\n",
    "\n",
    "def create_doc_index(term_index, term_dictionary):\n",
    "    return term_index \\\n",
    "        .flatMap(lambda (term, postings_list): get_doc_index(term, postings_list, term_dictionary)) \\\n",
    "        .reduceByKey(lambda x, y: merge_postings(x, y))\n",
    "\n",
    "\n",
    "def get_rf_stats(postings, classification):\n",
    "    a_plus_c = set(postings.keys())\n",
    "    a_plus_b = set(classifications_index[classification])\n",
    "    # first intersection is to get (a), second difference is to get (c) (checkout tf-rf paper for reference)\n",
    "    a = a_plus_c.intersection(a_plus_b)\n",
    "    c = a_plus_c.difference(a_plus_b)\n",
    "    size_a = len(a)\n",
    "    size_c = len(c)\n",
    "    return size_a, size_c\n",
    "\n",
    "\n",
    "def get_rf_postings(classification):\n",
    "    def get_rf_postings_internal(postings):\n",
    "        size_a, size_c = get_rf_stats(postings, classification)\n",
    "        return {docId: calculate_rf(size_a, size_c)\n",
    "                for docId, tf in postings.items()}\n",
    "    return get_rf_postings_internal\n",
    "\n",
    "\n",
    "def get_tf_rf_postings(classification):\n",
    "    def get_tf_rf_postings_internal(postings):\n",
    "        size_a, size_c = get_rf_stats(postings, classification)\n",
    "        return {docId: calculate_tf_rf(tf, size_a, size_c)\n",
    "                for docId, tf in postings.items()}\n",
    "    return get_tf_rf_postings_internal\n",
    "\n",
    "\n",
    "get_binary = lambda x: 1 if x > 0 else 0\n",
    "get_binary = np.vectorize(get_binary)\n",
    "\n",
    "def get_row_top_N(y_score_row, y_true_row):\n",
    "    desc_score_indices = np.argsort(y_score_row)[::-1]\n",
    "    # print y_score_row\n",
    "    # print y_true_row\n",
    "    true_indices = np.where(y_true_row ==1)[0]\n",
    "    # print desc_score_indices\n",
    "    found = 0\n",
    "    top_N = 0\n",
    "    for i, score in enumerate(desc_score_indices):\n",
    "        if score in true_indices:\n",
    "            found += 1\n",
    "            if found == len(true_indices):\n",
    "                top_N = i + 1\n",
    "    # print top_N\n",
    "    return top_N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input/Output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATIO = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sc = SparkContext(\"\", \"Generate Inverted Index Job\")\n",
    "es_server = \"deka.cip.ifi.lmu.de\"\n",
    "es_port = \"9200\"\n",
    "\n",
    "original_parent_save_location = \"hdfs://deka.cip.ifi.lmu.de/extended_pv/\"\n",
    "save_parent_location = original_parent_save_location\n",
    "\n",
    "root_location = \"/big/s/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "\n",
    "docs_output = save_parent_location + \"docs_output\"\n",
    "postings_list_output = save_parent_location + \"postings_list_full.json\"\n",
    "\n",
    "accepted_terms_list_output = original_parent_save_location + \"accepted_terms_list_{}.pkl\"\n",
    "accepted_terms_with_scores_list_output = original_parent_save_location + \"accepted_terms_with_scores_list_{}.pkl\"\n",
    "postings_list_chi_selected_output = original_parent_save_location + \"postings_list_{}.json\"\n",
    "term_df_map_output = original_parent_save_location + \"term_df_map_output_{}.json\"\n",
    "doc_index_chi_selected_output = original_parent_save_location + \"doc_index_for_postings_{}.json\"\n",
    "term_dictionary_output = original_parent_save_location + \"term_dictionary_{}.pkl\"\n",
    "\n",
    "\n",
    "postings_list_training_chi_selected_output = save_parent_location + \"training_postings_list_{}.json\"\n",
    "postings_list_validation_chi_selected_output = save_parent_location + \"validation_postings_list_{}.json\"\n",
    "postings_list_test_chi_selected_output = save_parent_location + \"test_postings_list_{}.json\"\n",
    "\n",
    "\n",
    "# data location\n",
    "preprocessed_docs_location = root_location + \"preprocessed_data/\" + \"extended_pv_docs_only_for_spark\"\n",
    "\n",
    "\n",
    "# Classification objects, unrelated to sample size\n",
    "classifications_index_output = exports_location + \"extended_pv_classifications_index.pkl\"\n",
    "doc_classifications_map_file = exports_location + \"extended_pv_doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "valid_classes_file = exports_location + \"valid_classes.pkl\"\n",
    "valid_subclasses_file = exports_location + \"valid_subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "doc_lengths_map_output = exports_location + \"extended_pv_doc_lengths_map.pkl\"\n",
    "training_docs_list_file = exports_location + \"extended_pv_training_docs_list_\" + str(SAMPLE_RATIO) + \".pkl\"\n",
    "validation_docs_list_file = exports_location + \"extended_pv_validation_docs_list_\" + str(SAMPLE_RATIO) + \".pkl\"\n",
    "test_docs_list_file = exports_location + \"extended_pv_test_docs_list_\" + str(SAMPLE_RATIO) + \".pkl\"\n",
    "\n",
    "\n",
    "training_predictions_sections_output = save_parent_location + \"training_predictions_sections_list.pkl\"\n",
    "training_labels_sections_list_output = save_parent_location + \"training_labels_sections_list.pkl\"\n",
    "valdiation_predictions_sections_output = save_parent_location + \"validation_predictions_sections_list.pkl\"\n",
    "validation_labels_sections_list_output = save_parent_location + \"validation_labels_sections_list.pkl\"\n",
    "\n",
    "\n",
    "test_postings_list_output = save_parent_location + \"test_postings_list_50000.json\"\n",
    "training_errors_output = save_parent_location + \"training_errors.json\"\n",
    "model_output = save_parent_location + \"models/\" + \"iter_\" + str(SVM_ITERATIONS) + \"_reg_\" + str(SVM_REG) + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data_output_name(method, no_of_features=TOP_N_FEATURES, data_type=\"training\"):\n",
    "    return save_parent_location + \"models/\" + data_type + \"_data/\" + method  + \"_data.json\"\n",
    "def get_save_location(location, sample=False):\n",
    "    if sample:\n",
    "        return location.replace(save_parent_location, sample_save_parent_location)\n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.75 s, sys: 348 ms, total: 8.1 s\n",
      "Wall time: 8.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "valid_classes = pickle.load(open(valid_classes_file))\n",
    "valid_subclasses = pickle.load(open(valid_subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))\n",
    "classifications_index = pickle.load(open(classifications_index_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395509\n"
     ]
    }
   ],
   "source": [
    "extended_pv_docs = set(training_docs_list) | set(validation_docs_list) | set(test_docs_list)\n",
    "doc_count = len(extended_pv_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accelerates the chi squared calculation a lot\n",
    "classifications_index_set = {k:set(docs) for k,docs in classifications_index.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395509"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'G-20-B', [u'07433566', u'07896523', u'07218441', u'07116477', u'06985663'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications_index.items()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'07390846', [u'C', u'C-08', u'C-08-K', u'C-08-L'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_classification_map.items()[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'A', u'B', u'C', u'D', u'E', u'F', u'G', u'H']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Once only) Create the doc classification and classifications index for extended pv docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extended_pv_doc_classification_map = {}\n",
    "for doc_id in extended_pv_docs:\n",
    "    extended_pv_doc_classification_map[doc_id] = doc_classification_map[doc_id]\n",
    "pickle.dump(extended_pv_doc_classification_map, open(exports_location + \"extended_pv_doc_classification_map.pkl\", \"w\"))\n",
    "\n",
    "extended_pv_classifications_index = {}\n",
    "for classf in classifications_index:\n",
    "    classf_docs_set = set(classifications_index[classf])\n",
    "    valid_extended_pv_docs = classf_docs_set & extended_pv_docs\n",
    "    extended_pv_classifications_index[classf] = list(valid_extended_pv_docs)\n",
    "pickle.dump(extended_pv_classifications_index, open(exports_location + \"extended_pv_classifications_index.pkl\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading document texts from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_text_objs = sc.textFile(preprocessed_docs_location).map(lambda line: line.split(\" \", 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "### Create Postings List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 880 ms, sys: 236 ms, total: 1.12 s\n",
      "Wall time: 2h 15min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create Postings List\n",
    "postings_lists = doc_text_objs.flatMap(lambda (doc_id, doc): simple_tokenizer(doc, doc_id)).reduceByKey(lambda x,y: merge_postings(x,y))\n",
    "# the second condition is made specifically for num_indic, as it usually is in all docs and that messes up chi\n",
    "min_doc_postings_lists = postings_lists.filter(lambda (x,y): len(y) > MIN_DOCUMENTS and len(y) < doc_count)\n",
    "#number_of_terms = min_doc_postings_lists.count()\n",
    "\n",
    "# min_doc_postings_lists.map(lambda (term, postings_list): \",\".join([term, json.dumps(postings_list)])).repartition(1).saveAsTextFile(postings_list_output)\n",
    "min_doc_postings_lists.map(lambda postings: json.dumps(postings)).saveAsTextFile(postings_list_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Document Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_lengths_dict = doc_text_objs.map(lambda (doc_id, document_text): (doc_id, len(document_text))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_doc_length = sum(doc_lengths_dict.values())/len(doc_lengths_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'08369259', 85861)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lengths_dict.items()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46477"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_doc_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Document Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(doc_lengths_dict, open(doc_lengths_map_output, \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Postings List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save Postings List\n",
    "# min_doc_postings_lists.map(lambda (term, postings_list): \",\".join([term, json.dumps(postings_list)])).repartition(1).saveAsTextFile(postings_list_output)\n",
    "min_doc_postings_lists.map(lambda postings: json.dumps(postings)).saveAsTextFile(postings_list_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Postings List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Postings Lists\n",
    "min_doc_postings_lists = sc.textFile(postings_list_output).map(lambda json_postings: json.loads(json_postings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_chi_index(term_index, classifications_index_set, subclasses, number_of_docs):\n",
    "    return term_index.map(lambda (term, postings_list): (term, calculate_chi_squared(postings_list.keys(), classifications_index_set, subclasses, number_of_docs)))\n",
    "\n",
    "def calculate_chi_squared(document_list, classifications_index_set, subclasses, number_of_docs):\n",
    "    \"\"\"\n",
    "    Chi squared is the ratio of the difference between actual frequency and expected frequency of a term relative to the expected frequency\n",
    "    summed up across all classes and whether the term appears or not\n",
    "    Here we calculate the average chi squared score which is one of two options in multi-label classification (the other being max)\n",
    "    \"\"\"\n",
    "    chi_score = 0\n",
    "    N = len(document_list)\n",
    "    doc_set = set(document_list)\n",
    "    Nt1 = N # actual collection frequency of having the word\n",
    "    Nt0 = number_of_docs - N # actual collection frequency of not having the word\n",
    "    Pt1 = float(N)/ number_of_docs # probability of the term happening\n",
    "    Pt0 = float(number_of_docs - N)/ number_of_docs # probablility of the term not happening\n",
    "    #print \"Docs Stats: Term present in %d (%.7f), Not Present in %d (%.7f) \" % (Nt1, Pt1, Nt0, Pt0)\n",
    "    for subclass in subclasses:\n",
    "        # this condition is only required for when using a sample because some subclasses may not have docs\n",
    "        if len(classifications_index_set[subclass]) > 0:\n",
    "            Pc1 = float(len(classifications_index_set[subclass]))/ number_of_docs # probability of the class happening\n",
    "            Pc0 = 1 - Pc1\n",
    "            Pt1c1 = float(len(doc_set & classifications_index_set[subclass])) / number_of_docs\n",
    "            Pt1c0 = Pt1 - Pt1c1\n",
    "            Pt0c1 = Pc1 - Pt1c1\n",
    "            Pt0c0 = 1 - Pt1c0 - Pt0c1 - Pt1c1\n",
    "\n",
    "            cat_chi_score = (number_of_docs * math.pow(Pt1c1 * Pt0c0 - Pt1c0 * Pt0c1, 2))/(Pt1 * Pt0 * Pc1 * Pc0)\n",
    "            # calculate average chi score\n",
    "            chi_score += Pc1 * cat_chi_score\n",
    "            #print \"subclass %s: %.7f, %.7f, %.7f, %.7f, %.7f, %.7f\" % (subclass, Pc1, Pt1c1, Pt1c0, Pt0c1, Pt0c0, chi_score)\n",
    "    return chi_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for subclass in subclasses:\n",
    "    if len(classifications_index_set[subclass]) == 0:\n",
    "        print subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16714599"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_doc_postings_lists.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "postings_length_dict = min_doc_postings_lists.map(lambda (term, postings_list): (term, len(postings_list))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395509"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(postings_length_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'disease inclusion'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postings_length_dict.keys()[395509]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postings_length_dict['disease inclusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'num_indic', 395509)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(postings_length_dict.items(), key= lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_postings_items = sorted(postings_length_dict.items(), key= lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_postings_items[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# min_doc_postings_lists = sc.parallelize(min_doc_postings_lists.take(10000))\n",
    "\n",
    "# term_accepted_chi_list_with_scores = get_chi_index(min_doc_postings_lists, classifications_index, subclasses, doc_count).takeOrdered(TOP_N_FEATURES, lambda (term,score): -score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order by Chi Squared and get Top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 10000\n",
      "CPU times: user 45.7 s, sys: 4.08 s, total: 49.7 s\n",
      "Wall time: 2h 56min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "term_accepted_chi_list_with_scores = get_chi_index(min_doc_postings_lists, classifications_index_set, subclasses, doc_count).takeOrdered(TOP_N_FEATURES, lambda (term,score): -score)\n",
    "term_accepted_chi_list = map(lambda (x,y): x, term_accepted_chi_list_with_scores)\n",
    "# gets a bit slower at the end but finishes eventually \n",
    "term_dictionary = get_term_dictionary(term_accepted_chi_list)\n",
    "min_doc_postings_lists = min_doc_postings_lists.filter(lambda (term, postings): term in term_accepted_chi_list).cache()\n",
    "number_of_terms = min_doc_postings_lists.count()\n",
    "term_df_map = min_doc_postings_lists.map(lambda (term, postings): (term, len(postings))).collectAsMap()\n",
    "\n",
    "# Save Postings List and the supporting objects\n",
    "# min_doc_postings_lists.map(lambda (term, postings_list): \",\".join([term, json.dumps(postings_list)])).repartition(1).saveAsTextFile(postings_list_output)\n",
    "min_doc_postings_lists.map(lambda postings: json.dumps(postings)).saveAsTextFile(postings_list_chi_selected_output.format(str(TOP_N_FEATURES)))\n",
    "sc.parallelize(term_dictionary.items()).saveAsPickleFile(term_dictionary_output.format(str(TOP_N_FEATURES)))\n",
    "sc.parallelize(term_accepted_chi_list).saveAsPickleFile(accepted_terms_list_output.format(str(TOP_N_FEATURES)))\n",
    "sc.parallelize(term_accepted_chi_list_with_scores).saveAsPickleFile(accepted_terms_with_scores_list_output.format(str(TOP_N_FEATURES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#min_doc_postings_lists.map(lambda postings: json.dumps(postings)).repartition(100).saveAsTextFile(postings_list_chi_selected_output.format(str(TOP_N_FEATURES)))\n",
    "#term_df_map = min_doc_postings_lists.map(lambda (term, postings): (term, len(postings))).collectAsMap()\n",
    "sc.parallelize(term_dictionary.items()).repartition(1).saveAsPickleFile(term_dictionary_output.format(str(TOP_N_FEATURES)))\n",
    "sc.parallelize(term_df_map.items()).saveAsPickleFile(term_df_map_output.format(str(TOP_N_FEATURES)))\n",
    "sc.parallelize(term_accepted_chi_list).repartition(1).saveAsPickleFile(accepted_terms_list_output.format(str(TOP_N_FEATURES)))\n",
    "sc.parallelize(term_accepted_chi_list_with_scores).repartition(1).saveAsPickleFile(accepted_terms_with_scores_list_output.format(str(TOP_N_FEATURES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'pharmaceutically', 15411.494644150624),\n",
       " (u'pharmaceutically acceptable', 15121.398337111134),\n",
       " (u'pharmaceutical', 13925.085275880954),\n",
       " (u'server', 13792.377376291814),\n",
       " (u'administered', 13558.053009137388),\n",
       " (u'network', 13110.796900314415),\n",
       " (u'protein', 12995.0009762267),\n",
       " (u'pharmaceutical composition', 12956.506131944068),\n",
       " (u'administering', 12904.26814543066),\n",
       " (u'oral', 12873.917238804137)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_accepted_chi_list_with_scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.parallelize(term_accepted_chi_list_with_scores).saveAsPickleFile(accepted_terms_with_scores_list_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'pharmaceutically',\n",
       " u'pharmaceutically acceptable',\n",
       " u'pharmaceutical',\n",
       " u'server',\n",
       " u'administered',\n",
       " u'network',\n",
       " u'protein',\n",
       " u'pharmaceutical composition',\n",
       " u'administering',\n",
       " u'oral',\n",
       " u'assay',\n",
       " u'therapeutic',\n",
       " u'effective amount',\n",
       " u'purified',\n",
       " u'computer',\n",
       " u'vitro',\n",
       " u'pharmaceutical compositions',\n",
       " u'amino',\n",
       " u'disease',\n",
       " u'executed',\n",
       " u'vivo',\n",
       " u'dosage',\n",
       " u'memory',\n",
       " u'acids',\n",
       " u'internet',\n",
       " u'incubated',\n",
       " u'hardware',\n",
       " u'acid',\n",
       " u'administration',\n",
       " u'request',\n",
       " u'proteins',\n",
       " u'diseases',\n",
       " u'gene',\n",
       " u'culture',\n",
       " u'therapeutically',\n",
       " u'processor',\n",
       " u'parenteral',\n",
       " u'serum',\n",
       " u'program',\n",
       " u'excipients',\n",
       " u'capsules',\n",
       " u'software',\n",
       " u'sodium',\n",
       " u'acceptable carrier',\n",
       " u'recombinant',\n",
       " u'phosphate',\n",
       " u'intravenous',\n",
       " u'enzyme',\n",
       " u'drug',\n",
       " u'amino acid',\n",
       " u'dose',\n",
       " u'chromatography',\n",
       " u'dna',\n",
       " u'peptide',\n",
       " u'37\\xb0',\n",
       " u'acceptable salts',\n",
       " u'inhibition',\n",
       " u'washed',\n",
       " u'biol',\n",
       " u'store',\n",
       " u'receptor',\n",
       " u'amino acids',\n",
       " u'assays',\n",
       " u'animal',\n",
       " u'oral administration',\n",
       " u'acceptable salt',\n",
       " u'requests',\n",
       " u'comprising administering',\n",
       " u'information',\n",
       " u'instructions',\n",
       " u'execution',\n",
       " u'num_indic pharmaceutical',\n",
       " u'lactose',\n",
       " u'tablets',\n",
       " u'storing',\n",
       " u'active ingredient',\n",
       " u'purification',\n",
       " u'executing',\n",
       " u'genes',\n",
       " u'chem',\n",
       " u'gel',\n",
       " u'surface',\n",
       " u'intramuscular',\n",
       " u'therapeutically effective',\n",
       " u'client',\n",
       " u'operating system',\n",
       " u'compounds',\n",
       " u'tissue',\n",
       " u'animals',\n",
       " u'suspensions',\n",
       " u'preparations',\n",
       " u'user',\n",
       " u'access',\n",
       " u'aqueous',\n",
       " u'nucleic',\n",
       " u'composition comprising',\n",
       " u'biological',\n",
       " u'orally',\n",
       " u'doses',\n",
       " u'nucleotide']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_accepted_chi_list[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recreate term dictionary with just the accepted terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 10000\n"
     ]
    }
   ],
   "source": [
    "# gets a bit slower at the end but finishes eventually \n",
    "term_dictionary = get_term_dictionary(term_accepted_chi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_doc_postings_lists = min_doc_postings_lists.filter(lambda (term, postings): term in term_accepted_chi_list).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_terms = min_doc_postings_lists.count()\n",
    "number_of_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Reduced Postings List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save Postings List\n",
    "## min_doc_postings_lists.map(lambda (term, postings_list): \",\".join([term, json.dumps(postings_list)])).repartition(1).saveAsTextFile(postings_list_output)\n",
    "min_doc_postings_lists.map(lambda postings: json.dumps(postings)).saveAsTextFile(postings_list_chi_selected_output.format(str(TOP_N_FEATURES)))\n",
    "#sc.parallelize(term_dictionary.items()).saveAsPickleFile(term_dictionary_output)\n",
    "#sc.parallelize(term_accepted_chi_list).saveAsPickleFile(accepted_terms_list_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Reduced Postings List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-89dbcbd1ab08>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-89dbcbd1ab08>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    term_dictionary = dict(sc.pickleFile(term_dictionary_output).collect())\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "min_doc_postings_lists = sc.textFile(postings_list_chi_selected_output.format(str(TOP_N_FEATURES)).map(lambda json_postings: json.loads(json_postings)).cache()\n",
    "term_dictionary = dict(sc.pickleFile(term_dictionary_output).collect())\n",
    "number_of_terms = min_doc_postings_lists.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect document lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_doc_index = create_doc_index(min_doc_postings_lists, term_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to collect the document lengths since they are used in the BM25 calculation\n",
    "all_doc_index = create_doc_index(min_doc_postings_lists, term_dictionary)\n",
    "\n",
    "doc_lengths_rdd = all_doc_index.mapValues(lambda postings_dictionary: reduce(lambda x, term: x + postings_dictionary[term], postings_dictionary, 0))\n",
    "avg_doc_length = doc_lengths_rdd.map(lambda (term, count): count).reduce(lambda count1, count2: count1 + count2) / doc_count\n",
    "doc_lengths_dict = doc_lengths_rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_doc_index.map(lambda postings: json.dumps(postings)).saveAsTextFile(doc_index_chi_selected_output.format(str(TOP_N_FEATURES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Document Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.parallelize(doc_lengths_dict.items()).saveAsPickleFile(doc_lengths_map_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_doc_index.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_doc_index.saveAsPickleFile(doc_index_chi_selected_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Document Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_lengths_dict = dict(sc.pickleFile(doc_lengths_map_output).collect())\n",
    "avg_doc_length = sum(doc_lengths_dict.values())/len(doc_lengths_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'08226314', 3466)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lengths_dict.items()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2009750"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_lengths_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load everything for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_doc_postings_lists = sc.textFile(postings_list_chi_selected_output.format(str(TOP_N_FEATURES))).map(lambda json_postings: json.loads(json_postings)).cache()\n",
    "term_dictionary = dict(sc.pickleFile(term_dictionary_output.format(str(TOP_N_FEATURES))).collect())\n",
    "term_df_map = dict(sc.pickleFile(term_df_map_output.format(str(TOP_N_FEATURES))).collect())\n",
    "number_of_terms = len(term_df_map) # min_doc_postings_lists.count()\n",
    "doc_lengths_dict = dict(sc.pickleFile(doc_lengths_map_output).collect())\n",
    "avg_doc_length = sum(doc_lengths_dict.values())/len(doc_lengths_dict)\n",
    "#all_doc_index = sc.textFile(doc_index_chi_selected_output.format(str(TOP_N_FEATURES))).map(lambda json_postings: json.loads(json_postings)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_doc_index = all_doc_index.map(lambda (doc_id, postings): (doc_id, {int(key): postings[key] for key in postings})).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get min_doc_postings_lists for the sample only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_docs_set = set(training_docs_list)\n",
    "validation_docs_set = set(validation_docs_list)\n",
    "test_docs_set = set(test_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_documents = training_docs_list\n",
    "validation_documents = validation_docs_list\n",
    "test_documents = test_docs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_doc_postings_lists = sc.textFile(postings_list_chi_selected_output.format(str(TOP_N_FEATURES))).map(lambda json_postings: json.loads(json_postings)).cache()\n",
    "min_doc_postings_lists = min_doc_postings_lists.map(lambda (term, postings): (term, {doc_id:postings[doc_id] for doc_id in postings if doc_id in training_docs_set or doc_id in validation_docs_set or doc_id in test_docs_set}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_doc_postings_lists.map(lambda postings: json.dumps(postings)).saveAsTextFile(get_save_location(postings_list_chi_selected_output.format(str(TOP_N_FEATURES)), sample=IS_SAMPLE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_postings_output = get_save_location(postings_list_training_chi_selected_output.format(str(TOP_N_FEATURES)), sample=IS_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_min_doc_postings_lists = min_doc_postings_lists.map(lambda (term, postings): (term, {doc_id:postings[doc_id] for doc_id in postings if doc_id in training_docs_set}))\n",
    "training_min_doc_postings_lists.map(lambda postings: json.dumps(postings)).saveAsTextFile(training_postings_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_min_doc_postings_lists = sc.textFile(training_postings_output).map(get_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_postings_output = get_save_location(postings_list_validation_chi_selected_output.format(str(TOP_N_FEATURES)), sample=IS_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_min_doc_postings_lists = min_doc_postings_lists.map(lambda (term, postings): (term, {doc_id:postings[doc_id] for doc_id in postings if doc_id in validation_docs_set}))\n",
    "validation_min_doc_postings_lists.map(lambda postings: json.dumps(postings)).saveAsTextFile(validation_postings_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_min_doc_postings_lists = sc.textFile(validation_postings_output).map(get_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_postings_output = get_save_location(postings_list_test_chi_selected_output.format(str(TOP_N_FEATURES)), sample=IS_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_min_doc_postings_lists = min_doc_postings_lists.map(lambda (term, postings): (term, {doc_id:postings[doc_id] for doc_id in postings if doc_id in test_docs_set}))\n",
    "test_min_doc_postings_lists.map(lambda postings: json.dumps(postings)).saveAsTextFile(test_postings_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_min_doc_postings_lists = sc.textFile(test_postings_output).map(get_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start creating term weighting postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_written_doc_index(term_index, name, data_type=\"training\"):\n",
    "    doc_index = create_doc_index(term_index, term_dictionary)\n",
    "    output_name = get_data_output_name(name, data_type=data_type)\n",
    "    doc_index.map(lambda postings: json.dumps(postings)).repartition(100).saveAsTextFile(output_name)\n",
    "    doc_index = sc.textFile(output_name).map(get_json_convert_num)# .cache()\n",
    "    return doc_index\n",
    "\n",
    "def read_written_doc_index(name, data_type=\"training\"):\n",
    "    output_name = get_data_output_name(name, data_type=data_type)\n",
    "    doc_index = sc.textFile(output_name).map(get_json_convert_num)\n",
    "    return doc_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.4 s, sys: 832 ms, total: 13.2 s\n",
      "Wall time: 33min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tf_postings = training_min_doc_postings_lists\n",
    "# tf_doc_index_training = create_written_doc_index(tf_postings, \"tf\")\n",
    "\n",
    "# sublinear_tf_postings = tf_postings.mapValues(lambda postings: {docId:  calculate_sublinear_tf(tf) for docId, tf in postings.items()})\n",
    "# sublinear_tf_doc_index_training = create_written_doc_index(sublinear_tf_postings, \"tf-sublinear\")\n",
    "\n",
    "# tf_idf_postings = tf_postings.mapValues(lambda postings: {docId:  calculate_tf_idf(tf, len(postings), len(training_documents)) for docId, tf in postings.items()})\n",
    "# tf_idf_doc_index_training = create_written_doc_index(tf_idf_postings, \"tf-idf\")\n",
    "\n",
    "sublinear_tf_idf_postings = tf_postings.mapValues(lambda postings: {docId:  calculate_sublinear_tf_idf(tf, len(postings), len(training_documents)) for docId, tf in postings.items()})\n",
    "sublinear_tf_idf_doc_index_training = create_written_doc_index(sublinear_tf_idf_postings, \"sublinear-tf-idf\")\n",
    "\n",
    "bm25_postings = tf_postings.mapValues(lambda postings: {docId: calculate_bm25(tf, len(postings), len(training_documents), doc_lengths_dict[docId], avg_doc_length) for docId, tf in postings.items()})\n",
    "bm25_doc_index_training = create_written_doc_index(bm25_postings, \"bm25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_doc_index_training = read_written_doc_index(\"tf\")\n",
    "sublinear_tf_doc_index_training = read_written_doc_index(\"tf-sublinear\")\n",
    "tf_idf_doc_index_training = read_written_doc_index(\"tf-idf\")\n",
    "sublinear_tf_idf_doc_index_training = read_written_doc_index(\"sublinear-tf-idf\")\n",
    "bm25_doc_index_training = read_written_doc_index(\"bm25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.38 s, sys: 420 ms, total: 6.8 s\n",
      "Wall time: 13min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tf_postings_validation = validation_min_doc_postings_lists\n",
    "# tf_doc_index_validation = create_written_doc_index(tf_postings_validation, \"tf\", data_type=\"validation\")\n",
    "\n",
    "# sublinear_tf_postings_validation = tf_postings_validation.mapValues(lambda postings: {docId:  calculate_sublinear_tf(tf) for docId, tf in postings.items()})\n",
    "# sublinear_tf_doc_index_validation = create_written_doc_index(sublinear_tf_postings_validation, \"tf-sublinear\", data_type=\"validation\")\n",
    "\n",
    "# tf_idf_postings_validation = tf_postings_validation.mapValues(lambda postings: {docId:  calculate_tf_idf(tf, len(postings), len(validation_documents)) for docId, tf in postings.items()})\n",
    "# tf_idf_doc_index_validation = create_written_doc_index(tf_idf_postings_validation, \"tf-idf\", data_type=\"validation\")\n",
    "\n",
    "# sublinear_tf_idf_postings_validation = tf_postings_validation.mapValues(lambda postings: {docId:  calculate_sublinear_tf_idf(tf, len(postings), len(validation_documents)) for docId, tf in postings.items()})\n",
    "# sublinear_tf_idf_doc_index_validation = create_written_doc_index(sublinear_tf_idf_postings_validation, \"sublinear-tf-idf\", data_type=\"validation\")\n",
    "\n",
    "bm25_postings_validation = tf_postings_validation.mapValues(lambda postings: {docId: calculate_bm25(tf, len(postings), len(validation_documents), doc_lengths_dict[docId], avg_doc_length) for docId, tf in postings.items()})\n",
    "bm25_doc_index_validation = create_written_doc_index(bm25_postings_validation, \"bm25\", data_type=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_doc_index_validation = read_written_doc_index(\"tf\", data_type=\"validation\")\n",
    "sublinear_tf_doc_index_validation = read_written_doc_index(\"tf-sublinear\", data_type=\"validation\")\n",
    "tf_idf_doc_index_validation = read_written_doc_index(\"tf-idf\", data_type=\"validation\")\n",
    "sublinear_tf_idf_doc_index_validation = read_written_doc_index(\"sublinear-tf-idf\", data_type=\"validation\")\n",
    "bm25_doc_index_validation = read_written_doc_index(\"bm25\", data_type=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.3 s, sys: 884 ms, total: 12.2 s\n",
      "Wall time: 1h 4min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tf_postings_test = test_min_doc_postings_lists\n",
    "tf_doc_index_test = create_written_doc_index(tf_postings_test, \"tf\", data_type=\"test\")\n",
    "\n",
    "sublinear_tf_postings_test = tf_postings_test.mapValues(lambda postings: {docId:  calculate_sublinear_tf(tf) for docId, tf in postings.items()})\n",
    "sublinear_tf_doc_index_test = create_written_doc_index(sublinear_tf_postings_test, \"tf-sublinear\", data_type=\"test\")\n",
    "\n",
    "tf_idf_postings_test = tf_postings_test.mapValues(lambda postings: {docId:  calculate_tf_idf(tf, len(postings), len(test_documents)) for docId, tf in postings.items()})\n",
    "tf_idf_doc_index_test = create_written_doc_index(tf_idf_postings_test, \"tf-idf\", data_type=\"test\")\n",
    "\n",
    "sublinear_tf_idf_postings_test = tf_postings_test.mapValues(lambda postings: {docId:  calculate_sublinear_tf_idf(tf, len(postings), len(test_documents)) for docId, tf in postings.items()})\n",
    "sublinear_tf_idf_doc_index_test = create_written_doc_index(sublinear_tf_idf_postings_test, \"sublinear-tf-idf\", data_type=\"test\")\n",
    "\n",
    "bm25_postings_test = tf_postings_test.mapValues(lambda postings: {docId: calculate_bm25(tf, len(postings), len(test_documents), doc_lengths_dict[docId], avg_doc_length) for docId, tf in postings.items()})\n",
    "bm25_doc_index_test = create_written_doc_index(bm25_postings_test, \"bm25\", data_type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_doc_index_test = read_written_doc_index(\"tf\", data_type=\"test\")\n",
    "sublinear_tf_doc_index_test = read_written_doc_index(\"tf-sublinear\", data_type=\"test\")\n",
    "tf_idf_doc_index_test = read_written_doc_index(\"tf-idf\", data_type=\"test\")\n",
    "sublinear_tf_idf_doc_index_test = read_written_doc_index(\"sublinear-tf-idf\", data_type=\"test\")\n",
    "bm25_doc_index_test = read_written_doc_index(\"bm25\", data_type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'oooidii', {3: 4}], [u'232323', {3: 2}]]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def jsonKV2str(x):\n",
    "#     if isinstance(x, dict):\n",
    "#             return {int(k):(int(v) if isinstance(v, unicode) else v) for k,v in x.items()}\n",
    "#     return x\n",
    "\n",
    "# output_namee = \"hdfs://deka.cip.ifi.lmu.de/svm/new/lskd4.json\"\n",
    "# dd = {\"232323\":{3:2},\"oooidii\": {3:4}}\n",
    "# #sc.parallelize(dd.items()).take(1)\n",
    "# #sc.parallelize(dd.items()).map(lambda postings: json.dumps(postings)).saveAsTextFile(output_namee)\n",
    "# sc.parallelize(dd.items()).map(lambda postings: json.dumps(postings)).take(1)\n",
    "# sc.parallelize(dd.items()).map(lambda postings: json.dumps(postings)).map(lambda postings: json.loads(postings, object_hook=jsonKV2str)).collect()\n",
    "\n",
    "#map(json.dumps,dd.items() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_idf_doc_index_validation.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 s, sys: 332 ms, total: 11.5 s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_doc_index_val = all_doc_index.filter(lambda (doc_id, postings): doc_id in validation_documents).cache()\n",
    "sublinear_tf_doc_index_val = sublinear_tf_doc_index.filter(lambda (doc_id, postings): doc_id in validation_documents).cache()\n",
    "tf_id_doc_index_val = tf_id_doc_index.filter(lambda (doc_id, postings): doc_id in validation_documents).cache()\n",
    "bm25_doc_index_val = bm25_doc_index.filter(lambda (doc_id, postings): doc_id in validation_documents).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Actual Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.6.1)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
