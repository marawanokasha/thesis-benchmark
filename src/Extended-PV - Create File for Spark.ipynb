{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc installation and try again.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import itertools\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "\n",
    "import logging\n",
    "from logging import info\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "root = logging.getLogger()\n",
    "for handler in root.handlers[:]:\n",
    "    root.removeHandler(handler)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # adds a default StreamHanlder\n",
    "#root.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SVM_SEED = 1234\n",
    "DOC2VEC_SEED = 1234\n",
    "WORD2VEC_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MIN_WORD_COUNT = 100\n",
    "NUM_CORES = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "GLOBAL_VARS = namedtuple('GLOBAL_VARS', ['MODEL_NAME', 'DOC2VEC_MODEL_NAME', 'DOC2VEC_MODEL', \n",
    "                                         'SVM_MODEL_NAME', 'NN_MODEL_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "VOCAB_MODEL = \"vocab_model\"\n",
    "MODEL_PREFIX = \"model\"\n",
    "VALIDATION_MATRIX = \"validation_matrix.pkl\"\n",
    "VALIDATION_DICT = \"validation_dict.pkl\"\n",
    "TEST_MATRIX = \"test_matrix.pkl\"\n",
    "TEST_DICT = \"test_dict.pkl\"\n",
    "METRICS = \"metrics.pkl\"\n",
    "CLASSIFIER = \"classifier.pkl\"\n",
    "TYPE_CLASSIFIER= \"{}_classifier.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATIO = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "root_location = \"/home/local/shalaby/\"\n",
    "exports_location = root_location + \"exported_data/\"\n",
    "\n",
    "doc2vec_model_save_location = os.path.join(root_location, \"parameter_search_doc2vec_models_extended_abs_desc_claims_large_sample_chunks\", \"full\")\n",
    "if not os.path.exists(doc2vec_model_save_location):\n",
    "    os.makedirs(doc2vec_model_save_location)\n",
    "if not os.path.exists(os.path.join(doc2vec_model_save_location, VOCAB_MODEL)):\n",
    "    os.makedirs(os.path.join(doc2vec_model_save_location, VOCAB_MODEL))\n",
    "\n",
    "#training_file = root_location + \"docs_output.json\"\n",
    "training_file = root_location + 'docs_output.json'\n",
    "\n",
    "doc_classifications_map_file = exports_location + \"doc_classification_map.pkl\"\n",
    "sections_file = exports_location + \"sections.pkl\"\n",
    "classes_file = exports_location + \"classes.pkl\"\n",
    "subclasses_file = exports_location + \"subclasses.pkl\"\n",
    "valid_classes_file = exports_location + \"valid_classes.pkl\"\n",
    "valid_subclasses_file = exports_location + \"valid_subclasses.pkl\"\n",
    "classifications_output = exports_location + \"classifications.pkl\"\n",
    "# training_docs_list_file = exports_location + \"extended_pv_training_docs_list.pkl\"\n",
    "# validation_docs_list_file = exports_location + \"extended_pv_validation_docs_list.pkl\"\n",
    "# test_docs_list_file = exports_location + \"extended_pv_test_docs_list.pkl\"\n",
    "training_docs_list_file = exports_location + \"extended_pv_training_docs_list_\" + str(SAMPLE_RATIO) + \".pkl\"\n",
    "validation_docs_list_file = exports_location + \"extended_pv_validation_docs_list_\" + str(SAMPLE_RATIO) + \".pkl\"\n",
    "test_docs_list_file = exports_location + \"extended_pv_test_docs_list_\" + str(SAMPLE_RATIO) + \".pkl\"\n",
    "\n",
    "preprocessed_location = root_location + \"preprocessed_data/\" + \"extended_pv_abs_desc_claims_large_sample_chunks/\"\n",
    "\n",
    "training_preprocessed_files_prefix = preprocessed_location + \"extended_pv_training_docs_data_preprocessed-\"\n",
    "validation_preprocessed_files_prefix = preprocessed_location + \"extended_pv_validation_docs_data_preprocessed-\"\n",
    "test_preprocessed_files_prefix = preprocessed_location + \"extended_pv_test_docs_data_preprocessed-\"\n",
    "\n",
    "docs_only_preprocessed_file = root_location + \"preprocessed_data/\" + \"extended_pv_docs_only_for_spark\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.2 s, sys: 3min 11s, total: 3min 36s\n",
      "Wall time: 3min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_classification_map = pickle.load(open(doc_classifications_map_file))\n",
    "sections = pickle.load(open(sections_file))\n",
    "classes = pickle.load(open(classes_file))\n",
    "subclasses = pickle.load(open(subclasses_file))\n",
    "valid_classes = pickle.load(open(valid_classes_file))\n",
    "valid_subclasses = pickle.load(open(valid_subclasses_file))\n",
    "training_docs_list = pickle.load(open(training_docs_list_file))\n",
    "validation_docs_list = pickle.load(open(validation_docs_list_file))\n",
    "test_docs_list = pickle.load(open(test_docs_list_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254767"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60957"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79785"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ensure_disk_location_exists(location):\n",
    "    if not os.path.exists(location):\n",
    "        os.makedirs(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ExtendedPVDocsOnlyWriterDocumentBatchIterator(object):\n",
    "    def __init__(self, filename_prefix, file_to_write_path):\n",
    "        self.filename_prefix = filename_prefix\n",
    "        self.file_to_write_path = file_to_write_path\n",
    "        self.file_to_write = open(file_to_write_path, \"a\")\n",
    "        self.curr_doc_index = 0\n",
    "    def load_new_batch_in_memory(self):\n",
    "        info(\"Loading new batch for index: {}\".format(self.curr_doc_index))\n",
    "        try:\n",
    "            preproc_file  = open(self.filename_prefix + str(self.curr_doc_index), \"r\")\n",
    "            info(\"Finished loading new batch of {} documents\".format(self.curr_doc_index))\n",
    "            return preproc_file\n",
    "        except:\n",
    "            return False\n",
    "    def iterate_over_all_docs(self): \n",
    "        preproc_file = self.load_new_batch_in_memory()\n",
    "        while preproc_file is not False:\n",
    "            for line in preproc_file:\n",
    "                try:\n",
    "                    doc_id, doc_content = line.split(\" \", 1)\n",
    "                    if is_real_doc(doc_id):\n",
    "                        #print line[:100]\n",
    "#                         self.file_to_write.write(line + \"\\n\")\n",
    "                        self.file_to_write.write(line)\n",
    "                        self.curr_doc_index += 1\n",
    "#                         if self.curr_doc_index % 100 == 0:\n",
    "#                             break\n",
    "                except:\n",
    "                    continue\n",
    "            preproc_file = self.load_new_batch_in_memory()\n",
    "        self.file_to_write.close()\n",
    "def is_real_doc(doc_id):\n",
    "    return doc_id.find(\"_\") == -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove old file contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dd = open(docs_only_preprocessed_file, \"r+\")\n",
    "dd.seek(0)\n",
    "dd.truncate()\n",
    "dd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate to fill it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-30 01:44:13,689 : INFO : Loading new batch for index: 0\n",
      "2017-03-30 01:44:13,691 : INFO : Finished loading new batch of 0 documents\n",
      "2017-03-30 01:44:15,834 : INFO : Loading new batch for index: 10000\n",
      "2017-03-30 01:44:15,836 : INFO : Finished loading new batch of 10000 documents\n",
      "2017-03-30 01:44:18,226 : INFO : Loading new batch for index: 20000\n",
      "2017-03-30 01:44:18,229 : INFO : Finished loading new batch of 20000 documents\n",
      "2017-03-30 01:44:20,433 : INFO : Loading new batch for index: 30000\n",
      "2017-03-30 01:44:20,434 : INFO : Finished loading new batch of 30000 documents\n",
      "2017-03-30 01:44:23,698 : INFO : Loading new batch for index: 40000\n",
      "2017-03-30 01:44:23,700 : INFO : Finished loading new batch of 40000 documents\n",
      "2017-03-30 01:44:26,007 : INFO : Loading new batch for index: 50000\n",
      "2017-03-30 01:44:26,010 : INFO : Finished loading new batch of 50000 documents\n",
      "2017-03-30 01:44:28,281 : INFO : Loading new batch for index: 60000\n",
      "2017-03-30 01:44:28,283 : INFO : Finished loading new batch of 60000 documents\n",
      "2017-03-30 01:44:30,594 : INFO : Loading new batch for index: 70000\n",
      "2017-03-30 01:44:30,595 : INFO : Finished loading new batch of 70000 documents\n",
      "2017-03-30 01:44:33,035 : INFO : Loading new batch for index: 80000\n",
      "2017-03-30 01:44:33,036 : INFO : Finished loading new batch of 80000 documents\n",
      "2017-03-30 01:44:35,726 : INFO : Loading new batch for index: 90000\n",
      "2017-03-30 01:44:35,728 : INFO : Finished loading new batch of 90000 documents\n",
      "2017-03-30 01:44:38,086 : INFO : Loading new batch for index: 100000\n",
      "2017-03-30 01:44:38,087 : INFO : Finished loading new batch of 100000 documents\n",
      "2017-03-30 01:44:40,432 : INFO : Loading new batch for index: 110000\n",
      "2017-03-30 01:44:40,433 : INFO : Finished loading new batch of 110000 documents\n",
      "2017-03-30 01:44:42,715 : INFO : Loading new batch for index: 120000\n",
      "2017-03-30 01:44:42,716 : INFO : Finished loading new batch of 120000 documents\n",
      "2017-03-30 01:44:44,967 : INFO : Loading new batch for index: 130000\n",
      "2017-03-30 01:44:44,969 : INFO : Finished loading new batch of 130000 documents\n",
      "2017-03-30 01:44:47,332 : INFO : Loading new batch for index: 140000\n",
      "2017-03-30 01:44:47,335 : INFO : Finished loading new batch of 140000 documents\n",
      "2017-03-30 01:44:49,785 : INFO : Loading new batch for index: 150000\n",
      "2017-03-30 01:44:49,786 : INFO : Finished loading new batch of 150000 documents\n",
      "2017-03-30 01:44:52,084 : INFO : Loading new batch for index: 160000\n",
      "2017-03-30 01:44:52,086 : INFO : Finished loading new batch of 160000 documents\n",
      "2017-03-30 01:44:56,356 : INFO : Loading new batch for index: 170000\n",
      "2017-03-30 01:44:56,359 : INFO : Finished loading new batch of 170000 documents\n",
      "2017-03-30 01:44:59,386 : INFO : Loading new batch for index: 180000\n",
      "2017-03-30 01:44:59,389 : INFO : Finished loading new batch of 180000 documents\n",
      "2017-03-30 01:45:02,212 : INFO : Loading new batch for index: 190000\n",
      "2017-03-30 01:45:02,214 : INFO : Finished loading new batch of 190000 documents\n",
      "2017-03-30 01:45:04,847 : INFO : Loading new batch for index: 200000\n",
      "2017-03-30 01:45:04,849 : INFO : Finished loading new batch of 200000 documents\n",
      "2017-03-30 01:45:07,557 : INFO : Loading new batch for index: 210000\n",
      "2017-03-30 01:45:07,559 : INFO : Finished loading new batch of 210000 documents\n",
      "2017-03-30 01:45:09,837 : INFO : Loading new batch for index: 220000\n",
      "2017-03-30 01:45:09,839 : INFO : Finished loading new batch of 220000 documents\n",
      "2017-03-30 01:45:12,504 : INFO : Loading new batch for index: 230000\n",
      "2017-03-30 01:45:12,507 : INFO : Finished loading new batch of 230000 documents\n",
      "2017-03-30 01:45:14,957 : INFO : Loading new batch for index: 240000\n",
      "2017-03-30 01:45:14,958 : INFO : Finished loading new batch of 240000 documents\n",
      "2017-03-30 01:45:17,615 : INFO : Loading new batch for index: 250000\n",
      "2017-03-30 01:45:17,617 : INFO : Finished loading new batch of 250000 documents\n",
      "2017-03-30 01:45:18,552 : INFO : Loading new batch for index: 254767\n"
     ]
    }
   ],
   "source": [
    "training_iterator = ExtendedPVDocsOnlyWriterDocumentBatchIterator(training_preprocessed_files_prefix, \n",
    "                                                                  docs_only_preprocessed_file)\n",
    "training_iterator.iterate_over_all_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-30 02:05:56,583 : INFO : Loading new batch for index: 0\n",
      "2017-03-30 02:05:56,584 : INFO : Finished loading new batch of 0 documents\n",
      "2017-03-30 02:05:58,713 : INFO : Loading new batch for index: 10000\n",
      "2017-03-30 02:05:58,714 : INFO : Finished loading new batch of 10000 documents\n",
      "2017-03-30 02:06:14,361 : INFO : Loading new batch for index: 20000\n",
      "2017-03-30 02:06:14,362 : INFO : Finished loading new batch of 20000 documents\n",
      "2017-03-30 02:06:22,290 : INFO : Loading new batch for index: 30000\n",
      "2017-03-30 02:06:22,291 : INFO : Finished loading new batch of 30000 documents\n",
      "2017-03-30 02:06:47,156 : INFO : Loading new batch for index: 40000\n",
      "2017-03-30 02:06:47,158 : INFO : Finished loading new batch of 40000 documents\n",
      "2017-03-30 02:06:50,129 : INFO : Loading new batch for index: 50000\n",
      "2017-03-30 02:06:50,130 : INFO : Finished loading new batch of 50000 documents\n",
      "2017-03-30 02:06:55,668 : INFO : Loading new batch for index: 60000\n",
      "2017-03-30 02:06:55,670 : INFO : Finished loading new batch of 60000 documents\n",
      "2017-03-30 02:06:55,859 : INFO : Loading new batch for index: 60957\n"
     ]
    }
   ],
   "source": [
    "validation_iterator = ExtendedPVDocsOnlyWriterDocumentBatchIterator(validation_preprocessed_files_prefix, \n",
    "                                                                  docs_only_preprocessed_file)\n",
    "validation_iterator.iterate_over_all_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-30 02:47:40,894 : INFO : Loading new batch for index: 0\n",
      "2017-03-30 02:47:40,897 : INFO : Finished loading new batch of 0 documents\n",
      "2017-03-30 02:47:43,318 : INFO : Loading new batch for index: 10000\n",
      "2017-03-30 02:47:43,319 : INFO : Finished loading new batch of 10000 documents\n",
      "2017-03-30 02:47:46,640 : INFO : Loading new batch for index: 20000\n",
      "2017-03-30 02:47:46,642 : INFO : Finished loading new batch of 20000 documents\n",
      "2017-03-30 02:47:49,034 : INFO : Loading new batch for index: 30000\n",
      "2017-03-30 02:47:49,036 : INFO : Finished loading new batch of 30000 documents\n",
      "2017-03-30 02:47:51,969 : INFO : Loading new batch for index: 40000\n",
      "2017-03-30 02:47:51,971 : INFO : Finished loading new batch of 40000 documents\n",
      "2017-03-30 02:47:54,571 : INFO : Loading new batch for index: 50000\n",
      "2017-03-30 02:47:54,572 : INFO : Finished loading new batch of 50000 documents\n",
      "2017-03-30 02:47:57,298 : INFO : Loading new batch for index: 60000\n",
      "2017-03-30 02:47:57,301 : INFO : Finished loading new batch of 60000 documents\n",
      "2017-03-30 02:48:00,073 : INFO : Loading new batch for index: 70000\n",
      "2017-03-30 02:48:00,075 : INFO : Finished loading new batch of 70000 documents\n",
      "2017-03-30 02:48:02,965 : INFO : Loading new batch for index: 79785\n"
     ]
    }
   ],
   "source": [
    "test_iterator = ExtendedPVDocsOnlyWriterDocumentBatchIterator(test_preprocessed_files_prefix, \n",
    "                                                                  docs_only_preprocessed_file)\n",
    "test_iterator.iterate_over_all_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
